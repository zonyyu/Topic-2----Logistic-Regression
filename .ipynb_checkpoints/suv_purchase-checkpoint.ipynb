{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f33ffb",
   "metadata": {},
   "source": [
    "# SUV Purchase Predictor\n",
    "\n",
    "In this notebook, we will train a logistic regression model to predict whether or not one should buy an SUV given their age, gender, and annual salary. The purpose of this programming exercise is to expose students to the **PyTorch** framework, as well as diving deeper into the architecture of learning algorithms.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Suppose you are thinking about buying an SUV, however you are not sure whether or not you should drop the money on a new car. You decide to make a classifier that learns people's decisions in the past to aid you in your own decision. You are given a dataset with the features `UserID`, `Gender`, `Age`, `EstimatedSalary`, as well as your label `Purchased`.\n",
    "\n",
    "## Unpacking the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa5aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b3d45",
   "metadata": {},
   "source": [
    "## Training the Logistic Regression Model\n",
    "\n",
    "Recall from the Topic 2 Notebook, Logistic Regression differs from Linear Regression in terms of the Activation function (sigmoid) and the Cost function (BCE). Most other aspects are similar. Here we will use the `BatchNorm1d` layer to normalize the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "295c7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c7c7c",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "After training our Logistic Regression Classifier, we observe a very high Accuracy and F1 Score, which is awesome! Lets keep track of our progress:\n",
    "\n",
    "| Model | Observations | Accuracy | F1 | \n",
    "|:- |:- |:- | :- |\n",
    "| Logistic Regression | High accuracy, very few false positives and false negatives | 88% | 88% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57d2328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
