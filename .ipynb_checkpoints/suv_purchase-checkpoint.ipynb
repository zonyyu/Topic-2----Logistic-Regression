{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f33ffb",
   "metadata": {},
   "source": [
    "# SUV Purchase Predictor\n",
    "\n",
    "In this notebook, we will train a logistic regression model to predict whether or not one should buy an SUV given their age, gender, and annual salary. The purpose of this programming exercise is to expose students to the **PyTorch** framework, as well as diving deeper into the architecture of learning algorithms.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Suppose you are thinking about buying an SUV, however you are not sure whether or not you should drop the money on a new car. You decide to make a classifier that learns people's decisions in the past to aid you in your own decision. You are given a dataset with the features `UserID`, `Gender`, `Age`, `EstimatedSalary`, as well as your label `Purchased`.\n",
    "\n",
    "## Unpacking the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa5aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa706a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15624510</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15810944</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15668575</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15603246</td>\n",
       "      <td>Female</td>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15804002</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>15691863</td>\n",
       "      <td>Female</td>\n",
       "      <td>46</td>\n",
       "      <td>41000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>15706071</td>\n",
       "      <td>Male</td>\n",
       "      <td>51</td>\n",
       "      <td>23000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>15654296</td>\n",
       "      <td>Female</td>\n",
       "      <td>50</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>15755018</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>33000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>15594041</td>\n",
       "      <td>Female</td>\n",
       "      <td>49</td>\n",
       "      <td>36000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      User ID  Gender  Age  EstimatedSalary  Purchased\n",
       "0    15624510    Male   19            19000          0\n",
       "1    15810944    Male   35            20000          0\n",
       "2    15668575  Female   26            43000          0\n",
       "3    15603246  Female   27            57000          0\n",
       "4    15804002    Male   19            76000          0\n",
       "..        ...     ...  ...              ...        ...\n",
       "395  15691863  Female   46            41000          1\n",
       "396  15706071    Male   51            23000          1\n",
       "397  15654296  Female   50            20000          1\n",
       "398  15755018    Male   36            33000          0\n",
       "399  15594041  Female   49            36000          1\n",
       "\n",
       "[400 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('datasets/SUV_Purchase.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e9c7f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Female  Male\n",
       "0         0     1\n",
       "1         0     1\n",
       "2         1     0\n",
       "3         1     0\n",
       "4         0     1\n",
       "..      ...   ...\n",
       "395       1     0\n",
       "396       0     1\n",
       "397       1     0\n",
       "398       0     1\n",
       "399       1     0\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting Gender into categorical values\n",
    "gender_dummies = pd.get_dummies(data['Gender'])\n",
    "gender_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6903c155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "      <th>gender_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15624510</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15810944</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15668575</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15603246</td>\n",
       "      <td>Female</td>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15804002</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>15691863</td>\n",
       "      <td>Female</td>\n",
       "      <td>46</td>\n",
       "      <td>41000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>15706071</td>\n",
       "      <td>Male</td>\n",
       "      <td>51</td>\n",
       "      <td>23000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>15654296</td>\n",
       "      <td>Female</td>\n",
       "      <td>50</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>15755018</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>33000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>15594041</td>\n",
       "      <td>Female</td>\n",
       "      <td>49</td>\n",
       "      <td>36000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      User ID  Gender  Age  EstimatedSalary  Purchased  gender_num\n",
       "0    15624510    Male   19            19000          0           0\n",
       "1    15810944    Male   35            20000          0           0\n",
       "2    15668575  Female   26            43000          0           1\n",
       "3    15603246  Female   27            57000          0           1\n",
       "4    15804002    Male   19            76000          0           0\n",
       "..        ...     ...  ...              ...        ...         ...\n",
       "395  15691863  Female   46            41000          1           1\n",
       "396  15706071    Male   51            23000          1           0\n",
       "397  15654296  Female   50            20000          1           1\n",
       "398  15755018    Male   36            33000          0           0\n",
       "399  15594041  Female   49            36000          1           1\n",
       "\n",
       "[400 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let 0 = male, 1 = female\n",
    "data['gender_num'] = gender_dummies['Female']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8939ee7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 20)\n",
      "(320, 1)\n",
      "(80, 20)\n",
      "(80, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data selection\n",
    "features = data[['Age', 'EstimatedSalary', 'gender_num']].to_numpy()\n",
    "labels = data[['Purchased']].to_numpy()\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size = 0.2)\n",
    "poly= PolynomialFeatures(degree=3)\n",
    "\n",
    "# Create Polynomial Features\n",
    "X_train = poly.fit_transform(X_train)\n",
    "X_test = poly.fit_transform(X_test)\n",
    "\n",
    "# Display shapes\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "# Convert into pytorch tensors \n",
    "X_train_t = torch.from_numpy(X_train).float()\n",
    "Y_train_t = torch.from_numpy(Y_train).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d507b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of positives: 143\n",
      "Num of negatives: 257\n",
      "\n",
      "positive weight: 1.3986013986013985\n",
      "negative weight: 0.7782101167315175\n",
      "(320, 1)\n"
     ]
    }
   ],
   "source": [
    "# Number of positive and negative (females and males)\n",
    "positives = np.sum(data['Purchased'].to_numpy())\n",
    "negatives = int(len(data.index) - positives)\n",
    "\n",
    "print(f'Num of positives: {positives}')\n",
    "print(f'Num of negatives: {negatives}\\n')\n",
    "\n",
    "# Calculate Class Weights to balance classes\n",
    "pos_weight = len(data.index)/(2 * positives)\n",
    "neg_weight = len(data.index)/(2 * negatives)\n",
    "\n",
    "print(f\"positive weight: {pos_weight}\")\n",
    "print(f\"negative weight: {neg_weight}\")\n",
    "\n",
    "\n",
    "def class_weights(Y_train, pos_weight, neg_weight):\n",
    "    \"\"\"\n",
    "    calculates the weights for each and every training example\n",
    "    Args:\n",
    "        Y_train: training labels\n",
    "        pos_weight: Weight of positive labels\n",
    "        neg_weight: Weight of negative labels\n",
    "    Returns:\n",
    "        weights: an np.array where its shape is identical to Y_train\n",
    "    \"\"\"\n",
    "    \n",
    "    pos_mask = Y_train.astype(bool)\n",
    "    neg_mask = ~Y_train.astype(bool)\n",
    "\n",
    "    pos_mask = pos_mask.astype(np.float32) * pos_weight\n",
    "\n",
    "    neg_mask = neg_mask.astype(np.float32) * neg_weight\n",
    "    return pos_mask + neg_mask\n",
    "    \n",
    "\n",
    "weights = class_weights(Y_train, pos_weight, neg_weight)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b3d45",
   "metadata": {},
   "source": [
    "## Training the Logistic Regression Model\n",
    "\n",
    "Recall from the Topic 2 Notebook, Logistic Regression differs from Linear Regression in terms of the Activation function (sigmoid) and the Cost function (BCE). Most other aspects are similar. Here we will use the `BatchNorm1d` layer to normalize the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c974717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Linear, BCELoss, BatchNorm1d\n",
    "from torch.optim import SGD\n",
    "\n",
    "class Logreg(Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear  = Linear(input_dim, output_dim)\n",
    "        self.bn = BatchNorm1d(input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Batch norm is used solely to normalize the inputs\n",
    "        x = self.bn(x)\n",
    "        z = self.linear(x)\n",
    "        y_pred = torch.sigmoid(z)\n",
    "        return y_pred\n",
    "    \n",
    "    def fit(self, X_train, Y_train, epochs, loss_func, opt):\n",
    "        \"\"\"\n",
    "        Trains the model.\n",
    "        Args:\n",
    "            X_train, Y_train: training set and labels. Must be \n",
    "                            torch.tensor\n",
    "            epochs: The number of passes over your training dataset\n",
    "            loss_func: optimizing criterion\n",
    "            opt: optimizing algorithm\n",
    "        \"\"\"\n",
    "        for i in range(epochs):\n",
    "            self.train()\n",
    "            opt.zero_grad()\n",
    "        \n",
    "            Y_pred = self(X_train)\n",
    "            loss = loss_func(Y_pred, Y_train)\n",
    "            \n",
    "            print(f\"iteration {i+1}: loss: {loss.item()}\")\n",
    "        \n",
    "            loss.backward()\n",
    "            opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2584aecd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1: loss: 0.7855066061019897\n",
      "iteration 2: loss: 0.7525612711906433\n",
      "iteration 3: loss: 0.7111790776252747\n",
      "iteration 4: loss: 0.6666046977043152\n",
      "iteration 5: loss: 0.6228680610656738\n",
      "iteration 6: loss: 0.5824532508850098\n",
      "iteration 7: loss: 0.5464895367622375\n",
      "iteration 8: loss: 0.5152049660682678\n",
      "iteration 9: loss: 0.488351047039032\n",
      "iteration 10: loss: 0.4654792249202728\n",
      "iteration 11: loss: 0.4460868239402771\n",
      "iteration 12: loss: 0.42968472838401794\n",
      "iteration 13: loss: 0.4158264696598053\n",
      "iteration 14: loss: 0.4041185975074768\n",
      "iteration 15: loss: 0.3942214846611023\n",
      "iteration 16: loss: 0.3858463168144226\n",
      "iteration 17: loss: 0.37874937057495117\n",
      "iteration 18: loss: 0.3727259039878845\n",
      "iteration 19: loss: 0.3676045835018158\n",
      "iteration 20: loss: 0.36324167251586914\n",
      "iteration 21: loss: 0.35951679944992065\n",
      "iteration 22: loss: 0.35632866621017456\n",
      "iteration 23: loss: 0.3535920977592468\n",
      "iteration 24: loss: 0.35123521089553833\n",
      "iteration 25: loss: 0.3491972088813782\n",
      "iteration 26: loss: 0.3474270701408386\n",
      "iteration 27: loss: 0.3458816409111023\n",
      "iteration 28: loss: 0.3445243537425995\n",
      "iteration 29: loss: 0.3433247208595276\n",
      "iteration 30: loss: 0.3422570824623108\n",
      "iteration 31: loss: 0.3412996530532837\n",
      "iteration 32: loss: 0.3404347002506256\n",
      "iteration 33: loss: 0.339647114276886\n",
      "iteration 34: loss: 0.3389240801334381\n",
      "iteration 35: loss: 0.33825528621673584\n",
      "iteration 36: loss: 0.33763203024864197\n",
      "iteration 37: loss: 0.33704695105552673\n",
      "iteration 38: loss: 0.336493581533432\n",
      "iteration 39: loss: 0.33596712350845337\n",
      "iteration 40: loss: 0.3354629874229431\n",
      "iteration 41: loss: 0.33497780561447144\n",
      "iteration 42: loss: 0.3345082402229309\n",
      "iteration 43: loss: 0.3340516984462738\n",
      "iteration 44: loss: 0.3336062729358673\n",
      "iteration 45: loss: 0.3331697881221771\n",
      "iteration 46: loss: 0.3327409029006958\n",
      "iteration 47: loss: 0.33231836557388306\n",
      "iteration 48: loss: 0.33190107345581055\n",
      "iteration 49: loss: 0.33148813247680664\n",
      "iteration 50: loss: 0.33107900619506836\n",
      "iteration 51: loss: 0.33067336678504944\n",
      "iteration 52: loss: 0.3302707076072693\n",
      "iteration 53: loss: 0.32987093925476074\n",
      "iteration 54: loss: 0.3294738531112671\n",
      "iteration 55: loss: 0.32907888293266296\n",
      "iteration 56: loss: 0.3286867141723633\n",
      "iteration 57: loss: 0.32829684019088745\n",
      "iteration 58: loss: 0.3279092013835907\n",
      "iteration 59: loss: 0.32752448320388794\n",
      "iteration 60: loss: 0.32714220881462097\n",
      "iteration 61: loss: 0.3267626166343689\n",
      "iteration 62: loss: 0.3263856768608093\n",
      "iteration 63: loss: 0.3260114789009094\n",
      "iteration 64: loss: 0.32564014196395874\n",
      "iteration 65: loss: 0.3252711892127991\n",
      "iteration 66: loss: 0.3249050974845886\n",
      "iteration 67: loss: 0.32454222440719604\n",
      "iteration 68: loss: 0.32418155670166016\n",
      "iteration 69: loss: 0.3238238990306854\n",
      "iteration 70: loss: 0.3234688937664032\n",
      "iteration 71: loss: 0.3231167197227478\n",
      "iteration 72: loss: 0.3227672874927521\n",
      "iteration 73: loss: 0.3224203288555145\n",
      "iteration 74: loss: 0.32207635045051575\n",
      "iteration 75: loss: 0.32173481583595276\n",
      "iteration 76: loss: 0.3213958740234375\n",
      "iteration 77: loss: 0.3210594952106476\n",
      "iteration 78: loss: 0.32072576880455017\n",
      "iteration 79: loss: 0.3203946352005005\n",
      "iteration 80: loss: 0.3200659453868866\n",
      "iteration 81: loss: 0.3197398781776428\n",
      "iteration 82: loss: 0.319416344165802\n",
      "iteration 83: loss: 0.3190951943397522\n",
      "iteration 84: loss: 0.31877660751342773\n",
      "iteration 85: loss: 0.3184603154659271\n",
      "iteration 86: loss: 0.31814688444137573\n",
      "iteration 87: loss: 0.31783556938171387\n",
      "iteration 88: loss: 0.317526638507843\n",
      "iteration 89: loss: 0.31722015142440796\n",
      "iteration 90: loss: 0.31691616773605347\n",
      "iteration 91: loss: 0.31661444902420044\n",
      "iteration 92: loss: 0.31631505489349365\n",
      "iteration 93: loss: 0.31601810455322266\n",
      "iteration 94: loss: 0.3157230317592621\n",
      "iteration 95: loss: 0.3154307007789612\n",
      "iteration 96: loss: 0.3151403069496155\n",
      "iteration 97: loss: 0.314852237701416\n",
      "iteration 98: loss: 0.314566433429718\n",
      "iteration 99: loss: 0.31428250670433044\n",
      "iteration 100: loss: 0.3140009641647339\n",
      "iteration 101: loss: 0.3137214481830597\n",
      "iteration 102: loss: 0.313444048166275\n",
      "iteration 103: loss: 0.31316879391670227\n",
      "iteration 104: loss: 0.3128954768180847\n",
      "iteration 105: loss: 0.3126242458820343\n",
      "iteration 106: loss: 0.3123549818992615\n",
      "iteration 107: loss: 0.31208765506744385\n",
      "iteration 108: loss: 0.3118223547935486\n",
      "iteration 109: loss: 0.31155896186828613\n",
      "iteration 110: loss: 0.3112976551055908\n",
      "iteration 111: loss: 0.31103816628456116\n",
      "iteration 112: loss: 0.31078046560287476\n",
      "iteration 113: loss: 0.31052473187446594\n",
      "iteration 114: loss: 0.31027087569236755\n",
      "iteration 115: loss: 0.31001877784729004\n",
      "iteration 116: loss: 0.30976858735084534\n",
      "iteration 117: loss: 0.30951982736587524\n",
      "iteration 118: loss: 0.30927330255508423\n",
      "iteration 119: loss: 0.3090282380580902\n",
      "iteration 120: loss: 0.3087850511074066\n",
      "iteration 121: loss: 0.308543860912323\n",
      "iteration 122: loss: 0.3083037734031677\n",
      "iteration 123: loss: 0.30806592106819153\n",
      "iteration 124: loss: 0.3078295886516571\n",
      "iteration 125: loss: 0.3075949251651764\n",
      "iteration 126: loss: 0.30736175179481506\n",
      "iteration 127: loss: 0.30713018774986267\n",
      "iteration 128: loss: 0.3069002330303192\n",
      "iteration 129: loss: 0.30667218565940857\n",
      "iteration 130: loss: 0.3064454197883606\n",
      "iteration 131: loss: 0.30622047185897827\n",
      "iteration 132: loss: 0.30599671602249146\n",
      "iteration 133: loss: 0.30577489733695984\n",
      "iteration 134: loss: 0.30555450916290283\n",
      "iteration 135: loss: 0.3053353428840637\n",
      "iteration 136: loss: 0.3051181137561798\n",
      "iteration 137: loss: 0.3049018979072571\n",
      "iteration 138: loss: 0.3046875298023224\n",
      "iteration 139: loss: 0.304474413394928\n",
      "iteration 140: loss: 0.30426302552223206\n",
      "iteration 141: loss: 0.30405282974243164\n",
      "iteration 142: loss: 0.3038438558578491\n",
      "iteration 143: loss: 0.3036365807056427\n",
      "iteration 144: loss: 0.30343058705329895\n",
      "iteration 145: loss: 0.303226113319397\n",
      "iteration 146: loss: 0.30302298069000244\n",
      "iteration 147: loss: 0.30282121896743774\n",
      "iteration 148: loss: 0.30262047052383423\n",
      "iteration 149: loss: 0.30242109298706055\n",
      "iteration 150: loss: 0.3022235035896301\n",
      "iteration 151: loss: 0.30202698707580566\n",
      "iteration 152: loss: 0.30183175206184387\n",
      "iteration 153: loss: 0.3016376495361328\n",
      "iteration 154: loss: 0.3014448881149292\n",
      "iteration 155: loss: 0.30125361680984497\n",
      "iteration 156: loss: 0.30106326937675476\n",
      "iteration 157: loss: 0.3008743226528168\n",
      "iteration 158: loss: 0.3006867468357086\n",
      "iteration 159: loss: 0.3005000948905945\n",
      "iteration 160: loss: 0.30031487345695496\n",
      "iteration 161: loss: 0.3001307249069214\n",
      "iteration 162: loss: 0.2999480664730072\n",
      "iteration 163: loss: 0.29976633191108704\n",
      "iteration 164: loss: 0.29958575963974\n",
      "iteration 165: loss: 0.29940617084503174\n",
      "iteration 166: loss: 0.29922813177108765\n",
      "iteration 167: loss: 0.299050897359848\n",
      "iteration 168: loss: 0.2988748550415039\n",
      "iteration 169: loss: 0.2987002432346344\n",
      "iteration 170: loss: 0.29852649569511414\n",
      "iteration 171: loss: 0.29835376143455505\n",
      "iteration 172: loss: 0.29818230867385864\n",
      "iteration 173: loss: 0.2980118989944458\n",
      "iteration 174: loss: 0.29784220457077026\n",
      "iteration 175: loss: 0.2976740300655365\n",
      "iteration 176: loss: 0.2975068986415863\n",
      "iteration 177: loss: 0.2973406910896301\n",
      "iteration 178: loss: 0.2971755862236023\n",
      "iteration 179: loss: 0.2970113456249237\n",
      "iteration 180: loss: 0.2968480587005615\n",
      "iteration 181: loss: 0.29668617248535156\n",
      "iteration 182: loss: 0.2965250611305237\n",
      "iteration 183: loss: 0.29636499285697937\n",
      "iteration 184: loss: 0.29620569944381714\n",
      "iteration 185: loss: 0.29604777693748474\n",
      "iteration 186: loss: 0.2958903908729553\n",
      "iteration 187: loss: 0.2957341969013214\n",
      "iteration 188: loss: 0.29557910561561584\n",
      "iteration 189: loss: 0.2954248785972595\n",
      "iteration 190: loss: 0.29527148604393005\n",
      "iteration 191: loss: 0.29511940479278564\n",
      "iteration 192: loss: 0.2949678301811218\n",
      "iteration 193: loss: 0.294817179441452\n",
      "iteration 194: loss: 0.2946675717830658\n",
      "iteration 195: loss: 0.29451870918273926\n",
      "iteration 196: loss: 0.29437118768692017\n",
      "iteration 197: loss: 0.2942243218421936\n",
      "iteration 198: loss: 0.2940780520439148\n",
      "iteration 199: loss: 0.29393312335014343\n",
      "iteration 200: loss: 0.2937887907028198\n",
      "iteration 201: loss: 0.29364559054374695\n",
      "iteration 202: loss: 0.29350313544273376\n",
      "iteration 203: loss: 0.29336103796958923\n",
      "iteration 204: loss: 0.2932204306125641\n",
      "iteration 205: loss: 0.2930803596973419\n",
      "iteration 206: loss: 0.29294145107269287\n",
      "iteration 207: loss: 0.2928031384944916\n",
      "iteration 208: loss: 0.2926654815673828\n",
      "iteration 209: loss: 0.2925289273262024\n",
      "iteration 210: loss: 0.2923928499221802\n",
      "iteration 211: loss: 0.292258083820343\n",
      "iteration 212: loss: 0.29212385416030884\n",
      "iteration 213: loss: 0.29199033975601196\n",
      "iteration 214: loss: 0.29185742139816284\n",
      "iteration 215: loss: 0.29172569513320923\n",
      "iteration 216: loss: 0.2915944755077362\n",
      "iteration 217: loss: 0.29146409034729004\n",
      "iteration 218: loss: 0.29133450984954834\n",
      "iteration 219: loss: 0.2912057340145111\n",
      "iteration 220: loss: 0.2910779118537903\n",
      "iteration 221: loss: 0.29095038771629333\n",
      "iteration 222: loss: 0.29082411527633667\n",
      "iteration 223: loss: 0.2906982898712158\n",
      "iteration 224: loss: 0.29057303071022034\n",
      "iteration 225: loss: 0.2904483675956726\n",
      "iteration 226: loss: 0.2903248965740204\n",
      "iteration 227: loss: 0.2902018427848816\n",
      "iteration 228: loss: 0.29007989168167114\n",
      "iteration 229: loss: 0.2899583876132965\n",
      "iteration 230: loss: 0.2898375391960144\n",
      "iteration 231: loss: 0.28971749544143677\n",
      "iteration 232: loss: 0.28959792852401733\n",
      "iteration 233: loss: 0.2894793152809143\n",
      "iteration 234: loss: 0.28936120867729187\n",
      "iteration 235: loss: 0.28924378752708435\n",
      "iteration 236: loss: 0.28912705183029175\n",
      "iteration 237: loss: 0.2890109419822693\n",
      "iteration 238: loss: 0.2888956367969513\n",
      "iteration 239: loss: 0.2887807786464691\n",
      "iteration 240: loss: 0.2886669337749481\n",
      "iteration 241: loss: 0.28855347633361816\n",
      "iteration 242: loss: 0.28844067454338074\n",
      "iteration 243: loss: 0.2883285582065582\n",
      "iteration 244: loss: 0.28821712732315063\n",
      "iteration 245: loss: 0.28810644149780273\n",
      "iteration 246: loss: 0.2879960536956787\n",
      "iteration 247: loss: 0.28788667917251587\n",
      "iteration 248: loss: 0.2877776622772217\n",
      "iteration 249: loss: 0.28766924142837524\n",
      "iteration 250: loss: 0.2875615358352661\n",
      "iteration 251: loss: 0.2874542474746704\n",
      "iteration 252: loss: 0.28734785318374634\n",
      "iteration 253: loss: 0.28724169731140137\n",
      "iteration 254: loss: 0.28713661432266235\n",
      "iteration 255: loss: 0.28703173995018005\n",
      "iteration 256: loss: 0.2869275212287903\n",
      "iteration 257: loss: 0.28682392835617065\n",
      "iteration 258: loss: 0.2867209315299988\n",
      "iteration 259: loss: 0.2866186201572418\n",
      "iteration 260: loss: 0.2865169048309326\n",
      "iteration 261: loss: 0.2864153981208801\n",
      "iteration 262: loss: 0.28631487488746643\n",
      "iteration 263: loss: 0.286214679479599\n",
      "iteration 264: loss: 0.28611496090888977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 265: loss: 0.28601592779159546\n",
      "iteration 266: loss: 0.2859174609184265\n",
      "iteration 267: loss: 0.2858193516731262\n",
      "iteration 268: loss: 0.28572165966033936\n",
      "iteration 269: loss: 0.28562501072883606\n",
      "iteration 270: loss: 0.2855284810066223\n",
      "iteration 271: loss: 0.285432904958725\n",
      "iteration 272: loss: 0.2853375971317291\n",
      "iteration 273: loss: 0.28524285554885864\n",
      "iteration 274: loss: 0.2851485311985016\n",
      "iteration 275: loss: 0.28505486249923706\n",
      "iteration 276: loss: 0.2849618196487427\n",
      "iteration 277: loss: 0.28486886620521545\n",
      "iteration 278: loss: 0.28477683663368225\n",
      "iteration 279: loss: 0.28468501567840576\n",
      "iteration 280: loss: 0.2845936715602875\n",
      "iteration 281: loss: 0.284503310918808\n",
      "iteration 282: loss: 0.2844131290912628\n",
      "iteration 283: loss: 0.28432339429855347\n",
      "iteration 284: loss: 0.28423383831977844\n",
      "iteration 285: loss: 0.28414517641067505\n",
      "iteration 286: loss: 0.28405705094337463\n",
      "iteration 287: loss: 0.28396880626678467\n",
      "iteration 288: loss: 0.283881813287735\n",
      "iteration 289: loss: 0.2837947905063629\n",
      "iteration 290: loss: 0.28370827436447144\n",
      "iteration 291: loss: 0.2836224436759949\n",
      "iteration 292: loss: 0.2835368812084198\n",
      "iteration 293: loss: 0.2834518849849701\n",
      "iteration 294: loss: 0.28336748480796814\n",
      "iteration 295: loss: 0.28328362107276917\n",
      "iteration 296: loss: 0.28319963812828064\n",
      "iteration 297: loss: 0.2831166684627533\n",
      "iteration 298: loss: 0.2830337882041931\n",
      "iteration 299: loss: 0.2829517722129822\n",
      "iteration 300: loss: 0.282869815826416\n",
      "iteration 301: loss: 0.28278833627700806\n",
      "iteration 302: loss: 0.2827073037624359\n",
      "iteration 303: loss: 0.28262680768966675\n",
      "iteration 304: loss: 0.2825467586517334\n",
      "iteration 305: loss: 0.28246715664863586\n",
      "iteration 306: loss: 0.2823875844478607\n",
      "iteration 307: loss: 0.2823089063167572\n",
      "iteration 308: loss: 0.2822302281856537\n",
      "iteration 309: loss: 0.2821525037288666\n",
      "iteration 310: loss: 0.282074511051178\n",
      "iteration 311: loss: 0.2819971740245819\n",
      "iteration 312: loss: 0.28192025423049927\n",
      "iteration 313: loss: 0.281843900680542\n",
      "iteration 314: loss: 0.28176793456077576\n",
      "iteration 315: loss: 0.28169238567352295\n",
      "iteration 316: loss: 0.28161725401878357\n",
      "iteration 317: loss: 0.2815420925617218\n",
      "iteration 318: loss: 0.28146788477897644\n",
      "iteration 319: loss: 0.2813935875892639\n",
      "iteration 320: loss: 0.28132015466690063\n",
      "iteration 321: loss: 0.28124672174453735\n",
      "iteration 322: loss: 0.2811737060546875\n",
      "iteration 323: loss: 0.2811010777950287\n",
      "iteration 324: loss: 0.2810288965702057\n",
      "iteration 325: loss: 0.2809571325778961\n",
      "iteration 326: loss: 0.2808857560157776\n",
      "iteration 327: loss: 0.2808148264884949\n",
      "iteration 328: loss: 0.2807442247867584\n",
      "iteration 329: loss: 0.2806735634803772\n",
      "iteration 330: loss: 0.2806037664413452\n",
      "iteration 331: loss: 0.28053396940231323\n",
      "iteration 332: loss: 0.2804649770259857\n",
      "iteration 333: loss: 0.2803959548473358\n",
      "iteration 334: loss: 0.2803272604942322\n",
      "iteration 335: loss: 0.2802594304084778\n",
      "iteration 336: loss: 0.2801915109157562\n",
      "iteration 337: loss: 0.2801240086555481\n",
      "iteration 338: loss: 0.280056893825531\n",
      "iteration 339: loss: 0.279989629983902\n",
      "iteration 340: loss: 0.2799232006072998\n",
      "iteration 341: loss: 0.27985697984695435\n",
      "iteration 342: loss: 0.2797907590866089\n",
      "iteration 343: loss: 0.27972549200057983\n",
      "iteration 344: loss: 0.27966007590293884\n",
      "iteration 345: loss: 0.2795954942703247\n",
      "iteration 346: loss: 0.2795308530330658\n",
      "iteration 347: loss: 0.2794664800167084\n",
      "iteration 348: loss: 0.2794025242328644\n",
      "iteration 349: loss: 0.27933937311172485\n",
      "iteration 350: loss: 0.27927613258361816\n",
      "iteration 351: loss: 0.27921319007873535\n",
      "iteration 352: loss: 0.2791500985622406\n",
      "iteration 353: loss: 0.2790878415107727\n",
      "iteration 354: loss: 0.27902594208717346\n",
      "iteration 355: loss: 0.27896440029144287\n",
      "iteration 356: loss: 0.27890267968177795\n",
      "iteration 357: loss: 0.2788417637348175\n",
      "iteration 358: loss: 0.2787807583808899\n",
      "iteration 359: loss: 0.278719961643219\n",
      "iteration 360: loss: 0.2786601483821869\n",
      "iteration 361: loss: 0.2786000669002533\n",
      "iteration 362: loss: 0.27854031324386597\n",
      "iteration 363: loss: 0.2784808874130249\n",
      "iteration 364: loss: 0.2784218192100525\n",
      "iteration 365: loss: 0.2783629894256592\n",
      "iteration 366: loss: 0.2783045172691345\n",
      "iteration 367: loss: 0.27824634313583374\n",
      "iteration 368: loss: 0.278188556432724\n",
      "iteration 369: loss: 0.27813053131103516\n",
      "iteration 370: loss: 0.2780732810497284\n",
      "iteration 371: loss: 0.2780159115791321\n",
      "iteration 372: loss: 0.27795925736427307\n",
      "iteration 373: loss: 0.27790242433547974\n",
      "iteration 374: loss: 0.27784648537635803\n",
      "iteration 375: loss: 0.27779021859169006\n",
      "iteration 376: loss: 0.27773427963256836\n",
      "iteration 377: loss: 0.2776786684989929\n",
      "iteration 378: loss: 0.27762389183044434\n",
      "iteration 379: loss: 0.27756887674331665\n",
      "iteration 380: loss: 0.2775140702724457\n",
      "iteration 381: loss: 0.2774588167667389\n",
      "iteration 382: loss: 0.2774046063423157\n",
      "iteration 383: loss: 0.27735066413879395\n",
      "iteration 384: loss: 0.27729710936546326\n",
      "iteration 385: loss: 0.27724379301071167\n",
      "iteration 386: loss: 0.2771902084350586\n",
      "iteration 387: loss: 0.2771373987197876\n",
      "iteration 388: loss: 0.2770843207836151\n",
      "iteration 389: loss: 0.27703219652175903\n",
      "iteration 390: loss: 0.2769796848297119\n",
      "iteration 391: loss: 0.2769274115562439\n",
      "iteration 392: loss: 0.2768760621547699\n",
      "iteration 393: loss: 0.27682435512542725\n",
      "iteration 394: loss: 0.2767729163169861\n",
      "iteration 395: loss: 0.2767217755317688\n",
      "iteration 396: loss: 0.2766708731651306\n",
      "iteration 397: loss: 0.27662020921707153\n",
      "iteration 398: loss: 0.27656984329223633\n",
      "iteration 399: loss: 0.2765197455883026\n",
      "iteration 400: loss: 0.27646929025650024\n",
      "iteration 401: loss: 0.27641960978507996\n",
      "iteration 402: loss: 0.2763703167438507\n",
      "iteration 403: loss: 0.2763206362724304\n",
      "iteration 404: loss: 0.2762717306613922\n",
      "iteration 405: loss: 0.27622315287590027\n",
      "iteration 406: loss: 0.27617424726486206\n",
      "iteration 407: loss: 0.2761255204677582\n",
      "iteration 408: loss: 0.2760776877403259\n",
      "iteration 409: loss: 0.27602943778038025\n",
      "iteration 410: loss: 0.27598148584365845\n",
      "iteration 411: loss: 0.27593374252319336\n",
      "iteration 412: loss: 0.2758868336677551\n",
      "iteration 413: loss: 0.27583953738212585\n",
      "iteration 414: loss: 0.27579253911972046\n",
      "iteration 415: loss: 0.27574577927589417\n",
      "iteration 416: loss: 0.27569860219955444\n",
      "iteration 417: loss: 0.2756522595882416\n",
      "iteration 418: loss: 0.2756061255931854\n",
      "iteration 419: loss: 0.27556031942367554\n",
      "iteration 420: loss: 0.2755146324634552\n",
      "iteration 421: loss: 0.2754685878753662\n",
      "iteration 422: loss: 0.27542346715927124\n",
      "iteration 423: loss: 0.2753778100013733\n",
      "iteration 424: loss: 0.27533307671546936\n",
      "iteration 425: loss: 0.275287926197052\n",
      "iteration 426: loss: 0.2752435803413391\n",
      "iteration 427: loss: 0.27519887685775757\n",
      "iteration 428: loss: 0.2751549780368805\n",
      "iteration 429: loss: 0.27511072158813477\n",
      "iteration 430: loss: 0.2750665545463562\n",
      "iteration 431: loss: 0.2750227451324463\n",
      "iteration 432: loss: 0.2749790549278259\n",
      "iteration 433: loss: 0.27493560314178467\n",
      "iteration 434: loss: 0.27489233016967773\n",
      "iteration 435: loss: 0.2748493254184723\n",
      "iteration 436: loss: 0.2748064398765564\n",
      "iteration 437: loss: 0.2747637927532196\n",
      "iteration 438: loss: 0.27472132444381714\n",
      "iteration 439: loss: 0.27467912435531616\n",
      "iteration 440: loss: 0.27463650703430176\n",
      "iteration 441: loss: 0.27459463477134705\n",
      "iteration 442: loss: 0.27455297112464905\n",
      "iteration 443: loss: 0.27451092004776\n",
      "iteration 444: loss: 0.2744693160057068\n",
      "iteration 445: loss: 0.274427592754364\n",
      "iteration 446: loss: 0.2743867039680481\n",
      "iteration 447: loss: 0.2743454575538635\n",
      "iteration 448: loss: 0.27430489659309387\n",
      "iteration 449: loss: 0.2742639482021332\n",
      "iteration 450: loss: 0.2742232382297516\n",
      "iteration 451: loss: 0.2741832137107849\n",
      "iteration 452: loss: 0.27414292097091675\n",
      "iteration 453: loss: 0.27410268783569336\n",
      "iteration 454: loss: 0.2740626037120819\n",
      "iteration 455: loss: 0.27402275800704956\n",
      "iteration 456: loss: 0.27398309111595154\n",
      "iteration 457: loss: 0.27394360303878784\n",
      "iteration 458: loss: 0.2739042639732361\n",
      "iteration 459: loss: 0.27386513352394104\n",
      "iteration 460: loss: 0.2738262116909027\n",
      "iteration 461: loss: 0.2737874388694763\n",
      "iteration 462: loss: 0.27374815940856934\n",
      "iteration 463: loss: 0.2737097144126892\n",
      "iteration 464: loss: 0.27367132902145386\n",
      "iteration 465: loss: 0.27363264560699463\n",
      "iteration 466: loss: 0.2735946774482727\n",
      "iteration 467: loss: 0.2735568881034851\n",
      "iteration 468: loss: 0.2735186219215393\n",
      "iteration 469: loss: 0.273481160402298\n",
      "iteration 470: loss: 0.2734432816505432\n",
      "iteration 471: loss: 0.27340611815452576\n",
      "iteration 472: loss: 0.2733685076236725\n",
      "iteration 473: loss: 0.2733309864997864\n",
      "iteration 474: loss: 0.273293673992157\n",
      "iteration 475: loss: 0.2732572555541992\n",
      "iteration 476: loss: 0.2732202708721161\n",
      "iteration 477: loss: 0.2731834352016449\n",
      "iteration 478: loss: 0.27314671874046326\n",
      "iteration 479: loss: 0.27311015129089355\n",
      "iteration 480: loss: 0.27307385206222534\n",
      "iteration 481: loss: 0.2730376124382019\n",
      "iteration 482: loss: 0.2730015516281128\n",
      "iteration 483: loss: 0.27296561002731323\n",
      "iteration 484: loss: 0.2729299068450928\n",
      "iteration 485: loss: 0.2728942334651947\n",
      "iteration 486: loss: 0.27285870909690857\n",
      "iteration 487: loss: 0.2728234529495239\n",
      "iteration 488: loss: 0.27278822660446167\n",
      "iteration 489: loss: 0.2727524936199188\n",
      "iteration 490: loss: 0.2727176547050476\n",
      "iteration 491: loss: 0.27268287539482117\n",
      "iteration 492: loss: 0.27264755964279175\n",
      "iteration 493: loss: 0.2726131081581116\n",
      "iteration 494: loss: 0.27257809042930603\n",
      "iteration 495: loss: 0.27254390716552734\n",
      "iteration 496: loss: 0.2725091576576233\n",
      "iteration 497: loss: 0.2724752426147461\n",
      "iteration 498: loss: 0.2724408507347107\n",
      "iteration 499: loss: 0.2724071145057678\n",
      "iteration 500: loss: 0.27237293124198914\n",
      "iteration 501: loss: 0.2723389267921448\n",
      "iteration 502: loss: 0.2723049521446228\n",
      "iteration 503: loss: 0.2722718417644501\n",
      "iteration 504: loss: 0.272238165140152\n",
      "iteration 505: loss: 0.2722046375274658\n",
      "iteration 506: loss: 0.2721713185310364\n",
      "iteration 507: loss: 0.27213793992996216\n",
      "iteration 508: loss: 0.27210479974746704\n",
      "iteration 509: loss: 0.27207186818122864\n",
      "iteration 510: loss: 0.27203893661499023\n",
      "iteration 511: loss: 0.272006094455719\n",
      "iteration 512: loss: 0.27197346091270447\n",
      "iteration 513: loss: 0.2719409167766571\n",
      "iteration 514: loss: 0.27190858125686646\n",
      "iteration 515: loss: 0.2718762457370758\n",
      "iteration 516: loss: 0.2718440890312195\n",
      "iteration 517: loss: 0.2718120217323303\n",
      "iteration 518: loss: 0.2717794179916382\n",
      "iteration 519: loss: 0.2717476189136505\n",
      "iteration 520: loss: 0.2717159390449524\n",
      "iteration 521: loss: 0.2716836929321289\n",
      "iteration 522: loss: 0.2716521918773651\n",
      "iteration 523: loss: 0.27162083983421326\n",
      "iteration 524: loss: 0.2715889811515808\n",
      "iteration 525: loss: 0.27155783772468567\n",
      "iteration 526: loss: 0.27152615785598755\n",
      "iteration 527: loss: 0.2714952826499939\n",
      "iteration 528: loss: 0.2714638411998749\n",
      "iteration 529: loss: 0.27143239974975586\n",
      "iteration 530: loss: 0.27140194177627563\n",
      "iteration 531: loss: 0.2713707387447357\n",
      "iteration 532: loss: 0.2713397145271301\n",
      "iteration 533: loss: 0.2713095545768738\n",
      "iteration 534: loss: 0.2712787091732025\n",
      "iteration 535: loss: 0.2712479531764984\n",
      "iteration 536: loss: 0.271217405796051\n",
      "iteration 537: loss: 0.27118757367134094\n",
      "iteration 538: loss: 0.2711572051048279\n",
      "iteration 539: loss: 0.27112695574760437\n",
      "iteration 540: loss: 0.27109670639038086\n",
      "iteration 541: loss: 0.2710666358470917\n",
      "iteration 542: loss: 0.27103665471076965\n",
      "iteration 543: loss: 0.2710067629814148\n",
      "iteration 544: loss: 0.2709769606590271\n",
      "iteration 545: loss: 0.2709473669528961\n",
      "iteration 546: loss: 0.27091777324676514\n",
      "iteration 547: loss: 0.27088838815689087\n",
      "iteration 548: loss: 0.27085885405540466\n",
      "iteration 549: loss: 0.27082961797714233\n",
      "iteration 550: loss: 0.2707996964454651\n",
      "iteration 551: loss: 0.2707706093788147\n",
      "iteration 552: loss: 0.27074167132377625\n",
      "iteration 553: loss: 0.27071279287338257\n",
      "iteration 554: loss: 0.270683228969574\n",
      "iteration 555: loss: 0.270654559135437\n",
      "iteration 556: loss: 0.27062588930130005\n",
      "iteration 557: loss: 0.2705967426300049\n",
      "iteration 558: loss: 0.27056828141212463\n",
      "iteration 559: loss: 0.27053993940353394\n",
      "iteration 560: loss: 0.2705109715461731\n",
      "iteration 561: loss: 0.2704828083515167\n",
      "iteration 562: loss: 0.270454078912735\n",
      "iteration 563: loss: 0.27042606472969055\n",
      "iteration 564: loss: 0.27039748430252075\n",
      "iteration 565: loss: 0.27036893367767334\n",
      "iteration 566: loss: 0.2703412175178528\n",
      "iteration 567: loss: 0.2703128755092621\n",
      "iteration 568: loss: 0.27028536796569824\n",
      "iteration 569: loss: 0.27025723457336426\n",
      "iteration 570: loss: 0.2702290415763855\n",
      "iteration 571: loss: 0.270201176404953\n",
      "iteration 572: loss: 0.27017390727996826\n",
      "iteration 573: loss: 0.27014607191085815\n",
      "iteration 574: loss: 0.2701182961463928\n",
      "iteration 575: loss: 0.27009063959121704\n",
      "iteration 576: loss: 0.2700631022453308\n",
      "iteration 577: loss: 0.2700362801551819\n",
      "iteration 578: loss: 0.2700088918209076\n",
      "iteration 579: loss: 0.2699815332889557\n",
      "iteration 580: loss: 0.2699543535709381\n",
      "iteration 581: loss: 0.26992717385292053\n",
      "iteration 582: loss: 0.2699001133441925\n",
      "iteration 583: loss: 0.2698729634284973\n",
      "iteration 584: loss: 0.2698461413383484\n",
      "iteration 585: loss: 0.2698192894458771\n",
      "iteration 586: loss: 0.26979243755340576\n",
      "iteration 587: loss: 0.2697656750679016\n",
      "iteration 588: loss: 0.2697383761405945\n",
      "iteration 589: loss: 0.26971182227134705\n",
      "iteration 590: loss: 0.2696853578090668\n",
      "iteration 591: loss: 0.2696589529514313\n",
      "iteration 592: loss: 0.26963263750076294\n",
      "iteration 593: loss: 0.2696056365966797\n",
      "iteration 594: loss: 0.2695794701576233\n",
      "iteration 595: loss: 0.26955336332321167\n",
      "iteration 596: loss: 0.2695273756980896\n",
      "iteration 597: loss: 0.26950061321258545\n",
      "iteration 598: loss: 0.2694747745990753\n",
      "iteration 599: loss: 0.269448846578598\n",
      "iteration 600: loss: 0.2694224417209625\n",
      "iteration 601: loss: 0.26939672231674194\n",
      "iteration 602: loss: 0.2693704664707184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 603: loss: 0.26934489607810974\n",
      "iteration 604: loss: 0.26931872963905334\n",
      "iteration 605: loss: 0.2692928910255432\n",
      "iteration 606: loss: 0.26926684379577637\n",
      "iteration 607: loss: 0.26924166083335876\n",
      "iteration 608: loss: 0.2692156434059143\n",
      "iteration 609: loss: 0.2691905200481415\n",
      "iteration 610: loss: 0.2691647410392761\n",
      "iteration 611: loss: 0.26913899183273315\n",
      "iteration 612: loss: 0.26911407709121704\n",
      "iteration 613: loss: 0.2690884470939636\n",
      "iteration 614: loss: 0.26906290650367737\n",
      "iteration 615: loss: 0.26903823018074036\n",
      "iteration 616: loss: 0.26901280879974365\n",
      "iteration 617: loss: 0.2689875066280365\n",
      "iteration 618: loss: 0.26896294951438904\n",
      "iteration 619: loss: 0.26893776655197144\n",
      "iteration 620: loss: 0.26891258358955383\n",
      "iteration 621: loss: 0.26888754963874817\n",
      "iteration 622: loss: 0.2688625156879425\n",
      "iteration 623: loss: 0.2688375413417816\n",
      "iteration 624: loss: 0.2688134014606476\n",
      "iteration 625: loss: 0.26878851652145386\n",
      "iteration 626: loss: 0.26876378059387207\n",
      "iteration 627: loss: 0.26873910427093506\n",
      "iteration 628: loss: 0.26871436834335327\n",
      "iteration 629: loss: 0.2686898410320282\n",
      "iteration 630: loss: 0.2686653137207031\n",
      "iteration 631: loss: 0.26864075660705566\n",
      "iteration 632: loss: 0.26861637830734253\n",
      "iteration 633: loss: 0.268591970205307\n",
      "iteration 634: loss: 0.2685677111148834\n",
      "iteration 635: loss: 0.2685435116291046\n",
      "iteration 636: loss: 0.26851850748062134\n",
      "iteration 637: loss: 0.2684944272041321\n",
      "iteration 638: loss: 0.26847025752067566\n",
      "iteration 639: loss: 0.2684463560581207\n",
      "iteration 640: loss: 0.268422394990921\n",
      "iteration 641: loss: 0.2683984339237213\n",
      "iteration 642: loss: 0.26837387681007385\n",
      "iteration 643: loss: 0.2683500349521637\n",
      "iteration 644: loss: 0.26832637190818787\n",
      "iteration 645: loss: 0.26830270886421204\n",
      "iteration 646: loss: 0.2682783007621765\n",
      "iteration 647: loss: 0.26825469732284546\n",
      "iteration 648: loss: 0.2682311534881592\n",
      "iteration 649: loss: 0.26820698380470276\n",
      "iteration 650: loss: 0.26818355917930603\n",
      "iteration 651: loss: 0.2681601643562317\n",
      "iteration 652: loss: 0.2681361138820648\n",
      "iteration 653: loss: 0.2681128978729248\n",
      "iteration 654: loss: 0.2680889070034027\n",
      "iteration 655: loss: 0.26806581020355225\n",
      "iteration 656: loss: 0.2680419385433197\n",
      "iteration 657: loss: 0.2680188715457916\n",
      "iteration 658: loss: 0.2679951786994934\n",
      "iteration 659: loss: 0.2679721713066101\n",
      "iteration 660: loss: 0.2679485082626343\n",
      "iteration 661: loss: 0.2679257094860077\n",
      "iteration 662: loss: 0.2679021656513214\n",
      "iteration 663: loss: 0.2678794264793396\n",
      "iteration 664: loss: 0.2678559720516205\n",
      "iteration 665: loss: 0.2678333520889282\n",
      "iteration 666: loss: 0.2678099274635315\n",
      "iteration 667: loss: 0.2677866816520691\n",
      "iteration 668: loss: 0.267764151096344\n",
      "iteration 669: loss: 0.2677409052848816\n",
      "iteration 670: loss: 0.2677178382873535\n",
      "iteration 671: loss: 0.2676946520805359\n",
      "iteration 672: loss: 0.2676723599433899\n",
      "iteration 673: loss: 0.2676493227481842\n",
      "iteration 674: loss: 0.2676263153553009\n",
      "iteration 675: loss: 0.2676033675670624\n",
      "iteration 676: loss: 0.2675812840461731\n",
      "iteration 677: loss: 0.26755839586257935\n",
      "iteration 678: loss: 0.26753565669059753\n",
      "iteration 679: loss: 0.26751285791397095\n",
      "iteration 680: loss: 0.2674901783466339\n",
      "iteration 681: loss: 0.2674674689769745\n",
      "iteration 682: loss: 0.2674456834793091\n",
      "iteration 683: loss: 0.2674230635166168\n",
      "iteration 684: loss: 0.2674005329608917\n",
      "iteration 685: loss: 0.2673780620098114\n",
      "iteration 686: loss: 0.26735568046569824\n",
      "iteration 687: loss: 0.26733332872390747\n",
      "iteration 688: loss: 0.26731088757514954\n",
      "iteration 689: loss: 0.26728856563568115\n",
      "iteration 690: loss: 0.26726633310317993\n",
      "iteration 691: loss: 0.2672441601753235\n",
      "iteration 692: loss: 0.26722192764282227\n",
      "iteration 693: loss: 0.2671998143196106\n",
      "iteration 694: loss: 0.2671777009963989\n",
      "iteration 695: loss: 0.2671556770801544\n",
      "iteration 696: loss: 0.26713284850120544\n",
      "iteration 697: loss: 0.2671108841896057\n",
      "iteration 698: loss: 0.26708894968032837\n",
      "iteration 699: loss: 0.2670670449733734\n",
      "iteration 700: loss: 0.267045259475708\n",
      "iteration 701: loss: 0.2670234441757202\n",
      "iteration 702: loss: 0.2670016586780548\n",
      "iteration 703: loss: 0.2669791579246521\n",
      "iteration 704: loss: 0.26695746183395386\n",
      "iteration 705: loss: 0.2669358253479004\n",
      "iteration 706: loss: 0.2669141888618469\n",
      "iteration 707: loss: 0.2668918967247009\n",
      "iteration 708: loss: 0.2668703496456146\n",
      "iteration 709: loss: 0.2668488621711731\n",
      "iteration 710: loss: 0.26682743430137634\n",
      "iteration 711: loss: 0.2668052315711975\n",
      "iteration 712: loss: 0.26678383350372314\n",
      "iteration 713: loss: 0.26676255464553833\n",
      "iteration 714: loss: 0.26674047112464905\n",
      "iteration 715: loss: 0.266719251871109\n",
      "iteration 716: loss: 0.2666972279548645\n",
      "iteration 717: loss: 0.26667600870132446\n",
      "iteration 718: loss: 0.2666548788547516\n",
      "iteration 719: loss: 0.26663297414779663\n",
      "iteration 720: loss: 0.26661187410354614\n",
      "iteration 721: loss: 0.2665901184082031\n",
      "iteration 722: loss: 0.2665690779685974\n",
      "iteration 723: loss: 0.2665473520755768\n",
      "iteration 724: loss: 0.26652634143829346\n",
      "iteration 725: loss: 0.26650482416152954\n",
      "iteration 726: loss: 0.26648396253585815\n",
      "iteration 727: loss: 0.2664622962474823\n",
      "iteration 728: loss: 0.2664415240287781\n",
      "iteration 729: loss: 0.26642003655433655\n",
      "iteration 730: loss: 0.2663993239402771\n",
      "iteration 731: loss: 0.2663778066635132\n",
      "iteration 732: loss: 0.2663564085960388\n",
      "iteration 733: loss: 0.2663358449935913\n",
      "iteration 734: loss: 0.26631441712379456\n",
      "iteration 735: loss: 0.26629313826560974\n",
      "iteration 736: loss: 0.2662726044654846\n",
      "iteration 737: loss: 0.26625141501426697\n",
      "iteration 738: loss: 0.26623016595840454\n",
      "iteration 739: loss: 0.2662096917629242\n",
      "iteration 740: loss: 0.2661886513233185\n",
      "iteration 741: loss: 0.266167551279068\n",
      "iteration 742: loss: 0.2661471962928772\n",
      "iteration 743: loss: 0.2661261558532715\n",
      "iteration 744: loss: 0.26610517501831055\n",
      "iteration 745: loss: 0.2660841941833496\n",
      "iteration 746: loss: 0.26606398820877075\n",
      "iteration 747: loss: 0.26604312658309937\n",
      "iteration 748: loss: 0.2660221457481384\n",
      "iteration 749: loss: 0.2660013735294342\n",
      "iteration 750: loss: 0.2659805417060852\n",
      "iteration 751: loss: 0.2659597396850586\n",
      "iteration 752: loss: 0.2659398019313812\n",
      "iteration 753: loss: 0.2659190595149994\n",
      "iteration 754: loss: 0.26589828729629517\n",
      "iteration 755: loss: 0.26587769389152527\n",
      "iteration 756: loss: 0.2658570408821106\n",
      "iteration 757: loss: 0.2658364176750183\n",
      "iteration 758: loss: 0.2658158838748932\n",
      "iteration 759: loss: 0.26579540967941284\n",
      "iteration 760: loss: 0.2657749056816101\n",
      "iteration 761: loss: 0.26575446128845215\n",
      "iteration 762: loss: 0.2657340168952942\n",
      "iteration 763: loss: 0.26571351289749146\n",
      "iteration 764: loss: 0.26569315791130066\n",
      "iteration 765: loss: 0.26567280292510986\n",
      "iteration 766: loss: 0.26565247774124146\n",
      "iteration 767: loss: 0.26563215255737305\n",
      "iteration 768: loss: 0.2656118869781494\n",
      "iteration 769: loss: 0.26559171080589294\n",
      "iteration 770: loss: 0.2655714750289917\n",
      "iteration 771: loss: 0.26555135846138\n",
      "iteration 772: loss: 0.26553112268447876\n",
      "iteration 773: loss: 0.2655109763145447\n",
      "iteration 774: loss: 0.26549094915390015\n",
      "iteration 775: loss: 0.26547011733055115\n",
      "iteration 776: loss: 0.26545003056526184\n",
      "iteration 777: loss: 0.2654300332069397\n",
      "iteration 778: loss: 0.26541006565093994\n",
      "iteration 779: loss: 0.26539018750190735\n",
      "iteration 780: loss: 0.2653701901435852\n",
      "iteration 781: loss: 0.26534950733184814\n",
      "iteration 782: loss: 0.2653300166130066\n",
      "iteration 783: loss: 0.2653101980686188\n",
      "iteration 784: loss: 0.26529043912887573\n",
      "iteration 785: loss: 0.2652706205844879\n",
      "iteration 786: loss: 0.26525014638900757\n",
      "iteration 787: loss: 0.26523035764694214\n",
      "iteration 788: loss: 0.26521068811416626\n",
      "iteration 789: loss: 0.2651902437210083\n",
      "iteration 790: loss: 0.2651706039905548\n",
      "iteration 791: loss: 0.2651509642601013\n",
      "iteration 792: loss: 0.2651306688785553\n",
      "iteration 793: loss: 0.2651110589504242\n",
      "iteration 794: loss: 0.2650914788246155\n",
      "iteration 795: loss: 0.26507124304771423\n",
      "iteration 796: loss: 0.26505178213119507\n",
      "iteration 797: loss: 0.2650322914123535\n",
      "iteration 798: loss: 0.26501208543777466\n",
      "iteration 799: loss: 0.2649926543235779\n",
      "iteration 800: loss: 0.26497259736061096\n",
      "iteration 801: loss: 0.26495322585105896\n",
      "iteration 802: loss: 0.26493382453918457\n",
      "iteration 803: loss: 0.26491376757621765\n",
      "iteration 804: loss: 0.2648945450782776\n",
      "iteration 805: loss: 0.2648744583129883\n",
      "iteration 806: loss: 0.2648552656173706\n",
      "iteration 807: loss: 0.26483532786369324\n",
      "iteration 808: loss: 0.2648161053657532\n",
      "iteration 809: loss: 0.2647961974143982\n",
      "iteration 810: loss: 0.2647770941257477\n",
      "iteration 811: loss: 0.2647571563720703\n",
      "iteration 812: loss: 0.26473817229270935\n",
      "iteration 813: loss: 0.26471829414367676\n",
      "iteration 814: loss: 0.2646985650062561\n",
      "iteration 815: loss: 0.2646794617176056\n",
      "iteration 816: loss: 0.2646598219871521\n",
      "iteration 817: loss: 0.2646408677101135\n",
      "iteration 818: loss: 0.26462113857269287\n",
      "iteration 819: loss: 0.2646014094352722\n",
      "iteration 820: loss: 0.2645826041698456\n",
      "iteration 821: loss: 0.2645629644393921\n",
      "iteration 822: loss: 0.26454415917396545\n",
      "iteration 823: loss: 0.26452454924583435\n",
      "iteration 824: loss: 0.26450496912002563\n",
      "iteration 825: loss: 0.264486163854599\n",
      "iteration 826: loss: 0.26446670293807983\n",
      "iteration 827: loss: 0.26444724202156067\n",
      "iteration 828: loss: 0.2644277811050415\n",
      "iteration 829: loss: 0.2644091248512268\n",
      "iteration 830: loss: 0.2643897533416748\n",
      "iteration 831: loss: 0.26437026262283325\n",
      "iteration 832: loss: 0.26435166597366333\n",
      "iteration 833: loss: 0.2643323540687561\n",
      "iteration 834: loss: 0.26431307196617126\n",
      "iteration 835: loss: 0.26429376006126404\n",
      "iteration 836: loss: 0.264274537563324\n",
      "iteration 837: loss: 0.26425594091415405\n",
      "iteration 838: loss: 0.26423677802085876\n",
      "iteration 839: loss: 0.2642175555229187\n",
      "iteration 840: loss: 0.2641984224319458\n",
      "iteration 841: loss: 0.2641792297363281\n",
      "iteration 842: loss: 0.2641600966453552\n",
      "iteration 843: loss: 0.2641417384147644\n",
      "iteration 844: loss: 0.2641226053237915\n",
      "iteration 845: loss: 0.26410356163978577\n",
      "iteration 846: loss: 0.26408451795578003\n",
      "iteration 847: loss: 0.2640655040740967\n",
      "iteration 848: loss: 0.26404646039009094\n",
      "iteration 849: loss: 0.26402753591537476\n",
      "iteration 850: loss: 0.2640085816383362\n",
      "iteration 851: loss: 0.2639896273612976\n",
      "iteration 852: loss: 0.2639707624912262\n",
      "iteration 853: loss: 0.26395183801651\n",
      "iteration 854: loss: 0.2639329433441162\n",
      "iteration 855: loss: 0.2639140784740448\n",
      "iteration 856: loss: 0.26389530301094055\n",
      "iteration 857: loss: 0.2638764977455139\n",
      "iteration 858: loss: 0.26385772228240967\n",
      "iteration 859: loss: 0.26383882761001587\n",
      "iteration 860: loss: 0.2638201117515564\n",
      "iteration 861: loss: 0.2638014256954193\n",
      "iteration 862: loss: 0.2637827694416046\n",
      "iteration 863: loss: 0.26376402378082275\n",
      "iteration 864: loss: 0.26374539732933044\n",
      "iteration 865: loss: 0.26372671127319336\n",
      "iteration 866: loss: 0.26370805501937866\n",
      "iteration 867: loss: 0.26368948817253113\n",
      "iteration 868: loss: 0.2636708617210388\n",
      "iteration 869: loss: 0.26365238428115845\n",
      "iteration 870: loss: 0.2636337876319885\n",
      "iteration 871: loss: 0.26361459493637085\n",
      "iteration 872: loss: 0.2635959982872009\n",
      "iteration 873: loss: 0.2635776102542877\n",
      "iteration 874: loss: 0.26355913281440735\n",
      "iteration 875: loss: 0.263540655374527\n",
      "iteration 876: loss: 0.26352229714393616\n",
      "iteration 877: loss: 0.26350313425064087\n",
      "iteration 878: loss: 0.2634847164154053\n",
      "iteration 879: loss: 0.26346641778945923\n",
      "iteration 880: loss: 0.2634480595588684\n",
      "iteration 881: loss: 0.2634297013282776\n",
      "iteration 882: loss: 0.26341068744659424\n",
      "iteration 883: loss: 0.2633923888206482\n",
      "iteration 884: loss: 0.26337409019470215\n",
      "iteration 885: loss: 0.2633557915687561\n",
      "iteration 886: loss: 0.2633368968963623\n",
      "iteration 887: loss: 0.2633186876773834\n",
      "iteration 888: loss: 0.26330047845840454\n",
      "iteration 889: loss: 0.26328161358833313\n",
      "iteration 890: loss: 0.2632634937763214\n",
      "iteration 891: loss: 0.26324528455734253\n",
      "iteration 892: loss: 0.2632271945476532\n",
      "iteration 893: loss: 0.26320841908454895\n",
      "iteration 894: loss: 0.263190358877182\n",
      "iteration 895: loss: 0.26317161321640015\n",
      "iteration 896: loss: 0.2631534934043884\n",
      "iteration 897: loss: 0.26313549280166626\n",
      "iteration 898: loss: 0.2631167769432068\n",
      "iteration 899: loss: 0.2630988359451294\n",
      "iteration 900: loss: 0.26308080554008484\n",
      "iteration 901: loss: 0.26306217908859253\n",
      "iteration 902: loss: 0.2630442976951599\n",
      "iteration 903: loss: 0.2630256414413452\n",
      "iteration 904: loss: 0.2630077302455902\n",
      "iteration 905: loss: 0.2629891037940979\n",
      "iteration 906: loss: 0.26297128200531006\n",
      "iteration 907: loss: 0.26295337080955505\n",
      "iteration 908: loss: 0.26293492317199707\n",
      "iteration 909: loss: 0.26291707158088684\n",
      "iteration 910: loss: 0.26289862394332886\n",
      "iteration 911: loss: 0.262880802154541\n",
      "iteration 912: loss: 0.2628623843193054\n",
      "iteration 913: loss: 0.26284462213516235\n",
      "iteration 914: loss: 0.262826144695282\n",
      "iteration 915: loss: 0.26280853152275085\n",
      "iteration 916: loss: 0.26279014348983765\n",
      "iteration 917: loss: 0.26277175545692444\n",
      "iteration 918: loss: 0.2627541124820709\n",
      "iteration 919: loss: 0.2627357840538025\n",
      "iteration 920: loss: 0.262718141078949\n",
      "iteration 921: loss: 0.26269984245300293\n",
      "iteration 922: loss: 0.2626822590827942\n",
      "iteration 923: loss: 0.2626640796661377\n",
      "iteration 924: loss: 0.2626458704471588\n",
      "iteration 925: loss: 0.2626282572746277\n",
      "iteration 926: loss: 0.2626100778579712\n",
      "iteration 927: loss: 0.26259180903434753\n",
      "iteration 928: loss: 0.26257434487342834\n",
      "iteration 929: loss: 0.2625562250614166\n",
      "iteration 930: loss: 0.2625381052494049\n",
      "iteration 931: loss: 0.2625206410884857\n",
      "iteration 932: loss: 0.262502521276474\n",
      "iteration 933: loss: 0.2624844014644623\n",
      "iteration 934: loss: 0.26246705651283264\n",
      "iteration 935: loss: 0.2624490261077881\n",
      "iteration 936: loss: 0.26243099570274353\n",
      "iteration 937: loss: 0.2624136805534363\n",
      "iteration 938: loss: 0.2623957097530365\n",
      "iteration 939: loss: 0.26237770915031433\n",
      "iteration 940: loss: 0.2623596787452698\n",
      "iteration 941: loss: 0.2623424530029297\n",
      "iteration 942: loss: 0.2623245120048523\n",
      "iteration 943: loss: 0.2623066306114197\n",
      "iteration 944: loss: 0.2622886598110199\n",
      "iteration 945: loss: 0.26227083802223206\n",
      "iteration 946: loss: 0.26225361227989197\n",
      "iteration 947: loss: 0.2622358202934265\n",
      "iteration 948: loss: 0.2622179388999939\n",
      "iteration 949: loss: 0.26220011711120605\n",
      "iteration 950: loss: 0.2621823847293854\n",
      "iteration 951: loss: 0.26216524839401245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 952: loss: 0.2621474862098694\n",
      "iteration 953: loss: 0.2621297538280487\n",
      "iteration 954: loss: 0.26211193203926086\n",
      "iteration 955: loss: 0.26209431886672974\n",
      "iteration 956: loss: 0.26207655668258667\n",
      "iteration 957: loss: 0.26205888390541077\n",
      "iteration 958: loss: 0.2620411813259125\n",
      "iteration 959: loss: 0.26202359795570374\n",
      "iteration 960: loss: 0.26200658082962036\n",
      "iteration 961: loss: 0.26198896765708923\n",
      "iteration 962: loss: 0.2619713246822357\n",
      "iteration 963: loss: 0.26195377111434937\n",
      "iteration 964: loss: 0.2619361877441406\n",
      "iteration 965: loss: 0.2619185745716095\n",
      "iteration 966: loss: 0.2619010806083679\n",
      "iteration 967: loss: 0.26188355684280396\n",
      "iteration 968: loss: 0.2618660032749176\n",
      "iteration 969: loss: 0.2618485689163208\n",
      "iteration 970: loss: 0.26183104515075684\n",
      "iteration 971: loss: 0.26181358098983765\n",
      "iteration 972: loss: 0.26179617643356323\n",
      "iteration 973: loss: 0.26177871227264404\n",
      "iteration 974: loss: 0.26176124811172485\n",
      "iteration 975: loss: 0.2617438733577728\n",
      "iteration 976: loss: 0.2617265582084656\n",
      "iteration 977: loss: 0.2617090940475464\n",
      "iteration 978: loss: 0.26169174909591675\n",
      "iteration 979: loss: 0.2616744041442871\n",
      "iteration 980: loss: 0.2616564631462097\n",
      "iteration 981: loss: 0.26163917779922485\n",
      "iteration 982: loss: 0.2616218328475952\n",
      "iteration 983: loss: 0.26160454750061035\n",
      "iteration 984: loss: 0.26158732175827026\n",
      "iteration 985: loss: 0.261570006608963\n",
      "iteration 986: loss: 0.26155275106430054\n",
      "iteration 987: loss: 0.26153555512428284\n",
      "iteration 988: loss: 0.26151832938194275\n",
      "iteration 989: loss: 0.2615004777908325\n",
      "iteration 990: loss: 0.261483371257782\n",
      "iteration 991: loss: 0.26146623492240906\n",
      "iteration 992: loss: 0.26144909858703613\n",
      "iteration 993: loss: 0.2614319324493408\n",
      "iteration 994: loss: 0.26141414046287537\n",
      "iteration 995: loss: 0.2613970637321472\n",
      "iteration 996: loss: 0.26138001680374146\n",
      "iteration 997: loss: 0.2613629400730133\n",
      "iteration 998: loss: 0.2613458037376404\n",
      "iteration 999: loss: 0.2613280713558197\n",
      "iteration 1000: loss: 0.2613110840320587\n",
      "iteration 1001: loss: 0.26129406690597534\n",
      "iteration 1002: loss: 0.26127707958221436\n",
      "iteration 1003: loss: 0.26125943660736084\n",
      "iteration 1004: loss: 0.26124247908592224\n",
      "iteration 1005: loss: 0.26122555136680603\n",
      "iteration 1006: loss: 0.2612079977989197\n",
      "iteration 1007: loss: 0.2611910104751587\n",
      "iteration 1008: loss: 0.26117411255836487\n",
      "iteration 1009: loss: 0.26115718483924866\n",
      "iteration 1010: loss: 0.2611396610736847\n",
      "iteration 1011: loss: 0.26112285256385803\n",
      "iteration 1012: loss: 0.2611059248447418\n",
      "iteration 1013: loss: 0.26108843088150024\n",
      "iteration 1014: loss: 0.26107165217399597\n",
      "iteration 1015: loss: 0.26105421781539917\n",
      "iteration 1016: loss: 0.2610374391078949\n",
      "iteration 1017: loss: 0.26102057099342346\n",
      "iteration 1018: loss: 0.26100319623947144\n",
      "iteration 1019: loss: 0.26098647713661194\n",
      "iteration 1020: loss: 0.26096969842910767\n",
      "iteration 1021: loss: 0.26095232367515564\n",
      "iteration 1022: loss: 0.2609356939792633\n",
      "iteration 1023: loss: 0.2609182894229889\n",
      "iteration 1024: loss: 0.26090162992477417\n",
      "iteration 1025: loss: 0.2608843743801117\n",
      "iteration 1026: loss: 0.26086756587028503\n",
      "iteration 1027: loss: 0.26085036993026733\n",
      "iteration 1028: loss: 0.26083406805992126\n",
      "iteration 1029: loss: 0.26081737875938416\n",
      "iteration 1030: loss: 0.26080021262168884\n",
      "iteration 1031: loss: 0.2607835829257965\n",
      "iteration 1032: loss: 0.2607664465904236\n",
      "iteration 1033: loss: 0.2607499063014984\n",
      "iteration 1034: loss: 0.26073265075683594\n",
      "iteration 1035: loss: 0.26071617007255554\n",
      "iteration 1036: loss: 0.26069894433021545\n",
      "iteration 1037: loss: 0.2606818377971649\n",
      "iteration 1038: loss: 0.26066532731056213\n",
      "iteration 1039: loss: 0.26064828038215637\n",
      "iteration 1040: loss: 0.2606317400932312\n",
      "iteration 1041: loss: 0.2606147527694702\n",
      "iteration 1042: loss: 0.26059824228286743\n",
      "iteration 1043: loss: 0.26058122515678406\n",
      "iteration 1044: loss: 0.2605641484260559\n",
      "iteration 1045: loss: 0.26054781675338745\n",
      "iteration 1046: loss: 0.2605307400226593\n",
      "iteration 1047: loss: 0.26051443815231323\n",
      "iteration 1048: loss: 0.26049742102622986\n",
      "iteration 1049: loss: 0.26048049330711365\n",
      "iteration 1050: loss: 0.2604641318321228\n",
      "iteration 1051: loss: 0.26044726371765137\n",
      "iteration 1052: loss: 0.2604302763938904\n",
      "iteration 1053: loss: 0.26041391491889954\n",
      "iteration 1054: loss: 0.2603970766067505\n",
      "iteration 1055: loss: 0.26038017868995667\n",
      "iteration 1056: loss: 0.26036399602890015\n",
      "iteration 1057: loss: 0.26034706830978394\n",
      "iteration 1058: loss: 0.2603302597999573\n",
      "iteration 1059: loss: 0.26031404733657837\n",
      "iteration 1060: loss: 0.2602972388267517\n",
      "iteration 1061: loss: 0.2602803707122803\n",
      "iteration 1062: loss: 0.2602636218070984\n",
      "iteration 1063: loss: 0.26024746894836426\n",
      "iteration 1064: loss: 0.2602306306362152\n",
      "iteration 1065: loss: 0.2602140009403229\n",
      "iteration 1066: loss: 0.2601972222328186\n",
      "iteration 1067: loss: 0.26018109917640686\n",
      "iteration 1068: loss: 0.2601644694805145\n",
      "iteration 1069: loss: 0.26014766097068787\n",
      "iteration 1070: loss: 0.26013100147247314\n",
      "iteration 1071: loss: 0.26011502742767334\n",
      "iteration 1072: loss: 0.2600983679294586\n",
      "iteration 1073: loss: 0.2600816786289215\n",
      "iteration 1074: loss: 0.2600650191307068\n",
      "iteration 1075: loss: 0.2600484788417816\n",
      "iteration 1076: loss: 0.26003187894821167\n",
      "iteration 1077: loss: 0.26001590490341187\n",
      "iteration 1078: loss: 0.25999927520751953\n",
      "iteration 1079: loss: 0.25998276472091675\n",
      "iteration 1080: loss: 0.2599661946296692\n",
      "iteration 1081: loss: 0.259949654340744\n",
      "iteration 1082: loss: 0.25993308424949646\n",
      "iteration 1083: loss: 0.2599166929721832\n",
      "iteration 1084: loss: 0.2599002420902252\n",
      "iteration 1085: loss: 0.2598842978477478\n",
      "iteration 1086: loss: 0.2598678469657898\n",
      "iteration 1087: loss: 0.259851336479187\n",
      "iteration 1088: loss: 0.2598349452018738\n",
      "iteration 1089: loss: 0.25981849431991577\n",
      "iteration 1090: loss: 0.25980204343795776\n",
      "iteration 1091: loss: 0.25978565216064453\n",
      "iteration 1092: loss: 0.2597692608833313\n",
      "iteration 1093: loss: 0.25975295901298523\n",
      "iteration 1094: loss: 0.2597365975379944\n",
      "iteration 1095: loss: 0.2597202658653259\n",
      "iteration 1096: loss: 0.2597038745880127\n",
      "iteration 1097: loss: 0.25968748331069946\n",
      "iteration 1098: loss: 0.2596712112426758\n",
      "iteration 1099: loss: 0.2596549093723297\n",
      "iteration 1100: loss: 0.2596386969089508\n",
      "iteration 1101: loss: 0.2596224248409271\n",
      "iteration 1102: loss: 0.25960618257522583\n",
      "iteration 1103: loss: 0.25958991050720215\n",
      "iteration 1104: loss: 0.25957363843917847\n",
      "iteration 1105: loss: 0.25955742597579956\n",
      "iteration 1106: loss: 0.25954118371009827\n",
      "iteration 1107: loss: 0.25952500104904175\n",
      "iteration 1108: loss: 0.25950875878334045\n",
      "iteration 1109: loss: 0.25949209928512573\n",
      "iteration 1110: loss: 0.2594760060310364\n",
      "iteration 1111: loss: 0.25945982336997986\n",
      "iteration 1112: loss: 0.2594437003135681\n",
      "iteration 1113: loss: 0.259427547454834\n",
      "iteration 1114: loss: 0.25941142439842224\n",
      "iteration 1115: loss: 0.2593953013420105\n",
      "iteration 1116: loss: 0.25937923789024353\n",
      "iteration 1117: loss: 0.2593631446361542\n",
      "iteration 1118: loss: 0.25934654474258423\n",
      "iteration 1119: loss: 0.2593304514884949\n",
      "iteration 1120: loss: 0.25931447744369507\n",
      "iteration 1121: loss: 0.2592984437942505\n",
      "iteration 1122: loss: 0.2592824399471283\n",
      "iteration 1123: loss: 0.2592659592628479\n",
      "iteration 1124: loss: 0.25924986600875854\n",
      "iteration 1125: loss: 0.2592339515686035\n",
      "iteration 1126: loss: 0.2592180073261261\n",
      "iteration 1127: loss: 0.2592020630836487\n",
      "iteration 1128: loss: 0.2591855823993683\n",
      "iteration 1129: loss: 0.25916963815689087\n",
      "iteration 1130: loss: 0.2591537535190582\n",
      "iteration 1131: loss: 0.2591378092765808\n",
      "iteration 1132: loss: 0.2591213881969452\n",
      "iteration 1133: loss: 0.25910553336143494\n",
      "iteration 1134: loss: 0.2590896487236023\n",
      "iteration 1135: loss: 0.25907379388809204\n",
      "iteration 1136: loss: 0.2590574324131012\n",
      "iteration 1137: loss: 0.25904157757759094\n",
      "iteration 1138: loss: 0.25902581214904785\n",
      "iteration 1139: loss: 0.259009450674057\n",
      "iteration 1140: loss: 0.2589936852455139\n",
      "iteration 1141: loss: 0.25897789001464844\n",
      "iteration 1142: loss: 0.25896158814430237\n",
      "iteration 1143: loss: 0.2589457631111145\n",
      "iteration 1144: loss: 0.2589300572872162\n",
      "iteration 1145: loss: 0.2589137554168701\n",
      "iteration 1146: loss: 0.2588980793952942\n",
      "iteration 1147: loss: 0.2588823437690735\n",
      "iteration 1148: loss: 0.2588661313056946\n",
      "iteration 1149: loss: 0.2588505148887634\n",
      "iteration 1150: loss: 0.25883427262306213\n",
      "iteration 1151: loss: 0.2588185966014862\n",
      "iteration 1152: loss: 0.25880295038223267\n",
      "iteration 1153: loss: 0.25878673791885376\n",
      "iteration 1154: loss: 0.258771151304245\n",
      "iteration 1155: loss: 0.25875502824783325\n",
      "iteration 1156: loss: 0.25873932242393494\n",
      "iteration 1157: loss: 0.2587231993675232\n",
      "iteration 1158: loss: 0.2587079405784607\n",
      "iteration 1159: loss: 0.2586924135684967\n",
      "iteration 1160: loss: 0.25867632031440735\n",
      "iteration 1161: loss: 0.25866076350212097\n",
      "iteration 1162: loss: 0.258644700050354\n",
      "iteration 1163: loss: 0.25862917304039\n",
      "iteration 1164: loss: 0.25861313939094543\n",
      "iteration 1165: loss: 0.2585976719856262\n",
      "iteration 1166: loss: 0.25858163833618164\n",
      "iteration 1167: loss: 0.2585662305355072\n",
      "iteration 1168: loss: 0.258550226688385\n",
      "iteration 1169: loss: 0.2585342526435852\n",
      "iteration 1170: loss: 0.25851887464523315\n",
      "iteration 1171: loss: 0.25850290060043335\n",
      "iteration 1172: loss: 0.2584874629974365\n",
      "iteration 1173: loss: 0.2584715485572815\n",
      "iteration 1174: loss: 0.2584560811519623\n",
      "iteration 1175: loss: 0.25844019651412964\n",
      "iteration 1176: loss: 0.2584248483181\n",
      "iteration 1177: loss: 0.25840896368026733\n",
      "iteration 1178: loss: 0.2583930790424347\n",
      "iteration 1179: loss: 0.2583778202533722\n",
      "iteration 1180: loss: 0.2583619952201843\n",
      "iteration 1181: loss: 0.2583461403846741\n",
      "iteration 1182: loss: 0.2583308219909668\n",
      "iteration 1183: loss: 0.2583150267601013\n",
      "iteration 1184: loss: 0.2582997977733612\n",
      "iteration 1185: loss: 0.2582840025424957\n",
      "iteration 1186: loss: 0.25826823711395264\n",
      "iteration 1187: loss: 0.25825294852256775\n",
      "iteration 1188: loss: 0.25823718309402466\n",
      "iteration 1189: loss: 0.25822144746780396\n",
      "iteration 1190: loss: 0.258206307888031\n",
      "iteration 1191: loss: 0.2581906318664551\n",
      "iteration 1192: loss: 0.2581748366355896\n",
      "iteration 1193: loss: 0.25815919041633606\n",
      "iteration 1194: loss: 0.2581440806388855\n",
      "iteration 1195: loss: 0.25812840461730957\n",
      "iteration 1196: loss: 0.25811275839805603\n",
      "iteration 1197: loss: 0.2580971121788025\n",
      "iteration 1198: loss: 0.25808197259902954\n",
      "iteration 1199: loss: 0.258066326379776\n",
      "iteration 1200: loss: 0.258050799369812\n",
      "iteration 1201: loss: 0.2580351233482361\n",
      "iteration 1202: loss: 0.2580201029777527\n",
      "iteration 1203: loss: 0.2580045461654663\n",
      "iteration 1204: loss: 0.25798898935317993\n",
      "iteration 1205: loss: 0.25797346234321594\n",
      "iteration 1206: loss: 0.25795793533325195\n",
      "iteration 1207: loss: 0.25794297456741333\n",
      "iteration 1208: loss: 0.25792741775512695\n",
      "iteration 1209: loss: 0.2579118609428406\n",
      "iteration 1210: loss: 0.25789642333984375\n",
      "iteration 1211: loss: 0.25788092613220215\n",
      "iteration 1212: loss: 0.2578654885292053\n",
      "iteration 1213: loss: 0.2578505277633667\n",
      "iteration 1214: loss: 0.25783511996269226\n",
      "iteration 1215: loss: 0.2578197121620178\n",
      "iteration 1216: loss: 0.25780433416366577\n",
      "iteration 1217: loss: 0.25778892636299133\n",
      "iteration 1218: loss: 0.2577734589576721\n",
      "iteration 1219: loss: 0.25775808095932007\n",
      "iteration 1220: loss: 0.2577427327632904\n",
      "iteration 1221: loss: 0.25772786140441895\n",
      "iteration 1222: loss: 0.25771254301071167\n",
      "iteration 1223: loss: 0.257697194814682\n",
      "iteration 1224: loss: 0.2576819062232971\n",
      "iteration 1225: loss: 0.2576666474342346\n",
      "iteration 1226: loss: 0.25765132904052734\n",
      "iteration 1227: loss: 0.25763604044914246\n",
      "iteration 1228: loss: 0.2576206922531128\n",
      "iteration 1229: loss: 0.2576054632663727\n",
      "iteration 1230: loss: 0.25759023427963257\n",
      "iteration 1231: loss: 0.2575749158859253\n",
      "iteration 1232: loss: 0.25755974650382996\n",
      "iteration 1233: loss: 0.2575445771217346\n",
      "iteration 1234: loss: 0.2575293183326721\n",
      "iteration 1235: loss: 0.25751417875289917\n",
      "iteration 1236: loss: 0.2574990391731262\n",
      "iteration 1237: loss: 0.2574838697910309\n",
      "iteration 1238: loss: 0.25746864080429077\n",
      "iteration 1239: loss: 0.2574535012245178\n",
      "iteration 1240: loss: 0.2574383616447449\n",
      "iteration 1241: loss: 0.2574232220649719\n",
      "iteration 1242: loss: 0.25740811228752136\n",
      "iteration 1243: loss: 0.2573925256729126\n",
      "iteration 1244: loss: 0.2573774456977844\n",
      "iteration 1245: loss: 0.25736236572265625\n",
      "iteration 1246: loss: 0.2573472857475281\n",
      "iteration 1247: loss: 0.25733229517936707\n",
      "iteration 1248: loss: 0.2573171555995941\n",
      "iteration 1249: loss: 0.2573021948337555\n",
      "iteration 1250: loss: 0.2572871446609497\n",
      "iteration 1251: loss: 0.2572721242904663\n",
      "iteration 1252: loss: 0.2572566270828247\n",
      "iteration 1253: loss: 0.2572418749332428\n",
      "iteration 1254: loss: 0.25722694396972656\n",
      "iteration 1255: loss: 0.25721198320388794\n",
      "iteration 1256: loss: 0.2571970522403717\n",
      "iteration 1257: loss: 0.25718212127685547\n",
      "iteration 1258: loss: 0.25716662406921387\n",
      "iteration 1259: loss: 0.25715169310569763\n",
      "iteration 1260: loss: 0.25713682174682617\n",
      "iteration 1261: loss: 0.25712189078330994\n",
      "iteration 1262: loss: 0.2571069896221161\n",
      "iteration 1263: loss: 0.2570917010307312\n",
      "iteration 1264: loss: 0.25707682967185974\n",
      "iteration 1265: loss: 0.25706201791763306\n",
      "iteration 1266: loss: 0.257047176361084\n",
      "iteration 1267: loss: 0.2570318579673767\n",
      "iteration 1268: loss: 0.2570170760154724\n",
      "iteration 1269: loss: 0.2570022940635681\n",
      "iteration 1270: loss: 0.25698739290237427\n",
      "iteration 1271: loss: 0.2569721043109894\n",
      "iteration 1272: loss: 0.25695735216140747\n",
      "iteration 1273: loss: 0.25694260001182556\n",
      "iteration 1274: loss: 0.25692784786224365\n",
      "iteration 1275: loss: 0.25691261887550354\n",
      "iteration 1276: loss: 0.2568979263305664\n",
      "iteration 1277: loss: 0.25688326358795166\n",
      "iteration 1278: loss: 0.25686806440353394\n",
      "iteration 1279: loss: 0.2568533718585968\n",
      "iteration 1280: loss: 0.25683873891830444\n",
      "iteration 1281: loss: 0.25682350993156433\n",
      "iteration 1282: loss: 0.25680890679359436\n",
      "iteration 1283: loss: 0.256794273853302\n",
      "iteration 1284: loss: 0.2567790448665619\n",
      "iteration 1285: loss: 0.25676441192626953\n",
      "iteration 1286: loss: 0.25674933195114136\n",
      "iteration 1287: loss: 0.2567347586154938\n",
      "iteration 1288: loss: 0.2567201554775238\n",
      "iteration 1289: loss: 0.2567051351070404\n",
      "iteration 1290: loss: 0.2566905617713928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1291: loss: 0.25667548179626465\n",
      "iteration 1292: loss: 0.25666096806526184\n",
      "iteration 1293: loss: 0.25664594769477844\n",
      "iteration 1294: loss: 0.256631463766098\n",
      "iteration 1295: loss: 0.2566169500350952\n",
      "iteration 1296: loss: 0.2566019594669342\n",
      "iteration 1297: loss: 0.2565874755382538\n",
      "iteration 1298: loss: 0.25657254457473755\n",
      "iteration 1299: loss: 0.25655800104141235\n",
      "iteration 1300: loss: 0.2565430700778961\n",
      "iteration 1301: loss: 0.2565285861492157\n",
      "iteration 1302: loss: 0.25651365518569946\n",
      "iteration 1303: loss: 0.2564992308616638\n",
      "iteration 1304: loss: 0.25648438930511475\n",
      "iteration 1305: loss: 0.2564700245857239\n",
      "iteration 1306: loss: 0.25645512342453003\n",
      "iteration 1307: loss: 0.25644081830978394\n",
      "iteration 1308: loss: 0.2564259469509125\n",
      "iteration 1309: loss: 0.2564111351966858\n",
      "iteration 1310: loss: 0.2563968002796173\n",
      "iteration 1311: loss: 0.25638192892074585\n",
      "iteration 1312: loss: 0.25636762380599976\n",
      "iteration 1313: loss: 0.25635284185409546\n",
      "iteration 1314: loss: 0.25633859634399414\n",
      "iteration 1315: loss: 0.25632381439208984\n",
      "iteration 1316: loss: 0.2563090920448303\n",
      "iteration 1317: loss: 0.2562948167324066\n",
      "iteration 1318: loss: 0.2562800943851471\n",
      "iteration 1319: loss: 0.25626587867736816\n",
      "iteration 1320: loss: 0.25625115633010864\n",
      "iteration 1321: loss: 0.25623640418052673\n",
      "iteration 1322: loss: 0.2562222480773926\n",
      "iteration 1323: loss: 0.2562074065208435\n",
      "iteration 1324: loss: 0.25619280338287354\n",
      "iteration 1325: loss: 0.2561786472797394\n",
      "iteration 1326: loss: 0.256164014339447\n",
      "iteration 1327: loss: 0.25614938139915466\n",
      "iteration 1328: loss: 0.2561352849006653\n",
      "iteration 1329: loss: 0.25612062215805054\n",
      "iteration 1330: loss: 0.25610631704330444\n",
      "iteration 1331: loss: 0.2560921907424927\n",
      "iteration 1332: loss: 0.2560775876045227\n",
      "iteration 1333: loss: 0.2560630440711975\n",
      "iteration 1334: loss: 0.2560485005378723\n",
      "iteration 1335: loss: 0.2560344338417053\n",
      "iteration 1336: loss: 0.2560199499130249\n",
      "iteration 1337: loss: 0.2560054063796997\n",
      "iteration 1338: loss: 0.2559909224510193\n",
      "iteration 1339: loss: 0.2559768557548523\n",
      "iteration 1340: loss: 0.25596243143081665\n",
      "iteration 1341: loss: 0.2559479773044586\n",
      "iteration 1342: loss: 0.2559334635734558\n",
      "iteration 1343: loss: 0.255919486284256\n",
      "iteration 1344: loss: 0.25590506196022034\n",
      "iteration 1345: loss: 0.2558906376361847\n",
      "iteration 1346: loss: 0.25587624311447144\n",
      "iteration 1347: loss: 0.2558618187904358\n",
      "iteration 1348: loss: 0.2558474540710449\n",
      "iteration 1349: loss: 0.25583356618881226\n",
      "iteration 1350: loss: 0.255819171667099\n",
      "iteration 1351: loss: 0.2558048367500305\n",
      "iteration 1352: loss: 0.2557904124259949\n",
      "iteration 1353: loss: 0.2557761073112488\n",
      "iteration 1354: loss: 0.2557617723941803\n",
      "iteration 1355: loss: 0.2557474970817566\n",
      "iteration 1356: loss: 0.2557336390018463\n",
      "iteration 1357: loss: 0.2557193636894226\n",
      "iteration 1358: loss: 0.2557050883769989\n",
      "iteration 1359: loss: 0.2556908428668976\n",
      "iteration 1360: loss: 0.25567659735679626\n",
      "iteration 1361: loss: 0.25566232204437256\n",
      "iteration 1362: loss: 0.25564807653427124\n",
      "iteration 1363: loss: 0.2556338608264923\n",
      "iteration 1364: loss: 0.2556196451187134\n",
      "iteration 1365: loss: 0.25560545921325684\n",
      "iteration 1366: loss: 0.2555912435054779\n",
      "iteration 1367: loss: 0.2555771768093109\n",
      "iteration 1368: loss: 0.2555629312992096\n",
      "iteration 1369: loss: 0.255548894405365\n",
      "iteration 1370: loss: 0.25553470849990845\n",
      "iteration 1371: loss: 0.2555205821990967\n",
      "iteration 1372: loss: 0.2555064558982849\n",
      "iteration 1373: loss: 0.25549229979515076\n",
      "iteration 1374: loss: 0.2554782032966614\n",
      "iteration 1375: loss: 0.2554641366004944\n",
      "iteration 1376: loss: 0.2554500699043274\n",
      "iteration 1377: loss: 0.2554360032081604\n",
      "iteration 1378: loss: 0.2554219365119934\n",
      "iteration 1379: loss: 0.2554078996181488\n",
      "iteration 1380: loss: 0.2553938329219818\n",
      "iteration 1381: loss: 0.255379855632782\n",
      "iteration 1382: loss: 0.2553658187389374\n",
      "iteration 1383: loss: 0.25535181164741516\n",
      "iteration 1384: loss: 0.25533780455589294\n",
      "iteration 1385: loss: 0.2553237974643707\n",
      "iteration 1386: loss: 0.2553098499774933\n",
      "iteration 1387: loss: 0.25529587268829346\n",
      "iteration 1388: loss: 0.2552819550037384\n",
      "iteration 1389: loss: 0.25526756048202515\n",
      "iteration 1390: loss: 0.25525376200675964\n",
      "iteration 1391: loss: 0.2552398443222046\n",
      "iteration 1392: loss: 0.25522592663764954\n",
      "iteration 1393: loss: 0.2552120089530945\n",
      "iteration 1394: loss: 0.25519809126853943\n",
      "iteration 1395: loss: 0.25518423318862915\n",
      "iteration 1396: loss: 0.25517016649246216\n",
      "iteration 1397: loss: 0.25515633821487427\n",
      "iteration 1398: loss: 0.255142480134964\n",
      "iteration 1399: loss: 0.2551286816596985\n",
      "iteration 1400: loss: 0.2551148533821106\n",
      "iteration 1401: loss: 0.2551006078720093\n",
      "iteration 1402: loss: 0.25508683919906616\n",
      "iteration 1403: loss: 0.25507301092147827\n",
      "iteration 1404: loss: 0.2550591826438904\n",
      "iteration 1405: loss: 0.2550455629825592\n",
      "iteration 1406: loss: 0.2550313472747803\n",
      "iteration 1407: loss: 0.25501763820648193\n",
      "iteration 1408: loss: 0.2550038695335388\n",
      "iteration 1409: loss: 0.2549901604652405\n",
      "iteration 1410: loss: 0.2549760341644287\n",
      "iteration 1411: loss: 0.25496232509613037\n",
      "iteration 1412: loss: 0.2549486458301544\n",
      "iteration 1413: loss: 0.25493496656417847\n",
      "iteration 1414: loss: 0.2549208104610443\n",
      "iteration 1415: loss: 0.25490713119506836\n",
      "iteration 1416: loss: 0.25489360094070435\n",
      "iteration 1417: loss: 0.25487953424453735\n",
      "iteration 1418: loss: 0.25486594438552856\n",
      "iteration 1419: loss: 0.254852294921875\n",
      "iteration 1420: loss: 0.2548382580280304\n",
      "iteration 1421: loss: 0.2548246681690216\n",
      "iteration 1422: loss: 0.2548111081123352\n",
      "iteration 1423: loss: 0.2547971308231354\n",
      "iteration 1424: loss: 0.254783570766449\n",
      "iteration 1425: loss: 0.2547699809074402\n",
      "iteration 1426: loss: 0.2547560930252075\n",
      "iteration 1427: loss: 0.2547425627708435\n",
      "iteration 1428: loss: 0.25472864508628845\n",
      "iteration 1429: loss: 0.25471511483192444\n",
      "iteration 1430: loss: 0.2547016739845276\n",
      "iteration 1431: loss: 0.25468772649765015\n",
      "iteration 1432: loss: 0.2546742558479309\n",
      "iteration 1433: loss: 0.25466036796569824\n",
      "iteration 1434: loss: 0.254646897315979\n",
      "iteration 1435: loss: 0.254633367061615\n",
      "iteration 1436: loss: 0.25461968779563904\n",
      "iteration 1437: loss: 0.2546062171459198\n",
      "iteration 1438: loss: 0.2545923888683319\n",
      "iteration 1439: loss: 0.25457900762557983\n",
      "iteration 1440: loss: 0.25456517934799194\n",
      "iteration 1441: loss: 0.25455182790756226\n",
      "iteration 1442: loss: 0.25453805923461914\n",
      "iteration 1443: loss: 0.2545246481895447\n",
      "iteration 1444: loss: 0.2545109987258911\n",
      "iteration 1445: loss: 0.25449761748313904\n",
      "iteration 1446: loss: 0.2544838786125183\n",
      "iteration 1447: loss: 0.2544705867767334\n",
      "iteration 1448: loss: 0.2544568181037903\n",
      "iteration 1449: loss: 0.25444355607032776\n",
      "iteration 1450: loss: 0.2544298470020294\n",
      "iteration 1451: loss: 0.2544165551662445\n",
      "iteration 1452: loss: 0.2544029951095581\n",
      "iteration 1453: loss: 0.2543899416923523\n",
      "iteration 1454: loss: 0.25437629222869873\n",
      "iteration 1455: loss: 0.25436267256736755\n",
      "iteration 1456: loss: 0.25434935092926025\n",
      "iteration 1457: loss: 0.2543357312679291\n",
      "iteration 1458: loss: 0.25432252883911133\n",
      "iteration 1459: loss: 0.2543090581893921\n",
      "iteration 1460: loss: 0.25429588556289673\n",
      "iteration 1461: loss: 0.25428229570388794\n",
      "iteration 1462: loss: 0.25426873564720154\n",
      "iteration 1463: loss: 0.2542555630207062\n",
      "iteration 1464: loss: 0.2542420029640198\n",
      "iteration 1465: loss: 0.25422847270965576\n",
      "iteration 1466: loss: 0.25421544909477234\n",
      "iteration 1467: loss: 0.2542019188404083\n",
      "iteration 1468: loss: 0.2541883885860443\n",
      "iteration 1469: loss: 0.2541753649711609\n",
      "iteration 1470: loss: 0.2541618347167969\n",
      "iteration 1471: loss: 0.25414836406707764\n",
      "iteration 1472: loss: 0.2541353106498718\n",
      "iteration 1473: loss: 0.25412195920944214\n",
      "iteration 1474: loss: 0.2541085183620453\n",
      "iteration 1475: loss: 0.25409549474716187\n",
      "iteration 1476: loss: 0.254082053899765\n",
      "iteration 1477: loss: 0.25406864285469055\n",
      "iteration 1478: loss: 0.2540552020072937\n",
      "iteration 1479: loss: 0.2540423274040222\n",
      "iteration 1480: loss: 0.25402897596359253\n",
      "iteration 1481: loss: 0.25401559472084045\n",
      "iteration 1482: loss: 0.25400227308273315\n",
      "iteration 1483: loss: 0.2539892792701721\n",
      "iteration 1484: loss: 0.2539759576320648\n",
      "iteration 1485: loss: 0.25396275520324707\n",
      "iteration 1486: loss: 0.25394946336746216\n",
      "iteration 1487: loss: 0.2539365291595459\n",
      "iteration 1488: loss: 0.2539231777191162\n",
      "iteration 1489: loss: 0.2539099156856537\n",
      "iteration 1490: loss: 0.25389665365219116\n",
      "iteration 1491: loss: 0.2538835108280182\n",
      "iteration 1492: loss: 0.2538706362247467\n",
      "iteration 1493: loss: 0.25385740399360657\n",
      "iteration 1494: loss: 0.25384417176246643\n",
      "iteration 1495: loss: 0.2538309693336487\n",
      "iteration 1496: loss: 0.25381773710250854\n",
      "iteration 1497: loss: 0.25380468368530273\n",
      "iteration 1498: loss: 0.25379183888435364\n",
      "iteration 1499: loss: 0.2537786662578583\n",
      "iteration 1500: loss: 0.2537654936313629\n",
      "iteration 1501: loss: 0.25375235080718994\n",
      "iteration 1502: loss: 0.25373920798301697\n",
      "iteration 1503: loss: 0.25372642278671265\n",
      "iteration 1504: loss: 0.2537132501602173\n",
      "iteration 1505: loss: 0.2537001669406891\n",
      "iteration 1506: loss: 0.2536870539188385\n",
      "iteration 1507: loss: 0.25367432832717896\n",
      "iteration 1508: loss: 0.25366124510765076\n",
      "iteration 1509: loss: 0.2536482512950897\n",
      "iteration 1510: loss: 0.2536352276802063\n",
      "iteration 1511: loss: 0.2536221742630005\n",
      "iteration 1512: loss: 0.2536091208457947\n",
      "iteration 1513: loss: 0.25359612703323364\n",
      "iteration 1514: loss: 0.25358322262763977\n",
      "iteration 1515: loss: 0.25357016921043396\n",
      "iteration 1516: loss: 0.2535572052001953\n",
      "iteration 1517: loss: 0.25354424118995667\n",
      "iteration 1518: loss: 0.25353124737739563\n",
      "iteration 1519: loss: 0.253518283367157\n",
      "iteration 1520: loss: 0.2535053789615631\n",
      "iteration 1521: loss: 0.25349244475364685\n",
      "iteration 1522: loss: 0.2534795105457306\n",
      "iteration 1523: loss: 0.2534666061401367\n",
      "iteration 1524: loss: 0.25345370173454285\n",
      "iteration 1525: loss: 0.25344088673591614\n",
      "iteration 1526: loss: 0.25342804193496704\n",
      "iteration 1527: loss: 0.25341516733169556\n",
      "iteration 1528: loss: 0.2534022629261017\n",
      "iteration 1529: loss: 0.2533894181251526\n",
      "iteration 1530: loss: 0.25337666273117065\n",
      "iteration 1531: loss: 0.25336381793022156\n",
      "iteration 1532: loss: 0.2533505856990814\n",
      "iteration 1533: loss: 0.2533377707004547\n",
      "iteration 1534: loss: 0.253324955701828\n",
      "iteration 1535: loss: 0.2533121705055237\n",
      "iteration 1536: loss: 0.2532995045185089\n",
      "iteration 1537: loss: 0.2532866895198822\n",
      "iteration 1538: loss: 0.25327396392822266\n",
      "iteration 1539: loss: 0.2532612383365631\n",
      "iteration 1540: loss: 0.25324851274490356\n",
      "iteration 1541: loss: 0.25323548913002014\n",
      "iteration 1542: loss: 0.2532227039337158\n",
      "iteration 1543: loss: 0.25321000814437866\n",
      "iteration 1544: loss: 0.2531972825527191\n",
      "iteration 1545: loss: 0.25318461656570435\n",
      "iteration 1546: loss: 0.25317203998565674\n",
      "iteration 1547: loss: 0.2531590163707733\n",
      "iteration 1548: loss: 0.25314635038375854\n",
      "iteration 1549: loss: 0.2531338632106781\n",
      "iteration 1550: loss: 0.2531212270259857\n",
      "iteration 1551: loss: 0.2531087398529053\n",
      "iteration 1552: loss: 0.25309574604034424\n",
      "iteration 1553: loss: 0.25308308005332947\n",
      "iteration 1554: loss: 0.2530704736709595\n",
      "iteration 1555: loss: 0.25305789709091187\n",
      "iteration 1556: loss: 0.2530454099178314\n",
      "iteration 1557: loss: 0.25303250551223755\n",
      "iteration 1558: loss: 0.25301989912986755\n",
      "iteration 1559: loss: 0.2530073821544647\n",
      "iteration 1560: loss: 0.2529948353767395\n",
      "iteration 1561: loss: 0.25298207998275757\n",
      "iteration 1562: loss: 0.25296956300735474\n",
      "iteration 1563: loss: 0.25295698642730713\n",
      "iteration 1564: loss: 0.25294414162635803\n",
      "iteration 1565: loss: 0.2529316544532776\n",
      "iteration 1566: loss: 0.2529192864894867\n",
      "iteration 1567: loss: 0.252906858921051\n",
      "iteration 1568: loss: 0.25289398431777954\n",
      "iteration 1569: loss: 0.25288158655166626\n",
      "iteration 1570: loss: 0.2528691291809082\n",
      "iteration 1571: loss: 0.25285640358924866\n",
      "iteration 1572: loss: 0.25284406542778015\n",
      "iteration 1573: loss: 0.2528316378593445\n",
      "iteration 1574: loss: 0.25281885266304016\n",
      "iteration 1575: loss: 0.25280648469924927\n",
      "iteration 1576: loss: 0.252794086933136\n",
      "iteration 1577: loss: 0.2527815103530884\n",
      "iteration 1578: loss: 0.2527691423892975\n",
      "iteration 1579: loss: 0.2527564465999603\n",
      "iteration 1580: loss: 0.2527441084384918\n",
      "iteration 1581: loss: 0.2527318000793457\n",
      "iteration 1582: loss: 0.2527192234992981\n",
      "iteration 1583: loss: 0.25270694494247437\n",
      "iteration 1584: loss: 0.252694308757782\n",
      "iteration 1585: loss: 0.25268203020095825\n",
      "iteration 1586: loss: 0.25266969203948975\n",
      "iteration 1587: loss: 0.25265708565711975\n",
      "iteration 1588: loss: 0.25264495611190796\n",
      "iteration 1589: loss: 0.25263234972953796\n",
      "iteration 1590: loss: 0.252620130777359\n",
      "iteration 1591: loss: 0.252607524394989\n",
      "iteration 1592: loss: 0.2525954842567444\n",
      "iteration 1593: loss: 0.2525830864906311\n",
      "iteration 1594: loss: 0.25257089734077454\n",
      "iteration 1595: loss: 0.25255870819091797\n",
      "iteration 1596: loss: 0.25254619121551514\n",
      "iteration 1597: loss: 0.2525339722633362\n",
      "iteration 1598: loss: 0.2525216042995453\n",
      "iteration 1599: loss: 0.2525094747543335\n",
      "iteration 1600: loss: 0.25249701738357544\n",
      "iteration 1601: loss: 0.25248485803604126\n",
      "iteration 1602: loss: 0.2524724304676056\n",
      "iteration 1603: loss: 0.25245997309684753\n",
      "iteration 1604: loss: 0.2524479925632477\n",
      "iteration 1605: loss: 0.2524355947971344\n",
      "iteration 1606: loss: 0.2524234652519226\n",
      "iteration 1607: loss: 0.2524111270904541\n",
      "iteration 1608: loss: 0.2523989975452423\n",
      "iteration 1609: loss: 0.2523867189884186\n",
      "iteration 1610: loss: 0.25237470865249634\n",
      "iteration 1611: loss: 0.25236234068870544\n",
      "iteration 1612: loss: 0.25234997272491455\n",
      "iteration 1613: loss: 0.2523380219936371\n",
      "iteration 1614: loss: 0.25232577323913574\n",
      "iteration 1615: loss: 0.2523137927055359\n",
      "iteration 1616: loss: 0.25230151414871216\n",
      "iteration 1617: loss: 0.25228920578956604\n",
      "iteration 1618: loss: 0.2522772550582886\n",
      "iteration 1619: loss: 0.2522648870944977\n",
      "iteration 1620: loss: 0.25225311517715454\n",
      "iteration 1621: loss: 0.2522408366203308\n",
      "iteration 1622: loss: 0.25222858786582947\n",
      "iteration 1623: loss: 0.2522166967391968\n",
      "iteration 1624: loss: 0.2522044777870178\n",
      "iteration 1625: loss: 0.2521922290325165\n",
      "iteration 1626: loss: 0.2521805167198181\n",
      "iteration 1627: loss: 0.25216832756996155\n",
      "iteration 1628: loss: 0.2521561086177826\n",
      "iteration 1629: loss: 0.2521442770957947\n",
      "iteration 1630: loss: 0.2521320581436157\n",
      "iteration 1631: loss: 0.2521200478076935\n",
      "iteration 1632: loss: 0.2521083950996399\n",
      "iteration 1633: loss: 0.2520962655544281\n",
      "iteration 1634: loss: 0.2520841062068939\n",
      "iteration 1635: loss: 0.2520720362663269\n",
      "iteration 1636: loss: 0.2520602345466614\n",
      "iteration 1637: loss: 0.252048134803772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1638: loss: 0.2520361840724945\n",
      "iteration 1639: loss: 0.2520241141319275\n",
      "iteration 1640: loss: 0.2520124018192291\n",
      "iteration 1641: loss: 0.2520003616809845\n",
      "iteration 1642: loss: 0.2519882321357727\n",
      "iteration 1643: loss: 0.25197622179985046\n",
      "iteration 1644: loss: 0.25196465849876404\n",
      "iteration 1645: loss: 0.2519526183605194\n",
      "iteration 1646: loss: 0.25194060802459717\n",
      "iteration 1647: loss: 0.2519286572933197\n",
      "iteration 1648: loss: 0.2519169747829437\n",
      "iteration 1649: loss: 0.25190502405166626\n",
      "iteration 1650: loss: 0.25189319252967834\n",
      "iteration 1651: loss: 0.2518812119960785\n",
      "iteration 1652: loss: 0.2518692910671234\n",
      "iteration 1653: loss: 0.25185731053352356\n",
      "iteration 1654: loss: 0.25184571743011475\n",
      "iteration 1655: loss: 0.25183379650115967\n",
      "iteration 1656: loss: 0.251821905374527\n",
      "iteration 1657: loss: 0.2518101632595062\n",
      "iteration 1658: loss: 0.25179827213287354\n",
      "iteration 1659: loss: 0.25178641080856323\n",
      "iteration 1660: loss: 0.2517748475074768\n",
      "iteration 1661: loss: 0.2517630457878113\n",
      "iteration 1662: loss: 0.25175121426582336\n",
      "iteration 1663: loss: 0.25173935294151306\n",
      "iteration 1664: loss: 0.25172752141952515\n",
      "iteration 1665: loss: 0.2517158091068268\n",
      "iteration 1666: loss: 0.25170400738716125\n",
      "iteration 1667: loss: 0.25169238448143005\n",
      "iteration 1668: loss: 0.2516806125640869\n",
      "iteration 1669: loss: 0.2516688108444214\n",
      "iteration 1670: loss: 0.2516574263572693\n",
      "iteration 1671: loss: 0.25164565443992615\n",
      "iteration 1672: loss: 0.25163406133651733\n",
      "iteration 1673: loss: 0.25162234902381897\n",
      "iteration 1674: loss: 0.25161057710647583\n",
      "iteration 1675: loss: 0.25159892439842224\n",
      "iteration 1676: loss: 0.2515871524810791\n",
      "iteration 1677: loss: 0.25157544016838074\n",
      "iteration 1678: loss: 0.25156378746032715\n",
      "iteration 1679: loss: 0.25155210494995117\n",
      "iteration 1680: loss: 0.25154048204421997\n",
      "iteration 1681: loss: 0.25152894854545593\n",
      "iteration 1682: loss: 0.25151729583740234\n",
      "iteration 1683: loss: 0.25150567293167114\n",
      "iteration 1684: loss: 0.25149407982826233\n",
      "iteration 1685: loss: 0.25148242712020874\n",
      "iteration 1686: loss: 0.2514708936214447\n",
      "iteration 1687: loss: 0.2514593005180359\n",
      "iteration 1688: loss: 0.2514476776123047\n",
      "iteration 1689: loss: 0.2514360845088959\n",
      "iteration 1690: loss: 0.2514247000217438\n",
      "iteration 1691: loss: 0.25141310691833496\n",
      "iteration 1692: loss: 0.2514016032218933\n",
      "iteration 1693: loss: 0.2513900399208069\n",
      "iteration 1694: loss: 0.2513785660266876\n",
      "iteration 1695: loss: 0.2513670325279236\n",
      "iteration 1696: loss: 0.2513555586338043\n",
      "iteration 1697: loss: 0.25134405493736267\n",
      "iteration 1698: loss: 0.2513325810432434\n",
      "iteration 1699: loss: 0.2513207793235779\n",
      "iteration 1700: loss: 0.25130945444107056\n",
      "iteration 1701: loss: 0.25129812955856323\n",
      "iteration 1702: loss: 0.25128668546676636\n",
      "iteration 1703: loss: 0.25127530097961426\n",
      "iteration 1704: loss: 0.25126388669013977\n",
      "iteration 1705: loss: 0.2512524724006653\n",
      "iteration 1706: loss: 0.2512410581111908\n",
      "iteration 1707: loss: 0.2512297034263611\n",
      "iteration 1708: loss: 0.2512180209159851\n",
      "iteration 1709: loss: 0.2512066662311554\n",
      "iteration 1710: loss: 0.25119534134864807\n",
      "iteration 1711: loss: 0.2511839270591736\n",
      "iteration 1712: loss: 0.2511725425720215\n",
      "iteration 1713: loss: 0.2511611878871918\n",
      "iteration 1714: loss: 0.2511500418186188\n",
      "iteration 1715: loss: 0.2511384189128876\n",
      "iteration 1716: loss: 0.25112709403038025\n",
      "iteration 1717: loss: 0.2511158287525177\n",
      "iteration 1718: loss: 0.25110459327697754\n",
      "iteration 1719: loss: 0.251093327999115\n",
      "iteration 1720: loss: 0.25108176469802856\n",
      "iteration 1721: loss: 0.25107046961784363\n",
      "iteration 1722: loss: 0.25105926394462585\n",
      "iteration 1723: loss: 0.2510480284690857\n",
      "iteration 1724: loss: 0.25103676319122314\n",
      "iteration 1725: loss: 0.2510252594947815\n",
      "iteration 1726: loss: 0.2510140538215637\n",
      "iteration 1727: loss: 0.25100287795066833\n",
      "iteration 1728: loss: 0.25099167227745056\n",
      "iteration 1729: loss: 0.25098052620887756\n",
      "iteration 1730: loss: 0.2509690523147583\n",
      "iteration 1731: loss: 0.2509579062461853\n",
      "iteration 1732: loss: 0.2509468197822571\n",
      "iteration 1733: loss: 0.2509356737136841\n",
      "iteration 1734: loss: 0.25092440843582153\n",
      "iteration 1735: loss: 0.2509132921695709\n",
      "iteration 1736: loss: 0.25090211629867554\n",
      "iteration 1737: loss: 0.25089073181152344\n",
      "iteration 1738: loss: 0.2508796751499176\n",
      "iteration 1739: loss: 0.25086861848831177\n",
      "iteration 1740: loss: 0.25085753202438354\n",
      "iteration 1741: loss: 0.25084632635116577\n",
      "iteration 1742: loss: 0.25083526968955994\n",
      "iteration 1743: loss: 0.2508242428302765\n",
      "iteration 1744: loss: 0.25081294775009155\n",
      "iteration 1745: loss: 0.2508019208908081\n",
      "iteration 1746: loss: 0.25079092383384705\n",
      "iteration 1747: loss: 0.2507796585559845\n",
      "iteration 1748: loss: 0.25076860189437866\n",
      "iteration 1749: loss: 0.25075763463974\n",
      "iteration 1750: loss: 0.25074639916419983\n",
      "iteration 1751: loss: 0.2507352828979492\n",
      "iteration 1752: loss: 0.25072434544563293\n",
      "iteration 1753: loss: 0.2507131099700928\n",
      "iteration 1754: loss: 0.2507022023200989\n",
      "iteration 1755: loss: 0.2506912648677826\n",
      "iteration 1756: loss: 0.2506800889968872\n",
      "iteration 1757: loss: 0.2506692111492157\n",
      "iteration 1758: loss: 0.2506580352783203\n",
      "iteration 1759: loss: 0.2506471574306488\n",
      "iteration 1760: loss: 0.2506362497806549\n",
      "iteration 1761: loss: 0.2506250739097595\n",
      "iteration 1762: loss: 0.2506142556667328\n",
      "iteration 1763: loss: 0.2506031394004822\n",
      "iteration 1764: loss: 0.25059229135513306\n",
      "iteration 1765: loss: 0.25058165192604065\n",
      "iteration 1766: loss: 0.25057050585746765\n",
      "iteration 1767: loss: 0.2505597472190857\n",
      "iteration 1768: loss: 0.25054866075515747\n",
      "iteration 1769: loss: 0.2505378723144531\n",
      "iteration 1770: loss: 0.2505268156528473\n",
      "iteration 1771: loss: 0.2505160868167877\n",
      "iteration 1772: loss: 0.2505049407482147\n",
      "iteration 1773: loss: 0.25049424171447754\n",
      "iteration 1774: loss: 0.25048351287841797\n",
      "iteration 1775: loss: 0.2504725158214569\n",
      "iteration 1776: loss: 0.2504616677761078\n",
      "iteration 1777: loss: 0.2504507005214691\n",
      "iteration 1778: loss: 0.25043997168540955\n",
      "iteration 1779: loss: 0.25042903423309326\n",
      "iteration 1780: loss: 0.25041836500167847\n",
      "iteration 1781: loss: 0.2504073977470398\n",
      "iteration 1782: loss: 0.2503967881202698\n",
      "iteration 1783: loss: 0.2503858506679535\n",
      "iteration 1784: loss: 0.2503751814365387\n",
      "iteration 1785: loss: 0.2503642439842224\n",
      "iteration 1786: loss: 0.25035360455513\n",
      "iteration 1787: loss: 0.2503427267074585\n",
      "iteration 1788: loss: 0.2503320276737213\n",
      "iteration 1789: loss: 0.2503211498260498\n",
      "iteration 1790: loss: 0.2503105700016022\n",
      "iteration 1791: loss: 0.25029975175857544\n",
      "iteration 1792: loss: 0.2502889037132263\n",
      "iteration 1793: loss: 0.2502783536911011\n",
      "iteration 1794: loss: 0.2502676844596863\n",
      "iteration 1795: loss: 0.25025713443756104\n",
      "iteration 1796: loss: 0.2502463459968567\n",
      "iteration 1797: loss: 0.2502356767654419\n",
      "iteration 1798: loss: 0.25022488832473755\n",
      "iteration 1799: loss: 0.2502143681049347\n",
      "iteration 1800: loss: 0.25020360946655273\n",
      "iteration 1801: loss: 0.25019288063049316\n",
      "iteration 1802: loss: 0.2501824200153351\n",
      "iteration 1803: loss: 0.2501716911792755\n",
      "iteration 1804: loss: 0.2501611113548279\n",
      "iteration 1805: loss: 0.2501503825187683\n",
      "iteration 1806: loss: 0.2501397430896759\n",
      "iteration 1807: loss: 0.2501293122768402\n",
      "iteration 1808: loss: 0.250118613243103\n",
      "iteration 1809: loss: 0.25010794401168823\n",
      "iteration 1810: loss: 0.25009748339653015\n",
      "iteration 1811: loss: 0.2500867247581482\n",
      "iteration 1812: loss: 0.2500763535499573\n",
      "iteration 1813: loss: 0.25006571412086487\n",
      "iteration 1814: loss: 0.25005510449409485\n",
      "iteration 1815: loss: 0.2500447630882263\n",
      "iteration 1816: loss: 0.2500341534614563\n",
      "iteration 1817: loss: 0.2500234544277191\n",
      "iteration 1818: loss: 0.25001317262649536\n",
      "iteration 1819: loss: 0.25000256299972534\n",
      "iteration 1820: loss: 0.2499919831752777\n",
      "iteration 1821: loss: 0.24998171627521515\n",
      "iteration 1822: loss: 0.2499711960554123\n",
      "iteration 1823: loss: 0.24996058642864227\n",
      "iteration 1824: loss: 0.2499503344297409\n",
      "iteration 1825: loss: 0.24993982911109924\n",
      "iteration 1826: loss: 0.24992933869361877\n",
      "iteration 1827: loss: 0.24991896748542786\n",
      "iteration 1828: loss: 0.24990849196910858\n",
      "iteration 1829: loss: 0.24989798665046692\n",
      "iteration 1830: loss: 0.24988755583763123\n",
      "iteration 1831: loss: 0.24987733364105225\n",
      "iteration 1832: loss: 0.2498667687177658\n",
      "iteration 1833: loss: 0.24985632300376892\n",
      "iteration 1834: loss: 0.2498461753129959\n",
      "iteration 1835: loss: 0.2498357743024826\n",
      "iteration 1836: loss: 0.2498251497745514\n",
      "iteration 1837: loss: 0.24981474876403809\n",
      "iteration 1838: loss: 0.24980464577674866\n",
      "iteration 1839: loss: 0.24979424476623535\n",
      "iteration 1840: loss: 0.24978375434875488\n",
      "iteration 1841: loss: 0.24977341294288635\n",
      "iteration 1842: loss: 0.24976329505443573\n",
      "iteration 1843: loss: 0.2497529685497284\n",
      "iteration 1844: loss: 0.2497425079345703\n",
      "iteration 1845: loss: 0.24973218142986298\n",
      "iteration 1846: loss: 0.24972185492515564\n",
      "iteration 1847: loss: 0.24971182644367218\n",
      "iteration 1848: loss: 0.2497013807296753\n",
      "iteration 1849: loss: 0.2496912032365799\n",
      "iteration 1850: loss: 0.24968092143535614\n",
      "iteration 1851: loss: 0.2496708184480667\n",
      "iteration 1852: loss: 0.24966053664684296\n",
      "iteration 1853: loss: 0.2496502846479416\n",
      "iteration 1854: loss: 0.2496400624513626\n",
      "iteration 1855: loss: 0.2496296614408493\n",
      "iteration 1856: loss: 0.2496197521686554\n",
      "iteration 1857: loss: 0.2496095448732376\n",
      "iteration 1858: loss: 0.24959918856620789\n",
      "iteration 1859: loss: 0.24958904087543488\n",
      "iteration 1860: loss: 0.24957886338233948\n",
      "iteration 1861: loss: 0.24956853687763214\n",
      "iteration 1862: loss: 0.24955856800079346\n",
      "iteration 1863: loss: 0.24954843521118164\n",
      "iteration 1864: loss: 0.2495381385087967\n",
      "iteration 1865: loss: 0.24952802062034607\n",
      "iteration 1866: loss: 0.24951788783073425\n",
      "iteration 1867: loss: 0.24950766563415527\n",
      "iteration 1868: loss: 0.24949756264686584\n",
      "iteration 1869: loss: 0.2494877129793167\n",
      "iteration 1870: loss: 0.2494775354862213\n",
      "iteration 1871: loss: 0.24946744740009308\n",
      "iteration 1872: loss: 0.24945738911628723\n",
      "iteration 1873: loss: 0.24944718182086945\n",
      "iteration 1874: loss: 0.2494371384382248\n",
      "iteration 1875: loss: 0.24942722916603088\n",
      "iteration 1876: loss: 0.2494170218706131\n",
      "iteration 1877: loss: 0.24940724670886993\n",
      "iteration 1878: loss: 0.24939708411693573\n",
      "iteration 1879: loss: 0.24938711524009705\n",
      "iteration 1880: loss: 0.24937710165977478\n",
      "iteration 1881: loss: 0.24936696887016296\n",
      "iteration 1882: loss: 0.24935702979564667\n",
      "iteration 1883: loss: 0.24934692680835724\n",
      "iteration 1884: loss: 0.24933698773384094\n",
      "iteration 1885: loss: 0.24932703375816345\n",
      "iteration 1886: loss: 0.2493169754743576\n",
      "iteration 1887: loss: 0.2493070363998413\n",
      "iteration 1888: loss: 0.24929697811603546\n",
      "iteration 1889: loss: 0.24928729236125946\n",
      "iteration 1890: loss: 0.24927739799022675\n",
      "iteration 1891: loss: 0.2492673695087433\n",
      "iteration 1892: loss: 0.24925751984119415\n",
      "iteration 1893: loss: 0.2492474764585495\n",
      "iteration 1894: loss: 0.24923762679100037\n",
      "iteration 1895: loss: 0.24922767281532288\n",
      "iteration 1896: loss: 0.24921779334545135\n",
      "iteration 1897: loss: 0.249208003282547\n",
      "iteration 1898: loss: 0.2491980344057083\n",
      "iteration 1899: loss: 0.24918821454048157\n",
      "iteration 1900: loss: 0.24917837977409363\n",
      "iteration 1901: loss: 0.24916860461235046\n",
      "iteration 1902: loss: 0.24915871024131775\n",
      "iteration 1903: loss: 0.2491488754749298\n",
      "iteration 1904: loss: 0.2491389513015747\n",
      "iteration 1905: loss: 0.24912917613983154\n",
      "iteration 1906: loss: 0.24911931157112122\n",
      "iteration 1907: loss: 0.24910958111286163\n",
      "iteration 1908: loss: 0.2490997016429901\n",
      "iteration 1909: loss: 0.24909000098705292\n",
      "iteration 1910: loss: 0.2490801066160202\n",
      "iteration 1911: loss: 0.2490704506635666\n",
      "iteration 1912: loss: 0.24906063079833984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1913: loss: 0.24905094504356384\n",
      "iteration 1914: loss: 0.24904115498065948\n",
      "iteration 1915: loss: 0.24903146922588348\n",
      "iteration 1916: loss: 0.24902167916297913\n",
      "iteration 1917: loss: 0.24901199340820312\n",
      "iteration 1918: loss: 0.24900218844413757\n",
      "iteration 1919: loss: 0.24899256229400635\n",
      "iteration 1920: loss: 0.24898286163806915\n",
      "iteration 1921: loss: 0.24897320568561554\n",
      "iteration 1922: loss: 0.24896351993083954\n",
      "iteration 1923: loss: 0.2489538937807083\n",
      "iteration 1924: loss: 0.24894431233406067\n",
      "iteration 1925: loss: 0.24893474578857422\n",
      "iteration 1926: loss: 0.24892501533031464\n",
      "iteration 1927: loss: 0.24891510605812073\n",
      "iteration 1928: loss: 0.24890558421611786\n",
      "iteration 1929: loss: 0.24889588356018066\n",
      "iteration 1930: loss: 0.2488863468170166\n",
      "iteration 1931: loss: 0.2488766461610794\n",
      "iteration 1932: loss: 0.24886712431907654\n",
      "iteration 1933: loss: 0.2488574981689453\n",
      "iteration 1934: loss: 0.2488478720188141\n",
      "iteration 1935: loss: 0.2488383948802948\n",
      "iteration 1936: loss: 0.24882879853248596\n",
      "iteration 1937: loss: 0.24881935119628906\n",
      "iteration 1938: loss: 0.24880973994731903\n",
      "iteration 1939: loss: 0.2488001585006714\n",
      "iteration 1940: loss: 0.24879047274589539\n",
      "iteration 1941: loss: 0.24878092110157013\n",
      "iteration 1942: loss: 0.24877135455608368\n",
      "iteration 1943: loss: 0.24876198172569275\n",
      "iteration 1944: loss: 0.2487524300813675\n",
      "iteration 1945: loss: 0.2487429678440094\n",
      "iteration 1946: loss: 0.2487334907054901\n",
      "iteration 1947: loss: 0.24872393906116486\n",
      "iteration 1948: loss: 0.24871468544006348\n",
      "iteration 1949: loss: 0.2487049549818039\n",
      "iteration 1950: loss: 0.2486955225467682\n",
      "iteration 1951: loss: 0.24868616461753845\n",
      "iteration 1952: loss: 0.24867670238018036\n",
      "iteration 1953: loss: 0.24866726994514465\n",
      "iteration 1954: loss: 0.2486579716205597\n",
      "iteration 1955: loss: 0.2486485242843628\n",
      "iteration 1956: loss: 0.2486390769481659\n",
      "iteration 1957: loss: 0.24862957000732422\n",
      "iteration 1958: loss: 0.2486201524734497\n",
      "iteration 1959: loss: 0.2486107349395752\n",
      "iteration 1960: loss: 0.24860143661499023\n",
      "iteration 1961: loss: 0.2485920488834381\n",
      "iteration 1962: loss: 0.24858269095420837\n",
      "iteration 1963: loss: 0.24857322871685028\n",
      "iteration 1964: loss: 0.24856385588645935\n",
      "iteration 1965: loss: 0.2485545128583908\n",
      "iteration 1966: loss: 0.24854516983032227\n",
      "iteration 1967: loss: 0.24853599071502686\n",
      "iteration 1968: loss: 0.2485266625881195\n",
      "iteration 1969: loss: 0.24851711094379425\n",
      "iteration 1970: loss: 0.24850793182849884\n",
      "iteration 1971: loss: 0.248498797416687\n",
      "iteration 1972: loss: 0.24848949909210205\n",
      "iteration 1973: loss: 0.24848023056983948\n",
      "iteration 1974: loss: 0.2484709769487381\n",
      "iteration 1975: loss: 0.2484615296125412\n",
      "iteration 1976: loss: 0.24845227599143982\n",
      "iteration 1977: loss: 0.24844305217266083\n",
      "iteration 1978: loss: 0.24843382835388184\n",
      "iteration 1979: loss: 0.2484247237443924\n",
      "iteration 1980: loss: 0.2484152764081955\n",
      "iteration 1981: loss: 0.2484060823917389\n",
      "iteration 1982: loss: 0.2483968734741211\n",
      "iteration 1983: loss: 0.24838769435882568\n",
      "iteration 1984: loss: 0.2483786642551422\n",
      "iteration 1985: loss: 0.2483692616224289\n",
      "iteration 1986: loss: 0.24836008250713348\n",
      "iteration 1987: loss: 0.24835094809532166\n",
      "iteration 1988: loss: 0.24834179878234863\n",
      "iteration 1989: loss: 0.24833258986473083\n",
      "iteration 1990: loss: 0.24832339584827423\n",
      "iteration 1991: loss: 0.2483142912387848\n",
      "iteration 1992: loss: 0.24830515682697296\n",
      "iteration 1993: loss: 0.24829618632793427\n",
      "iteration 1994: loss: 0.2482868731021881\n",
      "iteration 1995: loss: 0.248277947306633\n",
      "iteration 1996: loss: 0.24826888740062714\n",
      "iteration 1997: loss: 0.2482597827911377\n",
      "iteration 1998: loss: 0.24825051426887512\n",
      "iteration 1999: loss: 0.24824145436286926\n",
      "iteration 2000: loss: 0.2482324093580246\n",
      "iteration 2001: loss: 0.24822337925434113\n",
      "iteration 2002: loss: 0.24821431934833527\n",
      "iteration 2003: loss: 0.2482052743434906\n",
      "iteration 2004: loss: 0.24819627404212952\n",
      "iteration 2005: loss: 0.24818725883960724\n",
      "iteration 2006: loss: 0.24817803502082825\n",
      "iteration 2007: loss: 0.24816903471946716\n",
      "iteration 2008: loss: 0.24816004931926727\n",
      "iteration 2009: loss: 0.24815110862255096\n",
      "iteration 2010: loss: 0.24814191460609436\n",
      "iteration 2011: loss: 0.24813294410705566\n",
      "iteration 2012: loss: 0.24812407791614532\n",
      "iteration 2013: loss: 0.24811527132987976\n",
      "iteration 2014: loss: 0.2481062412261963\n",
      "iteration 2015: loss: 0.24809733033180237\n",
      "iteration 2016: loss: 0.24808844923973083\n",
      "iteration 2017: loss: 0.2480793297290802\n",
      "iteration 2018: loss: 0.24807043373584747\n",
      "iteration 2019: loss: 0.24806158244609833\n",
      "iteration 2020: loss: 0.24805264174938202\n",
      "iteration 2021: loss: 0.24804356694221497\n",
      "iteration 2022: loss: 0.24803471565246582\n",
      "iteration 2023: loss: 0.24802586436271667\n",
      "iteration 2024: loss: 0.2480168342590332\n",
      "iteration 2025: loss: 0.24800798296928406\n",
      "iteration 2026: loss: 0.2479991912841797\n",
      "iteration 2027: loss: 0.24799039959907532\n",
      "iteration 2028: loss: 0.24798135459423065\n",
      "iteration 2029: loss: 0.24797256290912628\n",
      "iteration 2030: loss: 0.2479638159275055\n",
      "iteration 2031: loss: 0.24795480072498322\n",
      "iteration 2032: loss: 0.24794605374336243\n",
      "iteration 2033: loss: 0.24793727695941925\n",
      "iteration 2034: loss: 0.24792857468128204\n",
      "iteration 2035: loss: 0.24791963398456573\n",
      "iteration 2036: loss: 0.24791090190410614\n",
      "iteration 2037: loss: 0.24790219962596893\n",
      "iteration 2038: loss: 0.247893288731575\n",
      "iteration 2039: loss: 0.24788454174995422\n",
      "iteration 2040: loss: 0.2478758841753006\n",
      "iteration 2041: loss: 0.24786698818206787\n",
      "iteration 2042: loss: 0.24785831570625305\n",
      "iteration 2043: loss: 0.24784965813159943\n",
      "iteration 2044: loss: 0.24784061312675476\n",
      "iteration 2045: loss: 0.24783198535442352\n",
      "iteration 2046: loss: 0.2478233277797699\n",
      "iteration 2047: loss: 0.24781446158885956\n",
      "iteration 2048: loss: 0.24780583381652832\n",
      "iteration 2049: loss: 0.24779722094535828\n",
      "iteration 2050: loss: 0.24778839945793152\n",
      "iteration 2051: loss: 0.24777980148792267\n",
      "iteration 2052: loss: 0.24777114391326904\n",
      "iteration 2053: loss: 0.24776235222816467\n",
      "iteration 2054: loss: 0.24775364995002747\n",
      "iteration 2055: loss: 0.2477450668811798\n",
      "iteration 2056: loss: 0.24773640930652618\n",
      "iteration 2057: loss: 0.2477278709411621\n",
      "iteration 2058: loss: 0.24771931767463684\n",
      "iteration 2059: loss: 0.24771055579185486\n",
      "iteration 2060: loss: 0.24770204722881317\n",
      "iteration 2061: loss: 0.24769341945648193\n",
      "iteration 2062: loss: 0.24768467247486115\n",
      "iteration 2063: loss: 0.24767616391181946\n",
      "iteration 2064: loss: 0.24766770005226135\n",
      "iteration 2065: loss: 0.24765899777412415\n",
      "iteration 2066: loss: 0.24765053391456604\n",
      "iteration 2067: loss: 0.247641921043396\n",
      "iteration 2068: loss: 0.2476331740617752\n",
      "iteration 2069: loss: 0.2476247102022171\n",
      "iteration 2070: loss: 0.24761632084846497\n",
      "iteration 2071: loss: 0.24760766327381134\n",
      "iteration 2072: loss: 0.24759908020496368\n",
      "iteration 2073: loss: 0.24759063124656677\n",
      "iteration 2074: loss: 0.24758204817771912\n",
      "iteration 2075: loss: 0.24757365882396698\n",
      "iteration 2076: loss: 0.2475651204586029\n",
      "iteration 2077: loss: 0.2475566416978836\n",
      "iteration 2078: loss: 0.24754826724529266\n",
      "iteration 2079: loss: 0.247539684176445\n",
      "iteration 2080: loss: 0.2475312054157257\n",
      "iteration 2081: loss: 0.24752287566661835\n",
      "iteration 2082: loss: 0.2475142925977707\n",
      "iteration 2083: loss: 0.24750597774982452\n",
      "iteration 2084: loss: 0.24749748408794403\n",
      "iteration 2085: loss: 0.24748890101909637\n",
      "iteration 2086: loss: 0.2474806010723114\n",
      "iteration 2087: loss: 0.24747228622436523\n",
      "iteration 2088: loss: 0.247463658452034\n",
      "iteration 2089: loss: 0.2474554032087326\n",
      "iteration 2090: loss: 0.24744689464569092\n",
      "iteration 2091: loss: 0.2474384754896164\n",
      "iteration 2092: loss: 0.2474302053451538\n",
      "iteration 2093: loss: 0.2474217712879181\n",
      "iteration 2094: loss: 0.24741356074810028\n",
      "iteration 2095: loss: 0.24740517139434814\n",
      "iteration 2096: loss: 0.24739673733711243\n",
      "iteration 2097: loss: 0.24738860130310059\n",
      "iteration 2098: loss: 0.24738004803657532\n",
      "iteration 2099: loss: 0.2473718672990799\n",
      "iteration 2100: loss: 0.24736352264881134\n",
      "iteration 2101: loss: 0.24735502898693085\n",
      "iteration 2102: loss: 0.2473468780517578\n",
      "iteration 2103: loss: 0.24733857810497284\n",
      "iteration 2104: loss: 0.2473302185535431\n",
      "iteration 2105: loss: 0.24732205271720886\n",
      "iteration 2106: loss: 0.24731352925300598\n",
      "iteration 2107: loss: 0.24730543792247772\n",
      "iteration 2108: loss: 0.24729713797569275\n",
      "iteration 2109: loss: 0.24728885293006897\n",
      "iteration 2110: loss: 0.24728071689605713\n",
      "iteration 2111: loss: 0.24727246165275574\n",
      "iteration 2112: loss: 0.24726422131061554\n",
      "iteration 2113: loss: 0.24725596606731415\n",
      "iteration 2114: loss: 0.24724769592285156\n",
      "iteration 2115: loss: 0.2472396194934845\n",
      "iteration 2116: loss: 0.2472314089536667\n",
      "iteration 2117: loss: 0.24722325801849365\n",
      "iteration 2118: loss: 0.24721506237983704\n",
      "iteration 2119: loss: 0.24720697104930878\n",
      "iteration 2120: loss: 0.24719858169555664\n",
      "iteration 2121: loss: 0.24719059467315674\n",
      "iteration 2122: loss: 0.24718233942985535\n",
      "iteration 2123: loss: 0.24717417359352112\n",
      "iteration 2124: loss: 0.24716618657112122\n",
      "iteration 2125: loss: 0.24715784192085266\n",
      "iteration 2126: loss: 0.24714989960193634\n",
      "iteration 2127: loss: 0.24714155495166779\n",
      "iteration 2128: loss: 0.24713361263275146\n",
      "iteration 2129: loss: 0.24712547659873962\n",
      "iteration 2130: loss: 0.24711735546588898\n",
      "iteration 2131: loss: 0.24710926413536072\n",
      "iteration 2132: loss: 0.24710135161876678\n",
      "iteration 2133: loss: 0.2470930516719818\n",
      "iteration 2134: loss: 0.24708513915538788\n",
      "iteration 2135: loss: 0.2470768690109253\n",
      "iteration 2136: loss: 0.2470688819885254\n",
      "iteration 2137: loss: 0.24706098437309265\n",
      "iteration 2138: loss: 0.24705275893211365\n",
      "iteration 2139: loss: 0.24704492092132568\n",
      "iteration 2140: loss: 0.2470366656780243\n",
      "iteration 2141: loss: 0.24702878296375275\n",
      "iteration 2142: loss: 0.24702081084251404\n",
      "iteration 2143: loss: 0.24701276421546936\n",
      "iteration 2144: loss: 0.24700479209423065\n",
      "iteration 2145: loss: 0.24699683487415314\n",
      "iteration 2146: loss: 0.24698877334594727\n",
      "iteration 2147: loss: 0.24698083102703094\n",
      "iteration 2148: loss: 0.24697284400463104\n",
      "iteration 2149: loss: 0.24696488678455353\n",
      "iteration 2150: loss: 0.2469569444656372\n",
      "iteration 2151: loss: 0.2469489872455597\n",
      "iteration 2152: loss: 0.24694108963012695\n",
      "iteration 2153: loss: 0.24693307280540466\n",
      "iteration 2154: loss: 0.24692514538764954\n",
      "iteration 2155: loss: 0.24691733717918396\n",
      "iteration 2156: loss: 0.24690940976142883\n",
      "iteration 2157: loss: 0.2469015121459961\n",
      "iteration 2158: loss: 0.24689361453056335\n",
      "iteration 2159: loss: 0.246885746717453\n",
      "iteration 2160: loss: 0.24687786400318146\n",
      "iteration 2161: loss: 0.24686980247497559\n",
      "iteration 2162: loss: 0.24686214327812195\n",
      "iteration 2163: loss: 0.24685434997081757\n",
      "iteration 2164: loss: 0.2468462884426117\n",
      "iteration 2165: loss: 0.24683859944343567\n",
      "iteration 2166: loss: 0.24683058261871338\n",
      "iteration 2167: loss: 0.24682283401489258\n",
      "iteration 2168: loss: 0.24681517481803894\n",
      "iteration 2169: loss: 0.24680717289447784\n",
      "iteration 2170: loss: 0.24679942429065704\n",
      "iteration 2171: loss: 0.24679164588451385\n",
      "iteration 2172: loss: 0.24678373336791992\n",
      "iteration 2173: loss: 0.24677598476409912\n",
      "iteration 2174: loss: 0.24676814675331116\n",
      "iteration 2175: loss: 0.24676041305065155\n",
      "iteration 2176: loss: 0.24675282835960388\n",
      "iteration 2177: loss: 0.24674491584300995\n",
      "iteration 2178: loss: 0.24673719704151154\n",
      "iteration 2179: loss: 0.24672949314117432\n",
      "iteration 2180: loss: 0.24672174453735352\n",
      "iteration 2181: loss: 0.24671407043933868\n",
      "iteration 2182: loss: 0.24670617282390594\n",
      "iteration 2183: loss: 0.2466984987258911\n",
      "iteration 2184: loss: 0.24669098854064941\n",
      "iteration 2185: loss: 0.24668315052986145\n",
      "iteration 2186: loss: 0.24667546153068542\n",
      "iteration 2187: loss: 0.24666781723499298\n",
      "iteration 2188: loss: 0.2466599941253662\n",
      "iteration 2189: loss: 0.2466525137424469\n",
      "iteration 2190: loss: 0.24664489924907684\n",
      "iteration 2191: loss: 0.2466370165348053\n",
      "iteration 2192: loss: 0.2466295063495636\n",
      "iteration 2193: loss: 0.24662168323993683\n",
      "iteration 2194: loss: 0.24661409854888916\n",
      "iteration 2195: loss: 0.246606707572937\n",
      "iteration 2196: loss: 0.24659891426563263\n",
      "iteration 2197: loss: 0.24659132957458496\n",
      "iteration 2198: loss: 0.2465837448835373\n",
      "iteration 2199: loss: 0.24657604098320007\n",
      "iteration 2200: loss: 0.2465684860944748\n",
      "iteration 2201: loss: 0.2465609312057495\n",
      "iteration 2202: loss: 0.2465532273054123\n",
      "iteration 2203: loss: 0.24654583632946014\n",
      "iteration 2204: loss: 0.24653835594654083\n",
      "iteration 2205: loss: 0.24653062224388123\n",
      "iteration 2206: loss: 0.24652314186096191\n",
      "iteration 2207: loss: 0.2465154230594635\n",
      "iteration 2208: loss: 0.24650795757770538\n",
      "iteration 2209: loss: 0.24650049209594727\n",
      "iteration 2210: loss: 0.24649281799793243\n",
      "iteration 2211: loss: 0.2464853823184967\n",
      "iteration 2212: loss: 0.24647793173789978\n",
      "iteration 2213: loss: 0.24647028744220734\n",
      "iteration 2214: loss: 0.24646282196044922\n",
      "iteration 2215: loss: 0.2464553862810135\n",
      "iteration 2216: loss: 0.24644777178764343\n",
      "iteration 2217: loss: 0.2464403659105301\n",
      "iteration 2218: loss: 0.24643294513225555\n",
      "iteration 2219: loss: 0.2464255392551422\n",
      "iteration 2220: loss: 0.24641814827919006\n",
      "iteration 2221: loss: 0.24641072750091553\n",
      "iteration 2222: loss: 0.24640317261219025\n",
      "iteration 2223: loss: 0.2463957816362381\n",
      "iteration 2224: loss: 0.24638843536376953\n",
      "iteration 2225: loss: 0.24638088047504425\n",
      "iteration 2226: loss: 0.24637353420257568\n",
      "iteration 2227: loss: 0.24636605381965637\n",
      "iteration 2228: loss: 0.24635858833789825\n",
      "iteration 2229: loss: 0.2463511973619461\n",
      "iteration 2230: loss: 0.24634388089179993\n",
      "iteration 2231: loss: 0.24633637070655823\n",
      "iteration 2232: loss: 0.24632909893989563\n",
      "iteration 2233: loss: 0.24632176756858826\n",
      "iteration 2234: loss: 0.24631428718566895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2235: loss: 0.24630701541900635\n",
      "iteration 2236: loss: 0.24629977345466614\n",
      "iteration 2237: loss: 0.24629227817058563\n",
      "iteration 2238: loss: 0.24628500640392303\n",
      "iteration 2239: loss: 0.24627776443958282\n",
      "iteration 2240: loss: 0.2462705373764038\n",
      "iteration 2241: loss: 0.2462630718946457\n",
      "iteration 2242: loss: 0.24625568091869354\n",
      "iteration 2243: loss: 0.24624848365783691\n",
      "iteration 2244: loss: 0.24624106287956238\n",
      "iteration 2245: loss: 0.24623386561870575\n",
      "iteration 2246: loss: 0.24622678756713867\n",
      "iteration 2247: loss: 0.24621936678886414\n",
      "iteration 2248: loss: 0.2462121993303299\n",
      "iteration 2249: loss: 0.2462049424648285\n",
      "iteration 2250: loss: 0.2461976259946823\n",
      "iteration 2251: loss: 0.24619026482105255\n",
      "iteration 2252: loss: 0.24618308246135712\n",
      "iteration 2253: loss: 0.24617592990398407\n",
      "iteration 2254: loss: 0.2461686134338379\n",
      "iteration 2255: loss: 0.24616146087646484\n",
      "iteration 2256: loss: 0.24615418910980225\n",
      "iteration 2257: loss: 0.2461470663547516\n",
      "iteration 2258: loss: 0.2461397647857666\n",
      "iteration 2259: loss: 0.24613268673419952\n",
      "iteration 2260: loss: 0.24612560868263245\n",
      "iteration 2261: loss: 0.24611833691596985\n",
      "iteration 2262: loss: 0.24611103534698486\n",
      "iteration 2263: loss: 0.24610397219657898\n",
      "iteration 2264: loss: 0.24609701335430145\n",
      "iteration 2265: loss: 0.2460896074771881\n",
      "iteration 2266: loss: 0.24608254432678223\n",
      "iteration 2267: loss: 0.24607551097869873\n",
      "iteration 2268: loss: 0.24606847763061523\n",
      "iteration 2269: loss: 0.24606108665466309\n",
      "iteration 2270: loss: 0.24605396389961243\n",
      "iteration 2271: loss: 0.24604694545269012\n",
      "iteration 2272: loss: 0.24603994190692902\n",
      "iteration 2273: loss: 0.24603256583213806\n",
      "iteration 2274: loss: 0.24602560698986053\n",
      "iteration 2275: loss: 0.24601861834526062\n",
      "iteration 2276: loss: 0.2460116147994995\n",
      "iteration 2277: loss: 0.24600431323051453\n",
      "iteration 2278: loss: 0.2459973394870758\n",
      "iteration 2279: loss: 0.24599036574363708\n",
      "iteration 2280: loss: 0.2459832727909088\n",
      "iteration 2281: loss: 0.2459762990474701\n",
      "iteration 2282: loss: 0.2459692656993866\n",
      "iteration 2283: loss: 0.24596218764781952\n",
      "iteration 2284: loss: 0.24595527350902557\n",
      "iteration 2285: loss: 0.24594834446907043\n",
      "iteration 2286: loss: 0.24594111740589142\n",
      "iteration 2287: loss: 0.24593420326709747\n",
      "iteration 2288: loss: 0.24592728912830353\n",
      "iteration 2289: loss: 0.24592025578022003\n",
      "iteration 2290: loss: 0.24591335654258728\n",
      "iteration 2291: loss: 0.2459060698747635\n",
      "iteration 2292: loss: 0.24589920043945312\n",
      "iteration 2293: loss: 0.24589233100414276\n",
      "iteration 2294: loss: 0.24588532745838165\n",
      "iteration 2295: loss: 0.24587848782539368\n",
      "iteration 2296: loss: 0.24587127566337585\n",
      "iteration 2297: loss: 0.24586448073387146\n",
      "iteration 2298: loss: 0.24585767090320587\n",
      "iteration 2299: loss: 0.24585075676441193\n",
      "iteration 2300: loss: 0.24584393203258514\n",
      "iteration 2301: loss: 0.2458367794752121\n",
      "iteration 2302: loss: 0.24583002924919128\n",
      "iteration 2303: loss: 0.24582307040691376\n",
      "iteration 2304: loss: 0.24581627547740936\n",
      "iteration 2305: loss: 0.24580934643745422\n",
      "iteration 2306: loss: 0.24580255150794983\n",
      "iteration 2307: loss: 0.24579563736915588\n",
      "iteration 2308: loss: 0.24578869342803955\n",
      "iteration 2309: loss: 0.24578197300434113\n",
      "iteration 2310: loss: 0.24577505886554718\n",
      "iteration 2311: loss: 0.24576830863952637\n",
      "iteration 2312: loss: 0.2457614243030548\n",
      "iteration 2313: loss: 0.2457544505596161\n",
      "iteration 2314: loss: 0.24574756622314453\n",
      "iteration 2315: loss: 0.2457408607006073\n",
      "iteration 2316: loss: 0.24573412537574768\n",
      "iteration 2317: loss: 0.24572741985321045\n",
      "iteration 2318: loss: 0.24572055041790009\n",
      "iteration 2319: loss: 0.2457137107849121\n",
      "iteration 2320: loss: 0.24570688605308533\n",
      "iteration 2321: loss: 0.24570004642009735\n",
      "iteration 2322: loss: 0.2456934005022049\n",
      "iteration 2323: loss: 0.2456866055727005\n",
      "iteration 2324: loss: 0.24567992985248566\n",
      "iteration 2325: loss: 0.24567313492298126\n",
      "iteration 2326: loss: 0.2456665337085724\n",
      "iteration 2327: loss: 0.24565954506397247\n",
      "iteration 2328: loss: 0.24565276503562927\n",
      "iteration 2329: loss: 0.2456461638212204\n",
      "iteration 2330: loss: 0.24563941359519958\n",
      "iteration 2331: loss: 0.24563279747962952\n",
      "iteration 2332: loss: 0.24562609195709229\n",
      "iteration 2333: loss: 0.24561938643455505\n",
      "iteration 2334: loss: 0.24561282992362976\n",
      "iteration 2335: loss: 0.24560599029064178\n",
      "iteration 2336: loss: 0.24559946358203888\n",
      "iteration 2337: loss: 0.24559256434440613\n",
      "iteration 2338: loss: 0.2455858439207077\n",
      "iteration 2339: loss: 0.2455793172121048\n",
      "iteration 2340: loss: 0.24557261168956757\n",
      "iteration 2341: loss: 0.24556592106819153\n",
      "iteration 2342: loss: 0.245559424161911\n",
      "iteration 2343: loss: 0.24555273354053497\n",
      "iteration 2344: loss: 0.24554605782032013\n",
      "iteration 2345: loss: 0.2455395758152008\n",
      "iteration 2346: loss: 0.24553294479846954\n",
      "iteration 2347: loss: 0.2455262690782547\n",
      "iteration 2348: loss: 0.24551963806152344\n",
      "iteration 2349: loss: 0.2455131560564041\n",
      "iteration 2350: loss: 0.24550659954547882\n",
      "iteration 2351: loss: 0.24550001323223114\n",
      "iteration 2352: loss: 0.24549353122711182\n",
      "iteration 2353: loss: 0.24548694491386414\n",
      "iteration 2354: loss: 0.24548014998435974\n",
      "iteration 2355: loss: 0.24547354876995087\n",
      "iteration 2356: loss: 0.24546711146831512\n",
      "iteration 2357: loss: 0.24546051025390625\n",
      "iteration 2358: loss: 0.245453879237175\n",
      "iteration 2359: loss: 0.2454473227262497\n",
      "iteration 2360: loss: 0.24544093012809753\n",
      "iteration 2361: loss: 0.24543437361717224\n",
      "iteration 2362: loss: 0.24542780220508575\n",
      "iteration 2363: loss: 0.24542126059532166\n",
      "iteration 2364: loss: 0.24541492760181427\n",
      "iteration 2365: loss: 0.24540838599205017\n",
      "iteration 2366: loss: 0.24540185928344727\n",
      "iteration 2367: loss: 0.2453954517841339\n",
      "iteration 2368: loss: 0.2453889101743698\n",
      "iteration 2369: loss: 0.2453823983669281\n",
      "iteration 2370: loss: 0.2453760802745819\n",
      "iteration 2371: loss: 0.24536962807178497\n",
      "iteration 2372: loss: 0.24536311626434326\n",
      "iteration 2373: loss: 0.24535682797431946\n",
      "iteration 2374: loss: 0.24535039067268372\n",
      "iteration 2375: loss: 0.2453438937664032\n",
      "iteration 2376: loss: 0.2453376054763794\n",
      "iteration 2377: loss: 0.24533119797706604\n",
      "iteration 2378: loss: 0.2453247308731079\n",
      "iteration 2379: loss: 0.24531829357147217\n",
      "iteration 2380: loss: 0.2453118860721588\n",
      "iteration 2381: loss: 0.24530544877052307\n",
      "iteration 2382: loss: 0.24529893696308136\n",
      "iteration 2383: loss: 0.24529249966144562\n",
      "iteration 2384: loss: 0.24528619647026062\n",
      "iteration 2385: loss: 0.24527998268604279\n",
      "iteration 2386: loss: 0.245273619890213\n",
      "iteration 2387: loss: 0.24526719748973846\n",
      "iteration 2388: loss: 0.2452608346939087\n",
      "iteration 2389: loss: 0.24525447189807892\n",
      "iteration 2390: loss: 0.24524827301502228\n",
      "iteration 2391: loss: 0.2452419251203537\n",
      "iteration 2392: loss: 0.24523556232452393\n",
      "iteration 2393: loss: 0.24522921442985535\n",
      "iteration 2394: loss: 0.24522288143634796\n",
      "iteration 2395: loss: 0.24521656334400177\n",
      "iteration 2396: loss: 0.2452102154493332\n",
      "iteration 2397: loss: 0.245203897356987\n",
      "iteration 2398: loss: 0.2451976090669632\n",
      "iteration 2399: loss: 0.24519145488739014\n",
      "iteration 2400: loss: 0.24518516659736633\n",
      "iteration 2401: loss: 0.2451789677143097\n",
      "iteration 2402: loss: 0.2451726645231247\n",
      "iteration 2403: loss: 0.2451663762331009\n",
      "iteration 2404: loss: 0.24516010284423828\n",
      "iteration 2405: loss: 0.24515385925769806\n",
      "iteration 2406: loss: 0.24514766037464142\n",
      "iteration 2407: loss: 0.2451414167881012\n",
      "iteration 2408: loss: 0.24513515830039978\n",
      "iteration 2409: loss: 0.24512894451618195\n",
      "iteration 2410: loss: 0.24512270092964172\n",
      "iteration 2411: loss: 0.2451164424419403\n",
      "iteration 2412: loss: 0.245110422372818\n",
      "iteration 2413: loss: 0.24510419368743896\n",
      "iteration 2414: loss: 0.24509799480438232\n",
      "iteration 2415: loss: 0.24509179592132568\n",
      "iteration 2416: loss: 0.24508559703826904\n",
      "iteration 2417: loss: 0.24507936835289001\n",
      "iteration 2418: loss: 0.24507328867912292\n",
      "iteration 2419: loss: 0.24506714940071106\n",
      "iteration 2420: loss: 0.2450609654188156\n",
      "iteration 2421: loss: 0.24505476653575897\n",
      "iteration 2422: loss: 0.24504879117012024\n",
      "iteration 2423: loss: 0.24504263699054718\n",
      "iteration 2424: loss: 0.2450365126132965\n",
      "iteration 2425: loss: 0.24503035843372345\n",
      "iteration 2426: loss: 0.24502423405647278\n",
      "iteration 2427: loss: 0.2450180947780609\n",
      "iteration 2428: loss: 0.24501200020313263\n",
      "iteration 2429: loss: 0.24500584602355957\n",
      "iteration 2430: loss: 0.24499991536140442\n",
      "iteration 2431: loss: 0.24499383568763733\n",
      "iteration 2432: loss: 0.24498748779296875\n",
      "iteration 2433: loss: 0.24498136341571808\n",
      "iteration 2434: loss: 0.2449754774570465\n",
      "iteration 2435: loss: 0.24496948719024658\n",
      "iteration 2436: loss: 0.2449633777141571\n",
      "iteration 2437: loss: 0.2449573576450348\n",
      "iteration 2438: loss: 0.2449512779712677\n",
      "iteration 2439: loss: 0.2449452430009842\n",
      "iteration 2440: loss: 0.2449391782283783\n",
      "iteration 2441: loss: 0.2449333220720291\n",
      "iteration 2442: loss: 0.24492709338665009\n",
      "iteration 2443: loss: 0.24492108821868896\n",
      "iteration 2444: loss: 0.24491524696350098\n",
      "iteration 2445: loss: 0.24490924179553986\n",
      "iteration 2446: loss: 0.24490304291248322\n",
      "iteration 2447: loss: 0.24489720165729523\n",
      "iteration 2448: loss: 0.2448912113904953\n",
      "iteration 2449: loss: 0.24488525092601776\n",
      "iteration 2450: loss: 0.24487924575805664\n",
      "iteration 2451: loss: 0.24487335979938507\n",
      "iteration 2452: loss: 0.24486739933490753\n",
      "iteration 2453: loss: 0.2448614090681076\n",
      "iteration 2454: loss: 0.24485544860363007\n",
      "iteration 2455: loss: 0.24484951794147491\n",
      "iteration 2456: loss: 0.24484355747699738\n",
      "iteration 2457: loss: 0.24483759701251984\n",
      "iteration 2458: loss: 0.24483156204223633\n",
      "iteration 2459: loss: 0.2448258101940155\n",
      "iteration 2460: loss: 0.24481971561908722\n",
      "iteration 2461: loss: 0.2448139637708664\n",
      "iteration 2462: loss: 0.24480807781219482\n",
      "iteration 2463: loss: 0.24480195343494415\n",
      "iteration 2464: loss: 0.24479623138904572\n",
      "iteration 2465: loss: 0.24479016661643982\n",
      "iteration 2466: loss: 0.24478447437286377\n",
      "iteration 2467: loss: 0.2447785884141922\n",
      "iteration 2468: loss: 0.2447727918624878\n",
      "iteration 2469: loss: 0.24476692080497742\n",
      "iteration 2470: loss: 0.24476107954978943\n",
      "iteration 2471: loss: 0.24475522339344025\n",
      "iteration 2472: loss: 0.24474938213825226\n",
      "iteration 2473: loss: 0.24474354088306427\n",
      "iteration 2474: loss: 0.24473769962787628\n",
      "iteration 2475: loss: 0.2447318285703659\n",
      "iteration 2476: loss: 0.2447260320186615\n",
      "iteration 2477: loss: 0.24472036957740784\n",
      "iteration 2478: loss: 0.2447144091129303\n",
      "iteration 2479: loss: 0.2447085827589035\n",
      "iteration 2480: loss: 0.2447027713060379\n",
      "iteration 2481: loss: 0.2446969449520111\n",
      "iteration 2482: loss: 0.2446911782026291\n",
      "iteration 2483: loss: 0.244685560464859\n",
      "iteration 2484: loss: 0.24467960000038147\n",
      "iteration 2485: loss: 0.24467399716377258\n",
      "iteration 2486: loss: 0.24466800689697266\n",
      "iteration 2487: loss: 0.24466244876384735\n",
      "iteration 2488: loss: 0.24465668201446533\n",
      "iteration 2489: loss: 0.2446509301662445\n",
      "iteration 2490: loss: 0.2446451634168625\n",
      "iteration 2491: loss: 0.24463944137096405\n",
      "iteration 2492: loss: 0.2446337193250656\n",
      "iteration 2493: loss: 0.24462798237800598\n",
      "iteration 2494: loss: 0.24462208151817322\n",
      "iteration 2495: loss: 0.2446165531873703\n",
      "iteration 2496: loss: 0.24461066722869873\n",
      "iteration 2497: loss: 0.2446051388978958\n",
      "iteration 2498: loss: 0.24459943175315857\n",
      "iteration 2499: loss: 0.2445937693119049\n",
      "iteration 2500: loss: 0.24458804726600647\n",
      "iteration 2501: loss: 0.24458245933055878\n",
      "iteration 2502: loss: 0.24457678198814392\n",
      "iteration 2503: loss: 0.24457108974456787\n",
      "iteration 2504: loss: 0.2445654422044754\n",
      "iteration 2505: loss: 0.24455976486206055\n",
      "iteration 2506: loss: 0.24455413222312927\n",
      "iteration 2507: loss: 0.2445484846830368\n",
      "iteration 2508: loss: 0.24454279243946075\n",
      "iteration 2509: loss: 0.24453718960285187\n",
      "iteration 2510: loss: 0.2445315569639206\n",
      "iteration 2511: loss: 0.24452610313892365\n",
      "iteration 2512: loss: 0.24452033638954163\n",
      "iteration 2513: loss: 0.24451467394828796\n",
      "iteration 2514: loss: 0.24450917541980743\n",
      "iteration 2515: loss: 0.24450340867042542\n",
      "iteration 2516: loss: 0.24449780583381653\n",
      "iteration 2517: loss: 0.24449238181114197\n",
      "iteration 2518: loss: 0.24448668956756592\n",
      "iteration 2519: loss: 0.2444811314344406\n",
      "iteration 2520: loss: 0.24447572231292725\n",
      "iteration 2521: loss: 0.24446997046470642\n",
      "iteration 2522: loss: 0.2444644272327423\n",
      "iteration 2523: loss: 0.24445903301239014\n",
      "iteration 2524: loss: 0.2444533407688141\n",
      "iteration 2525: loss: 0.2444477528333664\n",
      "iteration 2526: loss: 0.244442418217659\n",
      "iteration 2527: loss: 0.2444368302822113\n",
      "iteration 2528: loss: 0.24443116784095764\n",
      "iteration 2529: loss: 0.24442562460899353\n",
      "iteration 2530: loss: 0.24442028999328613\n",
      "iteration 2531: loss: 0.24441476166248322\n",
      "iteration 2532: loss: 0.24440905451774597\n",
      "iteration 2533: loss: 0.24440376460552216\n",
      "iteration 2534: loss: 0.24439826607704163\n",
      "iteration 2535: loss: 0.24439282715320587\n",
      "iteration 2536: loss: 0.2443871945142746\n",
      "iteration 2537: loss: 0.2443818747997284\n",
      "iteration 2538: loss: 0.24437642097473145\n",
      "iteration 2539: loss: 0.2443709373474121\n",
      "iteration 2540: loss: 0.24436548352241516\n",
      "iteration 2541: loss: 0.2443598210811615\n",
      "iteration 2542: loss: 0.24435457587242126\n",
      "iteration 2543: loss: 0.24434903264045715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2544: loss: 0.24434354901313782\n",
      "iteration 2545: loss: 0.24433812499046326\n",
      "iteration 2546: loss: 0.24433255195617676\n",
      "iteration 2547: loss: 0.24432730674743652\n",
      "iteration 2548: loss: 0.24432186782360077\n",
      "iteration 2549: loss: 0.24431642889976501\n",
      "iteration 2550: loss: 0.24431106448173523\n",
      "iteration 2551: loss: 0.24430570006370544\n",
      "iteration 2552: loss: 0.24430029094219208\n",
      "iteration 2553: loss: 0.2442948818206787\n",
      "iteration 2554: loss: 0.2442895472049713\n",
      "iteration 2555: loss: 0.24428415298461914\n",
      "iteration 2556: loss: 0.24427878856658936\n",
      "iteration 2557: loss: 0.24427339434623718\n",
      "iteration 2558: loss: 0.2442680299282074\n",
      "iteration 2559: loss: 0.2442626655101776\n",
      "iteration 2560: loss: 0.24425730109214783\n",
      "iteration 2561: loss: 0.24425196647644043\n",
      "iteration 2562: loss: 0.24424681067466736\n",
      "iteration 2563: loss: 0.24424143135547638\n",
      "iteration 2564: loss: 0.2442360818386078\n",
      "iteration 2565: loss: 0.24423062801361084\n",
      "iteration 2566: loss: 0.24422529339790344\n",
      "iteration 2567: loss: 0.24421997368335724\n",
      "iteration 2568: loss: 0.2442147433757782\n",
      "iteration 2569: loss: 0.2442094385623932\n",
      "iteration 2570: loss: 0.24420413374900818\n",
      "iteration 2571: loss: 0.24419882893562317\n",
      "iteration 2572: loss: 0.24419352412223816\n",
      "iteration 2573: loss: 0.24418826401233673\n",
      "iteration 2574: loss: 0.24418286979198456\n",
      "iteration 2575: loss: 0.24417760968208313\n",
      "iteration 2576: loss: 0.2441723346710205\n",
      "iteration 2577: loss: 0.2441670447587967\n",
      "iteration 2578: loss: 0.24416179955005646\n",
      "iteration 2579: loss: 0.24415652453899384\n",
      "iteration 2580: loss: 0.2441512793302536\n",
      "iteration 2581: loss: 0.24414606392383575\n",
      "iteration 2582: loss: 0.24414095282554626\n",
      "iteration 2583: loss: 0.24413573741912842\n",
      "iteration 2584: loss: 0.24413049221038818\n",
      "iteration 2585: loss: 0.2441253364086151\n",
      "iteration 2586: loss: 0.24412009119987488\n",
      "iteration 2587: loss: 0.2441149204969406\n",
      "iteration 2588: loss: 0.24410970509052277\n",
      "iteration 2589: loss: 0.24410447478294373\n",
      "iteration 2590: loss: 0.24409928917884827\n",
      "iteration 2591: loss: 0.2440940886735916\n",
      "iteration 2592: loss: 0.24408891797065735\n",
      "iteration 2593: loss: 0.2440837174654007\n",
      "iteration 2594: loss: 0.24407871067523956\n",
      "iteration 2595: loss: 0.2440735548734665\n",
      "iteration 2596: loss: 0.24406838417053223\n",
      "iteration 2597: loss: 0.24406304955482483\n",
      "iteration 2598: loss: 0.24405786395072937\n",
      "iteration 2599: loss: 0.2440526932477951\n",
      "iteration 2600: loss: 0.2440475970506668\n",
      "iteration 2601: loss: 0.24404259026050568\n",
      "iteration 2602: loss: 0.24403758347034454\n",
      "iteration 2603: loss: 0.24403241276741028\n",
      "iteration 2604: loss: 0.2440272867679596\n",
      "iteration 2605: loss: 0.24402201175689697\n",
      "iteration 2606: loss: 0.2440168857574463\n",
      "iteration 2607: loss: 0.24401184916496277\n",
      "iteration 2608: loss: 0.2440067082643509\n",
      "iteration 2609: loss: 0.2440016269683838\n",
      "iteration 2610: loss: 0.2439965456724167\n",
      "iteration 2611: loss: 0.2439914047718048\n",
      "iteration 2612: loss: 0.2439863383769989\n",
      "iteration 2613: loss: 0.243981271982193\n",
      "iteration 2614: loss: 0.2439761906862259\n",
      "iteration 2615: loss: 0.24397125840187073\n",
      "iteration 2616: loss: 0.2439662218093872\n",
      "iteration 2617: loss: 0.24396094679832458\n",
      "iteration 2618: loss: 0.24395592510700226\n",
      "iteration 2619: loss: 0.24395108222961426\n",
      "iteration 2620: loss: 0.24394604563713074\n",
      "iteration 2621: loss: 0.2439408302307129\n",
      "iteration 2622: loss: 0.2439359724521637\n",
      "iteration 2623: loss: 0.2439309060573578\n",
      "iteration 2624: loss: 0.24392589926719666\n",
      "iteration 2625: loss: 0.2439207136631012\n",
      "iteration 2626: loss: 0.2439158856868744\n",
      "iteration 2627: loss: 0.24391083419322968\n",
      "iteration 2628: loss: 0.24390582740306854\n",
      "iteration 2629: loss: 0.2439008206129074\n",
      "iteration 2630: loss: 0.24389581382274628\n",
      "iteration 2631: loss: 0.24389080703258514\n",
      "iteration 2632: loss: 0.24388599395751953\n",
      "iteration 2633: loss: 0.24388083815574646\n",
      "iteration 2634: loss: 0.24387605488300323\n",
      "iteration 2635: loss: 0.24387101829051971\n",
      "iteration 2636: loss: 0.24386601150035858\n",
      "iteration 2637: loss: 0.24386119842529297\n",
      "iteration 2638: loss: 0.24385623633861542\n",
      "iteration 2639: loss: 0.24385125935077667\n",
      "iteration 2640: loss: 0.2438463270664215\n",
      "iteration 2641: loss: 0.24384132027626038\n",
      "iteration 2642: loss: 0.24383632838726044\n",
      "iteration 2643: loss: 0.24383139610290527\n",
      "iteration 2644: loss: 0.24382662773132324\n",
      "iteration 2645: loss: 0.24382153153419495\n",
      "iteration 2646: loss: 0.2438168078660965\n",
      "iteration 2647: loss: 0.24381189048290253\n",
      "iteration 2648: loss: 0.24380695819854736\n",
      "iteration 2649: loss: 0.2438020408153534\n",
      "iteration 2650: loss: 0.24379730224609375\n",
      "iteration 2651: loss: 0.24379220604896545\n",
      "iteration 2652: loss: 0.2437874972820282\n",
      "iteration 2653: loss: 0.24378249049186707\n",
      "iteration 2654: loss: 0.2437777817249298\n",
      "iteration 2655: loss: 0.24377290904521942\n",
      "iteration 2656: loss: 0.24376802146434784\n",
      "iteration 2657: loss: 0.24376317858695984\n",
      "iteration 2658: loss: 0.24375829100608826\n",
      "iteration 2659: loss: 0.24375343322753906\n",
      "iteration 2660: loss: 0.24374857544898987\n",
      "iteration 2661: loss: 0.24374370276927948\n",
      "iteration 2662: loss: 0.243739053606987\n",
      "iteration 2663: loss: 0.24373404681682587\n",
      "iteration 2664: loss: 0.2437293529510498\n",
      "iteration 2665: loss: 0.2437244951725006\n",
      "iteration 2666: loss: 0.2437196969985962\n",
      "iteration 2667: loss: 0.2437148541212082\n",
      "iteration 2668: loss: 0.24371008574962616\n",
      "iteration 2669: loss: 0.24370522797107697\n",
      "iteration 2670: loss: 0.24370057880878448\n",
      "iteration 2671: loss: 0.24369566142559052\n",
      "iteration 2672: loss: 0.24369105696678162\n",
      "iteration 2673: loss: 0.24368610978126526\n",
      "iteration 2674: loss: 0.24368147552013397\n",
      "iteration 2675: loss: 0.24367669224739075\n",
      "iteration 2676: loss: 0.24367189407348633\n",
      "iteration 2677: loss: 0.24366700649261475\n",
      "iteration 2678: loss: 0.24366240203380585\n",
      "iteration 2679: loss: 0.2436574399471283\n",
      "iteration 2680: loss: 0.2436528503894806\n",
      "iteration 2681: loss: 0.24364808201789856\n",
      "iteration 2682: loss: 0.24364331364631653\n",
      "iteration 2683: loss: 0.24363858997821808\n",
      "iteration 2684: loss: 0.24363382160663605\n",
      "iteration 2685: loss: 0.24362926185131073\n",
      "iteration 2686: loss: 0.24362432956695557\n",
      "iteration 2687: loss: 0.24361979961395264\n",
      "iteration 2688: loss: 0.24361512064933777\n",
      "iteration 2689: loss: 0.24361038208007812\n",
      "iteration 2690: loss: 0.24360568821430206\n",
      "iteration 2691: loss: 0.24360093474388123\n",
      "iteration 2692: loss: 0.2435964047908783\n",
      "iteration 2693: loss: 0.24359174072742462\n",
      "iteration 2694: loss: 0.24358685314655304\n",
      "iteration 2695: loss: 0.24358229339122772\n",
      "iteration 2696: loss: 0.24357759952545166\n",
      "iteration 2697: loss: 0.24357306957244873\n",
      "iteration 2698: loss: 0.24356837570667267\n",
      "iteration 2699: loss: 0.2435636818408966\n",
      "iteration 2700: loss: 0.24355904757976532\n",
      "iteration 2701: loss: 0.24355435371398926\n",
      "iteration 2702: loss: 0.2435496747493744\n",
      "iteration 2703: loss: 0.24354521930217743\n",
      "iteration 2704: loss: 0.24354052543640137\n",
      "iteration 2705: loss: 0.2435358464717865\n",
      "iteration 2706: loss: 0.24353131651878357\n",
      "iteration 2707: loss: 0.2435266673564911\n",
      "iteration 2708: loss: 0.2435220181941986\n",
      "iteration 2709: loss: 0.24351732432842255\n",
      "iteration 2710: loss: 0.24351289868354797\n",
      "iteration 2711: loss: 0.2435082644224167\n",
      "iteration 2712: loss: 0.243503600358963\n",
      "iteration 2713: loss: 0.24349918961524963\n",
      "iteration 2714: loss: 0.24349455535411835\n",
      "iteration 2715: loss: 0.2434898316860199\n",
      "iteration 2716: loss: 0.2434852123260498\n",
      "iteration 2717: loss: 0.24348080158233643\n",
      "iteration 2718: loss: 0.24347606301307678\n",
      "iteration 2719: loss: 0.24347157776355743\n",
      "iteration 2720: loss: 0.2434670478105545\n",
      "iteration 2721: loss: 0.2434624433517456\n",
      "iteration 2722: loss: 0.2434578388929367\n",
      "iteration 2723: loss: 0.2434532344341278\n",
      "iteration 2724: loss: 0.24344876408576965\n",
      "iteration 2725: loss: 0.24344436824321747\n",
      "iteration 2726: loss: 0.24343976378440857\n",
      "iteration 2727: loss: 0.24343518912792206\n",
      "iteration 2728: loss: 0.2434307038784027\n",
      "iteration 2729: loss: 0.2434261292219162\n",
      "iteration 2730: loss: 0.2434217482805252\n",
      "iteration 2731: loss: 0.24341733753681183\n",
      "iteration 2732: loss: 0.2434128075838089\n",
      "iteration 2733: loss: 0.24340827763080597\n",
      "iteration 2734: loss: 0.24340371787548065\n",
      "iteration 2735: loss: 0.24339938163757324\n",
      "iteration 2736: loss: 0.2433948516845703\n",
      "iteration 2737: loss: 0.24339032173156738\n",
      "iteration 2738: loss: 0.24338582158088684\n",
      "iteration 2739: loss: 0.2433813065290451\n",
      "iteration 2740: loss: 0.24337680637836456\n",
      "iteration 2741: loss: 0.24337248504161835\n",
      "iteration 2742: loss: 0.2433682233095169\n",
      "iteration 2743: loss: 0.24336370825767517\n",
      "iteration 2744: loss: 0.24335920810699463\n",
      "iteration 2745: loss: 0.24335472285747528\n",
      "iteration 2746: loss: 0.24335023760795593\n",
      "iteration 2747: loss: 0.24334576725959778\n",
      "iteration 2748: loss: 0.24334144592285156\n",
      "iteration 2749: loss: 0.24333696067333221\n",
      "iteration 2750: loss: 0.2433326691389084\n",
      "iteration 2751: loss: 0.24332821369171143\n",
      "iteration 2752: loss: 0.24332377314567566\n",
      "iteration 2753: loss: 0.24331946671009064\n",
      "iteration 2754: loss: 0.24331501126289368\n",
      "iteration 2755: loss: 0.24331048130989075\n",
      "iteration 2756: loss: 0.2433060109615326\n",
      "iteration 2757: loss: 0.24330158531665802\n",
      "iteration 2758: loss: 0.24329730868339539\n",
      "iteration 2759: loss: 0.24329285323619843\n",
      "iteration 2760: loss: 0.24328851699829102\n",
      "iteration 2761: loss: 0.24328415095806122\n",
      "iteration 2762: loss: 0.2432798594236374\n",
      "iteration 2763: loss: 0.24327543377876282\n",
      "iteration 2764: loss: 0.24327102303504944\n",
      "iteration 2765: loss: 0.2432667464017868\n",
      "iteration 2766: loss: 0.243262380361557\n",
      "iteration 2767: loss: 0.24325799942016602\n",
      "iteration 2768: loss: 0.24325373768806458\n",
      "iteration 2769: loss: 0.24324937164783478\n",
      "iteration 2770: loss: 0.2432449758052826\n",
      "iteration 2771: loss: 0.24324074387550354\n",
      "iteration 2772: loss: 0.24323639273643494\n",
      "iteration 2773: loss: 0.24323201179504395\n",
      "iteration 2774: loss: 0.2432277649641037\n",
      "iteration 2775: loss: 0.2432234287261963\n",
      "iteration 2776: loss: 0.24321909248828888\n",
      "iteration 2777: loss: 0.24321487545967102\n",
      "iteration 2778: loss: 0.24321052432060242\n",
      "iteration 2779: loss: 0.24320641160011292\n",
      "iteration 2780: loss: 0.24320204555988312\n",
      "iteration 2781: loss: 0.2431977242231369\n",
      "iteration 2782: loss: 0.24319355189800262\n",
      "iteration 2783: loss: 0.24318918585777283\n",
      "iteration 2784: loss: 0.24318504333496094\n",
      "iteration 2785: loss: 0.24318072199821472\n",
      "iteration 2786: loss: 0.24317654967308044\n",
      "iteration 2787: loss: 0.2431720495223999\n",
      "iteration 2788: loss: 0.24316787719726562\n",
      "iteration 2789: loss: 0.2431635856628418\n",
      "iteration 2790: loss: 0.2431594431400299\n",
      "iteration 2791: loss: 0.24315515160560608\n",
      "iteration 2792: loss: 0.243150994181633\n",
      "iteration 2793: loss: 0.24314673244953156\n",
      "iteration 2794: loss: 0.24314257502555847\n",
      "iteration 2795: loss: 0.24313810467720032\n",
      "iteration 2796: loss: 0.24313397705554962\n",
      "iteration 2797: loss: 0.243129700422287\n",
      "iteration 2798: loss: 0.24312558770179749\n",
      "iteration 2799: loss: 0.24312131106853485\n",
      "iteration 2800: loss: 0.24311721324920654\n",
      "iteration 2801: loss: 0.24311308562755585\n",
      "iteration 2802: loss: 0.2431086301803589\n",
      "iteration 2803: loss: 0.24310454726219177\n",
      "iteration 2804: loss: 0.24310031533241272\n",
      "iteration 2805: loss: 0.2430962324142456\n",
      "iteration 2806: loss: 0.2430921345949173\n",
      "iteration 2807: loss: 0.24308772385120392\n",
      "iteration 2808: loss: 0.2430836409330368\n",
      "iteration 2809: loss: 0.24307942390441895\n",
      "iteration 2810: loss: 0.24307537078857422\n",
      "iteration 2811: loss: 0.2430712729692459\n",
      "iteration 2812: loss: 0.2430669367313385\n",
      "iteration 2813: loss: 0.2430628091096878\n",
      "iteration 2814: loss: 0.24305877089500427\n",
      "iteration 2815: loss: 0.2430545836687088\n",
      "iteration 2816: loss: 0.24305030703544617\n",
      "iteration 2817: loss: 0.243046373128891\n",
      "iteration 2818: loss: 0.24304215610027313\n",
      "iteration 2819: loss: 0.243038147687912\n",
      "iteration 2820: loss: 0.24303393065929413\n",
      "iteration 2821: loss: 0.2430298775434494\n",
      "iteration 2822: loss: 0.24302570521831512\n",
      "iteration 2823: loss: 0.24302168190479279\n",
      "iteration 2824: loss: 0.24301747977733612\n",
      "iteration 2825: loss: 0.24301347136497498\n",
      "iteration 2826: loss: 0.24300932884216309\n",
      "iteration 2827: loss: 0.24300532042980194\n",
      "iteration 2828: loss: 0.24300114810466766\n",
      "iteration 2829: loss: 0.24299712479114532\n",
      "iteration 2830: loss: 0.24299314618110657\n",
      "iteration 2831: loss: 0.24298885464668274\n",
      "iteration 2832: loss: 0.2429848611354828\n",
      "iteration 2833: loss: 0.24298086762428284\n",
      "iteration 2834: loss: 0.24297669529914856\n",
      "iteration 2835: loss: 0.242972731590271\n",
      "iteration 2836: loss: 0.24296864867210388\n",
      "iteration 2837: loss: 0.24296453595161438\n",
      "iteration 2838: loss: 0.24296057224273682\n",
      "iteration 2839: loss: 0.24295659363269806\n",
      "iteration 2840: loss: 0.24295243620872498\n",
      "iteration 2841: loss: 0.2429484874010086\n",
      "iteration 2842: loss: 0.24294455349445343\n",
      "iteration 2843: loss: 0.24294042587280273\n",
      "iteration 2844: loss: 0.24293646216392517\n",
      "iteration 2845: loss: 0.24293211102485657\n",
      "iteration 2846: loss: 0.24292819201946259\n",
      "iteration 2847: loss: 0.24292424321174622\n",
      "iteration 2848: loss: 0.2429201304912567\n",
      "iteration 2849: loss: 0.24291622638702393\n",
      "iteration 2850: loss: 0.24291209876537323\n",
      "iteration 2851: loss: 0.24290819466114044\n",
      "iteration 2852: loss: 0.24290427565574646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2853: loss: 0.24290020763874054\n",
      "iteration 2854: loss: 0.24289628863334656\n",
      "iteration 2855: loss: 0.24289217591285706\n",
      "iteration 2856: loss: 0.24288837611675262\n",
      "iteration 2857: loss: 0.2428843080997467\n",
      "iteration 2858: loss: 0.2428804337978363\n",
      "iteration 2859: loss: 0.24287652969360352\n",
      "iteration 2860: loss: 0.2428724467754364\n",
      "iteration 2861: loss: 0.24286861717700958\n",
      "iteration 2862: loss: 0.24286453425884247\n",
      "iteration 2863: loss: 0.24286067485809326\n",
      "iteration 2864: loss: 0.24285662174224854\n",
      "iteration 2865: loss: 0.24285273253917694\n",
      "iteration 2866: loss: 0.2428487092256546\n",
      "iteration 2867: loss: 0.2428448498249054\n",
      "iteration 2868: loss: 0.2428409308195114\n",
      "iteration 2869: loss: 0.2428371012210846\n",
      "iteration 2870: loss: 0.24283306300640106\n",
      "iteration 2871: loss: 0.24282920360565186\n",
      "iteration 2872: loss: 0.2428251951932907\n",
      "iteration 2873: loss: 0.2428213655948639\n",
      "iteration 2874: loss: 0.24281735718250275\n",
      "iteration 2875: loss: 0.24281351268291473\n",
      "iteration 2876: loss: 0.2428097426891327\n",
      "iteration 2877: loss: 0.24280592799186707\n",
      "iteration 2878: loss: 0.24280193448066711\n",
      "iteration 2879: loss: 0.2427980899810791\n",
      "iteration 2880: loss: 0.24279411137104034\n",
      "iteration 2881: loss: 0.24279025197029114\n",
      "iteration 2882: loss: 0.2427864521741867\n",
      "iteration 2883: loss: 0.24278247356414795\n",
      "iteration 2884: loss: 0.2427786886692047\n",
      "iteration 2885: loss: 0.24277469515800476\n",
      "iteration 2886: loss: 0.24277107417583466\n",
      "iteration 2887: loss: 0.2427670955657959\n",
      "iteration 2888: loss: 0.24276311695575714\n",
      "iteration 2889: loss: 0.2427593469619751\n",
      "iteration 2890: loss: 0.24275553226470947\n",
      "iteration 2891: loss: 0.24275174736976624\n",
      "iteration 2892: loss: 0.24274782836437225\n",
      "iteration 2893: loss: 0.24274399876594543\n",
      "iteration 2894: loss: 0.24274024367332458\n",
      "iteration 2895: loss: 0.24273619055747986\n",
      "iteration 2896: loss: 0.242732435464859\n",
      "iteration 2897: loss: 0.24272875487804413\n",
      "iteration 2898: loss: 0.24272477626800537\n",
      "iteration 2899: loss: 0.2427210509777069\n",
      "iteration 2900: loss: 0.24271726608276367\n",
      "iteration 2901: loss: 0.2427133321762085\n",
      "iteration 2902: loss: 0.24270963668823242\n",
      "iteration 2903: loss: 0.24270586669445038\n",
      "iteration 2904: loss: 0.242701917886734\n",
      "iteration 2905: loss: 0.24269835650920868\n",
      "iteration 2906: loss: 0.2426944524049759\n",
      "iteration 2907: loss: 0.2426905333995819\n",
      "iteration 2908: loss: 0.24268698692321777\n",
      "iteration 2909: loss: 0.24268312752246857\n",
      "iteration 2910: loss: 0.24267932772636414\n",
      "iteration 2911: loss: 0.24267569184303284\n",
      "iteration 2912: loss: 0.2426719218492508\n",
      "iteration 2913: loss: 0.2426680326461792\n",
      "iteration 2914: loss: 0.24266433715820312\n",
      "iteration 2915: loss: 0.24266064167022705\n",
      "iteration 2916: loss: 0.24265673756599426\n",
      "iteration 2917: loss: 0.2426529824733734\n",
      "iteration 2918: loss: 0.2426494061946869\n",
      "iteration 2919: loss: 0.24264569580554962\n",
      "iteration 2920: loss: 0.24264183640480042\n",
      "iteration 2921: loss: 0.24263814091682434\n",
      "iteration 2922: loss: 0.24263449013233185\n",
      "iteration 2923: loss: 0.2426307648420334\n",
      "iteration 2924: loss: 0.24262705445289612\n",
      "iteration 2925: loss: 0.2426232397556305\n",
      "iteration 2926: loss: 0.24261970818042755\n",
      "iteration 2927: loss: 0.24261589348316193\n",
      "iteration 2928: loss: 0.24261221289634705\n",
      "iteration 2929: loss: 0.24260835349559784\n",
      "iteration 2930: loss: 0.24260489642620087\n",
      "iteration 2931: loss: 0.24260108172893524\n",
      "iteration 2932: loss: 0.24259738624095917\n",
      "iteration 2933: loss: 0.24259372055530548\n",
      "iteration 2934: loss: 0.24258990585803986\n",
      "iteration 2935: loss: 0.24258646368980408\n",
      "iteration 2936: loss: 0.2425827533006668\n",
      "iteration 2937: loss: 0.24257898330688477\n",
      "iteration 2938: loss: 0.24257531762123108\n",
      "iteration 2939: loss: 0.24257174134254456\n",
      "iteration 2940: loss: 0.24256794154644012\n",
      "iteration 2941: loss: 0.24256451427936554\n",
      "iteration 2942: loss: 0.24256086349487305\n",
      "iteration 2943: loss: 0.2425570785999298\n",
      "iteration 2944: loss: 0.2425534427165985\n",
      "iteration 2945: loss: 0.24254977703094482\n",
      "iteration 2946: loss: 0.24254605174064636\n",
      "iteration 2947: loss: 0.24254262447357178\n",
      "iteration 2948: loss: 0.24253900349140167\n",
      "iteration 2949: loss: 0.24253535270690918\n",
      "iteration 2950: loss: 0.24253153800964355\n",
      "iteration 2951: loss: 0.24252791702747345\n",
      "iteration 2952: loss: 0.24252429604530334\n",
      "iteration 2953: loss: 0.24252071976661682\n",
      "iteration 2954: loss: 0.24251694977283478\n",
      "iteration 2955: loss: 0.24251356720924377\n",
      "iteration 2956: loss: 0.24250993132591248\n",
      "iteration 2957: loss: 0.24250635504722595\n",
      "iteration 2958: loss: 0.24250276386737823\n",
      "iteration 2959: loss: 0.24249903857707977\n",
      "iteration 2960: loss: 0.24249546229839325\n",
      "iteration 2961: loss: 0.2424919307231903\n",
      "iteration 2962: loss: 0.24248838424682617\n",
      "iteration 2963: loss: 0.24248476326465607\n",
      "iteration 2964: loss: 0.24248120188713074\n",
      "iteration 2965: loss: 0.24247750639915466\n",
      "iteration 2966: loss: 0.24247395992279053\n",
      "iteration 2967: loss: 0.242470383644104\n",
      "iteration 2968: loss: 0.24246683716773987\n",
      "iteration 2969: loss: 0.24246326088905334\n",
      "iteration 2970: loss: 0.2424597293138504\n",
      "iteration 2971: loss: 0.2424563616514206\n",
      "iteration 2972: loss: 0.24245285987854004\n",
      "iteration 2973: loss: 0.24244928359985352\n",
      "iteration 2974: loss: 0.24244575202465057\n",
      "iteration 2975: loss: 0.2424420863389969\n",
      "iteration 2976: loss: 0.24243858456611633\n",
      "iteration 2977: loss: 0.242435023188591\n",
      "iteration 2978: loss: 0.24243152141571045\n",
      "iteration 2979: loss: 0.2424279749393463\n",
      "iteration 2980: loss: 0.24242444336414337\n",
      "iteration 2981: loss: 0.24242091178894043\n",
      "iteration 2982: loss: 0.24241742491722107\n",
      "iteration 2983: loss: 0.24241392314434052\n",
      "iteration 2984: loss: 0.24241046607494354\n",
      "iteration 2985: loss: 0.24240675568580627\n",
      "iteration 2986: loss: 0.24240326881408691\n",
      "iteration 2987: loss: 0.2423998862504959\n",
      "iteration 2988: loss: 0.24239639937877655\n",
      "iteration 2989: loss: 0.24239292740821838\n",
      "iteration 2990: loss: 0.24238939583301544\n",
      "iteration 2991: loss: 0.24238595366477966\n",
      "iteration 2992: loss: 0.24238243699073792\n",
      "iteration 2993: loss: 0.24237897992134094\n",
      "iteration 2994: loss: 0.24237552285194397\n",
      "iteration 2995: loss: 0.242372065782547\n",
      "iteration 2996: loss: 0.24236854910850525\n",
      "iteration 2997: loss: 0.24236509203910828\n",
      "iteration 2998: loss: 0.2423616349697113\n",
      "iteration 2999: loss: 0.24235832691192627\n",
      "iteration 3000: loss: 0.24235467612743378\n",
      "iteration 3001: loss: 0.2423512041568756\n",
      "iteration 3002: loss: 0.24234776198863983\n",
      "iteration 3003: loss: 0.24234429001808167\n",
      "iteration 3004: loss: 0.2423408478498459\n",
      "iteration 3005: loss: 0.24233753979206085\n",
      "iteration 3006: loss: 0.24233412742614746\n",
      "iteration 3007: loss: 0.24233070015907288\n",
      "iteration 3008: loss: 0.24232709407806396\n",
      "iteration 3009: loss: 0.24232368171215057\n",
      "iteration 3010: loss: 0.24232037365436554\n",
      "iteration 3011: loss: 0.2423168420791626\n",
      "iteration 3012: loss: 0.2423134297132492\n",
      "iteration 3013: loss: 0.2423100471496582\n",
      "iteration 3014: loss: 0.24230675399303436\n",
      "iteration 3015: loss: 0.24230310320854187\n",
      "iteration 3016: loss: 0.24229970574378967\n",
      "iteration 3017: loss: 0.24229630827903748\n",
      "iteration 3018: loss: 0.24229303002357483\n",
      "iteration 3019: loss: 0.24228966236114502\n",
      "iteration 3020: loss: 0.24228624999523163\n",
      "iteration 3021: loss: 0.24228279292583466\n",
      "iteration 3022: loss: 0.24227941036224365\n",
      "iteration 3023: loss: 0.24227602779865265\n",
      "iteration 3024: loss: 0.2422727793455124\n",
      "iteration 3025: loss: 0.24226942658424377\n",
      "iteration 3026: loss: 0.24226585030555725\n",
      "iteration 3027: loss: 0.2422625571489334\n",
      "iteration 3028: loss: 0.242259219288826\n",
      "iteration 3029: loss: 0.24225585162639618\n",
      "iteration 3030: loss: 0.2422523945569992\n",
      "iteration 3031: loss: 0.2422490417957306\n",
      "iteration 3032: loss: 0.2422458827495575\n",
      "iteration 3033: loss: 0.24224254488945007\n",
      "iteration 3034: loss: 0.24223895370960236\n",
      "iteration 3035: loss: 0.24223573505878448\n",
      "iteration 3036: loss: 0.24223236739635468\n",
      "iteration 3037: loss: 0.24222922325134277\n",
      "iteration 3038: loss: 0.24222567677497864\n",
      "iteration 3039: loss: 0.24222242832183838\n",
      "iteration 3040: loss: 0.24221912026405334\n",
      "iteration 3041: loss: 0.24221594631671906\n",
      "iteration 3042: loss: 0.24221237003803253\n",
      "iteration 3043: loss: 0.24220919609069824\n",
      "iteration 3044: loss: 0.24220585823059082\n",
      "iteration 3045: loss: 0.24220247566699982\n",
      "iteration 3046: loss: 0.24219918251037598\n",
      "iteration 3047: loss: 0.2421959638595581\n",
      "iteration 3048: loss: 0.24219267070293427\n",
      "iteration 3049: loss: 0.24218928813934326\n",
      "iteration 3050: loss: 0.24218598008155823\n",
      "iteration 3051: loss: 0.24218280613422394\n",
      "iteration 3052: loss: 0.24217930436134338\n",
      "iteration 3053: loss: 0.2421761453151703\n",
      "iteration 3054: loss: 0.24217286705970764\n",
      "iteration 3055: loss: 0.24216946959495544\n",
      "iteration 3056: loss: 0.24216632544994354\n",
      "iteration 3057: loss: 0.24216310679912567\n",
      "iteration 3058: loss: 0.24215975403785706\n",
      "iteration 3059: loss: 0.2421565055847168\n",
      "iteration 3060: loss: 0.2421533316373825\n",
      "iteration 3061: loss: 0.24215002357959747\n",
      "iteration 3062: loss: 0.24214673042297363\n",
      "iteration 3063: loss: 0.24214334785938263\n",
      "iteration 3064: loss: 0.2421402633190155\n",
      "iteration 3065: loss: 0.24213699996471405\n",
      "iteration 3066: loss: 0.24213366210460663\n",
      "iteration 3067: loss: 0.24213051795959473\n",
      "iteration 3068: loss: 0.24212709069252014\n",
      "iteration 3069: loss: 0.24212396144866943\n",
      "iteration 3070: loss: 0.2421208620071411\n",
      "iteration 3071: loss: 0.24211740493774414\n",
      "iteration 3072: loss: 0.24211427569389343\n",
      "iteration 3073: loss: 0.2421109676361084\n",
      "iteration 3074: loss: 0.2421078383922577\n",
      "iteration 3075: loss: 0.2421046495437622\n",
      "iteration 3076: loss: 0.24210134148597717\n",
      "iteration 3077: loss: 0.24209825694561005\n",
      "iteration 3078: loss: 0.24209491908550262\n",
      "iteration 3079: loss: 0.2420916110277176\n",
      "iteration 3080: loss: 0.24208831787109375\n",
      "iteration 3081: loss: 0.24208524823188782\n",
      "iteration 3082: loss: 0.2420821487903595\n",
      "iteration 3083: loss: 0.24207893013954163\n",
      "iteration 3084: loss: 0.24207572638988495\n",
      "iteration 3085: loss: 0.2420724630355835\n",
      "iteration 3086: loss: 0.24206939339637756\n",
      "iteration 3087: loss: 0.24206610023975372\n",
      "iteration 3088: loss: 0.24206307530403137\n",
      "iteration 3089: loss: 0.2420596331357956\n",
      "iteration 3090: loss: 0.24205657839775085\n",
      "iteration 3091: loss: 0.2420533001422882\n",
      "iteration 3092: loss: 0.24205024540424347\n",
      "iteration 3093: loss: 0.2420470267534256\n",
      "iteration 3094: loss: 0.24204392731189728\n",
      "iteration 3095: loss: 0.2420407086610794\n",
      "iteration 3096: loss: 0.24203765392303467\n",
      "iteration 3097: loss: 0.2420344054698944\n",
      "iteration 3098: loss: 0.24203138053417206\n",
      "iteration 3099: loss: 0.24202799797058105\n",
      "iteration 3100: loss: 0.2420249730348587\n",
      "iteration 3101: loss: 0.24202175438404083\n",
      "iteration 3102: loss: 0.24201872944831848\n",
      "iteration 3103: loss: 0.2420155256986618\n",
      "iteration 3104: loss: 0.24201250076293945\n",
      "iteration 3105: loss: 0.2420092523097992\n",
      "iteration 3106: loss: 0.24200625717639923\n",
      "iteration 3107: loss: 0.24200299382209778\n",
      "iteration 3108: loss: 0.2419997900724411\n",
      "iteration 3109: loss: 0.24199679493904114\n",
      "iteration 3110: loss: 0.24199357628822327\n",
      "iteration 3111: loss: 0.24199065566062927\n",
      "iteration 3112: loss: 0.24198739230632782\n",
      "iteration 3113: loss: 0.24198441207408905\n",
      "iteration 3114: loss: 0.24198123812675476\n",
      "iteration 3115: loss: 0.2419780194759369\n",
      "iteration 3116: loss: 0.24197503924369812\n",
      "iteration 3117: loss: 0.24197201430797577\n",
      "iteration 3118: loss: 0.2419690191745758\n",
      "iteration 3119: loss: 0.24196580052375793\n",
      "iteration 3120: loss: 0.24196262657642365\n",
      "iteration 3121: loss: 0.24195966124534607\n",
      "iteration 3122: loss: 0.24195647239685059\n",
      "iteration 3123: loss: 0.2419535219669342\n",
      "iteration 3124: loss: 0.24195030331611633\n",
      "iteration 3125: loss: 0.24194712936878204\n",
      "iteration 3126: loss: 0.2419443428516388\n",
      "iteration 3127: loss: 0.24194112420082092\n",
      "iteration 3128: loss: 0.24193796515464783\n",
      "iteration 3129: loss: 0.24193504452705383\n",
      "iteration 3130: loss: 0.24193187057971954\n",
      "iteration 3131: loss: 0.24192874133586884\n",
      "iteration 3132: loss: 0.241925910115242\n",
      "iteration 3133: loss: 0.2419227659702301\n",
      "iteration 3134: loss: 0.241919606924057\n",
      "iteration 3135: loss: 0.241916686296463\n",
      "iteration 3136: loss: 0.24191352725028992\n",
      "iteration 3137: loss: 0.24191053211688995\n",
      "iteration 3138: loss: 0.24190759658813477\n",
      "iteration 3139: loss: 0.24190452694892883\n",
      "iteration 3140: loss: 0.24190135300159454\n",
      "iteration 3141: loss: 0.24189861118793488\n",
      "iteration 3142: loss: 0.24189546704292297\n",
      "iteration 3143: loss: 0.24189233779907227\n",
      "iteration 3144: loss: 0.24188943207263947\n",
      "iteration 3145: loss: 0.2418864518404007\n",
      "iteration 3146: loss: 0.2418833076953888\n",
      "iteration 3147: loss: 0.24188017845153809\n",
      "iteration 3148: loss: 0.24187740683555603\n",
      "iteration 3149: loss: 0.2418743073940277\n",
      "iteration 3150: loss: 0.2418711632490158\n",
      "iteration 3151: loss: 0.24186845123767853\n",
      "iteration 3152: loss: 0.24186532199382782\n",
      "iteration 3153: loss: 0.2418622523546219\n",
      "iteration 3154: loss: 0.24185927212238312\n",
      "iteration 3155: loss: 0.24185636639595032\n",
      "iteration 3156: loss: 0.241853266954422\n",
      "iteration 3157: loss: 0.24185022711753845\n",
      "iteration 3158: loss: 0.24184712767601013\n",
      "iteration 3159: loss: 0.24184425175189972\n",
      "iteration 3160: loss: 0.24184128642082214\n",
      "iteration 3161: loss: 0.2418382167816162\n",
      "iteration 3162: loss: 0.24183526635169983\n",
      "iteration 3163: loss: 0.2418322116136551\n",
      "iteration 3164: loss: 0.24182946979999542\n",
      "iteration 3165: loss: 0.2418263852596283\n",
      "iteration 3166: loss: 0.24182331562042236\n",
      "iteration 3167: loss: 0.24182038009166718\n",
      "iteration 3168: loss: 0.24181754887104034\n",
      "iteration 3169: loss: 0.24181461334228516\n",
      "iteration 3170: loss: 0.2418115884065628\n",
      "iteration 3171: loss: 0.24180868268013\n",
      "iteration 3172: loss: 0.24180562794208527\n",
      "iteration 3173: loss: 0.2418026626110077\n",
      "iteration 3174: loss: 0.24179986119270325\n",
      "iteration 3175: loss: 0.24179694056510925\n",
      "iteration 3176: loss: 0.2417939007282257\n",
      "iteration 3177: loss: 0.24179096519947052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3178: loss: 0.24178794026374817\n",
      "iteration 3179: loss: 0.24178501963615417\n",
      "iteration 3180: loss: 0.24178233742713928\n",
      "iteration 3181: loss: 0.24177929759025574\n",
      "iteration 3182: loss: 0.24177642166614532\n",
      "iteration 3183: loss: 0.24177336692810059\n",
      "iteration 3184: loss: 0.24177046120166779\n",
      "iteration 3185: loss: 0.24176745116710663\n",
      "iteration 3186: loss: 0.24176457524299622\n",
      "iteration 3187: loss: 0.24176189303398132\n",
      "iteration 3188: loss: 0.24175886809825897\n",
      "iteration 3189: loss: 0.24175599217414856\n",
      "iteration 3190: loss: 0.2417529821395874\n",
      "iteration 3191: loss: 0.24175012111663818\n",
      "iteration 3192: loss: 0.24174723029136658\n",
      "iteration 3193: loss: 0.2417442351579666\n",
      "iteration 3194: loss: 0.241741344332695\n",
      "iteration 3195: loss: 0.24173851311206818\n",
      "iteration 3196: loss: 0.24173572659492493\n",
      "iteration 3197: loss: 0.2417328655719757\n",
      "iteration 3198: loss: 0.2417299747467041\n",
      "iteration 3199: loss: 0.24172711372375488\n",
      "iteration 3200: loss: 0.2417241334915161\n",
      "iteration 3201: loss: 0.2417212724685669\n",
      "iteration 3202: loss: 0.24171848595142365\n",
      "iteration 3203: loss: 0.24171550571918488\n",
      "iteration 3204: loss: 0.24171265959739685\n",
      "iteration 3205: loss: 0.24170982837677002\n",
      "iteration 3206: loss: 0.2417069375514984\n",
      "iteration 3207: loss: 0.24170401692390442\n",
      "iteration 3208: loss: 0.2417011559009552\n",
      "iteration 3209: loss: 0.24169833958148956\n",
      "iteration 3210: loss: 0.24169549345970154\n",
      "iteration 3211: loss: 0.24169273674488068\n",
      "iteration 3212: loss: 0.24168992042541504\n",
      "iteration 3213: loss: 0.2416871041059494\n",
      "iteration 3214: loss: 0.24168427288532257\n",
      "iteration 3215: loss: 0.24168142676353455\n",
      "iteration 3216: loss: 0.24167850613594055\n",
      "iteration 3217: loss: 0.2416757047176361\n",
      "iteration 3218: loss: 0.24167287349700928\n",
      "iteration 3219: loss: 0.24167008697986603\n",
      "iteration 3220: loss: 0.241667240858078\n",
      "iteration 3221: loss: 0.24166440963745117\n",
      "iteration 3222: loss: 0.24166162312030792\n",
      "iteration 3223: loss: 0.24165868759155273\n",
      "iteration 3224: loss: 0.24165591597557068\n",
      "iteration 3225: loss: 0.24165308475494385\n",
      "iteration 3226: loss: 0.24165034294128418\n",
      "iteration 3227: loss: 0.24164748191833496\n",
      "iteration 3228: loss: 0.2416447103023529\n",
      "iteration 3229: loss: 0.24164190888404846\n",
      "iteration 3230: loss: 0.2416389286518097\n",
      "iteration 3231: loss: 0.24163612723350525\n",
      "iteration 3232: loss: 0.2416333258152008\n",
      "iteration 3233: loss: 0.24163058400154114\n",
      "iteration 3234: loss: 0.24162781238555908\n",
      "iteration 3235: loss: 0.24162499606609344\n",
      "iteration 3236: loss: 0.24162223935127258\n",
      "iteration 3237: loss: 0.24161942303180695\n",
      "iteration 3238: loss: 0.2416166365146637\n",
      "iteration 3239: loss: 0.24161386489868164\n",
      "iteration 3240: loss: 0.24161109328269958\n",
      "iteration 3241: loss: 0.24160833656787872\n",
      "iteration 3242: loss: 0.24160568416118622\n",
      "iteration 3243: loss: 0.24160294234752655\n",
      "iteration 3244: loss: 0.2416001856327057\n",
      "iteration 3245: loss: 0.24159744381904602\n",
      "iteration 3246: loss: 0.24159446358680725\n",
      "iteration 3247: loss: 0.24159172177314758\n",
      "iteration 3248: loss: 0.24158897995948792\n",
      "iteration 3249: loss: 0.2415861189365387\n",
      "iteration 3250: loss: 0.24158337712287903\n",
      "iteration 3251: loss: 0.24158063530921936\n",
      "iteration 3252: loss: 0.2415778934955597\n",
      "iteration 3253: loss: 0.24157515168190002\n",
      "iteration 3254: loss: 0.24157242476940155\n",
      "iteration 3255: loss: 0.24156947433948517\n",
      "iteration 3256: loss: 0.2415667325258255\n",
      "iteration 3257: loss: 0.24156412482261658\n",
      "iteration 3258: loss: 0.2415613830089569\n",
      "iteration 3259: loss: 0.24155864119529724\n",
      "iteration 3260: loss: 0.24155592918395996\n",
      "iteration 3261: loss: 0.24155323207378387\n",
      "iteration 3262: loss: 0.2415502816438675\n",
      "iteration 3263: loss: 0.24154773354530334\n",
      "iteration 3264: loss: 0.24154499173164368\n",
      "iteration 3265: loss: 0.24154230952262878\n",
      "iteration 3266: loss: 0.24153956770896912\n",
      "iteration 3267: loss: 0.24153690040111542\n",
      "iteration 3268: loss: 0.24153394997119904\n",
      "iteration 3269: loss: 0.2415313720703125\n",
      "iteration 3270: loss: 0.2415286749601364\n",
      "iteration 3271: loss: 0.24152597784996033\n",
      "iteration 3272: loss: 0.24152329564094543\n",
      "iteration 3273: loss: 0.24152052402496338\n",
      "iteration 3274: loss: 0.2415178269147873\n",
      "iteration 3275: loss: 0.24151520431041718\n",
      "iteration 3276: loss: 0.2415125072002411\n",
      "iteration 3277: loss: 0.24150995910167694\n",
      "iteration 3278: loss: 0.24150702357292175\n",
      "iteration 3279: loss: 0.24150438606739044\n",
      "iteration 3280: loss: 0.24150168895721436\n",
      "iteration 3281: loss: 0.2414991408586502\n",
      "iteration 3282: loss: 0.2414964735507965\n",
      "iteration 3283: loss: 0.2414935827255249\n",
      "iteration 3284: loss: 0.24149104952812195\n",
      "iteration 3285: loss: 0.24148836731910706\n",
      "iteration 3286: loss: 0.24148574471473694\n",
      "iteration 3287: loss: 0.24148297309875488\n",
      "iteration 3288: loss: 0.2414802759885788\n",
      "iteration 3289: loss: 0.24147763848304749\n",
      "iteration 3290: loss: 0.24147510528564453\n",
      "iteration 3291: loss: 0.2414722442626953\n",
      "iteration 3292: loss: 0.24146966636180878\n",
      "iteration 3293: loss: 0.24146707355976105\n",
      "iteration 3294: loss: 0.24146440625190735\n",
      "iteration 3295: loss: 0.24146166443824768\n",
      "iteration 3296: loss: 0.24145901203155518\n",
      "iteration 3297: loss: 0.2414565086364746\n",
      "iteration 3298: loss: 0.241453617811203\n",
      "iteration 3299: loss: 0.2414509803056717\n",
      "iteration 3300: loss: 0.24144849181175232\n",
      "iteration 3301: loss: 0.241445854306221\n",
      "iteration 3302: loss: 0.24144312739372253\n",
      "iteration 3303: loss: 0.24144046008586884\n",
      "iteration 3304: loss: 0.24143800139427185\n",
      "iteration 3305: loss: 0.24143514037132263\n",
      "iteration 3306: loss: 0.24143263697624207\n",
      "iteration 3307: loss: 0.24143004417419434\n",
      "iteration 3308: loss: 0.24142730236053467\n",
      "iteration 3309: loss: 0.24142467975616455\n",
      "iteration 3310: loss: 0.24142220616340637\n",
      "iteration 3311: loss: 0.24141936004161835\n",
      "iteration 3312: loss: 0.24141690135002136\n",
      "iteration 3313: loss: 0.24141427874565125\n",
      "iteration 3314: loss: 0.24141159653663635\n",
      "iteration 3315: loss: 0.24140901863574982\n",
      "iteration 3316: loss: 0.24140658974647522\n",
      "iteration 3317: loss: 0.2414037436246872\n",
      "iteration 3318: loss: 0.24140127003192902\n",
      "iteration 3319: loss: 0.24139869213104248\n",
      "iteration 3320: loss: 0.2413959950208664\n",
      "iteration 3321: loss: 0.24139341711997986\n",
      "iteration 3322: loss: 0.24139094352722168\n",
      "iteration 3323: loss: 0.2413882464170456\n",
      "iteration 3324: loss: 0.24138565361499786\n",
      "iteration 3325: loss: 0.24138300120830536\n",
      "iteration 3326: loss: 0.24138054251670837\n",
      "iteration 3327: loss: 0.24137791991233826\n",
      "iteration 3328: loss: 0.24137528240680695\n",
      "iteration 3329: loss: 0.2413727045059204\n",
      "iteration 3330: loss: 0.241370290517807\n",
      "iteration 3331: loss: 0.24136754870414734\n",
      "iteration 3332: loss: 0.24136503040790558\n",
      "iteration 3333: loss: 0.24136237800121307\n",
      "iteration 3334: loss: 0.2413599044084549\n",
      "iteration 3335: loss: 0.24135735630989075\n",
      "iteration 3336: loss: 0.24135470390319824\n",
      "iteration 3337: loss: 0.24135224521160126\n",
      "iteration 3338: loss: 0.2413494884967804\n",
      "iteration 3339: loss: 0.2413470447063446\n",
      "iteration 3340: loss: 0.2413444072008133\n",
      "iteration 3341: loss: 0.24134185910224915\n",
      "iteration 3342: loss: 0.24133941531181335\n",
      "iteration 3343: loss: 0.24133677780628204\n",
      "iteration 3344: loss: 0.24133434891700745\n",
      "iteration 3345: loss: 0.24133160710334778\n",
      "iteration 3346: loss: 0.241329163312912\n",
      "iteration 3347: loss: 0.24132654070854187\n",
      "iteration 3348: loss: 0.24132411181926727\n",
      "iteration 3349: loss: 0.2413213700056076\n",
      "iteration 3350: loss: 0.241318941116333\n",
      "iteration 3351: loss: 0.24131634831428528\n",
      "iteration 3352: loss: 0.24131393432617188\n",
      "iteration 3353: loss: 0.24131152033805847\n",
      "iteration 3354: loss: 0.2413087785243988\n",
      "iteration 3355: loss: 0.2413063794374466\n",
      "iteration 3356: loss: 0.24130377173423767\n",
      "iteration 3357: loss: 0.24130134284496307\n",
      "iteration 3358: loss: 0.24129875004291534\n",
      "iteration 3359: loss: 0.24129633605480194\n",
      "iteration 3360: loss: 0.24129357933998108\n",
      "iteration 3361: loss: 0.24129119515419006\n",
      "iteration 3362: loss: 0.24128858745098114\n",
      "iteration 3363: loss: 0.24128615856170654\n",
      "iteration 3364: loss: 0.2412835657596588\n",
      "iteration 3365: loss: 0.2412811815738678\n",
      "iteration 3366: loss: 0.24127860367298126\n",
      "iteration 3367: loss: 0.24127618968486786\n",
      "iteration 3368: loss: 0.24127349257469177\n",
      "iteration 3369: loss: 0.24127110838890076\n",
      "iteration 3370: loss: 0.24126851558685303\n",
      "iteration 3371: loss: 0.2412659227848053\n",
      "iteration 3372: loss: 0.24126355350017548\n",
      "iteration 3373: loss: 0.24126096069812775\n",
      "iteration 3374: loss: 0.2412586212158203\n",
      "iteration 3375: loss: 0.24125604331493378\n",
      "iteration 3376: loss: 0.24125365912914276\n",
      "iteration 3377: loss: 0.24125108122825623\n",
      "iteration 3378: loss: 0.2412486970424652\n",
      "iteration 3379: loss: 0.24124614894390106\n",
      "iteration 3380: loss: 0.24124355614185333\n",
      "iteration 3381: loss: 0.2412412464618683\n",
      "iteration 3382: loss: 0.24123863875865936\n",
      "iteration 3383: loss: 0.24123629927635193\n",
      "iteration 3384: loss: 0.2412337362766266\n",
      "iteration 3385: loss: 0.24123139679431915\n",
      "iteration 3386: loss: 0.24122881889343262\n",
      "iteration 3387: loss: 0.24122627079486847\n",
      "iteration 3388: loss: 0.24122393131256104\n",
      "iteration 3389: loss: 0.2412213832139969\n",
      "iteration 3390: loss: 0.24121901392936707\n",
      "iteration 3391: loss: 0.2412165105342865\n",
      "iteration 3392: loss: 0.24121394753456116\n",
      "iteration 3393: loss: 0.24121162295341492\n",
      "iteration 3394: loss: 0.24120905995368958\n",
      "iteration 3395: loss: 0.24120673537254333\n",
      "iteration 3396: loss: 0.24120421707630157\n",
      "iteration 3397: loss: 0.24120166897773743\n",
      "iteration 3398: loss: 0.24119932949543\n",
      "iteration 3399: loss: 0.2411969155073166\n",
      "iteration 3400: loss: 0.24119457602500916\n",
      "iteration 3401: loss: 0.2411920577287674\n",
      "iteration 3402: loss: 0.24118950963020325\n",
      "iteration 3403: loss: 0.241187185049057\n",
      "iteration 3404: loss: 0.24118466675281525\n",
      "iteration 3405: loss: 0.24118217825889587\n",
      "iteration 3406: loss: 0.24117998778820038\n",
      "iteration 3407: loss: 0.24117740988731384\n",
      "iteration 3408: loss: 0.24117490649223328\n",
      "iteration 3409: loss: 0.24117259681224823\n",
      "iteration 3410: loss: 0.24117009341716766\n",
      "iteration 3411: loss: 0.2411675900220871\n",
      "iteration 3412: loss: 0.2411653995513916\n",
      "iteration 3413: loss: 0.24116286635398865\n",
      "iteration 3414: loss: 0.24116039276123047\n",
      "iteration 3415: loss: 0.24115809798240662\n",
      "iteration 3416: loss: 0.24115557968616486\n",
      "iteration 3417: loss: 0.24115321040153503\n",
      "iteration 3418: loss: 0.24115097522735596\n",
      "iteration 3419: loss: 0.2411484718322754\n",
      "iteration 3420: loss: 0.24114593863487244\n",
      "iteration 3421: loss: 0.24114379286766052\n",
      "iteration 3422: loss: 0.24114131927490234\n",
      "iteration 3423: loss: 0.2411387860774994\n",
      "iteration 3424: loss: 0.24113628268241882\n",
      "iteration 3425: loss: 0.2411341667175293\n",
      "iteration 3426: loss: 0.2411317080259323\n",
      "iteration 3427: loss: 0.24112915992736816\n",
      "iteration 3428: loss: 0.2411269247531891\n",
      "iteration 3429: loss: 0.24112455546855927\n",
      "iteration 3430: loss: 0.2411220818758011\n",
      "iteration 3431: loss: 0.2411195933818817\n",
      "iteration 3432: loss: 0.2411174774169922\n",
      "iteration 3433: loss: 0.241115003824234\n",
      "iteration 3434: loss: 0.24111247062683105\n",
      "iteration 3435: loss: 0.241110160946846\n",
      "iteration 3436: loss: 0.24110789597034454\n",
      "iteration 3437: loss: 0.24110540747642517\n",
      "iteration 3438: loss: 0.24110308289527893\n",
      "iteration 3439: loss: 0.24110059440135956\n",
      "iteration 3440: loss: 0.24109835922718048\n",
      "iteration 3441: loss: 0.24109601974487305\n",
      "iteration 3442: loss: 0.24109354615211487\n",
      "iteration 3443: loss: 0.24109108746051788\n",
      "iteration 3444: loss: 0.24108895659446716\n",
      "iteration 3445: loss: 0.24108652770519257\n",
      "iteration 3446: loss: 0.24108414351940155\n",
      "iteration 3447: loss: 0.24108168482780457\n",
      "iteration 3448: loss: 0.24107936024665833\n",
      "iteration 3449: loss: 0.24107715487480164\n",
      "iteration 3450: loss: 0.24107468128204346\n",
      "iteration 3451: loss: 0.24107234179973602\n",
      "iteration 3452: loss: 0.24106991291046143\n",
      "iteration 3453: loss: 0.2410678118467331\n",
      "iteration 3454: loss: 0.2410653829574585\n",
      "iteration 3455: loss: 0.24106304347515106\n",
      "iteration 3456: loss: 0.24106064438819885\n",
      "iteration 3457: loss: 0.24105815589427948\n",
      "iteration 3458: loss: 0.24105587601661682\n",
      "iteration 3459: loss: 0.24105365574359894\n",
      "iteration 3460: loss: 0.2410513460636139\n",
      "iteration 3461: loss: 0.2410489022731781\n",
      "iteration 3462: loss: 0.24104662239551544\n",
      "iteration 3463: loss: 0.24104420840740204\n",
      "iteration 3464: loss: 0.2410418540239334\n",
      "iteration 3465: loss: 0.24103966355323792\n",
      "iteration 3466: loss: 0.24103736877441406\n",
      "iteration 3467: loss: 0.24103505909442902\n",
      "iteration 3468: loss: 0.2410326451063156\n",
      "iteration 3469: loss: 0.24103036522865295\n",
      "iteration 3470: loss: 0.24102792143821716\n",
      "iteration 3471: loss: 0.24102585017681122\n",
      "iteration 3472: loss: 0.241023451089859\n",
      "iteration 3473: loss: 0.24102115631103516\n",
      "iteration 3474: loss: 0.24101874232292175\n",
      "iteration 3475: loss: 0.2410164326429367\n",
      "iteration 3476: loss: 0.24101415276527405\n",
      "iteration 3477: loss: 0.24101176857948303\n",
      "iteration 3478: loss: 0.2410096824169159\n",
      "iteration 3479: loss: 0.24100728332996368\n",
      "iteration 3480: loss: 0.24100501835346222\n",
      "iteration 3481: loss: 0.24100272357463837\n",
      "iteration 3482: loss: 0.24100033938884735\n",
      "iteration 3483: loss: 0.2409980595111847\n",
      "iteration 3484: loss: 0.24099579453468323\n",
      "iteration 3485: loss: 0.24099335074424744\n",
      "iteration 3486: loss: 0.24099119007587433\n",
      "iteration 3487: loss: 0.24098877608776093\n",
      "iteration 3488: loss: 0.24098677933216095\n",
      "iteration 3489: loss: 0.2409844845533371\n",
      "iteration 3490: loss: 0.24098220467567444\n",
      "iteration 3491: loss: 0.24097983539104462\n",
      "iteration 3492: loss: 0.24097760021686554\n",
      "iteration 3493: loss: 0.2409753054380417\n",
      "iteration 3494: loss: 0.24097295105457306\n",
      "iteration 3495: loss: 0.2409706562757492\n",
      "iteration 3496: loss: 0.24096842110157013\n",
      "iteration 3497: loss: 0.2409660816192627\n",
      "iteration 3498: loss: 0.24096381664276123\n",
      "iteration 3499: loss: 0.24096155166625977\n",
      "iteration 3500: loss: 0.2409592866897583\n",
      "iteration 3501: loss: 0.2409571409225464\n",
      "iteration 3502: loss: 0.2409549206495285\n",
      "iteration 3503: loss: 0.24095268547534943\n",
      "iteration 3504: loss: 0.24095043540000916\n",
      "iteration 3505: loss: 0.24094805121421814\n",
      "iteration 3506: loss: 0.24094581604003906\n",
      "iteration 3507: loss: 0.24094359576702118\n",
      "iteration 3508: loss: 0.2409413605928421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3509: loss: 0.24093899130821228\n",
      "iteration 3510: loss: 0.2409367561340332\n",
      "iteration 3511: loss: 0.24093452095985413\n",
      "iteration 3512: loss: 0.24093230068683624\n",
      "iteration 3513: loss: 0.24093011021614075\n",
      "iteration 3514: loss: 0.24092774093151093\n",
      "iteration 3515: loss: 0.24092552065849304\n",
      "iteration 3516: loss: 0.24092328548431396\n",
      "iteration 3517: loss: 0.24092106521129608\n",
      "iteration 3518: loss: 0.240918830037117\n",
      "iteration 3519: loss: 0.2409166395664215\n",
      "iteration 3520: loss: 0.24091430008411407\n",
      "iteration 3521: loss: 0.2409120798110962\n",
      "iteration 3522: loss: 0.2409098595380783\n",
      "iteration 3523: loss: 0.24090754985809326\n",
      "iteration 3524: loss: 0.24090532958507538\n",
      "iteration 3525: loss: 0.2409031093120575\n",
      "iteration 3526: loss: 0.240900918841362\n",
      "iteration 3527: loss: 0.24089869856834412\n",
      "iteration 3528: loss: 0.24089637398719788\n",
      "iteration 3529: loss: 0.24089415371418\n",
      "iteration 3530: loss: 0.2408919632434845\n",
      "iteration 3531: loss: 0.2408897429704666\n",
      "iteration 3532: loss: 0.24088755249977112\n",
      "iteration 3533: loss: 0.24088537693023682\n",
      "iteration 3534: loss: 0.24088315665721893\n",
      "iteration 3535: loss: 0.24088096618652344\n",
      "iteration 3536: loss: 0.24087853729724884\n",
      "iteration 3537: loss: 0.24087634682655334\n",
      "iteration 3538: loss: 0.24087420105934143\n",
      "iteration 3539: loss: 0.24087195098400116\n",
      "iteration 3540: loss: 0.24086980521678925\n",
      "iteration 3541: loss: 0.24086761474609375\n",
      "iteration 3542: loss: 0.24086543917655945\n",
      "iteration 3543: loss: 0.24086324870586395\n",
      "iteration 3544: loss: 0.2408609390258789\n",
      "iteration 3545: loss: 0.2408587634563446\n",
      "iteration 3546: loss: 0.2408566027879715\n",
      "iteration 3547: loss: 0.2408544272184372\n",
      "iteration 3548: loss: 0.2408522367477417\n",
      "iteration 3549: loss: 0.24085021018981934\n",
      "iteration 3550: loss: 0.24084778130054474\n",
      "iteration 3551: loss: 0.24084563553333282\n",
      "iteration 3552: loss: 0.24084344506263733\n",
      "iteration 3553: loss: 0.24084126949310303\n",
      "iteration 3554: loss: 0.24083909392356873\n",
      "iteration 3555: loss: 0.2408369481563568\n",
      "iteration 3556: loss: 0.24083475768566132\n",
      "iteration 3557: loss: 0.2408326417207718\n",
      "iteration 3558: loss: 0.2408304661512375\n",
      "iteration 3559: loss: 0.24082830548286438\n",
      "iteration 3560: loss: 0.24082593619823456\n",
      "iteration 3561: loss: 0.24082374572753906\n",
      "iteration 3562: loss: 0.24082162976264954\n",
      "iteration 3563: loss: 0.24081948399543762\n",
      "iteration 3564: loss: 0.24081730842590332\n",
      "iteration 3565: loss: 0.2408151626586914\n",
      "iteration 3566: loss: 0.24081310629844666\n",
      "iteration 3567: loss: 0.24081075191497803\n",
      "iteration 3568: loss: 0.2408086061477661\n",
      "iteration 3569: loss: 0.2408064603805542\n",
      "iteration 3570: loss: 0.24080431461334229\n",
      "iteration 3571: loss: 0.24080216884613037\n",
      "iteration 3572: loss: 0.24080002307891846\n",
      "iteration 3573: loss: 0.24079802632331848\n",
      "iteration 3574: loss: 0.24079565703868866\n",
      "iteration 3575: loss: 0.24079351127147675\n",
      "iteration 3576: loss: 0.24079139530658722\n",
      "iteration 3577: loss: 0.2407892495393753\n",
      "iteration 3578: loss: 0.2407871037721634\n",
      "iteration 3579: loss: 0.24078500270843506\n",
      "iteration 3580: loss: 0.24078276753425598\n",
      "iteration 3581: loss: 0.24078062176704407\n",
      "iteration 3582: loss: 0.24077849090099335\n",
      "iteration 3583: loss: 0.24077637493610382\n",
      "iteration 3584: loss: 0.2407742440700531\n",
      "iteration 3585: loss: 0.2407720386981964\n",
      "iteration 3586: loss: 0.24076998233795166\n",
      "iteration 3587: loss: 0.24076786637306213\n",
      "iteration 3588: loss: 0.2407657355070114\n",
      "iteration 3589: loss: 0.24076373875141144\n",
      "iteration 3590: loss: 0.2407614290714264\n",
      "iteration 3591: loss: 0.24075929820537567\n",
      "iteration 3592: loss: 0.24075718224048615\n",
      "iteration 3593: loss: 0.24075517058372498\n",
      "iteration 3594: loss: 0.24075308442115784\n",
      "iteration 3595: loss: 0.2407507598400116\n",
      "iteration 3596: loss: 0.24074864387512207\n",
      "iteration 3597: loss: 0.2407466471195221\n",
      "iteration 3598: loss: 0.24074454605579376\n",
      "iteration 3599: loss: 0.24074223637580872\n",
      "iteration 3600: loss: 0.24074013531208038\n",
      "iteration 3601: loss: 0.2407381534576416\n",
      "iteration 3602: loss: 0.24073605239391327\n",
      "iteration 3603: loss: 0.24073395133018494\n",
      "iteration 3604: loss: 0.24073174595832825\n",
      "iteration 3605: loss: 0.2407296597957611\n",
      "iteration 3606: loss: 0.24072757363319397\n",
      "iteration 3607: loss: 0.24072560667991638\n",
      "iteration 3608: loss: 0.24072328209877014\n",
      "iteration 3609: loss: 0.24072115123271942\n",
      "iteration 3610: loss: 0.24071919918060303\n",
      "iteration 3611: loss: 0.2407170981168747\n",
      "iteration 3612: loss: 0.24071478843688965\n",
      "iteration 3613: loss: 0.24071285128593445\n",
      "iteration 3614: loss: 0.2407107651233673\n",
      "iteration 3615: loss: 0.24070855975151062\n",
      "iteration 3616: loss: 0.24070651829242706\n",
      "iteration 3617: loss: 0.24070441722869873\n",
      "iteration 3618: loss: 0.24070248007774353\n",
      "iteration 3619: loss: 0.2407001554965973\n",
      "iteration 3620: loss: 0.24069809913635254\n",
      "iteration 3621: loss: 0.24069614708423615\n",
      "iteration 3622: loss: 0.2406938374042511\n",
      "iteration 3623: loss: 0.2406919002532959\n",
      "iteration 3624: loss: 0.24068984389305115\n",
      "iteration 3625: loss: 0.24068789184093475\n",
      "iteration 3626: loss: 0.2406856119632721\n",
      "iteration 3627: loss: 0.24068352580070496\n",
      "iteration 3628: loss: 0.24068160355091095\n",
      "iteration 3629: loss: 0.2406793087720871\n",
      "iteration 3630: loss: 0.2406773567199707\n",
      "iteration 3631: loss: 0.24067530035972595\n",
      "iteration 3632: loss: 0.24067310988903046\n",
      "iteration 3633: loss: 0.2406710684299469\n",
      "iteration 3634: loss: 0.2406691312789917\n",
      "iteration 3635: loss: 0.24066689610481262\n",
      "iteration 3636: loss: 0.24066495895385742\n",
      "iteration 3637: loss: 0.24066290259361267\n",
      "iteration 3638: loss: 0.24066071212291718\n",
      "iteration 3639: loss: 0.24065867066383362\n",
      "iteration 3640: loss: 0.24065673351287842\n",
      "iteration 3641: loss: 0.24065446853637695\n",
      "iteration 3642: loss: 0.24065256118774414\n",
      "iteration 3643: loss: 0.2406505048274994\n",
      "iteration 3644: loss: 0.24064834415912628\n",
      "iteration 3645: loss: 0.24064631760120392\n",
      "iteration 3646: loss: 0.24064438045024872\n",
      "iteration 3647: loss: 0.24064214527606964\n",
      "iteration 3648: loss: 0.24064023792743683\n",
      "iteration 3649: loss: 0.24063794314861298\n",
      "iteration 3650: loss: 0.24063603579998016\n",
      "iteration 3651: loss: 0.2406340092420578\n",
      "iteration 3652: loss: 0.2406318634748459\n",
      "iteration 3653: loss: 0.24062982201576233\n",
      "iteration 3654: loss: 0.2406279295682907\n",
      "iteration 3655: loss: 0.24062581360340118\n",
      "iteration 3656: loss: 0.24062375724315643\n",
      "iteration 3657: loss: 0.2406216412782669\n",
      "iteration 3658: loss: 0.24061961472034454\n",
      "iteration 3659: loss: 0.24061772227287292\n",
      "iteration 3660: loss: 0.240615576505661\n",
      "iteration 3661: loss: 0.24061354994773865\n",
      "iteration 3662: loss: 0.24061143398284912\n",
      "iteration 3663: loss: 0.24060940742492676\n",
      "iteration 3664: loss: 0.24060750007629395\n",
      "iteration 3665: loss: 0.2406053990125656\n",
      "iteration 3666: loss: 0.24060337245464325\n",
      "iteration 3667: loss: 0.24060125648975372\n",
      "iteration 3668: loss: 0.2405993938446045\n",
      "iteration 3669: loss: 0.24059736728668213\n",
      "iteration 3670: loss: 0.2405952513217926\n",
      "iteration 3671: loss: 0.24059326946735382\n",
      "iteration 3672: loss: 0.2405911237001419\n",
      "iteration 3673: loss: 0.24058926105499268\n",
      "iteration 3674: loss: 0.2405869960784912\n",
      "iteration 3675: loss: 0.24058513343334198\n",
      "iteration 3676: loss: 0.24058327078819275\n",
      "iteration 3677: loss: 0.24058112502098083\n",
      "iteration 3678: loss: 0.24057915806770325\n",
      "iteration 3679: loss: 0.24057702720165253\n",
      "iteration 3680: loss: 0.24057519435882568\n",
      "iteration 3681: loss: 0.24057292938232422\n",
      "iteration 3682: loss: 0.24057109653949738\n",
      "iteration 3683: loss: 0.24056896567344666\n",
      "iteration 3684: loss: 0.24056699872016907\n",
      "iteration 3685: loss: 0.24056510627269745\n",
      "iteration 3686: loss: 0.24056300520896912\n",
      "iteration 3687: loss: 0.2405611276626587\n",
      "iteration 3688: loss: 0.2405589371919632\n",
      "iteration 3689: loss: 0.24055707454681396\n",
      "iteration 3690: loss: 0.24055495858192444\n",
      "iteration 3691: loss: 0.2405531406402588\n",
      "iteration 3692: loss: 0.2405509054660797\n",
      "iteration 3693: loss: 0.24054904282093048\n",
      "iteration 3694: loss: 0.24054697155952454\n",
      "iteration 3695: loss: 0.2405451238155365\n",
      "iteration 3696: loss: 0.24054288864135742\n",
      "iteration 3697: loss: 0.24054105579853058\n",
      "iteration 3698: loss: 0.24053892493247986\n",
      "iteration 3699: loss: 0.24053709208965302\n",
      "iteration 3700: loss: 0.24053506553173065\n",
      "iteration 3701: loss: 0.24053306877613068\n",
      "iteration 3702: loss: 0.24053099751472473\n",
      "iteration 3703: loss: 0.2405291348695755\n",
      "iteration 3704: loss: 0.24052703380584717\n",
      "iteration 3705: loss: 0.24052521586418152\n",
      "iteration 3706: loss: 0.24052301049232483\n",
      "iteration 3707: loss: 0.2405211627483368\n",
      "iteration 3708: loss: 0.24051912128925323\n",
      "iteration 3709: loss: 0.240517258644104\n",
      "iteration 3710: loss: 0.24051520228385925\n",
      "iteration 3711: loss: 0.2405133694410324\n",
      "iteration 3712: loss: 0.24051114916801453\n",
      "iteration 3713: loss: 0.24050939083099365\n",
      "iteration 3714: loss: 0.24050728976726532\n",
      "iteration 3715: loss: 0.24050521850585938\n",
      "iteration 3716: loss: 0.24050338566303253\n",
      "iteration 3717: loss: 0.24050132930278778\n",
      "iteration 3718: loss: 0.24049952626228333\n",
      "iteration 3719: loss: 0.2404974400997162\n",
      "iteration 3720: loss: 0.2404954880475998\n",
      "iteration 3721: loss: 0.24049344658851624\n",
      "iteration 3722: loss: 0.2404915988445282\n",
      "iteration 3723: loss: 0.24048955738544464\n",
      "iteration 3724: loss: 0.240487739443779\n",
      "iteration 3725: loss: 0.24048569798469543\n",
      "iteration 3726: loss: 0.24048367142677307\n",
      "iteration 3727: loss: 0.24048185348510742\n",
      "iteration 3728: loss: 0.24047978222370148\n",
      "iteration 3729: loss: 0.24047796428203583\n",
      "iteration 3730: loss: 0.24047593772411346\n",
      "iteration 3731: loss: 0.24047401547431946\n",
      "iteration 3732: loss: 0.24047192931175232\n",
      "iteration 3733: loss: 0.24046993255615234\n",
      "iteration 3734: loss: 0.2404680997133255\n",
      "iteration 3735: loss: 0.24046607315540314\n",
      "iteration 3736: loss: 0.24046428501605988\n",
      "iteration 3737: loss: 0.24046221375465393\n",
      "iteration 3738: loss: 0.24046018719673157\n",
      "iteration 3739: loss: 0.2404583990573883\n",
      "iteration 3740: loss: 0.24045634269714355\n",
      "iteration 3741: loss: 0.2404545247554779\n",
      "iteration 3742: loss: 0.24045249819755554\n",
      "iteration 3743: loss: 0.24045047163963318\n",
      "iteration 3744: loss: 0.24044868350028992\n",
      "iteration 3745: loss: 0.24044664204120636\n",
      "iteration 3746: loss: 0.2404448539018631\n",
      "iteration 3747: loss: 0.24044284224510193\n",
      "iteration 3748: loss: 0.24044081568717957\n",
      "iteration 3749: loss: 0.2404390126466751\n",
      "iteration 3750: loss: 0.24043698608875275\n",
      "iteration 3751: loss: 0.24043497443199158\n",
      "iteration 3752: loss: 0.24043317139148712\n",
      "iteration 3753: loss: 0.24043115973472595\n",
      "iteration 3754: loss: 0.2404293566942215\n",
      "iteration 3755: loss: 0.24042734503746033\n",
      "iteration 3756: loss: 0.24042531847953796\n",
      "iteration 3757: loss: 0.2404235601425171\n",
      "iteration 3758: loss: 0.2404215782880783\n",
      "iteration 3759: loss: 0.2404196560382843\n",
      "iteration 3760: loss: 0.24041788280010223\n",
      "iteration 3761: loss: 0.24041585624217987\n",
      "iteration 3762: loss: 0.2404138594865799\n",
      "iteration 3763: loss: 0.24041207134723663\n",
      "iteration 3764: loss: 0.24041005969047546\n",
      "iteration 3765: loss: 0.24040809273719788\n",
      "iteration 3766: loss: 0.2404063194990158\n",
      "iteration 3767: loss: 0.24040432274341583\n",
      "iteration 3768: loss: 0.24040229618549347\n",
      "iteration 3769: loss: 0.2404005527496338\n",
      "iteration 3770: loss: 0.24039852619171143\n",
      "iteration 3771: loss: 0.2403966635465622\n",
      "iteration 3772: loss: 0.24039487540721893\n",
      "iteration 3773: loss: 0.24039287865161896\n",
      "iteration 3774: loss: 0.2403908669948578\n",
      "iteration 3775: loss: 0.24038910865783691\n",
      "iteration 3776: loss: 0.24038711190223694\n",
      "iteration 3777: loss: 0.24038517475128174\n",
      "iteration 3778: loss: 0.24038353562355042\n",
      "iteration 3779: loss: 0.24038152396678925\n",
      "iteration 3780: loss: 0.24037949740886688\n",
      "iteration 3781: loss: 0.24037757515907288\n",
      "iteration 3782: loss: 0.2403758019208908\n",
      "iteration 3783: loss: 0.24037380516529083\n",
      "iteration 3784: loss: 0.24037179350852966\n",
      "iteration 3785: loss: 0.24037019908428192\n",
      "iteration 3786: loss: 0.24036821722984314\n",
      "iteration 3787: loss: 0.24036619067192078\n",
      "iteration 3788: loss: 0.2403644621372223\n",
      "iteration 3789: loss: 0.2403624951839447\n",
      "iteration 3790: loss: 0.24036064743995667\n",
      "iteration 3791: loss: 0.2403586357831955\n",
      "iteration 3792: loss: 0.2403569221496582\n",
      "iteration 3793: loss: 0.24035492539405823\n",
      "iteration 3794: loss: 0.24035295844078064\n",
      "iteration 3795: loss: 0.24035108089447021\n",
      "iteration 3796: loss: 0.24034938216209412\n",
      "iteration 3797: loss: 0.24034741520881653\n",
      "iteration 3798: loss: 0.24034544825553894\n",
      "iteration 3799: loss: 0.24034357070922852\n",
      "iteration 3800: loss: 0.24034185707569122\n",
      "iteration 3801: loss: 0.24033990502357483\n",
      "iteration 3802: loss: 0.24033789336681366\n",
      "iteration 3803: loss: 0.24033598601818085\n",
      "iteration 3804: loss: 0.24033434689044952\n",
      "iteration 3805: loss: 0.24033239483833313\n",
      "iteration 3806: loss: 0.24033042788505554\n",
      "iteration 3807: loss: 0.2403285950422287\n",
      "iteration 3808: loss: 0.24032683670520782\n",
      "iteration 3809: loss: 0.24032488465309143\n",
      "iteration 3810: loss: 0.24032294750213623\n",
      "iteration 3811: loss: 0.2403210699558258\n",
      "iteration 3812: loss: 0.2403193712234497\n",
      "iteration 3813: loss: 0.2403174340724945\n",
      "iteration 3814: loss: 0.2403154820203781\n",
      "iteration 3815: loss: 0.24031361937522888\n",
      "iteration 3816: loss: 0.2403119057416916\n",
      "iteration 3817: loss: 0.2403099536895752\n",
      "iteration 3818: loss: 0.24030813574790955\n",
      "iteration 3819: loss: 0.24030622839927673\n",
      "iteration 3820: loss: 0.24030426144599915\n",
      "iteration 3821: loss: 0.2403026521205902\n",
      "iteration 3822: loss: 0.2403007298707962\n",
      "iteration 3823: loss: 0.2402987778186798\n",
      "iteration 3824: loss: 0.24029693007469177\n",
      "iteration 3825: loss: 0.24029502272605896\n",
      "iteration 3826: loss: 0.24029330909252167\n",
      "iteration 3827: loss: 0.2402915060520172\n",
      "iteration 3828: loss: 0.24028953909873962\n",
      "iteration 3829: loss: 0.24028761684894562\n",
      "iteration 3830: loss: 0.24028579890727997\n",
      "iteration 3831: loss: 0.24028384685516357\n",
      "iteration 3832: loss: 0.24028214812278748\n",
      "iteration 3833: loss: 0.24028034508228302\n",
      "iteration 3834: loss: 0.24027840793132782\n",
      "iteration 3835: loss: 0.24027660489082336\n",
      "iteration 3836: loss: 0.24027466773986816\n",
      "iteration 3837: loss: 0.24027296900749207\n",
      "iteration 3838: loss: 0.2402711808681488\n",
      "iteration 3839: loss: 0.2402692288160324\n",
      "iteration 3840: loss: 0.24026742577552795\n",
      "iteration 3841: loss: 0.24026551842689514\n",
      "iteration 3842: loss: 0.24026358127593994\n",
      "iteration 3843: loss: 0.24026203155517578\n",
      "iteration 3844: loss: 0.24026009440422058\n",
      "iteration 3845: loss: 0.24025829136371613\n",
      "iteration 3846: loss: 0.2402563840150833\n",
      "iteration 3847: loss: 0.24025455117225647\n",
      "iteration 3848: loss: 0.24025264382362366\n",
      "iteration 3849: loss: 0.24025073647499084\n",
      "iteration 3850: loss: 0.24024918675422668\n",
      "iteration 3851: loss: 0.24024724960327148\n",
      "iteration 3852: loss: 0.2402454912662506\n",
      "iteration 3853: loss: 0.2402435839176178\n",
      "iteration 3854: loss: 0.24024176597595215\n",
      "iteration 3855: loss: 0.24023985862731934\n",
      "iteration 3856: loss: 0.24023795127868652\n",
      "iteration 3857: loss: 0.24023616313934326\n",
      "iteration 3858: loss: 0.24023449420928955\n",
      "iteration 3859: loss: 0.2402327060699463\n",
      "iteration 3860: loss: 0.2402307689189911\n",
      "iteration 3861: loss: 0.24022898077964783\n",
      "iteration 3862: loss: 0.2402271330356598\n",
      "iteration 3863: loss: 0.24022531509399414\n",
      "iteration 3864: loss: 0.24022343754768372\n",
      "iteration 3865: loss: 0.24022164940834045\n",
      "iteration 3866: loss: 0.24021971225738525\n",
      "iteration 3867: loss: 0.24021820724010468\n",
      "iteration 3868: loss: 0.24021628499031067\n",
      "iteration 3869: loss: 0.240214541554451\n",
      "iteration 3870: loss: 0.24021263420581818\n",
      "iteration 3871: loss: 0.2402108609676361\n",
      "iteration 3872: loss: 0.2402089536190033\n",
      "iteration 3873: loss: 0.24020716547966003\n",
      "iteration 3874: loss: 0.2402052879333496\n",
      "iteration 3875: loss: 0.24020352959632874\n",
      "iteration 3876: loss: 0.24020163714885712\n",
      "iteration 3877: loss: 0.24019987881183624\n",
      "iteration 3878: loss: 0.24019820988178253\n",
      "iteration 3879: loss: 0.24019646644592285\n",
      "iteration 3880: loss: 0.24019470810890198\n",
      "iteration 3881: loss: 0.24019280076026917\n",
      "iteration 3882: loss: 0.2401910275220871\n",
      "iteration 3883: loss: 0.24018914997577667\n",
      "iteration 3884: loss: 0.240187406539917\n",
      "iteration 3885: loss: 0.24018552899360657\n",
      "iteration 3886: loss: 0.2401837557554245\n",
      "iteration 3887: loss: 0.24018187820911407\n",
      "iteration 3888: loss: 0.240180104970932\n",
      "iteration 3889: loss: 0.24017834663391113\n",
      "iteration 3890: loss: 0.2401764690876007\n",
      "iteration 3891: loss: 0.24017472565174103\n",
      "iteration 3892: loss: 0.2401728332042694\n",
      "iteration 3893: loss: 0.24017110466957092\n",
      "iteration 3894: loss: 0.24016936123371124\n",
      "iteration 3895: loss: 0.24016746878623962\n",
      "iteration 3896: loss: 0.24016594886779785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3897: loss: 0.24016407132148743\n",
      "iteration 3898: loss: 0.24016232788562775\n",
      "iteration 3899: loss: 0.24016058444976807\n",
      "iteration 3900: loss: 0.24015870690345764\n",
      "iteration 3901: loss: 0.24015697836875916\n",
      "iteration 3902: loss: 0.24015524983406067\n",
      "iteration 3903: loss: 0.24015335738658905\n",
      "iteration 3904: loss: 0.24015161395072937\n",
      "iteration 3905: loss: 0.2401498556137085\n",
      "iteration 3906: loss: 0.24014802277088165\n",
      "iteration 3907: loss: 0.24014627933502197\n",
      "iteration 3908: loss: 0.24014440178871155\n",
      "iteration 3909: loss: 0.24014267325401306\n",
      "iteration 3910: loss: 0.24014094471931458\n",
      "iteration 3911: loss: 0.24013908207416534\n",
      "iteration 3912: loss: 0.24013736844062805\n",
      "iteration 3913: loss: 0.24013559520244598\n",
      "iteration 3914: loss: 0.24013376235961914\n",
      "iteration 3915: loss: 0.24013201892375946\n",
      "iteration 3916: loss: 0.2401302605867386\n",
      "iteration 3917: loss: 0.24012836813926697\n",
      "iteration 3918: loss: 0.24012665450572968\n",
      "iteration 3919: loss: 0.24012494087219238\n",
      "iteration 3920: loss: 0.2401231825351715\n",
      "iteration 3921: loss: 0.24012136459350586\n",
      "iteration 3922: loss: 0.24011962115764618\n",
      "iteration 3923: loss: 0.2401178777217865\n",
      "iteration 3924: loss: 0.24011607468128204\n",
      "iteration 3925: loss: 0.24011436104774475\n",
      "iteration 3926: loss: 0.24011261761188507\n",
      "iteration 3927: loss: 0.2401108741760254\n",
      "iteration 3928: loss: 0.24010905623435974\n",
      "iteration 3929: loss: 0.24010734260082245\n",
      "iteration 3930: loss: 0.24010539054870605\n",
      "iteration 3931: loss: 0.24010352790355682\n",
      "iteration 3932: loss: 0.24010178446769714\n",
      "iteration 3933: loss: 0.24010011553764343\n",
      "iteration 3934: loss: 0.24009838700294495\n",
      "iteration 3935: loss: 0.2400965690612793\n",
      "iteration 3936: loss: 0.240094855427742\n",
      "iteration 3937: loss: 0.24009311199188232\n",
      "iteration 3938: loss: 0.24009141325950623\n",
      "iteration 3939: loss: 0.24008958041667938\n",
      "iteration 3940: loss: 0.24008789658546448\n",
      "iteration 3941: loss: 0.240086168050766\n",
      "iteration 3942: loss: 0.2400844842195511\n",
      "iteration 3943: loss: 0.24008265137672424\n",
      "iteration 3944: loss: 0.24008092284202576\n",
      "iteration 3945: loss: 0.24007923901081085\n",
      "iteration 3946: loss: 0.24007752537727356\n",
      "iteration 3947: loss: 0.24007582664489746\n",
      "iteration 3948: loss: 0.2400738000869751\n",
      "iteration 3949: loss: 0.24007205665111542\n",
      "iteration 3950: loss: 0.2400703728199005\n",
      "iteration 3951: loss: 0.24006867408752441\n",
      "iteration 3952: loss: 0.24006697535514832\n",
      "iteration 3953: loss: 0.24006517231464386\n",
      "iteration 3954: loss: 0.24006345868110657\n",
      "iteration 3955: loss: 0.24006178975105286\n",
      "iteration 3956: loss: 0.24006006121635437\n",
      "iteration 3957: loss: 0.24005837738513947\n",
      "iteration 3958: loss: 0.2400565892457962\n",
      "iteration 3959: loss: 0.2400548756122589\n",
      "iteration 3960: loss: 0.2400529384613037\n",
      "iteration 3961: loss: 0.2400512397289276\n",
      "iteration 3962: loss: 0.2400495558977127\n",
      "iteration 3963: loss: 0.2400478571653366\n",
      "iteration 3964: loss: 0.2400461733341217\n",
      "iteration 3965: loss: 0.24004435539245605\n",
      "iteration 3966: loss: 0.24004268646240234\n",
      "iteration 3967: loss: 0.24004101753234863\n",
      "iteration 3968: loss: 0.24003931879997253\n",
      "iteration 3969: loss: 0.24003741145133972\n",
      "iteration 3970: loss: 0.24003572762012482\n",
      "iteration 3971: loss: 0.24003390967845917\n",
      "iteration 3972: loss: 0.24003219604492188\n",
      "iteration 3973: loss: 0.24003055691719055\n",
      "iteration 3974: loss: 0.24002888798713684\n",
      "iteration 3975: loss: 0.24002718925476074\n",
      "iteration 3976: loss: 0.24002552032470703\n",
      "iteration 3977: loss: 0.24002361297607422\n",
      "iteration 3978: loss: 0.2400219440460205\n",
      "iteration 3979: loss: 0.2400202751159668\n",
      "iteration 3980: loss: 0.24001845717430115\n",
      "iteration 3981: loss: 0.24001681804656982\n",
      "iteration 3982: loss: 0.24001511931419373\n",
      "iteration 3983: loss: 0.24001345038414001\n",
      "iteration 3984: loss: 0.2400115430355072\n",
      "iteration 3985: loss: 0.2400098592042923\n",
      "iteration 3986: loss: 0.24000820517539978\n",
      "iteration 3987: loss: 0.24000652134418488\n",
      "iteration 3988: loss: 0.24000486731529236\n",
      "iteration 3989: loss: 0.24000318348407745\n",
      "iteration 3990: loss: 0.24000152945518494\n",
      "iteration 3991: loss: 0.23999950289726257\n",
      "iteration 3992: loss: 0.23999786376953125\n",
      "iteration 3993: loss: 0.23999616503715515\n",
      "iteration 3994: loss: 0.23999452590942383\n",
      "iteration 3995: loss: 0.2399928867816925\n",
      "iteration 3996: loss: 0.2399911880493164\n",
      "iteration 3997: loss: 0.23998932540416718\n",
      "iteration 3998: loss: 0.23998768627643585\n",
      "iteration 3999: loss: 0.23998601734638214\n",
      "iteration 4000: loss: 0.23998431861400604\n",
      "iteration 4001: loss: 0.2399826943874359\n",
      "iteration 4002: loss: 0.2399808168411255\n",
      "iteration 4003: loss: 0.2399791181087494\n",
      "iteration 4004: loss: 0.23997747898101807\n",
      "iteration 4005: loss: 0.23997583985328674\n",
      "iteration 4006: loss: 0.23997418582439423\n",
      "iteration 4007: loss: 0.2399723082780838\n",
      "iteration 4008: loss: 0.2399706393480301\n",
      "iteration 4009: loss: 0.23996898531913757\n",
      "iteration 4010: loss: 0.23996734619140625\n",
      "iteration 4011: loss: 0.23996567726135254\n",
      "iteration 4012: loss: 0.2399638444185257\n",
      "iteration 4013: loss: 0.23996219038963318\n",
      "iteration 4014: loss: 0.23996052145957947\n",
      "iteration 4015: loss: 0.23995891213417053\n",
      "iteration 4016: loss: 0.23995725810527802\n",
      "iteration 4017: loss: 0.2399553805589676\n",
      "iteration 4018: loss: 0.23995371162891388\n",
      "iteration 4019: loss: 0.23995208740234375\n",
      "iteration 4020: loss: 0.23995046317577362\n",
      "iteration 4021: loss: 0.2399487942457199\n",
      "iteration 4022: loss: 0.23994693160057068\n",
      "iteration 4023: loss: 0.23994533717632294\n",
      "iteration 4024: loss: 0.23994365334510803\n",
      "iteration 4025: loss: 0.23994199931621552\n",
      "iteration 4026: loss: 0.23994016647338867\n",
      "iteration 4027: loss: 0.23993852734565735\n",
      "iteration 4028: loss: 0.23993687331676483\n",
      "iteration 4029: loss: 0.2399352341890335\n",
      "iteration 4030: loss: 0.23993340134620667\n",
      "iteration 4031: loss: 0.23993177711963654\n",
      "iteration 4032: loss: 0.23993012309074402\n",
      "iteration 4033: loss: 0.23992852866649628\n",
      "iteration 4034: loss: 0.23992685973644257\n",
      "iteration 4035: loss: 0.23992499709129333\n",
      "iteration 4036: loss: 0.23992343246936798\n",
      "iteration 4037: loss: 0.23992183804512024\n",
      "iteration 4038: loss: 0.239919975399971\n",
      "iteration 4039: loss: 0.2399183213710785\n",
      "iteration 4040: loss: 0.23991671204566956\n",
      "iteration 4041: loss: 0.23991508781909943\n",
      "iteration 4042: loss: 0.2399132251739502\n",
      "iteration 4043: loss: 0.23991160094738007\n",
      "iteration 4044: loss: 0.23990997672080994\n",
      "iteration 4045: loss: 0.2399083822965622\n",
      "iteration 4046: loss: 0.23990651965141296\n",
      "iteration 4047: loss: 0.23990492522716522\n",
      "iteration 4048: loss: 0.23990340530872345\n",
      "iteration 4049: loss: 0.23990178108215332\n",
      "iteration 4050: loss: 0.2398999184370041\n",
      "iteration 4051: loss: 0.23989832401275635\n",
      "iteration 4052: loss: 0.23989669978618622\n",
      "iteration 4053: loss: 0.239894837141037\n",
      "iteration 4054: loss: 0.23989324271678925\n",
      "iteration 4055: loss: 0.2398916482925415\n",
      "iteration 4056: loss: 0.23989014327526093\n",
      "iteration 4057: loss: 0.23988831043243408\n",
      "iteration 4058: loss: 0.23988668620586395\n",
      "iteration 4059: loss: 0.2398850917816162\n",
      "iteration 4060: loss: 0.23988325893878937\n",
      "iteration 4061: loss: 0.23988163471221924\n",
      "iteration 4062: loss: 0.2398800402879715\n",
      "iteration 4063: loss: 0.23987844586372375\n",
      "iteration 4064: loss: 0.23987670242786407\n",
      "iteration 4065: loss: 0.23987512290477753\n",
      "iteration 4066: loss: 0.2398734986782074\n",
      "iteration 4067: loss: 0.23987166583538055\n",
      "iteration 4068: loss: 0.2398700714111328\n",
      "iteration 4069: loss: 0.23986844718456268\n",
      "iteration 4070: loss: 0.23986676335334778\n",
      "iteration 4071: loss: 0.23986518383026123\n",
      "iteration 4072: loss: 0.2398635596036911\n",
      "iteration 4073: loss: 0.23986172676086426\n",
      "iteration 4074: loss: 0.23986010253429413\n",
      "iteration 4075: loss: 0.23985853791236877\n",
      "iteration 4076: loss: 0.2398568093776703\n",
      "iteration 4077: loss: 0.23985521495342255\n",
      "iteration 4078: loss: 0.2398536652326584\n",
      "iteration 4079: loss: 0.23985183238983154\n",
      "iteration 4080: loss: 0.2398502081632614\n",
      "iteration 4081: loss: 0.2398487627506256\n",
      "iteration 4082: loss: 0.23984694480895996\n",
      "iteration 4083: loss: 0.23984535038471222\n",
      "iteration 4084: loss: 0.23984375596046448\n",
      "iteration 4085: loss: 0.23984189331531525\n",
      "iteration 4086: loss: 0.2398403137922287\n",
      "iteration 4087: loss: 0.23983891308307648\n",
      "iteration 4088: loss: 0.23983708024024963\n",
      "iteration 4089: loss: 0.2398354709148407\n",
      "iteration 4090: loss: 0.23983387649059296\n",
      "iteration 4091: loss: 0.23983219265937805\n",
      "iteration 4092: loss: 0.23983058333396912\n",
      "iteration 4093: loss: 0.23982901871204376\n",
      "iteration 4094: loss: 0.2398272305727005\n",
      "iteration 4095: loss: 0.23982563614845276\n",
      "iteration 4096: loss: 0.23982396721839905\n",
      "iteration 4097: loss: 0.2398223578929901\n",
      "iteration 4098: loss: 0.23982076346874237\n",
      "iteration 4099: loss: 0.2398189753293991\n",
      "iteration 4100: loss: 0.23981750011444092\n",
      "iteration 4101: loss: 0.23981590569019318\n",
      "iteration 4102: loss: 0.2398141622543335\n",
      "iteration 4103: loss: 0.23981256783008575\n",
      "iteration 4104: loss: 0.23981086909770966\n",
      "iteration 4105: loss: 0.2398093193769455\n",
      "iteration 4106: loss: 0.23980772495269775\n",
      "iteration 4107: loss: 0.2398058921098709\n",
      "iteration 4108: loss: 0.2398044764995575\n",
      "iteration 4109: loss: 0.23980288207530975\n",
      "iteration 4110: loss: 0.2398010939359665\n",
      "iteration 4111: loss: 0.2397996485233307\n",
      "iteration 4112: loss: 0.23979786038398743\n",
      "iteration 4113: loss: 0.23979628086090088\n",
      "iteration 4114: loss: 0.23979470133781433\n",
      "iteration 4115: loss: 0.239793062210083\n",
      "iteration 4116: loss: 0.23979148268699646\n",
      "iteration 4117: loss: 0.239789679646492\n",
      "iteration 4118: loss: 0.2397882491350174\n",
      "iteration 4119: loss: 0.23978666961193085\n",
      "iteration 4120: loss: 0.23978488147258759\n",
      "iteration 4121: loss: 0.23978331685066223\n",
      "iteration 4122: loss: 0.23978166282176971\n",
      "iteration 4123: loss: 0.23978009819984436\n",
      "iteration 4124: loss: 0.2397782802581787\n",
      "iteration 4125: loss: 0.23977677524089813\n",
      "iteration 4126: loss: 0.23977522552013397\n",
      "iteration 4127: loss: 0.2397734373807907\n",
      "iteration 4128: loss: 0.2397720068693161\n",
      "iteration 4129: loss: 0.23977024853229523\n",
      "iteration 4130: loss: 0.23976866900920868\n",
      "iteration 4131: loss: 0.23976722359657288\n",
      "iteration 4132: loss: 0.2397654503583908\n",
      "iteration 4133: loss: 0.23976390063762665\n",
      "iteration 4134: loss: 0.23976226150989532\n",
      "iteration 4135: loss: 0.23976068198680878\n",
      "iteration 4136: loss: 0.23975889384746552\n",
      "iteration 4137: loss: 0.2397574633359909\n",
      "iteration 4138: loss: 0.23975570499897003\n",
      "iteration 4139: loss: 0.23975412547588348\n",
      "iteration 4140: loss: 0.23975272476673126\n",
      "iteration 4141: loss: 0.2397509515285492\n",
      "iteration 4142: loss: 0.23974940180778503\n",
      "iteration 4143: loss: 0.2397477924823761\n",
      "iteration 4144: loss: 0.23974621295928955\n",
      "iteration 4145: loss: 0.2397444248199463\n",
      "iteration 4146: loss: 0.23974302411079407\n",
      "iteration 4147: loss: 0.239741250872612\n",
      "iteration 4148: loss: 0.2397398054599762\n",
      "iteration 4149: loss: 0.23973803222179413\n",
      "iteration 4150: loss: 0.23973648250102997\n",
      "iteration 4151: loss: 0.23973509669303894\n",
      "iteration 4152: loss: 0.23973329365253448\n",
      "iteration 4153: loss: 0.2397317886352539\n",
      "iteration 4154: loss: 0.23973014950752258\n",
      "iteration 4155: loss: 0.23972859978675842\n",
      "iteration 4156: loss: 0.2397269755601883\n",
      "iteration 4157: loss: 0.23972539603710175\n",
      "iteration 4158: loss: 0.23972365260124207\n",
      "iteration 4159: loss: 0.23972225189208984\n",
      "iteration 4160: loss: 0.2397204339504242\n",
      "iteration 4161: loss: 0.2397189438343048\n",
      "iteration 4162: loss: 0.2397172898054123\n",
      "iteration 4163: loss: 0.23971574008464813\n",
      "iteration 4164: loss: 0.2397141456604004\n",
      "iteration 4165: loss: 0.23971259593963623\n",
      "iteration 4166: loss: 0.2397109568119049\n",
      "iteration 4167: loss: 0.23970942199230194\n",
      "iteration 4168: loss: 0.23970766365528107\n",
      "iteration 4169: loss: 0.23970623314380646\n",
      "iteration 4170: loss: 0.23970451951026917\n",
      "iteration 4171: loss: 0.23970308899879456\n",
      "iteration 4172: loss: 0.2397013157606125\n",
      "iteration 4173: loss: 0.23969995975494385\n",
      "iteration 4174: loss: 0.23969817161560059\n",
      "iteration 4175: loss: 0.23969666659832\n",
      "iteration 4176: loss: 0.23969502747058868\n",
      "iteration 4177: loss: 0.2396935224533081\n",
      "iteration 4178: loss: 0.23969188332557678\n",
      "iteration 4179: loss: 0.23969034850597382\n",
      "iteration 4180: loss: 0.2396887242794037\n",
      "iteration 4181: loss: 0.2396872341632843\n",
      "iteration 4182: loss: 0.23968544602394104\n",
      "iteration 4183: loss: 0.23968406021595\n",
      "iteration 4184: loss: 0.23968231678009033\n",
      "iteration 4185: loss: 0.2396809309720993\n",
      "iteration 4186: loss: 0.23967917263507843\n",
      "iteration 4187: loss: 0.2396778166294098\n",
      "iteration 4188: loss: 0.23967602849006653\n",
      "iteration 4189: loss: 0.2396746575832367\n",
      "iteration 4190: loss: 0.2396729290485382\n",
      "iteration 4191: loss: 0.23967154324054718\n",
      "iteration 4192: loss: 0.2396697998046875\n",
      "iteration 4193: loss: 0.23966816067695618\n",
      "iteration 4194: loss: 0.2396666556596756\n",
      "iteration 4195: loss: 0.23966503143310547\n",
      "iteration 4196: loss: 0.2396635264158249\n",
      "iteration 4197: loss: 0.23966176807880402\n",
      "iteration 4198: loss: 0.239660382270813\n",
      "iteration 4199: loss: 0.2396586388349533\n",
      "iteration 4200: loss: 0.23965728282928467\n",
      "iteration 4201: loss: 0.239655539393425\n",
      "iteration 4202: loss: 0.23965410888195038\n",
      "iteration 4203: loss: 0.2396523654460907\n",
      "iteration 4204: loss: 0.23965077102184296\n",
      "iteration 4205: loss: 0.23964925110340118\n",
      "iteration 4206: loss: 0.23964765667915344\n",
      "iteration 4207: loss: 0.23964615166187286\n",
      "iteration 4208: loss: 0.23964452743530273\n",
      "iteration 4209: loss: 0.23964305222034454\n",
      "iteration 4210: loss: 0.23964139819145203\n",
      "iteration 4211: loss: 0.23963971436023712\n",
      "iteration 4212: loss: 0.2396383285522461\n",
      "iteration 4213: loss: 0.2396365851163864\n",
      "iteration 4214: loss: 0.23963522911071777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4215: loss: 0.23963363468647003\n",
      "iteration 4216: loss: 0.23963215947151184\n",
      "iteration 4217: loss: 0.23963050544261932\n",
      "iteration 4218: loss: 0.23962879180908203\n",
      "iteration 4219: loss: 0.239627406001091\n",
      "iteration 4220: loss: 0.2396257221698761\n",
      "iteration 4221: loss: 0.23962433636188507\n",
      "iteration 4222: loss: 0.23962262272834778\n",
      "iteration 4223: loss: 0.23962101340293884\n",
      "iteration 4224: loss: 0.23961953818798065\n",
      "iteration 4225: loss: 0.23961791396141052\n",
      "iteration 4226: loss: 0.23961642384529114\n",
      "iteration 4227: loss: 0.2396148145198822\n",
      "iteration 4228: loss: 0.2396133691072464\n",
      "iteration 4229: loss: 0.23961177468299866\n",
      "iteration 4230: loss: 0.23961015045642853\n",
      "iteration 4231: loss: 0.23960867524147034\n",
      "iteration 4232: loss: 0.2396070659160614\n",
      "iteration 4233: loss: 0.2396055907011032\n",
      "iteration 4234: loss: 0.23960399627685547\n",
      "iteration 4235: loss: 0.23960228264331818\n",
      "iteration 4236: loss: 0.23960094153881073\n",
      "iteration 4237: loss: 0.239599347114563\n",
      "iteration 4238: loss: 0.2395976483821869\n",
      "iteration 4239: loss: 0.23959627747535706\n",
      "iteration 4240: loss: 0.23959460854530334\n",
      "iteration 4241: loss: 0.2395932376384735\n",
      "iteration 4242: loss: 0.23959150910377502\n",
      "iteration 4243: loss: 0.2395898997783661\n",
      "iteration 4244: loss: 0.23958854377269745\n",
      "iteration 4245: loss: 0.23958687484264374\n",
      "iteration 4246: loss: 0.239585280418396\n",
      "iteration 4247: loss: 0.239583820104599\n",
      "iteration 4248: loss: 0.23958222568035126\n",
      "iteration 4249: loss: 0.23958086967468262\n",
      "iteration 4250: loss: 0.23957917094230652\n",
      "iteration 4251: loss: 0.23957757651805878\n",
      "iteration 4252: loss: 0.23957614600658417\n",
      "iteration 4253: loss: 0.23957455158233643\n",
      "iteration 4254: loss: 0.23957295715808868\n",
      "iteration 4255: loss: 0.2395714819431305\n",
      "iteration 4256: loss: 0.23956993222236633\n",
      "iteration 4257: loss: 0.23956820368766785\n",
      "iteration 4258: loss: 0.2395668774843216\n",
      "iteration 4259: loss: 0.23956529796123505\n",
      "iteration 4260: loss: 0.23956385254859924\n",
      "iteration 4261: loss: 0.2395622730255127\n",
      "iteration 4262: loss: 0.2395605742931366\n",
      "iteration 4263: loss: 0.23955921828746796\n",
      "iteration 4264: loss: 0.2395576536655426\n",
      "iteration 4265: loss: 0.2395559847354889\n",
      "iteration 4266: loss: 0.23955461382865906\n",
      "iteration 4267: loss: 0.2395530641078949\n",
      "iteration 4268: loss: 0.2395513504743576\n",
      "iteration 4269: loss: 0.23955002427101135\n",
      "iteration 4270: loss: 0.23954835534095764\n",
      "iteration 4271: loss: 0.23954670131206512\n",
      "iteration 4272: loss: 0.23954539000988007\n",
      "iteration 4273: loss: 0.23954367637634277\n",
      "iteration 4274: loss: 0.2395421266555786\n",
      "iteration 4275: loss: 0.23954077064990997\n",
      "iteration 4276: loss: 0.23953910171985626\n",
      "iteration 4277: loss: 0.23953752219676971\n",
      "iteration 4278: loss: 0.23953619599342346\n",
      "iteration 4279: loss: 0.23953449726104736\n",
      "iteration 4280: loss: 0.2395329475402832\n",
      "iteration 4281: loss: 0.23953163623809814\n",
      "iteration 4282: loss: 0.23952993750572205\n",
      "iteration 4283: loss: 0.23952838778495789\n",
      "iteration 4284: loss: 0.23952679336071014\n",
      "iteration 4285: loss: 0.23952536284923553\n",
      "iteration 4286: loss: 0.23952381312847137\n",
      "iteration 4287: loss: 0.23952226340770721\n",
      "iteration 4288: loss: 0.23952078819274902\n",
      "iteration 4289: loss: 0.23951926827430725\n",
      "iteration 4290: loss: 0.2395177185535431\n",
      "iteration 4291: loss: 0.2395162284374237\n",
      "iteration 4292: loss: 0.23951467871665955\n",
      "iteration 4293: loss: 0.23951312899589539\n",
      "iteration 4294: loss: 0.23951148986816406\n",
      "iteration 4295: loss: 0.23951014876365662\n",
      "iteration 4296: loss: 0.23950859904289246\n",
      "iteration 4297: loss: 0.23950691521167755\n",
      "iteration 4298: loss: 0.2395056039094925\n",
      "iteration 4299: loss: 0.23950405418872833\n",
      "iteration 4300: loss: 0.23950251936912537\n",
      "iteration 4301: loss: 0.23950085043907166\n",
      "iteration 4302: loss: 0.2394995391368866\n",
      "iteration 4303: loss: 0.23949798941612244\n",
      "iteration 4304: loss: 0.23949632048606873\n",
      "iteration 4305: loss: 0.23949499428272247\n",
      "iteration 4306: loss: 0.2394934594631195\n",
      "iteration 4307: loss: 0.2394917756319046\n",
      "iteration 4308: loss: 0.23949024081230164\n",
      "iteration 4309: loss: 0.23948891460895538\n",
      "iteration 4310: loss: 0.2394874095916748\n",
      "iteration 4311: loss: 0.2394857406616211\n",
      "iteration 4312: loss: 0.23948442935943604\n",
      "iteration 4313: loss: 0.23948287963867188\n",
      "iteration 4314: loss: 0.23948121070861816\n",
      "iteration 4315: loss: 0.239479660987854\n",
      "iteration 4316: loss: 0.23947837948799133\n",
      "iteration 4317: loss: 0.23947684466838837\n",
      "iteration 4318: loss: 0.23947516083717346\n",
      "iteration 4319: loss: 0.2394736260175705\n",
      "iteration 4320: loss: 0.23947231471538544\n",
      "iteration 4321: loss: 0.23947079479694366\n",
      "iteration 4322: loss: 0.23946912586688995\n",
      "iteration 4323: loss: 0.23946762084960938\n",
      "iteration 4324: loss: 0.23946630954742432\n",
      "iteration 4325: loss: 0.239464670419693\n",
      "iteration 4326: loss: 0.23946312069892883\n",
      "iteration 4327: loss: 0.23946158587932587\n",
      "iteration 4328: loss: 0.2394603043794632\n",
      "iteration 4329: loss: 0.23945863544940948\n",
      "iteration 4330: loss: 0.23945708572864532\n",
      "iteration 4331: loss: 0.23945558071136475\n",
      "iteration 4332: loss: 0.23945406079292297\n",
      "iteration 4333: loss: 0.23945274949073792\n",
      "iteration 4334: loss: 0.23945105075836182\n",
      "iteration 4335: loss: 0.23944953083992004\n",
      "iteration 4336: loss: 0.23944798111915588\n",
      "iteration 4337: loss: 0.23944666981697083\n",
      "iteration 4338: loss: 0.2394450604915619\n",
      "iteration 4339: loss: 0.23944354057312012\n",
      "iteration 4340: loss: 0.23944196105003357\n",
      "iteration 4341: loss: 0.2394407093524933\n",
      "iteration 4342: loss: 0.23943908512592316\n",
      "iteration 4343: loss: 0.239437535405159\n",
      "iteration 4344: loss: 0.23943598568439484\n",
      "iteration 4345: loss: 0.23943448066711426\n",
      "iteration 4346: loss: 0.2394331991672516\n",
      "iteration 4347: loss: 0.23943157494068146\n",
      "iteration 4348: loss: 0.2394299954175949\n",
      "iteration 4349: loss: 0.23942860960960388\n",
      "iteration 4350: loss: 0.2394271194934845\n",
      "iteration 4351: loss: 0.23942570388317108\n",
      "iteration 4352: loss: 0.2394241839647293\n",
      "iteration 4353: loss: 0.23942270874977112\n",
      "iteration 4354: loss: 0.23942117393016815\n",
      "iteration 4355: loss: 0.23941965401172638\n",
      "iteration 4356: loss: 0.23941826820373535\n",
      "iteration 4357: loss: 0.2394167184829712\n",
      "iteration 4358: loss: 0.2394152134656906\n",
      "iteration 4359: loss: 0.23941370844841003\n",
      "iteration 4360: loss: 0.23941218852996826\n",
      "iteration 4361: loss: 0.23941078782081604\n",
      "iteration 4362: loss: 0.23940928280353546\n",
      "iteration 4363: loss: 0.23940777778625488\n",
      "iteration 4364: loss: 0.2394062727689743\n",
      "iteration 4365: loss: 0.23940476775169373\n",
      "iteration 4366: loss: 0.2394033670425415\n",
      "iteration 4367: loss: 0.23940186202526093\n",
      "iteration 4368: loss: 0.23940035700798035\n",
      "iteration 4369: loss: 0.23939886689186096\n",
      "iteration 4370: loss: 0.239397332072258\n",
      "iteration 4371: loss: 0.23939570784568787\n",
      "iteration 4372: loss: 0.23939447104930878\n",
      "iteration 4373: loss: 0.23939292132854462\n",
      "iteration 4374: loss: 0.23939141631126404\n",
      "iteration 4375: loss: 0.23938992619514465\n",
      "iteration 4376: loss: 0.23938842117786407\n",
      "iteration 4377: loss: 0.23938682675361633\n",
      "iteration 4378: loss: 0.23938553035259247\n",
      "iteration 4379: loss: 0.2393840253353119\n",
      "iteration 4380: loss: 0.2393825501203537\n",
      "iteration 4381: loss: 0.23938104510307312\n",
      "iteration 4382: loss: 0.23937955498695374\n",
      "iteration 4383: loss: 0.2393779307603836\n",
      "iteration 4384: loss: 0.23937663435935974\n",
      "iteration 4385: loss: 0.23937515914440155\n",
      "iteration 4386: loss: 0.23937365412712097\n",
      "iteration 4387: loss: 0.23937217891216278\n",
      "iteration 4388: loss: 0.2393706738948822\n",
      "iteration 4389: loss: 0.23936906456947327\n",
      "iteration 4390: loss: 0.23936781287193298\n",
      "iteration 4391: loss: 0.2393662929534912\n",
      "iteration 4392: loss: 0.23936481773853302\n",
      "iteration 4393: loss: 0.23936334252357483\n",
      "iteration 4394: loss: 0.23936176300048828\n",
      "iteration 4395: loss: 0.2393602877855301\n",
      "iteration 4396: loss: 0.23935875296592712\n",
      "iteration 4397: loss: 0.23935715854167938\n",
      "iteration 4398: loss: 0.2393558770418167\n",
      "iteration 4399: loss: 0.23935441672801971\n",
      "iteration 4400: loss: 0.23935294151306152\n",
      "iteration 4401: loss: 0.23935143649578094\n",
      "iteration 4402: loss: 0.23934996128082275\n",
      "iteration 4403: loss: 0.23934844136238098\n",
      "iteration 4404: loss: 0.23934686183929443\n",
      "iteration 4405: loss: 0.23934559524059296\n",
      "iteration 4406: loss: 0.23934414982795715\n",
      "iteration 4407: loss: 0.23934265971183777\n",
      "iteration 4408: loss: 0.23934116959571838\n",
      "iteration 4409: loss: 0.2393396645784378\n",
      "iteration 4410: loss: 0.23933818936347961\n",
      "iteration 4411: loss: 0.23933672904968262\n",
      "iteration 4412: loss: 0.2393351048231125\n",
      "iteration 4413: loss: 0.2393335998058319\n",
      "iteration 4414: loss: 0.23933236300945282\n",
      "iteration 4415: loss: 0.23933088779449463\n",
      "iteration 4416: loss: 0.23932942748069763\n",
      "iteration 4417: loss: 0.23932795226573944\n",
      "iteration 4418: loss: 0.23932647705078125\n",
      "iteration 4419: loss: 0.23932500183582306\n",
      "iteration 4420: loss: 0.23932352662086487\n",
      "iteration 4421: loss: 0.23932191729545593\n",
      "iteration 4422: loss: 0.23932044208049774\n",
      "iteration 4423: loss: 0.23931920528411865\n",
      "iteration 4424: loss: 0.23931773006916046\n",
      "iteration 4425: loss: 0.23931626975536346\n",
      "iteration 4426: loss: 0.23931479454040527\n",
      "iteration 4427: loss: 0.23931331932544708\n",
      "iteration 4428: loss: 0.2393118441104889\n",
      "iteration 4429: loss: 0.2393103539943695\n",
      "iteration 4430: loss: 0.2393089085817337\n",
      "iteration 4431: loss: 0.2393074482679367\n",
      "iteration 4432: loss: 0.23930582404136658\n",
      "iteration 4433: loss: 0.2393043488264084\n",
      "iteration 4434: loss: 0.23930314183235168\n",
      "iteration 4435: loss: 0.2393016815185547\n",
      "iteration 4436: loss: 0.2393001765012741\n",
      "iteration 4437: loss: 0.2392987310886383\n",
      "iteration 4438: loss: 0.2392972707748413\n",
      "iteration 4439: loss: 0.23929579555988312\n",
      "iteration 4440: loss: 0.23929433524608612\n",
      "iteration 4441: loss: 0.23929286003112793\n",
      "iteration 4442: loss: 0.23929138481616974\n",
      "iteration 4443: loss: 0.23928995430469513\n",
      "iteration 4444: loss: 0.23928852379322052\n",
      "iteration 4445: loss: 0.23928704857826233\n",
      "iteration 4446: loss: 0.2392854243516922\n",
      "iteration 4447: loss: 0.23928415775299072\n",
      "iteration 4448: loss: 0.2392827570438385\n",
      "iteration 4449: loss: 0.2392812967300415\n",
      "iteration 4450: loss: 0.23927979171276093\n",
      "iteration 4451: loss: 0.23927836120128632\n",
      "iteration 4452: loss: 0.23927685618400574\n",
      "iteration 4453: loss: 0.23927536606788635\n",
      "iteration 4454: loss: 0.23927395045757294\n",
      "iteration 4455: loss: 0.23927247524261475\n",
      "iteration 4456: loss: 0.23927101492881775\n",
      "iteration 4457: loss: 0.23926956951618195\n",
      "iteration 4458: loss: 0.23926810920238495\n",
      "iteration 4459: loss: 0.23926666378974915\n",
      "iteration 4460: loss: 0.23926520347595215\n",
      "iteration 4461: loss: 0.23926374316215515\n",
      "iteration 4462: loss: 0.23926229774951935\n",
      "iteration 4463: loss: 0.23926083743572235\n",
      "iteration 4464: loss: 0.239259272813797\n",
      "iteration 4465: loss: 0.2392577826976776\n",
      "iteration 4466: loss: 0.239256352186203\n",
      "iteration 4467: loss: 0.2392549216747284\n",
      "iteration 4468: loss: 0.2392536848783493\n",
      "iteration 4469: loss: 0.2392522543668747\n",
      "iteration 4470: loss: 0.2392508089542389\n",
      "iteration 4471: loss: 0.2392493486404419\n",
      "iteration 4472: loss: 0.23924794793128967\n",
      "iteration 4473: loss: 0.2392464578151703\n",
      "iteration 4474: loss: 0.23924501240253448\n",
      "iteration 4475: loss: 0.23924359679222107\n",
      "iteration 4476: loss: 0.23924215137958527\n",
      "iteration 4477: loss: 0.23924072086811066\n",
      "iteration 4478: loss: 0.23923926055431366\n",
      "iteration 4479: loss: 0.23923781514167786\n",
      "iteration 4480: loss: 0.23923635482788086\n",
      "iteration 4481: loss: 0.23923492431640625\n",
      "iteration 4482: loss: 0.23923346400260925\n",
      "iteration 4483: loss: 0.23923203349113464\n",
      "iteration 4484: loss: 0.23923063278198242\n",
      "iteration 4485: loss: 0.2392292022705078\n",
      "iteration 4486: loss: 0.23922772705554962\n",
      "iteration 4487: loss: 0.2392263114452362\n",
      "iteration 4488: loss: 0.23922483623027802\n",
      "iteration 4489: loss: 0.2392234355211258\n",
      "iteration 4490: loss: 0.23922200500965118\n",
      "iteration 4491: loss: 0.2392205446958542\n",
      "iteration 4492: loss: 0.23921909928321838\n",
      "iteration 4493: loss: 0.23921766877174377\n",
      "iteration 4494: loss: 0.23921623826026917\n",
      "iteration 4495: loss: 0.23921477794647217\n",
      "iteration 4496: loss: 0.23921337723731995\n",
      "iteration 4497: loss: 0.23921191692352295\n",
      "iteration 4498: loss: 0.23921051621437073\n",
      "iteration 4499: loss: 0.23920908570289612\n",
      "iteration 4500: loss: 0.23920759558677673\n",
      "iteration 4501: loss: 0.23920616507530212\n",
      "iteration 4502: loss: 0.2392047941684723\n",
      "iteration 4503: loss: 0.23920336365699768\n",
      "iteration 4504: loss: 0.23920193314552307\n",
      "iteration 4505: loss: 0.23920050263404846\n",
      "iteration 4506: loss: 0.23919907212257385\n",
      "iteration 4507: loss: 0.23919756710529327\n",
      "iteration 4508: loss: 0.23919613659381866\n",
      "iteration 4509: loss: 0.23919470608234406\n",
      "iteration 4510: loss: 0.23919329047203064\n",
      "iteration 4511: loss: 0.23919185996055603\n",
      "iteration 4512: loss: 0.23919042944908142\n",
      "iteration 4513: loss: 0.2391889989376068\n",
      "iteration 4514: loss: 0.2391875982284546\n",
      "iteration 4515: loss: 0.23918619751930237\n",
      "iteration 4516: loss: 0.23918476700782776\n",
      "iteration 4517: loss: 0.23918330669403076\n",
      "iteration 4518: loss: 0.23918190598487854\n",
      "iteration 4519: loss: 0.2391802817583084\n",
      "iteration 4520: loss: 0.2391788214445114\n",
      "iteration 4521: loss: 0.2391773909330368\n",
      "iteration 4522: loss: 0.23917600512504578\n",
      "iteration 4523: loss: 0.23917455971240997\n",
      "iteration 4524: loss: 0.23917317390441895\n",
      "iteration 4525: loss: 0.23917174339294434\n",
      "iteration 4526: loss: 0.23917034268379211\n",
      "iteration 4527: loss: 0.2391689270734787\n",
      "iteration 4528: loss: 0.23916760087013245\n",
      "iteration 4529: loss: 0.23916621506214142\n",
      "iteration 4530: loss: 0.2391647845506668\n",
      "iteration 4531: loss: 0.2391633540391922\n",
      "iteration 4532: loss: 0.23916196823120117\n",
      "iteration 4533: loss: 0.23916053771972656\n",
      "iteration 4534: loss: 0.23915913701057434\n",
      "iteration 4535: loss: 0.23915770649909973\n",
      "iteration 4536: loss: 0.2391563206911087\n",
      "iteration 4537: loss: 0.23915493488311768\n",
      "iteration 4538: loss: 0.23915347456932068\n",
      "iteration 4539: loss: 0.23915183544158936\n",
      "iteration 4540: loss: 0.23915043473243713\n",
      "iteration 4541: loss: 0.23914901912212372\n",
      "iteration 4542: loss: 0.2391476184129715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4543: loss: 0.23914620280265808\n",
      "iteration 4544: loss: 0.23914480209350586\n",
      "iteration 4545: loss: 0.23914341628551483\n",
      "iteration 4546: loss: 0.23914198577404022\n",
      "iteration 4547: loss: 0.23914071917533875\n",
      "iteration 4548: loss: 0.23913931846618652\n",
      "iteration 4549: loss: 0.23913788795471191\n",
      "iteration 4550: loss: 0.2391365021467209\n",
      "iteration 4551: loss: 0.23913510143756866\n",
      "iteration 4552: loss: 0.23913368582725525\n",
      "iteration 4553: loss: 0.23913225531578064\n",
      "iteration 4554: loss: 0.2391306608915329\n",
      "iteration 4555: loss: 0.23912926018238068\n",
      "iteration 4556: loss: 0.23912782967090607\n",
      "iteration 4557: loss: 0.23912644386291504\n",
      "iteration 4558: loss: 0.23912516236305237\n",
      "iteration 4559: loss: 0.23912374675273895\n",
      "iteration 4560: loss: 0.23912236094474792\n",
      "iteration 4561: loss: 0.23912093043327332\n",
      "iteration 4562: loss: 0.23911964893341064\n",
      "iteration 4563: loss: 0.23911826312541962\n",
      "iteration 4564: loss: 0.239116832613945\n",
      "iteration 4565: loss: 0.23911520838737488\n",
      "iteration 4566: loss: 0.23911380767822266\n",
      "iteration 4567: loss: 0.23911242187023163\n",
      "iteration 4568: loss: 0.2391110360622406\n",
      "iteration 4569: loss: 0.23910963535308838\n",
      "iteration 4570: loss: 0.23910823464393616\n",
      "iteration 4571: loss: 0.23910681903362274\n",
      "iteration 4572: loss: 0.23910541832447052\n",
      "iteration 4573: loss: 0.23910418152809143\n",
      "iteration 4574: loss: 0.2391027957201004\n",
      "iteration 4575: loss: 0.23910114169120789\n",
      "iteration 4576: loss: 0.23909978568553925\n",
      "iteration 4577: loss: 0.23909835517406464\n",
      "iteration 4578: loss: 0.2390969693660736\n",
      "iteration 4579: loss: 0.2390955686569214\n",
      "iteration 4580: loss: 0.23909421265125275\n",
      "iteration 4581: loss: 0.23909282684326172\n",
      "iteration 4582: loss: 0.2390914410352707\n",
      "iteration 4583: loss: 0.2390902042388916\n",
      "iteration 4584: loss: 0.2390885353088379\n",
      "iteration 4585: loss: 0.23908719420433044\n",
      "iteration 4586: loss: 0.23908579349517822\n",
      "iteration 4587: loss: 0.2390844076871872\n",
      "iteration 4588: loss: 0.23908300697803497\n",
      "iteration 4589: loss: 0.23908165097236633\n",
      "iteration 4590: loss: 0.2390802651643753\n",
      "iteration 4591: loss: 0.23907864093780518\n",
      "iteration 4592: loss: 0.23907725512981415\n",
      "iteration 4593: loss: 0.23907598853111267\n",
      "iteration 4594: loss: 0.23907461762428284\n",
      "iteration 4595: loss: 0.2390732318162918\n",
      "iteration 4596: loss: 0.23907184600830078\n",
      "iteration 4597: loss: 0.23907046020030975\n",
      "iteration 4598: loss: 0.23906907439231873\n",
      "iteration 4599: loss: 0.23906750977039337\n",
      "iteration 4600: loss: 0.23906615376472473\n",
      "iteration 4601: loss: 0.23906473815441132\n",
      "iteration 4602: loss: 0.23906347155570984\n",
      "iteration 4603: loss: 0.2390621155500412\n",
      "iteration 4604: loss: 0.23906072974205017\n",
      "iteration 4605: loss: 0.23905935883522034\n",
      "iteration 4606: loss: 0.2390577495098114\n",
      "iteration 4607: loss: 0.23905637860298157\n",
      "iteration 4608: loss: 0.23905500769615173\n",
      "iteration 4609: loss: 0.2390536367893219\n",
      "iteration 4610: loss: 0.23905237019062042\n",
      "iteration 4611: loss: 0.239050954580307\n",
      "iteration 4612: loss: 0.23904938995838165\n",
      "iteration 4613: loss: 0.239048033952713\n",
      "iteration 4614: loss: 0.23904657363891602\n",
      "iteration 4615: loss: 0.239045187830925\n",
      "iteration 4616: loss: 0.23904387652873993\n",
      "iteration 4617: loss: 0.23904259502887726\n",
      "iteration 4618: loss: 0.23904123902320862\n",
      "iteration 4619: loss: 0.2390396147966385\n",
      "iteration 4620: loss: 0.23903827369213104\n",
      "iteration 4621: loss: 0.2390369176864624\n",
      "iteration 4622: loss: 0.23903556168079376\n",
      "iteration 4623: loss: 0.23903417587280273\n",
      "iteration 4624: loss: 0.23903270065784454\n",
      "iteration 4625: loss: 0.2390313595533371\n",
      "iteration 4626: loss: 0.23902995884418488\n",
      "iteration 4627: loss: 0.23902861773967743\n",
      "iteration 4628: loss: 0.2390272319316864\n",
      "iteration 4629: loss: 0.23902587592601776\n",
      "iteration 4630: loss: 0.23902425169944763\n",
      "iteration 4631: loss: 0.23902301490306854\n",
      "iteration 4632: loss: 0.2390216886997223\n",
      "iteration 4633: loss: 0.23902031779289246\n",
      "iteration 4634: loss: 0.23901894688606262\n",
      "iteration 4635: loss: 0.23901760578155518\n",
      "iteration 4636: loss: 0.23901602625846863\n",
      "iteration 4637: loss: 0.2390146553516388\n",
      "iteration 4638: loss: 0.2390134036540985\n",
      "iteration 4639: loss: 0.23901204764842987\n",
      "iteration 4640: loss: 0.23901069164276123\n",
      "iteration 4641: loss: 0.23900911211967468\n",
      "iteration 4642: loss: 0.23900775611400604\n",
      "iteration 4643: loss: 0.23900637030601501\n",
      "iteration 4644: loss: 0.2390051633119583\n",
      "iteration 4645: loss: 0.23900380730628967\n",
      "iteration 4646: loss: 0.23900222778320312\n",
      "iteration 4647: loss: 0.23900087177753448\n",
      "iteration 4648: loss: 0.23899951577186584\n",
      "iteration 4649: loss: 0.238998144865036\n",
      "iteration 4650: loss: 0.2389969378709793\n",
      "iteration 4651: loss: 0.23899535834789276\n",
      "iteration 4652: loss: 0.23899397253990173\n",
      "iteration 4653: loss: 0.2389926165342331\n",
      "iteration 4654: loss: 0.23899129033088684\n",
      "iteration 4655: loss: 0.238989919424057\n",
      "iteration 4656: loss: 0.2389884889125824\n",
      "iteration 4657: loss: 0.23898713290691376\n",
      "iteration 4658: loss: 0.23898577690124512\n",
      "iteration 4659: loss: 0.23898449540138245\n",
      "iteration 4660: loss: 0.23898306488990784\n",
      "iteration 4661: loss: 0.2389816790819168\n",
      "iteration 4662: loss: 0.2389802634716034\n",
      "iteration 4663: loss: 0.23897893726825714\n",
      "iteration 4664: loss: 0.2389775961637497\n",
      "iteration 4665: loss: 0.23897600173950195\n",
      "iteration 4666: loss: 0.23897461593151093\n",
      "iteration 4667: loss: 0.23897337913513184\n",
      "iteration 4668: loss: 0.2389720380306244\n",
      "iteration 4669: loss: 0.23897068202495575\n",
      "iteration 4670: loss: 0.2389691323041916\n",
      "iteration 4671: loss: 0.23896780610084534\n",
      "iteration 4672: loss: 0.2389664351940155\n",
      "iteration 4673: loss: 0.2389651983976364\n",
      "iteration 4674: loss: 0.23896367847919464\n",
      "iteration 4675: loss: 0.2389623373746872\n",
      "iteration 4676: loss: 0.23896098136901855\n",
      "iteration 4677: loss: 0.23895962536334991\n",
      "iteration 4678: loss: 0.2389582097530365\n",
      "iteration 4679: loss: 0.23895688354969025\n",
      "iteration 4680: loss: 0.23895549774169922\n",
      "iteration 4681: loss: 0.23895415663719177\n",
      "iteration 4682: loss: 0.23895259201526642\n",
      "iteration 4683: loss: 0.23895137012004852\n",
      "iteration 4684: loss: 0.23895005881786346\n",
      "iteration 4685: loss: 0.2389487326145172\n",
      "iteration 4686: loss: 0.23894712328910828\n",
      "iteration 4687: loss: 0.2389458417892456\n",
      "iteration 4688: loss: 0.2389446198940277\n",
      "iteration 4689: loss: 0.23894329369068146\n",
      "iteration 4690: loss: 0.23894169926643372\n",
      "iteration 4691: loss: 0.23894040286540985\n",
      "iteration 4692: loss: 0.2389390468597412\n",
      "iteration 4693: loss: 0.23893782496452332\n",
      "iteration 4694: loss: 0.23893630504608154\n",
      "iteration 4695: loss: 0.2389349639415741\n",
      "iteration 4696: loss: 0.23893363773822784\n",
      "iteration 4697: loss: 0.2389322817325592\n",
      "iteration 4698: loss: 0.23893089592456818\n",
      "iteration 4699: loss: 0.23892951011657715\n",
      "iteration 4700: loss: 0.2389281690120697\n",
      "iteration 4701: loss: 0.23892684280872345\n",
      "iteration 4702: loss: 0.23892530798912048\n",
      "iteration 4703: loss: 0.23892411589622498\n",
      "iteration 4704: loss: 0.23892275989055634\n",
      "iteration 4705: loss: 0.23892144858837128\n",
      "iteration 4706: loss: 0.23891989886760712\n",
      "iteration 4707: loss: 0.23891857266426086\n",
      "iteration 4708: loss: 0.23891739547252655\n",
      "iteration 4709: loss: 0.2389158308506012\n",
      "iteration 4710: loss: 0.23891445994377136\n",
      "iteration 4711: loss: 0.2389131486415863\n",
      "iteration 4712: loss: 0.23891198635101318\n",
      "iteration 4713: loss: 0.23891043663024902\n",
      "iteration 4714: loss: 0.23890912532806396\n",
      "iteration 4715: loss: 0.23890778422355652\n",
      "iteration 4716: loss: 0.23890642821788788\n",
      "iteration 4717: loss: 0.23890498280525208\n",
      "iteration 4718: loss: 0.23890364170074463\n",
      "iteration 4719: loss: 0.23890230059623718\n",
      "iteration 4720: loss: 0.2389008104801178\n",
      "iteration 4721: loss: 0.23889967799186707\n",
      "iteration 4722: loss: 0.238898366689682\n",
      "iteration 4723: loss: 0.23889681696891785\n",
      "iteration 4724: loss: 0.23889553546905518\n",
      "iteration 4725: loss: 0.23889431357383728\n",
      "iteration 4726: loss: 0.23889298737049103\n",
      "iteration 4727: loss: 0.23889143764972687\n",
      "iteration 4728: loss: 0.238890141248703\n",
      "iteration 4729: loss: 0.23888882994651794\n",
      "iteration 4730: loss: 0.23888742923736572\n",
      "iteration 4731: loss: 0.23888611793518066\n",
      "iteration 4732: loss: 0.23888477683067322\n",
      "iteration 4733: loss: 0.23888346552848816\n",
      "iteration 4734: loss: 0.23888206481933594\n",
      "iteration 4735: loss: 0.23888075351715088\n",
      "iteration 4736: loss: 0.23887941241264343\n",
      "iteration 4737: loss: 0.23887786269187927\n",
      "iteration 4738: loss: 0.23887673020362854\n",
      "iteration 4739: loss: 0.2388753890991211\n",
      "iteration 4740: loss: 0.23887386918067932\n",
      "iteration 4741: loss: 0.23887252807617188\n",
      "iteration 4742: loss: 0.2388712465763092\n",
      "iteration 4743: loss: 0.2388698309659958\n",
      "iteration 4744: loss: 0.23886851966381073\n",
      "iteration 4745: loss: 0.23886720836162567\n",
      "iteration 4746: loss: 0.2388656586408615\n",
      "iteration 4747: loss: 0.23886451125144958\n",
      "iteration 4748: loss: 0.23886319994926453\n",
      "iteration 4749: loss: 0.2388618439435959\n",
      "iteration 4750: loss: 0.2388603389263153\n",
      "iteration 4751: loss: 0.2388591766357422\n",
      "iteration 4752: loss: 0.23885786533355713\n",
      "iteration 4753: loss: 0.23885631561279297\n",
      "iteration 4754: loss: 0.2388550043106079\n",
      "iteration 4755: loss: 0.23885385692119598\n",
      "iteration 4756: loss: 0.2388523519039154\n",
      "iteration 4757: loss: 0.23885104060173035\n",
      "iteration 4758: loss: 0.2388497143983841\n",
      "iteration 4759: loss: 0.23884832859039307\n",
      "iteration 4760: loss: 0.238847017288208\n",
      "iteration 4761: loss: 0.23884570598602295\n",
      "iteration 4762: loss: 0.23884418606758118\n",
      "iteration 4763: loss: 0.23884303867816925\n",
      "iteration 4764: loss: 0.2388416826725006\n",
      "iteration 4765: loss: 0.23884019255638123\n",
      "iteration 4766: loss: 0.23883891105651855\n",
      "iteration 4767: loss: 0.23883771896362305\n",
      "iteration 4768: loss: 0.2388361245393753\n",
      "iteration 4769: loss: 0.23883482813835144\n",
      "iteration 4770: loss: 0.23883351683616638\n",
      "iteration 4771: loss: 0.23883211612701416\n",
      "iteration 4772: loss: 0.23883084952831268\n",
      "iteration 4773: loss: 0.23882929980754852\n",
      "iteration 4774: loss: 0.23882803320884705\n",
      "iteration 4775: loss: 0.23882687091827393\n",
      "iteration 4776: loss: 0.23882535099983215\n",
      "iteration 4777: loss: 0.2388240396976471\n",
      "iteration 4778: loss: 0.23882286250591278\n",
      "iteration 4779: loss: 0.2388213574886322\n",
      "iteration 4780: loss: 0.23882003128528595\n",
      "iteration 4781: loss: 0.23881876468658447\n",
      "iteration 4782: loss: 0.23881737887859344\n",
      "iteration 4783: loss: 0.2388160675764084\n",
      "iteration 4784: loss: 0.23881478607654572\n",
      "iteration 4785: loss: 0.23881325125694275\n",
      "iteration 4786: loss: 0.23881211876869202\n",
      "iteration 4787: loss: 0.23881062865257263\n",
      "iteration 4788: loss: 0.23880930244922638\n",
      "iteration 4789: loss: 0.23880815505981445\n",
      "iteration 4790: loss: 0.23880663514137268\n",
      "iteration 4791: loss: 0.2388053685426712\n",
      "iteration 4792: loss: 0.23880405724048615\n",
      "iteration 4793: loss: 0.23880267143249512\n",
      "iteration 4794: loss: 0.23880138993263245\n",
      "iteration 4795: loss: 0.23880012333393097\n",
      "iteration 4796: loss: 0.2387986183166504\n",
      "iteration 4797: loss: 0.23879742622375488\n",
      "iteration 4798: loss: 0.2387959510087967\n",
      "iteration 4799: loss: 0.23879463970661163\n",
      "iteration 4800: loss: 0.23879337310791016\n",
      "iteration 4801: loss: 0.23879198729991913\n",
      "iteration 4802: loss: 0.23879070580005646\n",
      "iteration 4803: loss: 0.2387894093990326\n",
      "iteration 4804: loss: 0.23878803849220276\n",
      "iteration 4805: loss: 0.2387867420911789\n",
      "iteration 4806: loss: 0.2387852668762207\n",
      "iteration 4807: loss: 0.23878395557403564\n",
      "iteration 4808: loss: 0.23878280818462372\n",
      "iteration 4809: loss: 0.23878133296966553\n",
      "iteration 4810: loss: 0.23877999186515808\n",
      "iteration 4811: loss: 0.23877863585948944\n",
      "iteration 4812: loss: 0.23877735435962677\n",
      "iteration 4813: loss: 0.23877611756324768\n",
      "iteration 4814: loss: 0.2387746125459671\n",
      "iteration 4815: loss: 0.2387734353542328\n",
      "iteration 4816: loss: 0.2387719601392746\n",
      "iteration 4817: loss: 0.23877069354057312\n",
      "iteration 4818: loss: 0.23876933753490448\n",
      "iteration 4819: loss: 0.23876798152923584\n",
      "iteration 4820: loss: 0.23876667022705078\n",
      "iteration 4821: loss: 0.2387651950120926\n",
      "iteration 4822: loss: 0.23876404762268066\n",
      "iteration 4823: loss: 0.238762766122818\n",
      "iteration 4824: loss: 0.2387612760066986\n",
      "iteration 4825: loss: 0.23876014351844788\n",
      "iteration 4826: loss: 0.2387586086988449\n",
      "iteration 4827: loss: 0.23875734210014343\n",
      "iteration 4828: loss: 0.2387562096118927\n",
      "iteration 4829: loss: 0.23875470459461212\n",
      "iteration 4830: loss: 0.23875343799591064\n",
      "iteration 4831: loss: 0.23875193297863007\n",
      "iteration 4832: loss: 0.23875078558921814\n",
      "iteration 4833: loss: 0.23874953389167786\n",
      "iteration 4834: loss: 0.23874804377555847\n",
      "iteration 4835: loss: 0.23874688148498535\n",
      "iteration 4836: loss: 0.23874540627002716\n",
      "iteration 4837: loss: 0.23874413967132568\n",
      "iteration 4838: loss: 0.2387426346540451\n",
      "iteration 4839: loss: 0.23874148726463318\n",
      "iteration 4840: loss: 0.2387402057647705\n",
      "iteration 4841: loss: 0.2387387454509735\n",
      "iteration 4842: loss: 0.23873762786388397\n",
      "iteration 4843: loss: 0.23873615264892578\n",
      "iteration 4844: loss: 0.2387348711490631\n",
      "iteration 4845: loss: 0.23873336613178253\n",
      "iteration 4846: loss: 0.2387322634458542\n",
      "iteration 4847: loss: 0.23873098194599152\n",
      "iteration 4848: loss: 0.23872947692871094\n",
      "iteration 4849: loss: 0.2387283593416214\n",
      "iteration 4850: loss: 0.2387268841266632\n",
      "iteration 4851: loss: 0.23872563242912292\n",
      "iteration 4852: loss: 0.23872427642345428\n",
      "iteration 4853: loss: 0.23872296512126923\n",
      "iteration 4854: loss: 0.23872172832489014\n",
      "iteration 4855: loss: 0.2387203723192215\n",
      "iteration 4856: loss: 0.2387191355228424\n",
      "iteration 4857: loss: 0.23871763050556183\n",
      "iteration 4858: loss: 0.23871639370918274\n",
      "iteration 4859: loss: 0.2387150228023529\n",
      "iteration 4860: loss: 0.23871389031410217\n",
      "iteration 4861: loss: 0.2387123852968216\n",
      "iteration 4862: loss: 0.23871126770973206\n",
      "iteration 4863: loss: 0.23871000111103058\n",
      "iteration 4864: loss: 0.23870854079723358\n",
      "iteration 4865: loss: 0.23870739340782166\n",
      "iteration 4866: loss: 0.23870594799518585\n",
      "iteration 4867: loss: 0.23870468139648438\n",
      "iteration 4868: loss: 0.23870325088500977\n",
      "iteration 4869: loss: 0.23870201408863068\n",
      "iteration 4870: loss: 0.2387005090713501\n",
      "iteration 4871: loss: 0.23869939148426056\n",
      "iteration 4872: loss: 0.23869793117046356\n",
      "iteration 4873: loss: 0.23869669437408447\n",
      "iteration 4874: loss: 0.2386954128742218\n",
      "iteration 4875: loss: 0.23869407176971436\n",
      "iteration 4876: loss: 0.23869280517101288\n",
      "iteration 4877: loss: 0.23869135975837708\n",
      "iteration 4878: loss: 0.23869022727012634\n",
      "iteration 4879: loss: 0.23868878185749054\n",
      "iteration 4880: loss: 0.23868751525878906\n",
      "iteration 4881: loss: 0.23868615925312042\n",
      "iteration 4882: loss: 0.23868492245674133\n",
      "iteration 4883: loss: 0.23868343234062195\n",
      "iteration 4884: loss: 0.2386823147535324\n",
      "iteration 4885: loss: 0.2386808693408966\n",
      "iteration 4886: loss: 0.23867961764335632\n",
      "iteration 4887: loss: 0.23867850005626678\n",
      "iteration 4888: loss: 0.2386769950389862\n",
      "iteration 4889: loss: 0.23867575824260712\n",
      "iteration 4890: loss: 0.23867443203926086\n",
      "iteration 4891: loss: 0.2386731654405594\n",
      "iteration 4892: loss: 0.23867173492908478\n",
      "iteration 4893: loss: 0.23867061734199524\n",
      "iteration 4894: loss: 0.23866915702819824\n",
      "iteration 4895: loss: 0.23866793513298035\n",
      "iteration 4896: loss: 0.2386665791273117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4897: loss: 0.23866531252861023\n",
      "iteration 4898: loss: 0.23866383731365204\n",
      "iteration 4899: loss: 0.23866276443004608\n",
      "iteration 4900: loss: 0.2386612892150879\n",
      "iteration 4901: loss: 0.2386600226163864\n",
      "iteration 4902: loss: 0.23865871131420135\n",
      "iteration 4903: loss: 0.23865744471549988\n",
      "iteration 4904: loss: 0.23865601420402527\n",
      "iteration 4905: loss: 0.23865476250648499\n",
      "iteration 4906: loss: 0.23865342140197754\n",
      "iteration 4907: loss: 0.23865218460559845\n",
      "iteration 4908: loss: 0.23865072429180145\n",
      "iteration 4909: loss: 0.2386496365070343\n",
      "iteration 4910: loss: 0.2386482059955597\n",
      "iteration 4911: loss: 0.23864693939685822\n",
      "iteration 4912: loss: 0.23864562809467316\n",
      "iteration 4913: loss: 0.23864436149597168\n",
      "iteration 4914: loss: 0.2386428862810135\n",
      "iteration 4915: loss: 0.23864178359508514\n",
      "iteration 4916: loss: 0.23864033818244934\n",
      "iteration 4917: loss: 0.238639235496521\n",
      "iteration 4918: loss: 0.23863771557807922\n",
      "iteration 4919: loss: 0.23863646388053894\n",
      "iteration 4920: loss: 0.23863515257835388\n",
      "iteration 4921: loss: 0.23863394558429718\n",
      "iteration 4922: loss: 0.23863248527050018\n",
      "iteration 4923: loss: 0.2386312484741211\n",
      "iteration 4924: loss: 0.23862996697425842\n",
      "iteration 4925: loss: 0.23862870037555695\n",
      "iteration 4926: loss: 0.2386273890733719\n",
      "iteration 4927: loss: 0.2386261522769928\n",
      "iteration 4928: loss: 0.2386246621608734\n",
      "iteration 4929: loss: 0.23862358927726746\n",
      "iteration 4930: loss: 0.23862209916114807\n",
      "iteration 4931: loss: 0.23862090706825256\n",
      "iteration 4932: loss: 0.2386196106672287\n",
      "iteration 4933: loss: 0.23861834406852722\n",
      "iteration 4934: loss: 0.2386169135570526\n",
      "iteration 4935: loss: 0.23861582577228546\n",
      "iteration 4936: loss: 0.23861436545848846\n",
      "iteration 4937: loss: 0.238613098859787\n",
      "iteration 4938: loss: 0.23861181735992432\n",
      "iteration 4939: loss: 0.2386106252670288\n",
      "iteration 4940: loss: 0.2386091649532318\n",
      "iteration 4941: loss: 0.23860783874988556\n",
      "iteration 4942: loss: 0.23860661685466766\n",
      "iteration 4943: loss: 0.23860517144203186\n",
      "iteration 4944: loss: 0.2386040985584259\n",
      "iteration 4945: loss: 0.2386026680469513\n",
      "iteration 4946: loss: 0.238601416349411\n",
      "iteration 4947: loss: 0.23860013484954834\n",
      "iteration 4948: loss: 0.23859889805316925\n",
      "iteration 4949: loss: 0.23859743773937225\n",
      "iteration 4950: loss: 0.2385963499546051\n",
      "iteration 4951: loss: 0.23859496414661407\n",
      "iteration 4952: loss: 0.2385936975479126\n",
      "iteration 4953: loss: 0.23859241604804993\n",
      "iteration 4954: loss: 0.23859116435050964\n",
      "iteration 4955: loss: 0.23858973383903503\n",
      "iteration 4956: loss: 0.23858842253684998\n",
      "iteration 4957: loss: 0.2385871857404709\n",
      "iteration 4958: loss: 0.23858587443828583\n",
      "iteration 4959: loss: 0.23858466744422913\n",
      "iteration 4960: loss: 0.23858323693275452\n",
      "iteration 4961: loss: 0.23858213424682617\n",
      "iteration 4962: loss: 0.23858070373535156\n",
      "iteration 4963: loss: 0.23857951164245605\n",
      "iteration 4964: loss: 0.238578200340271\n",
      "iteration 4965: loss: 0.2385767698287964\n",
      "iteration 4966: loss: 0.2385755330324173\n",
      "iteration 4967: loss: 0.23857417702674866\n",
      "iteration 4968: loss: 0.23857299983501434\n",
      "iteration 4969: loss: 0.23857152462005615\n",
      "iteration 4970: loss: 0.2385704517364502\n",
      "iteration 4971: loss: 0.23856902122497559\n",
      "iteration 4972: loss: 0.2385677993297577\n",
      "iteration 4973: loss: 0.23856651782989502\n",
      "iteration 4974: loss: 0.23856505751609802\n",
      "iteration 4975: loss: 0.23856386542320251\n",
      "iteration 4976: loss: 0.23856255412101746\n",
      "iteration 4977: loss: 0.23856136202812195\n",
      "iteration 4978: loss: 0.23855993151664734\n",
      "iteration 4979: loss: 0.23855885863304138\n",
      "iteration 4980: loss: 0.23855741322040558\n",
      "iteration 4981: loss: 0.23855623602867126\n",
      "iteration 4982: loss: 0.2385549247264862\n",
      "iteration 4983: loss: 0.2385534793138504\n",
      "iteration 4984: loss: 0.23855237662792206\n",
      "iteration 4985: loss: 0.2385510951280594\n",
      "iteration 4986: loss: 0.2385498732328415\n",
      "iteration 4987: loss: 0.23854860663414001\n",
      "iteration 4988: loss: 0.23854736983776093\n",
      "iteration 4989: loss: 0.23854593932628632\n",
      "iteration 4990: loss: 0.23854465782642365\n",
      "iteration 4991: loss: 0.23854346573352814\n",
      "iteration 4992: loss: 0.23854216933250427\n",
      "iteration 4993: loss: 0.23854096233844757\n",
      "iteration 4994: loss: 0.23853954672813416\n",
      "iteration 4995: loss: 0.2385382354259491\n",
      "iteration 4996: loss: 0.2385370284318924\n",
      "iteration 4997: loss: 0.23853567242622375\n",
      "iteration 4998: loss: 0.23853453993797302\n",
      "iteration 4999: loss: 0.2385331094264984\n",
      "iteration 5000: loss: 0.2385319173336029\n",
      "iteration 5001: loss: 0.23853063583374023\n",
      "iteration 5002: loss: 0.23852920532226562\n",
      "iteration 5003: loss: 0.2385280579328537\n",
      "iteration 5004: loss: 0.23852673172950745\n",
      "iteration 5005: loss: 0.23852553963661194\n",
      "iteration 5006: loss: 0.23852427303791046\n",
      "iteration 5007: loss: 0.23852281272411346\n",
      "iteration 5008: loss: 0.23852166533470154\n",
      "iteration 5009: loss: 0.23852035403251648\n",
      "iteration 5010: loss: 0.23851916193962097\n",
      "iteration 5011: loss: 0.23851776123046875\n",
      "iteration 5012: loss: 0.23851647973060608\n",
      "iteration 5013: loss: 0.23851525783538818\n",
      "iteration 5014: loss: 0.23851385712623596\n",
      "iteration 5015: loss: 0.2385127991437912\n",
      "iteration 5016: loss: 0.2385113686323166\n",
      "iteration 5017: loss: 0.23851004242897034\n",
      "iteration 5018: loss: 0.23850882053375244\n",
      "iteration 5019: loss: 0.2385074347257614\n",
      "iteration 5020: loss: 0.23850636184215546\n",
      "iteration 5021: loss: 0.23850493133068085\n",
      "iteration 5022: loss: 0.23850353062152863\n",
      "iteration 5023: loss: 0.23850247263908386\n",
      "iteration 5024: loss: 0.23850107192993164\n",
      "iteration 5025: loss: 0.23849985003471375\n",
      "iteration 5026: loss: 0.23849859833717346\n",
      "iteration 5027: loss: 0.23849721252918243\n",
      "iteration 5028: loss: 0.2384961098432541\n",
      "iteration 5029: loss: 0.23849472403526306\n",
      "iteration 5030: loss: 0.23849356174468994\n",
      "iteration 5031: loss: 0.23849229514598846\n",
      "iteration 5032: loss: 0.23849084973335266\n",
      "iteration 5033: loss: 0.23848967254161835\n",
      "iteration 5034: loss: 0.23848839104175568\n",
      "iteration 5035: loss: 0.23848700523376465\n",
      "iteration 5036: loss: 0.2384859323501587\n",
      "iteration 5037: loss: 0.23848453164100647\n",
      "iteration 5038: loss: 0.23848330974578857\n",
      "iteration 5039: loss: 0.23848207294940948\n",
      "iteration 5040: loss: 0.23848068714141846\n",
      "iteration 5041: loss: 0.23847949504852295\n",
      "iteration 5042: loss: 0.23847821354866028\n",
      "iteration 5043: loss: 0.23847679793834686\n",
      "iteration 5044: loss: 0.23847565054893494\n",
      "iteration 5045: loss: 0.23847436904907227\n",
      "iteration 5046: loss: 0.23847314715385437\n",
      "iteration 5047: loss: 0.23847191035747528\n",
      "iteration 5048: loss: 0.23847047984600067\n",
      "iteration 5049: loss: 0.23846931755542755\n",
      "iteration 5050: loss: 0.23846808075904846\n",
      "iteration 5051: loss: 0.23846669495105743\n",
      "iteration 5052: loss: 0.23846547305583954\n",
      "iteration 5053: loss: 0.23846420645713806\n",
      "iteration 5054: loss: 0.23846283555030823\n",
      "iteration 5055: loss: 0.23846176266670227\n",
      "iteration 5056: loss: 0.23846034705638885\n",
      "iteration 5057: loss: 0.23845918476581573\n",
      "iteration 5058: loss: 0.23845794796943665\n",
      "iteration 5059: loss: 0.23845653235912323\n",
      "iteration 5060: loss: 0.23845534026622772\n",
      "iteration 5061: loss: 0.23845410346984863\n",
      "iteration 5062: loss: 0.2384527176618576\n",
      "iteration 5063: loss: 0.23845164477825165\n",
      "iteration 5064: loss: 0.23845024406909943\n",
      "iteration 5065: loss: 0.2384488880634308\n",
      "iteration 5066: loss: 0.23844775557518005\n",
      "iteration 5067: loss: 0.23844632506370544\n",
      "iteration 5068: loss: 0.2384449690580368\n",
      "iteration 5069: loss: 0.23844392597675323\n",
      "iteration 5070: loss: 0.2384425401687622\n",
      "iteration 5071: loss: 0.23844127357006073\n",
      "iteration 5072: loss: 0.2384401112794876\n",
      "iteration 5073: loss: 0.2384386956691742\n",
      "iteration 5074: loss: 0.2384374439716339\n",
      "iteration 5075: loss: 0.23843629658222198\n",
      "iteration 5076: loss: 0.2384350597858429\n",
      "iteration 5077: loss: 0.23843364417552948\n",
      "iteration 5078: loss: 0.23843249678611755\n",
      "iteration 5079: loss: 0.2384311854839325\n",
      "iteration 5080: loss: 0.23842985928058624\n",
      "iteration 5081: loss: 0.23842866718769073\n",
      "iteration 5082: loss: 0.23842743039131165\n",
      "iteration 5083: loss: 0.23842623829841614\n",
      "iteration 5084: loss: 0.2384248524904251\n",
      "iteration 5085: loss: 0.23842361569404602\n",
      "iteration 5086: loss: 0.2384224683046341\n",
      "iteration 5087: loss: 0.23842120170593262\n",
      "iteration 5088: loss: 0.2384198158979416\n",
      "iteration 5089: loss: 0.23841862380504608\n",
      "iteration 5090: loss: 0.238417387008667\n",
      "iteration 5091: loss: 0.23841600120067596\n",
      "iteration 5092: loss: 0.23841485381126404\n",
      "iteration 5093: loss: 0.23841361701488495\n",
      "iteration 5094: loss: 0.23841221630573273\n",
      "iteration 5095: loss: 0.23841097950935364\n",
      "iteration 5096: loss: 0.23840980231761932\n",
      "iteration 5097: loss: 0.2384084016084671\n",
      "iteration 5098: loss: 0.23840728402137756\n",
      "iteration 5099: loss: 0.23840610682964325\n",
      "iteration 5100: loss: 0.23840484023094177\n",
      "iteration 5101: loss: 0.23840348422527313\n",
      "iteration 5102: loss: 0.2384023219347\n",
      "iteration 5103: loss: 0.23840108513832092\n",
      "iteration 5104: loss: 0.2383996695280075\n",
      "iteration 5105: loss: 0.23839867115020752\n",
      "iteration 5106: loss: 0.23839731514453888\n",
      "iteration 5107: loss: 0.23839592933654785\n",
      "iteration 5108: loss: 0.23839493095874786\n",
      "iteration 5109: loss: 0.23839351534843445\n",
      "iteration 5110: loss: 0.2383921593427658\n",
      "iteration 5111: loss: 0.23839092254638672\n",
      "iteration 5112: loss: 0.2383897304534912\n",
      "iteration 5113: loss: 0.23838850855827332\n",
      "iteration 5114: loss: 0.23838715255260468\n",
      "iteration 5115: loss: 0.23838599026203156\n",
      "iteration 5116: loss: 0.2383846938610077\n",
      "iteration 5117: loss: 0.23838332295417786\n",
      "iteration 5118: loss: 0.23838214576244354\n",
      "iteration 5119: loss: 0.23838090896606445\n",
      "iteration 5120: loss: 0.23837952315807343\n",
      "iteration 5121: loss: 0.23837852478027344\n",
      "iteration 5122: loss: 0.2383771389722824\n",
      "iteration 5123: loss: 0.23837578296661377\n",
      "iteration 5124: loss: 0.2383745163679123\n",
      "iteration 5125: loss: 0.23837339878082275\n",
      "iteration 5126: loss: 0.23837201297283173\n",
      "iteration 5127: loss: 0.23837080597877502\n",
      "iteration 5128: loss: 0.2383696287870407\n",
      "iteration 5129: loss: 0.23836839199066162\n",
      "iteration 5130: loss: 0.23836703598499298\n",
      "iteration 5131: loss: 0.23836585879325867\n",
      "iteration 5132: loss: 0.23836465179920197\n",
      "iteration 5133: loss: 0.23836326599121094\n",
      "iteration 5134: loss: 0.23836207389831543\n",
      "iteration 5135: loss: 0.2383609116077423\n",
      "iteration 5136: loss: 0.2383594959974289\n",
      "iteration 5137: loss: 0.2383583039045334\n",
      "iteration 5138: loss: 0.23835711181163788\n",
      "iteration 5139: loss: 0.23835578560829163\n",
      "iteration 5140: loss: 0.23835456371307373\n",
      "iteration 5141: loss: 0.2383532077074051\n",
      "iteration 5142: loss: 0.2383522093296051\n",
      "iteration 5143: loss: 0.23835082352161407\n",
      "iteration 5144: loss: 0.23834946751594543\n",
      "iteration 5145: loss: 0.23834845423698425\n",
      "iteration 5146: loss: 0.23834708333015442\n",
      "iteration 5147: loss: 0.2383458912372589\n",
      "iteration 5148: loss: 0.23834450542926788\n",
      "iteration 5149: loss: 0.23834338784217834\n",
      "iteration 5150: loss: 0.23834213614463806\n",
      "iteration 5151: loss: 0.23834078013896942\n",
      "iteration 5152: loss: 0.23833942413330078\n",
      "iteration 5153: loss: 0.2383384257555008\n",
      "iteration 5154: loss: 0.23833706974983215\n",
      "iteration 5155: loss: 0.23833584785461426\n",
      "iteration 5156: loss: 0.23833465576171875\n",
      "iteration 5157: loss: 0.23833346366882324\n",
      "iteration 5158: loss: 0.2383321076631546\n",
      "iteration 5159: loss: 0.23833075165748596\n",
      "iteration 5160: loss: 0.23832973837852478\n",
      "iteration 5161: loss: 0.23832838237285614\n",
      "iteration 5162: loss: 0.2383270263671875\n",
      "iteration 5163: loss: 0.2383258044719696\n",
      "iteration 5164: loss: 0.23832467198371887\n",
      "iteration 5165: loss: 0.23832349479198456\n",
      "iteration 5166: loss: 0.23832204937934875\n",
      "iteration 5167: loss: 0.23832067847251892\n",
      "iteration 5168: loss: 0.23831968009471893\n",
      "iteration 5169: loss: 0.2383183240890503\n",
      "iteration 5170: loss: 0.23831696808338165\n",
      "iteration 5171: loss: 0.23831577599048615\n",
      "iteration 5172: loss: 0.23831462860107422\n",
      "iteration 5173: loss: 0.23831340670585632\n",
      "iteration 5174: loss: 0.23831208050251007\n",
      "iteration 5175: loss: 0.23831072449684143\n",
      "iteration 5176: loss: 0.23830969631671906\n",
      "iteration 5177: loss: 0.238308385014534\n",
      "iteration 5178: loss: 0.23830702900886536\n",
      "iteration 5179: loss: 0.23830583691596985\n",
      "iteration 5180: loss: 0.23830468952655792\n",
      "iteration 5181: loss: 0.23830346763134003\n",
      "iteration 5182: loss: 0.23830214142799377\n",
      "iteration 5183: loss: 0.23830077052116394\n",
      "iteration 5184: loss: 0.23829977214336395\n",
      "iteration 5185: loss: 0.2382984161376953\n",
      "iteration 5186: loss: 0.2382972240447998\n",
      "iteration 5187: loss: 0.23829588294029236\n",
      "iteration 5188: loss: 0.23829475045204163\n",
      "iteration 5189: loss: 0.23829355835914612\n",
      "iteration 5190: loss: 0.23829218745231628\n",
      "iteration 5191: loss: 0.23829099535942078\n",
      "iteration 5192: loss: 0.23828986287117004\n",
      "iteration 5193: loss: 0.2382884919643402\n",
      "iteration 5194: loss: 0.2382873296737671\n",
      "iteration 5195: loss: 0.23828594386577606\n",
      "iteration 5196: loss: 0.23828499019145966\n",
      "iteration 5197: loss: 0.23828360438346863\n",
      "iteration 5198: loss: 0.23828229308128357\n",
      "iteration 5199: loss: 0.23828108608722687\n",
      "iteration 5200: loss: 0.23827974498271942\n",
      "iteration 5201: loss: 0.23827862739562988\n",
      "iteration 5202: loss: 0.23827746510505676\n",
      "iteration 5203: loss: 0.23827621340751648\n",
      "iteration 5204: loss: 0.23827500641345978\n",
      "iteration 5205: loss: 0.23827382922172546\n",
      "iteration 5206: loss: 0.2382725179195404\n",
      "iteration 5207: loss: 0.2382713109254837\n",
      "iteration 5208: loss: 0.23826999962329865\n",
      "iteration 5209: loss: 0.23826877772808075\n",
      "iteration 5210: loss: 0.2382676601409912\n",
      "iteration 5211: loss: 0.23826630413532257\n",
      "iteration 5212: loss: 0.23826511204242706\n",
      "iteration 5213: loss: 0.2382638156414032\n",
      "iteration 5214: loss: 0.2382628172636032\n",
      "iteration 5215: loss: 0.23826149106025696\n",
      "iteration 5216: loss: 0.23826012015342712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5217: loss: 0.23825888335704803\n",
      "iteration 5218: loss: 0.23825755715370178\n",
      "iteration 5219: loss: 0.23825640976428986\n",
      "iteration 5220: loss: 0.23825518786907196\n",
      "iteration 5221: loss: 0.2382538765668869\n",
      "iteration 5222: loss: 0.23825272917747498\n",
      "iteration 5223: loss: 0.23825159668922424\n",
      "iteration 5224: loss: 0.2382502257823944\n",
      "iteration 5225: loss: 0.2382490634918213\n",
      "iteration 5226: loss: 0.23824772238731384\n",
      "iteration 5227: loss: 0.23824641108512878\n",
      "iteration 5228: loss: 0.2382453978061676\n",
      "iteration 5229: loss: 0.23824408650398254\n",
      "iteration 5230: loss: 0.23824289441108704\n",
      "iteration 5231: loss: 0.2382415533065796\n",
      "iteration 5232: loss: 0.23824024200439453\n",
      "iteration 5233: loss: 0.23823924362659454\n",
      "iteration 5234: loss: 0.23823794722557068\n",
      "iteration 5235: loss: 0.23823674023151398\n",
      "iteration 5236: loss: 0.23823539912700653\n",
      "iteration 5237: loss: 0.23823408782482147\n",
      "iteration 5238: loss: 0.23823311924934387\n",
      "iteration 5239: loss: 0.23823177814483643\n",
      "iteration 5240: loss: 0.2382306158542633\n",
      "iteration 5241: loss: 0.23822931945323944\n",
      "iteration 5242: loss: 0.2382279336452484\n",
      "iteration 5243: loss: 0.2382269650697708\n",
      "iteration 5244: loss: 0.23822565376758575\n",
      "iteration 5245: loss: 0.23822446167469025\n",
      "iteration 5246: loss: 0.2382231205701828\n",
      "iteration 5247: loss: 0.23822180926799774\n",
      "iteration 5248: loss: 0.23822088539600372\n",
      "iteration 5249: loss: 0.23821952939033508\n",
      "iteration 5250: loss: 0.23821821808815002\n",
      "iteration 5251: loss: 0.23821702599525452\n",
      "iteration 5252: loss: 0.23821572959423065\n",
      "iteration 5253: loss: 0.23821453750133514\n",
      "iteration 5254: loss: 0.2382134199142456\n",
      "iteration 5255: loss: 0.23821207880973816\n",
      "iteration 5256: loss: 0.23821091651916504\n",
      "iteration 5257: loss: 0.2382095754146576\n",
      "iteration 5258: loss: 0.23820844292640686\n",
      "iteration 5259: loss: 0.23820731043815613\n",
      "iteration 5260: loss: 0.23820599913597107\n",
      "iteration 5261: loss: 0.23820480704307556\n",
      "iteration 5262: loss: 0.2382034957408905\n",
      "iteration 5263: loss: 0.23820233345031738\n",
      "iteration 5264: loss: 0.23820099234580994\n",
      "iteration 5265: loss: 0.2381998747587204\n",
      "iteration 5266: loss: 0.23819871246814728\n",
      "iteration 5267: loss: 0.23819732666015625\n",
      "iteration 5268: loss: 0.23819616436958313\n",
      "iteration 5269: loss: 0.23819486796855927\n",
      "iteration 5270: loss: 0.238193541765213\n",
      "iteration 5271: loss: 0.23819255828857422\n",
      "iteration 5272: loss: 0.23819127678871155\n",
      "iteration 5273: loss: 0.2381899356842041\n",
      "iteration 5274: loss: 0.23818878829479218\n",
      "iteration 5275: loss: 0.23818746209144592\n",
      "iteration 5276: loss: 0.23818626999855042\n",
      "iteration 5277: loss: 0.23818519711494446\n",
      "iteration 5278: loss: 0.2381838858127594\n",
      "iteration 5279: loss: 0.23818273842334747\n",
      "iteration 5280: loss: 0.23818139731884003\n",
      "iteration 5281: loss: 0.23818007111549377\n",
      "iteration 5282: loss: 0.23817892372608185\n",
      "iteration 5283: loss: 0.2381778210401535\n",
      "iteration 5284: loss: 0.238176628947258\n",
      "iteration 5285: loss: 0.23817534744739532\n",
      "iteration 5286: loss: 0.23817403614521027\n",
      "iteration 5287: loss: 0.23817285895347595\n",
      "iteration 5288: loss: 0.2381715476512909\n",
      "iteration 5289: loss: 0.23817045986652374\n",
      "iteration 5290: loss: 0.23816931247711182\n",
      "iteration 5291: loss: 0.23816800117492676\n",
      "iteration 5292: loss: 0.23816683888435364\n",
      "iteration 5293: loss: 0.23816554248332977\n",
      "iteration 5294: loss: 0.23816421627998352\n",
      "iteration 5295: loss: 0.23816323280334473\n",
      "iteration 5296: loss: 0.23816195130348206\n",
      "iteration 5297: loss: 0.23816077411174774\n",
      "iteration 5298: loss: 0.23815950751304626\n",
      "iteration 5299: loss: 0.2381581813097\n",
      "iteration 5300: loss: 0.23815703392028809\n",
      "iteration 5301: loss: 0.23815584182739258\n",
      "iteration 5302: loss: 0.23815473914146423\n",
      "iteration 5303: loss: 0.2381535768508911\n",
      "iteration 5304: loss: 0.23815229535102844\n",
      "iteration 5305: loss: 0.23815111815929413\n",
      "iteration 5306: loss: 0.23814980685710907\n",
      "iteration 5307: loss: 0.23814864456653595\n",
      "iteration 5308: loss: 0.2381473332643509\n",
      "iteration 5309: loss: 0.23814621567726135\n",
      "iteration 5310: loss: 0.23814508318901062\n",
      "iteration 5311: loss: 0.23814380168914795\n",
      "iteration 5312: loss: 0.23814263939857483\n",
      "iteration 5313: loss: 0.23814134299755096\n",
      "iteration 5314: loss: 0.2381400316953659\n",
      "iteration 5315: loss: 0.23813888430595398\n",
      "iteration 5316: loss: 0.2381376028060913\n",
      "iteration 5317: loss: 0.23813648521900177\n",
      "iteration 5318: loss: 0.23813529312610626\n",
      "iteration 5319: loss: 0.2381339967250824\n",
      "iteration 5320: loss: 0.23813271522521973\n",
      "iteration 5321: loss: 0.2381315529346466\n",
      "iteration 5322: loss: 0.23813025653362274\n",
      "iteration 5323: loss: 0.23812909424304962\n",
      "iteration 5324: loss: 0.23812802135944366\n",
      "iteration 5325: loss: 0.2381267100572586\n",
      "iteration 5326: loss: 0.2381255328655243\n",
      "iteration 5327: loss: 0.23812422156333923\n",
      "iteration 5328: loss: 0.2381231039762497\n",
      "iteration 5329: loss: 0.23812182247638702\n",
      "iteration 5330: loss: 0.23812051117420197\n",
      "iteration 5331: loss: 0.23811936378479004\n",
      "iteration 5332: loss: 0.23811829090118408\n",
      "iteration 5333: loss: 0.2381170243024826\n",
      "iteration 5334: loss: 0.23811586201190948\n",
      "iteration 5335: loss: 0.23811456561088562\n",
      "iteration 5336: loss: 0.2381134033203125\n",
      "iteration 5337: loss: 0.23811213672161102\n",
      "iteration 5338: loss: 0.23811085522174835\n",
      "iteration 5339: loss: 0.23810967803001404\n",
      "iteration 5340: loss: 0.23810860514640808\n",
      "iteration 5341: loss: 0.23810729384422302\n",
      "iteration 5342: loss: 0.23810617625713348\n",
      "iteration 5343: loss: 0.2381048947572708\n",
      "iteration 5344: loss: 0.2381037473678589\n",
      "iteration 5345: loss: 0.23810243606567383\n",
      "iteration 5346: loss: 0.23810119926929474\n",
      "iteration 5347: loss: 0.2381000518798828\n",
      "iteration 5348: loss: 0.23809878528118134\n",
      "iteration 5349: loss: 0.23809747397899628\n",
      "iteration 5350: loss: 0.23809655010700226\n",
      "iteration 5351: loss: 0.2380952388048172\n",
      "iteration 5352: loss: 0.23809409141540527\n",
      "iteration 5353: loss: 0.23809278011322021\n",
      "iteration 5354: loss: 0.23809154331684113\n",
      "iteration 5355: loss: 0.2380903661251068\n",
      "iteration 5356: loss: 0.23808908462524414\n",
      "iteration 5357: loss: 0.2380879670381546\n",
      "iteration 5358: loss: 0.23808665573596954\n",
      "iteration 5359: loss: 0.23808559775352478\n",
      "iteration 5360: loss: 0.23808448016643524\n",
      "iteration 5361: loss: 0.23808319866657257\n",
      "iteration 5362: loss: 0.2380819320678711\n",
      "iteration 5363: loss: 0.23808078467845917\n",
      "iteration 5364: loss: 0.2380794733762741\n",
      "iteration 5365: loss: 0.23807838559150696\n",
      "iteration 5366: loss: 0.2380770742893219\n",
      "iteration 5367: loss: 0.23807577788829803\n",
      "iteration 5368: loss: 0.2380746603012085\n",
      "iteration 5369: loss: 0.23807337880134583\n",
      "iteration 5370: loss: 0.23807236552238464\n",
      "iteration 5371: loss: 0.23807111382484436\n",
      "iteration 5372: loss: 0.2380698174238205\n",
      "iteration 5373: loss: 0.23806867003440857\n",
      "iteration 5374: loss: 0.23806743323802948\n",
      "iteration 5375: loss: 0.2380661517381668\n",
      "iteration 5376: loss: 0.23806500434875488\n",
      "iteration 5377: loss: 0.2380637675523758\n",
      "iteration 5378: loss: 0.23806247115135193\n",
      "iteration 5379: loss: 0.2380613535642624\n",
      "iteration 5380: loss: 0.23806004226207733\n",
      "iteration 5381: loss: 0.2380591183900833\n",
      "iteration 5382: loss: 0.23805785179138184\n",
      "iteration 5383: loss: 0.23805658519268036\n",
      "iteration 5384: loss: 0.23805546760559082\n",
      "iteration 5385: loss: 0.23805415630340576\n",
      "iteration 5386: loss: 0.2380530834197998\n",
      "iteration 5387: loss: 0.23805180191993713\n",
      "iteration 5388: loss: 0.23805049061775208\n",
      "iteration 5389: loss: 0.23804935812950134\n",
      "iteration 5390: loss: 0.23804812133312225\n",
      "iteration 5391: loss: 0.23804683983325958\n",
      "iteration 5392: loss: 0.23804572224617004\n",
      "iteration 5393: loss: 0.23804445564746857\n",
      "iteration 5394: loss: 0.2380436360836029\n",
      "iteration 5395: loss: 0.23804235458374023\n",
      "iteration 5396: loss: 0.23804108798503876\n",
      "iteration 5397: loss: 0.23803997039794922\n",
      "iteration 5398: loss: 0.23803868889808655\n",
      "iteration 5399: loss: 0.23803742229938507\n",
      "iteration 5400: loss: 0.23803630471229553\n",
      "iteration 5401: loss: 0.23803503811359406\n",
      "iteration 5402: loss: 0.23803392052650452\n",
      "iteration 5403: loss: 0.23803262412548065\n",
      "iteration 5404: loss: 0.23803138732910156\n",
      "iteration 5405: loss: 0.23803028464317322\n",
      "iteration 5406: loss: 0.23802895843982697\n",
      "iteration 5407: loss: 0.23802776634693146\n",
      "iteration 5408: loss: 0.23802664875984192\n",
      "iteration 5409: loss: 0.23802557587623596\n",
      "iteration 5410: loss: 0.23802432417869568\n",
      "iteration 5411: loss: 0.23802319169044495\n",
      "iteration 5412: loss: 0.23802189528942108\n",
      "iteration 5413: loss: 0.23802080750465393\n",
      "iteration 5414: loss: 0.23801955580711365\n",
      "iteration 5415: loss: 0.23801830410957336\n",
      "iteration 5416: loss: 0.23801717162132263\n",
      "iteration 5417: loss: 0.23801591992378235\n",
      "iteration 5418: loss: 0.23801465332508087\n",
      "iteration 5419: loss: 0.23801355063915253\n",
      "iteration 5420: loss: 0.23801223933696747\n",
      "iteration 5421: loss: 0.23801103234291077\n",
      "iteration 5422: loss: 0.23800981044769287\n",
      "iteration 5423: loss: 0.2380085438489914\n",
      "iteration 5424: loss: 0.2380073070526123\n",
      "iteration 5425: loss: 0.23800642788410187\n",
      "iteration 5426: loss: 0.2380051165819168\n",
      "iteration 5427: loss: 0.23800404369831085\n",
      "iteration 5428: loss: 0.23800280690193176\n",
      "iteration 5429: loss: 0.23800154030323029\n",
      "iteration 5430: loss: 0.23800039291381836\n",
      "iteration 5431: loss: 0.23799915611743927\n",
      "iteration 5432: loss: 0.23799791932106018\n",
      "iteration 5433: loss: 0.23799677193164825\n",
      "iteration 5434: loss: 0.23799553513526917\n",
      "iteration 5435: loss: 0.23799440264701843\n",
      "iteration 5436: loss: 0.23799316585063934\n",
      "iteration 5437: loss: 0.23799192905426025\n",
      "iteration 5438: loss: 0.23799081146717072\n",
      "iteration 5439: loss: 0.23798954486846924\n",
      "iteration 5440: loss: 0.23798830807209015\n",
      "iteration 5441: loss: 0.2379872053861618\n",
      "iteration 5442: loss: 0.23798593878746033\n",
      "iteration 5443: loss: 0.2379848212003708\n",
      "iteration 5444: loss: 0.2379835546016693\n",
      "iteration 5445: loss: 0.2379823625087738\n",
      "iteration 5446: loss: 0.23798123002052307\n",
      "iteration 5447: loss: 0.2379799783229828\n",
      "iteration 5448: loss: 0.2379787415266037\n",
      "iteration 5449: loss: 0.23797759413719177\n",
      "iteration 5450: loss: 0.23797640204429626\n",
      "iteration 5451: loss: 0.23797516524791718\n",
      "iteration 5452: loss: 0.23797424137592316\n",
      "iteration 5453: loss: 0.23797297477722168\n",
      "iteration 5454: loss: 0.23797185719013214\n",
      "iteration 5455: loss: 0.23797062039375305\n",
      "iteration 5456: loss: 0.23796936869621277\n",
      "iteration 5457: loss: 0.23796828091144562\n",
      "iteration 5458: loss: 0.23796701431274414\n",
      "iteration 5459: loss: 0.23796577751636505\n",
      "iteration 5460: loss: 0.2379647046327591\n",
      "iteration 5461: loss: 0.23796343803405762\n",
      "iteration 5462: loss: 0.2379622906446457\n",
      "iteration 5463: loss: 0.23796109855175018\n",
      "iteration 5464: loss: 0.2379598617553711\n",
      "iteration 5465: loss: 0.23795875906944275\n",
      "iteration 5466: loss: 0.23795747756958008\n",
      "iteration 5467: loss: 0.23795628547668457\n",
      "iteration 5468: loss: 0.23795516788959503\n",
      "iteration 5469: loss: 0.23795390129089355\n",
      "iteration 5470: loss: 0.23795267939567566\n",
      "iteration 5471: loss: 0.23795154690742493\n",
      "iteration 5472: loss: 0.23795035481452942\n",
      "iteration 5473: loss: 0.23794908821582794\n",
      "iteration 5474: loss: 0.2379479855298996\n",
      "iteration 5475: loss: 0.23794670403003693\n",
      "iteration 5476: loss: 0.23794560134410858\n",
      "iteration 5477: loss: 0.2379443645477295\n",
      "iteration 5478: loss: 0.2379431277513504\n",
      "iteration 5479: loss: 0.23794201016426086\n",
      "iteration 5480: loss: 0.23794075846672058\n",
      "iteration 5481: loss: 0.23793956637382507\n",
      "iteration 5482: loss: 0.23793847858905792\n",
      "iteration 5483: loss: 0.2379373013973236\n",
      "iteration 5484: loss: 0.23793606460094452\n",
      "iteration 5485: loss: 0.23793499171733856\n",
      "iteration 5486: loss: 0.23793378472328186\n",
      "iteration 5487: loss: 0.23793251812458038\n",
      "iteration 5488: loss: 0.23793141543865204\n",
      "iteration 5489: loss: 0.23793017864227295\n",
      "iteration 5490: loss: 0.23792894184589386\n",
      "iteration 5491: loss: 0.2379278689622879\n",
      "iteration 5492: loss: 0.2379266321659088\n",
      "iteration 5493: loss: 0.23792552947998047\n",
      "iteration 5494: loss: 0.23792429268360138\n",
      "iteration 5495: loss: 0.23792307078838348\n",
      "iteration 5496: loss: 0.23792198300361633\n",
      "iteration 5497: loss: 0.23792076110839844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5498: loss: 0.23791953921318054\n",
      "iteration 5499: loss: 0.237918421626091\n",
      "iteration 5500: loss: 0.2379172146320343\n",
      "iteration 5501: loss: 0.2379159927368164\n",
      "iteration 5502: loss: 0.23791489005088806\n",
      "iteration 5503: loss: 0.23791365325450897\n",
      "iteration 5504: loss: 0.23791241645812988\n",
      "iteration 5505: loss: 0.2379113733768463\n",
      "iteration 5506: loss: 0.23791012167930603\n",
      "iteration 5507: loss: 0.23790888488292694\n",
      "iteration 5508: loss: 0.2379077970981598\n",
      "iteration 5509: loss: 0.2379065454006195\n",
      "iteration 5510: loss: 0.23790530860424042\n",
      "iteration 5511: loss: 0.23790426552295685\n",
      "iteration 5512: loss: 0.23790304362773895\n",
      "iteration 5513: loss: 0.23790180683135986\n",
      "iteration 5514: loss: 0.23790070414543152\n",
      "iteration 5515: loss: 0.23789949715137482\n",
      "iteration 5516: loss: 0.2378983050584793\n",
      "iteration 5517: loss: 0.23789720237255096\n",
      "iteration 5518: loss: 0.23789596557617188\n",
      "iteration 5519: loss: 0.23789489269256592\n",
      "iteration 5520: loss: 0.23789362609386444\n",
      "iteration 5521: loss: 0.23789243400096893\n",
      "iteration 5522: loss: 0.23789136111736298\n",
      "iteration 5523: loss: 0.23789015412330627\n",
      "iteration 5524: loss: 0.237888902425766\n",
      "iteration 5525: loss: 0.23788781464099884\n",
      "iteration 5526: loss: 0.23788662254810333\n",
      "iteration 5527: loss: 0.23788538575172424\n",
      "iteration 5528: loss: 0.23788423836231232\n",
      "iteration 5529: loss: 0.23788301646709442\n",
      "iteration 5530: loss: 0.2378818243741989\n",
      "iteration 5531: loss: 0.23788073658943176\n",
      "iteration 5532: loss: 0.23787951469421387\n",
      "iteration 5533: loss: 0.23787827789783478\n",
      "iteration 5534: loss: 0.23787720501422882\n",
      "iteration 5535: loss: 0.2378760129213333\n",
      "iteration 5536: loss: 0.23787479102611542\n",
      "iteration 5537: loss: 0.23787371814250946\n",
      "iteration 5538: loss: 0.23787251114845276\n",
      "iteration 5539: loss: 0.23787108063697815\n",
      "iteration 5540: loss: 0.2378700077533722\n",
      "iteration 5541: loss: 0.23786881566047668\n",
      "iteration 5542: loss: 0.2378675490617752\n",
      "iteration 5543: loss: 0.23786649107933044\n",
      "iteration 5544: loss: 0.23786528408527374\n",
      "iteration 5545: loss: 0.23786406219005585\n",
      "iteration 5546: loss: 0.23786301910877228\n",
      "iteration 5547: loss: 0.2378617525100708\n",
      "iteration 5548: loss: 0.23786060512065887\n",
      "iteration 5549: loss: 0.23785948753356934\n",
      "iteration 5550: loss: 0.23785826563835144\n",
      "iteration 5551: loss: 0.23785707354545593\n",
      "iteration 5552: loss: 0.23785598576068878\n",
      "iteration 5553: loss: 0.23785479366779327\n",
      "iteration 5554: loss: 0.23785357177257538\n",
      "iteration 5555: loss: 0.23785249888896942\n",
      "iteration 5556: loss: 0.2378513067960739\n",
      "iteration 5557: loss: 0.23785023391246796\n",
      "iteration 5558: loss: 0.23784899711608887\n",
      "iteration 5559: loss: 0.23784780502319336\n",
      "iteration 5560: loss: 0.2378467321395874\n",
      "iteration 5561: loss: 0.2378455400466919\n",
      "iteration 5562: loss: 0.237844318151474\n",
      "iteration 5563: loss: 0.23784306645393372\n",
      "iteration 5564: loss: 0.2378418743610382\n",
      "iteration 5565: loss: 0.2378406524658203\n",
      "iteration 5566: loss: 0.23783960938453674\n",
      "iteration 5567: loss: 0.237838476896286\n",
      "iteration 5568: loss: 0.2378372848033905\n",
      "iteration 5569: loss: 0.23783619701862335\n",
      "iteration 5570: loss: 0.23783501982688904\n",
      "iteration 5571: loss: 0.23783381283283234\n",
      "iteration 5572: loss: 0.23783273994922638\n",
      "iteration 5573: loss: 0.23783151805400848\n",
      "iteration 5574: loss: 0.23783032596111298\n",
      "iteration 5575: loss: 0.23782925307750702\n",
      "iteration 5576: loss: 0.23782804608345032\n",
      "iteration 5577: loss: 0.23782679438591003\n",
      "iteration 5578: loss: 0.23782578110694885\n",
      "iteration 5579: loss: 0.23782455921173096\n",
      "iteration 5580: loss: 0.23782336711883545\n",
      "iteration 5581: loss: 0.23782213032245636\n",
      "iteration 5582: loss: 0.2378208190202713\n",
      "iteration 5583: loss: 0.2378196269273758\n",
      "iteration 5584: loss: 0.23781855404376984\n",
      "iteration 5585: loss: 0.23781737685203552\n",
      "iteration 5586: loss: 0.23781618475914001\n",
      "iteration 5587: loss: 0.23781514167785645\n",
      "iteration 5588: loss: 0.23781391978263855\n",
      "iteration 5589: loss: 0.23781272768974304\n",
      "iteration 5590: loss: 0.23781152069568634\n",
      "iteration 5591: loss: 0.23781046271324158\n",
      "iteration 5592: loss: 0.23780927062034607\n",
      "iteration 5593: loss: 0.23780806362628937\n",
      "iteration 5594: loss: 0.2378070056438446\n",
      "iteration 5595: loss: 0.2378058135509491\n",
      "iteration 5596: loss: 0.2378046214580536\n",
      "iteration 5597: loss: 0.23780333995819092\n",
      "iteration 5598: loss: 0.2378021776676178\n",
      "iteration 5599: loss: 0.23780092597007751\n",
      "iteration 5600: loss: 0.23779991269111633\n",
      "iteration 5601: loss: 0.23779869079589844\n",
      "iteration 5602: loss: 0.23779752850532532\n",
      "iteration 5603: loss: 0.23779647052288055\n",
      "iteration 5604: loss: 0.23779530823230743\n",
      "iteration 5605: loss: 0.23779407143592834\n",
      "iteration 5606: loss: 0.23779304325580597\n",
      "iteration 5607: loss: 0.23779182136058807\n",
      "iteration 5608: loss: 0.23779062926769257\n",
      "iteration 5609: loss: 0.237789586186409\n",
      "iteration 5610: loss: 0.23778817057609558\n",
      "iteration 5611: loss: 0.23778703808784485\n",
      "iteration 5612: loss: 0.2377859652042389\n",
      "iteration 5613: loss: 0.237784743309021\n",
      "iteration 5614: loss: 0.2377835512161255\n",
      "iteration 5615: loss: 0.2377825528383255\n",
      "iteration 5616: loss: 0.2377813309431076\n",
      "iteration 5617: loss: 0.2377801388502121\n",
      "iteration 5618: loss: 0.23777897655963898\n",
      "iteration 5619: loss: 0.23777790367603302\n",
      "iteration 5620: loss: 0.2377767264842987\n",
      "iteration 5621: loss: 0.2377755343914032\n",
      "iteration 5622: loss: 0.23777428269386292\n",
      "iteration 5623: loss: 0.2377731055021286\n",
      "iteration 5624: loss: 0.2377719134092331\n",
      "iteration 5625: loss: 0.23777088522911072\n",
      "iteration 5626: loss: 0.23776967823505402\n",
      "iteration 5627: loss: 0.2377685010433197\n",
      "iteration 5628: loss: 0.23776745796203613\n",
      "iteration 5629: loss: 0.23776626586914062\n",
      "iteration 5630: loss: 0.23776507377624512\n",
      "iteration 5631: loss: 0.2377639263868332\n",
      "iteration 5632: loss: 0.23776285350322723\n",
      "iteration 5633: loss: 0.2377617061138153\n",
      "iteration 5634: loss: 0.2377602756023407\n",
      "iteration 5635: loss: 0.23775926232337952\n",
      "iteration 5636: loss: 0.237758070230484\n",
      "iteration 5637: loss: 0.23775681853294373\n",
      "iteration 5638: loss: 0.23775577545166016\n",
      "iteration 5639: loss: 0.23775458335876465\n",
      "iteration 5640: loss: 0.23775342106819153\n",
      "iteration 5641: loss: 0.23775239288806915\n",
      "iteration 5642: loss: 0.23775120079517365\n",
      "iteration 5643: loss: 0.23775000870227814\n",
      "iteration 5644: loss: 0.23774877190589905\n",
      "iteration 5645: loss: 0.23774757981300354\n",
      "iteration 5646: loss: 0.2377464324235916\n",
      "iteration 5647: loss: 0.2377452403306961\n",
      "iteration 5648: loss: 0.2377443015575409\n",
      "iteration 5649: loss: 0.2377430945634842\n",
      "iteration 5650: loss: 0.23774190247058868\n",
      "iteration 5651: loss: 0.2377409040927887\n",
      "iteration 5652: loss: 0.2377397119998932\n",
      "iteration 5653: loss: 0.23773834109306335\n",
      "iteration 5654: loss: 0.23773732781410217\n",
      "iteration 5655: loss: 0.23773613572120667\n",
      "iteration 5656: loss: 0.23773495852947235\n",
      "iteration 5657: loss: 0.23773393034934998\n",
      "iteration 5658: loss: 0.23773273825645447\n",
      "iteration 5659: loss: 0.23773160576820374\n",
      "iteration 5660: loss: 0.23773035407066345\n",
      "iteration 5661: loss: 0.23772935569286346\n",
      "iteration 5662: loss: 0.23772796988487244\n",
      "iteration 5663: loss: 0.23772680759429932\n",
      "iteration 5664: loss: 0.23772577941417694\n",
      "iteration 5665: loss: 0.23772463202476501\n",
      "iteration 5666: loss: 0.23772339522838593\n",
      "iteration 5667: loss: 0.23772239685058594\n",
      "iteration 5668: loss: 0.237721249461174\n",
      "iteration 5669: loss: 0.2377200424671173\n",
      "iteration 5670: loss: 0.23771882057189941\n",
      "iteration 5671: loss: 0.2377176731824875\n",
      "iteration 5672: loss: 0.23771648108959198\n",
      "iteration 5673: loss: 0.23771528899669647\n",
      "iteration 5674: loss: 0.23771429061889648\n",
      "iteration 5675: loss: 0.23771309852600098\n",
      "iteration 5676: loss: 0.23771195113658905\n",
      "iteration 5677: loss: 0.23771090805530548\n",
      "iteration 5678: loss: 0.23770973086357117\n",
      "iteration 5679: loss: 0.23770837485790253\n",
      "iteration 5680: loss: 0.2377072274684906\n",
      "iteration 5681: loss: 0.23770618438720703\n",
      "iteration 5682: loss: 0.2377050220966339\n",
      "iteration 5683: loss: 0.2377038449048996\n",
      "iteration 5684: loss: 0.2377028465270996\n",
      "iteration 5685: loss: 0.2377016842365265\n",
      "iteration 5686: loss: 0.23770049214363098\n",
      "iteration 5687: loss: 0.23769915103912354\n",
      "iteration 5688: loss: 0.23769807815551758\n",
      "iteration 5689: loss: 0.23769697546958923\n",
      "iteration 5690: loss: 0.23769578337669373\n",
      "iteration 5691: loss: 0.23769478499889374\n",
      "iteration 5692: loss: 0.23769359290599823\n",
      "iteration 5693: loss: 0.23769235610961914\n",
      "iteration 5694: loss: 0.23769113421440125\n",
      "iteration 5695: loss: 0.23768997192382812\n",
      "iteration 5696: loss: 0.2376888245344162\n",
      "iteration 5697: loss: 0.23768766224384308\n",
      "iteration 5698: loss: 0.2376866340637207\n",
      "iteration 5699: loss: 0.23768547177314758\n",
      "iteration 5700: loss: 0.23768429458141327\n",
      "iteration 5701: loss: 0.2376832664012909\n",
      "iteration 5702: loss: 0.23768191039562225\n",
      "iteration 5703: loss: 0.23768076300621033\n",
      "iteration 5704: loss: 0.23767957091331482\n",
      "iteration 5705: loss: 0.23767857253551483\n",
      "iteration 5706: loss: 0.2376774251461029\n",
      "iteration 5707: loss: 0.23767629265785217\n",
      "iteration 5708: loss: 0.2376752644777298\n",
      "iteration 5709: loss: 0.23767390847206116\n",
      "iteration 5710: loss: 0.23767271637916565\n",
      "iteration 5711: loss: 0.23767158389091492\n",
      "iteration 5712: loss: 0.23767058551311493\n",
      "iteration 5713: loss: 0.2376694232225418\n",
      "iteration 5714: loss: 0.2376682460308075\n",
      "iteration 5715: loss: 0.23766693472862244\n",
      "iteration 5716: loss: 0.23766592144966125\n",
      "iteration 5717: loss: 0.23766477406024933\n",
      "iteration 5718: loss: 0.23766358196735382\n",
      "iteration 5719: loss: 0.23766255378723145\n",
      "iteration 5720: loss: 0.23766140639781952\n",
      "iteration 5721: loss: 0.2376602441072464\n",
      "iteration 5722: loss: 0.2376590222120285\n",
      "iteration 5723: loss: 0.23765787482261658\n",
      "iteration 5724: loss: 0.23765668272972107\n",
      "iteration 5725: loss: 0.23765556514263153\n",
      "iteration 5726: loss: 0.23765453696250916\n",
      "iteration 5727: loss: 0.23765349388122559\n",
      "iteration 5728: loss: 0.23765234649181366\n",
      "iteration 5729: loss: 0.23765099048614502\n",
      "iteration 5730: loss: 0.23764999210834503\n",
      "iteration 5731: loss: 0.2376488745212555\n",
      "iteration 5732: loss: 0.23764769732952118\n",
      "iteration 5733: loss: 0.2376466989517212\n",
      "iteration 5734: loss: 0.23764553666114807\n",
      "iteration 5735: loss: 0.23764419555664062\n",
      "iteration 5736: loss: 0.2376430481672287\n",
      "iteration 5737: loss: 0.23764201998710632\n",
      "iteration 5738: loss: 0.23764090240001678\n",
      "iteration 5739: loss: 0.23763975501060486\n",
      "iteration 5740: loss: 0.23763859272003174\n",
      "iteration 5741: loss: 0.23763737082481384\n",
      "iteration 5742: loss: 0.2376362383365631\n",
      "iteration 5743: loss: 0.23763509094715118\n",
      "iteration 5744: loss: 0.2376340627670288\n",
      "iteration 5745: loss: 0.23763291537761688\n",
      "iteration 5746: loss: 0.23763176798820496\n",
      "iteration 5747: loss: 0.2376304417848587\n",
      "iteration 5748: loss: 0.23762944340705872\n",
      "iteration 5749: loss: 0.23762822151184082\n",
      "iteration 5750: loss: 0.2376270592212677\n",
      "iteration 5751: loss: 0.23762604594230652\n",
      "iteration 5752: loss: 0.2376249134540558\n",
      "iteration 5753: loss: 0.23762357234954834\n",
      "iteration 5754: loss: 0.2376224249601364\n",
      "iteration 5755: loss: 0.2376214563846588\n",
      "iteration 5756: loss: 0.2376203089952469\n",
      "iteration 5757: loss: 0.23761916160583496\n",
      "iteration 5758: loss: 0.23761799931526184\n",
      "iteration 5759: loss: 0.23761682212352753\n",
      "iteration 5760: loss: 0.2376156747341156\n",
      "iteration 5761: loss: 0.23761451244354248\n",
      "iteration 5762: loss: 0.23761339485645294\n",
      "iteration 5763: loss: 0.23761236667633057\n",
      "iteration 5764: loss: 0.23761121928691864\n",
      "iteration 5765: loss: 0.23760986328125\n",
      "iteration 5766: loss: 0.2376089096069336\n",
      "iteration 5767: loss: 0.23760774731636047\n",
      "iteration 5768: loss: 0.23760659992694855\n",
      "iteration 5769: loss: 0.23760545253753662\n",
      "iteration 5770: loss: 0.23760423064231873\n",
      "iteration 5771: loss: 0.23760314285755157\n",
      "iteration 5772: loss: 0.23760196566581726\n",
      "iteration 5773: loss: 0.23760084807872772\n",
      "iteration 5774: loss: 0.23759984970092773\n",
      "iteration 5775: loss: 0.2375987321138382\n",
      "iteration 5776: loss: 0.23759742081165314\n",
      "iteration 5777: loss: 0.23759624361991882\n",
      "iteration 5778: loss: 0.23759524524211884\n",
      "iteration 5779: loss: 0.23759415745735168\n",
      "iteration 5780: loss: 0.23759298026561737\n",
      "iteration 5781: loss: 0.2375916689634323\n",
      "iteration 5782: loss: 0.2375907003879547\n",
      "iteration 5783: loss: 0.23758955299854279\n",
      "iteration 5784: loss: 0.23758840560913086\n",
      "iteration 5785: loss: 0.23758725821971893\n",
      "iteration 5786: loss: 0.2375860959291458\n",
      "iteration 5787: loss: 0.23758497834205627\n",
      "iteration 5788: loss: 0.23758380115032196\n",
      "iteration 5789: loss: 0.23758283257484436\n",
      "iteration 5790: loss: 0.23758168518543243\n",
      "iteration 5791: loss: 0.2375805377960205\n",
      "iteration 5792: loss: 0.23757919669151306\n",
      "iteration 5793: loss: 0.23757822811603546\n",
      "iteration 5794: loss: 0.23757711052894592\n",
      "iteration 5795: loss: 0.23757600784301758\n",
      "iteration 5796: loss: 0.23757486045360565\n",
      "iteration 5797: loss: 0.23757347464561462\n",
      "iteration 5798: loss: 0.23757252097129822\n",
      "iteration 5799: loss: 0.23757138848304749\n",
      "iteration 5800: loss: 0.23757028579711914\n",
      "iteration 5801: loss: 0.23756913840770721\n",
      "iteration 5802: loss: 0.23756806552410126\n",
      "iteration 5803: loss: 0.23756691813468933\n",
      "iteration 5804: loss: 0.2375657856464386\n",
      "iteration 5805: loss: 0.23756468296051025\n",
      "iteration 5806: loss: 0.23756369948387146\n",
      "iteration 5807: loss: 0.23756226897239685\n",
      "iteration 5808: loss: 0.23756113648414612\n",
      "iteration 5809: loss: 0.23756003379821777\n",
      "iteration 5810: loss: 0.23755905032157898\n",
      "iteration 5811: loss: 0.23755791783332825\n",
      "iteration 5812: loss: 0.2375566065311432\n",
      "iteration 5813: loss: 0.23755547404289246\n",
      "iteration 5814: loss: 0.23755452036857605\n",
      "iteration 5815: loss: 0.23755335807800293\n",
      "iteration 5816: loss: 0.23755225539207458\n",
      "iteration 5817: loss: 0.23755092918872833\n",
      "iteration 5818: loss: 0.23754997551441193\n",
      "iteration 5819: loss: 0.2375488579273224\n",
      "iteration 5820: loss: 0.23754771053791046\n",
      "iteration 5821: loss: 0.23754659295082092\n",
      "iteration 5822: loss: 0.2375454157590866\n",
      "iteration 5823: loss: 0.23754429817199707\n",
      "iteration 5824: loss: 0.23754318058490753\n",
      "iteration 5825: loss: 0.2375420331954956\n",
      "iteration 5826: loss: 0.23754072189331055\n",
      "iteration 5827: loss: 0.23753976821899414\n",
      "iteration 5828: loss: 0.2375386506319046\n",
      "iteration 5829: loss: 0.23753753304481506\n",
      "iteration 5830: loss: 0.23753638565540314\n",
      "iteration 5831: loss: 0.2375352382659912\n",
      "iteration 5832: loss: 0.23753412067890167\n",
      "iteration 5833: loss: 0.23753300309181213\n",
      "iteration 5834: loss: 0.23753182590007782\n",
      "iteration 5835: loss: 0.2375309020280838\n",
      "iteration 5836: loss: 0.23752956092357635\n",
      "iteration 5837: loss: 0.23752844333648682\n",
      "iteration 5838: loss: 0.2375272959470749\n",
      "iteration 5839: loss: 0.23752637207508087\n",
      "iteration 5840: loss: 0.2375250607728958\n",
      "iteration 5841: loss: 0.23752394318580627\n",
      "iteration 5842: loss: 0.23752281069755554\n",
      "iteration 5843: loss: 0.2375217229127884\n",
      "iteration 5844: loss: 0.2375207245349884\n",
      "iteration 5845: loss: 0.23751942813396454\n",
      "iteration 5846: loss: 0.23751835525035858\n",
      "iteration 5847: loss: 0.23751720786094666\n",
      "iteration 5848: loss: 0.23751625418663025\n",
      "iteration 5849: loss: 0.2375149428844452\n",
      "iteration 5850: loss: 0.23751382529735565\n",
      "iteration 5851: loss: 0.23751267790794373\n",
      "iteration 5852: loss: 0.2375117540359497\n",
      "iteration 5853: loss: 0.23751063644886017\n",
      "iteration 5854: loss: 0.23750929534435272\n",
      "iteration 5855: loss: 0.23750817775726318\n",
      "iteration 5856: loss: 0.23750707507133484\n",
      "iteration 5857: loss: 0.23750607669353485\n",
      "iteration 5858: loss: 0.2375047653913498\n",
      "iteration 5859: loss: 0.23750369250774384\n",
      "iteration 5860: loss: 0.2375025749206543\n",
      "iteration 5861: loss: 0.2375016212463379\n",
      "iteration 5862: loss: 0.23750030994415283\n",
      "iteration 5863: loss: 0.2374991923570633\n",
      "iteration 5864: loss: 0.23749808967113495\n",
      "iteration 5865: loss: 0.2374970018863678\n",
      "iteration 5866: loss: 0.2374957799911499\n",
      "iteration 5867: loss: 0.23749466240406036\n",
      "iteration 5868: loss: 0.23749351501464844\n",
      "iteration 5869: loss: 0.23749244213104248\n",
      "iteration 5870: loss: 0.2374914437532425\n",
      "iteration 5871: loss: 0.23749017715454102\n",
      "iteration 5872: loss: 0.23748905956745148\n",
      "iteration 5873: loss: 0.23748795688152313\n",
      "iteration 5874: loss: 0.2374868392944336\n",
      "iteration 5875: loss: 0.23748579621315002\n",
      "iteration 5876: loss: 0.23748469352722168\n",
      "iteration 5877: loss: 0.23748359084129333\n",
      "iteration 5878: loss: 0.2374824732542038\n",
      "iteration 5879: loss: 0.23748116195201874\n",
      "iteration 5880: loss: 0.2374802529811859\n",
      "iteration 5881: loss: 0.2374790608882904\n",
      "iteration 5882: loss: 0.23747801780700684\n",
      "iteration 5883: loss: 0.23747673630714417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5884: loss: 0.23747572302818298\n",
      "iteration 5885: loss: 0.23747465014457703\n",
      "iteration 5886: loss: 0.2374735325574875\n",
      "iteration 5887: loss: 0.23747222125530243\n",
      "iteration 5888: loss: 0.2374711036682129\n",
      "iteration 5889: loss: 0.23747017979621887\n",
      "iteration 5890: loss: 0.23746907711029053\n",
      "iteration 5891: loss: 0.23746776580810547\n",
      "iteration 5892: loss: 0.23746666312217712\n",
      "iteration 5893: loss: 0.23746557533740997\n",
      "iteration 5894: loss: 0.23746462166309357\n",
      "iteration 5895: loss: 0.2374633550643921\n",
      "iteration 5896: loss: 0.23746225237846375\n",
      "iteration 5897: loss: 0.2374611347913742\n",
      "iteration 5898: loss: 0.23746006190776825\n",
      "iteration 5899: loss: 0.23745889961719513\n",
      "iteration 5900: loss: 0.23745779693126678\n",
      "iteration 5901: loss: 0.23745670914649963\n",
      "iteration 5902: loss: 0.2374556064605713\n",
      "iteration 5903: loss: 0.23745429515838623\n",
      "iteration 5904: loss: 0.2374533712863922\n",
      "iteration 5905: loss: 0.23745223879814148\n",
      "iteration 5906: loss: 0.23745115101337433\n",
      "iteration 5907: loss: 0.23744983971118927\n",
      "iteration 5908: loss: 0.23744893074035645\n",
      "iteration 5909: loss: 0.2374478131532669\n",
      "iteration 5910: loss: 0.23744675517082214\n",
      "iteration 5911: loss: 0.23744544386863708\n",
      "iteration 5912: loss: 0.23744435608386993\n",
      "iteration 5913: loss: 0.23744340240955353\n",
      "iteration 5914: loss: 0.23744229972362518\n",
      "iteration 5915: loss: 0.2374410182237625\n",
      "iteration 5916: loss: 0.23743991553783417\n",
      "iteration 5917: loss: 0.23743882775306702\n",
      "iteration 5918: loss: 0.2374376803636551\n",
      "iteration 5919: loss: 0.23743657767772675\n",
      "iteration 5920: loss: 0.23743553459644318\n",
      "iteration 5921: loss: 0.23743443191051483\n",
      "iteration 5922: loss: 0.23743312060832977\n",
      "iteration 5923: loss: 0.23743216693401337\n",
      "iteration 5924: loss: 0.23743107914924622\n",
      "iteration 5925: loss: 0.23743000626564026\n",
      "iteration 5926: loss: 0.2374286651611328\n",
      "iteration 5927: loss: 0.23742754757404327\n",
      "iteration 5928: loss: 0.23742647469043732\n",
      "iteration 5929: loss: 0.2374255210161209\n",
      "iteration 5930: loss: 0.23742425441741943\n",
      "iteration 5931: loss: 0.2374231517314911\n",
      "iteration 5932: loss: 0.23742206394672394\n",
      "iteration 5933: loss: 0.23742076754570007\n",
      "iteration 5934: loss: 0.23741984367370605\n",
      "iteration 5935: loss: 0.2374187409877777\n",
      "iteration 5936: loss: 0.23741762340068817\n",
      "iteration 5937: loss: 0.2374163568019867\n",
      "iteration 5938: loss: 0.23741526901721954\n",
      "iteration 5939: loss: 0.23741436004638672\n",
      "iteration 5940: loss: 0.23741325736045837\n",
      "iteration 5941: loss: 0.2374119758605957\n",
      "iteration 5942: loss: 0.23741090297698975\n",
      "iteration 5943: loss: 0.2374098002910614\n",
      "iteration 5944: loss: 0.23740851879119873\n",
      "iteration 5945: loss: 0.2374076098203659\n",
      "iteration 5946: loss: 0.23740661144256592\n",
      "iteration 5947: loss: 0.23740549385547638\n",
      "iteration 5948: loss: 0.2374042570590973\n",
      "iteration 5949: loss: 0.23740315437316895\n",
      "iteration 5950: loss: 0.23740223050117493\n",
      "iteration 5951: loss: 0.23740115761756897\n",
      "iteration 5952: loss: 0.2373998612165451\n",
      "iteration 5953: loss: 0.23739878833293915\n",
      "iteration 5954: loss: 0.2373977154493332\n",
      "iteration 5955: loss: 0.23739643394947052\n",
      "iteration 5956: loss: 0.2373954802751541\n",
      "iteration 5957: loss: 0.23739442229270935\n",
      "iteration 5958: loss: 0.2373933494091034\n",
      "iteration 5959: loss: 0.23739203810691833\n",
      "iteration 5960: loss: 0.23739095032215118\n",
      "iteration 5961: loss: 0.23738989233970642\n",
      "iteration 5962: loss: 0.2373887598514557\n",
      "iteration 5963: loss: 0.23738770186901093\n",
      "iteration 5964: loss: 0.23738661408424377\n",
      "iteration 5965: loss: 0.23738551139831543\n",
      "iteration 5966: loss: 0.23738427460193634\n",
      "iteration 5967: loss: 0.23738333582878113\n",
      "iteration 5968: loss: 0.23738224804401398\n",
      "iteration 5969: loss: 0.2373810112476349\n",
      "iteration 5970: loss: 0.23737990856170654\n",
      "iteration 5971: loss: 0.23737883567810059\n",
      "iteration 5972: loss: 0.23737773299217224\n",
      "iteration 5973: loss: 0.2373766005039215\n",
      "iteration 5974: loss: 0.23737558722496033\n",
      "iteration 5975: loss: 0.2373744696378708\n",
      "iteration 5976: loss: 0.2373732030391693\n",
      "iteration 5977: loss: 0.23737213015556335\n",
      "iteration 5978: loss: 0.23737108707427979\n",
      "iteration 5979: loss: 0.23736998438835144\n",
      "iteration 5980: loss: 0.2373688668012619\n",
      "iteration 5981: loss: 0.23736782371997833\n",
      "iteration 5982: loss: 0.23736672103405\n",
      "iteration 5983: loss: 0.2373654544353485\n",
      "iteration 5984: loss: 0.23736438155174255\n",
      "iteration 5985: loss: 0.23736348748207092\n",
      "iteration 5986: loss: 0.23736223578453064\n",
      "iteration 5987: loss: 0.23736104369163513\n",
      "iteration 5988: loss: 0.23735997080802917\n",
      "iteration 5989: loss: 0.23735873401165009\n",
      "iteration 5990: loss: 0.23735764622688293\n",
      "iteration 5991: loss: 0.23735670745372772\n",
      "iteration 5992: loss: 0.23735561966896057\n",
      "iteration 5993: loss: 0.2373543679714203\n",
      "iteration 5994: loss: 0.23735332489013672\n",
      "iteration 5995: loss: 0.23735225200653076\n",
      "iteration 5996: loss: 0.23735098540782928\n",
      "iteration 5997: loss: 0.23735007643699646\n",
      "iteration 5998: loss: 0.2373489886522293\n",
      "iteration 5999: loss: 0.23734775185585022\n",
      "iteration 6000: loss: 0.23734667897224426\n",
      "iteration 6001: loss: 0.23734557628631592\n",
      "iteration 6002: loss: 0.23734453320503235\n",
      "iteration 6003: loss: 0.23734328150749207\n",
      "iteration 6004: loss: 0.23734238743782043\n",
      "iteration 6005: loss: 0.23734131455421448\n",
      "iteration 6006: loss: 0.237340047955513\n",
      "iteration 6007: loss: 0.23733898997306824\n",
      "iteration 6008: loss: 0.23733794689178467\n",
      "iteration 6009: loss: 0.237336665391922\n",
      "iteration 6010: loss: 0.23733560740947723\n",
      "iteration 6011: loss: 0.23733465373516083\n",
      "iteration 6012: loss: 0.23733341693878174\n",
      "iteration 6013: loss: 0.2373323142528534\n",
      "iteration 6014: loss: 0.2373313009738922\n",
      "iteration 6015: loss: 0.23733031749725342\n",
      "iteration 6016: loss: 0.23732900619506836\n",
      "iteration 6017: loss: 0.23732812702655792\n",
      "iteration 6018: loss: 0.23732705414295197\n",
      "iteration 6019: loss: 0.23732581734657288\n",
      "iteration 6020: loss: 0.2373247593641281\n",
      "iteration 6021: loss: 0.23732367157936096\n",
      "iteration 6022: loss: 0.23732247948646545\n",
      "iteration 6023: loss: 0.23732134699821472\n",
      "iteration 6024: loss: 0.23732049763202667\n",
      "iteration 6025: loss: 0.2373192310333252\n",
      "iteration 6026: loss: 0.23731818795204163\n",
      "iteration 6027: loss: 0.23731711506843567\n",
      "iteration 6028: loss: 0.23731586337089539\n",
      "iteration 6029: loss: 0.23731479048728943\n",
      "iteration 6030: loss: 0.23731374740600586\n",
      "iteration 6031: loss: 0.23731251060962677\n",
      "iteration 6032: loss: 0.23731157183647156\n",
      "iteration 6033: loss: 0.237310528755188\n",
      "iteration 6034: loss: 0.2373092919588089\n",
      "iteration 6035: loss: 0.23730821907520294\n",
      "iteration 6036: loss: 0.23730714619159698\n",
      "iteration 6037: loss: 0.23730608820915222\n",
      "iteration 6038: loss: 0.23730483651161194\n",
      "iteration 6039: loss: 0.2373039424419403\n",
      "iteration 6040: loss: 0.23730286955833435\n",
      "iteration 6041: loss: 0.23730163276195526\n",
      "iteration 6042: loss: 0.2373005896806717\n",
      "iteration 6043: loss: 0.23729948699474335\n",
      "iteration 6044: loss: 0.23729829490184784\n",
      "iteration 6045: loss: 0.23729725182056427\n",
      "iteration 6046: loss: 0.23729637265205383\n",
      "iteration 6047: loss: 0.23729510605335236\n",
      "iteration 6048: loss: 0.237294003367424\n",
      "iteration 6049: loss: 0.23729297518730164\n",
      "iteration 6050: loss: 0.23729164898395538\n",
      "iteration 6051: loss: 0.23729059100151062\n",
      "iteration 6052: loss: 0.23728957772254944\n",
      "iteration 6053: loss: 0.23728831112384796\n",
      "iteration 6054: loss: 0.23728743195533752\n",
      "iteration 6055: loss: 0.23728637397289276\n",
      "iteration 6056: loss: 0.23728513717651367\n",
      "iteration 6057: loss: 0.2372841089963913\n",
      "iteration 6058: loss: 0.23728303611278534\n",
      "iteration 6059: loss: 0.23728182911872864\n",
      "iteration 6060: loss: 0.23728075623512268\n",
      "iteration 6061: loss: 0.23727969825267792\n",
      "iteration 6062: loss: 0.23727858066558838\n",
      "iteration 6063: loss: 0.237277552485466\n",
      "iteration 6064: loss: 0.23727650940418243\n",
      "iteration 6065: loss: 0.23727527260780334\n",
      "iteration 6066: loss: 0.23727421462535858\n",
      "iteration 6067: loss: 0.23727314174175262\n",
      "iteration 6068: loss: 0.23727187514305115\n",
      "iteration 6069: loss: 0.23727087676525116\n",
      "iteration 6070: loss: 0.23726996779441833\n",
      "iteration 6071: loss: 0.23726873099803925\n",
      "iteration 6072: loss: 0.23726770281791687\n",
      "iteration 6073: loss: 0.23726646602153778\n",
      "iteration 6074: loss: 0.2372654229402542\n",
      "iteration 6075: loss: 0.23726435005664825\n",
      "iteration 6076: loss: 0.23726312816143036\n",
      "iteration 6077: loss: 0.23726212978363037\n",
      "iteration 6078: loss: 0.23726105690002441\n",
      "iteration 6079: loss: 0.23725982010364532\n",
      "iteration 6080: loss: 0.23725895583629608\n",
      "iteration 6081: loss: 0.2372579127550125\n",
      "iteration 6082: loss: 0.237256720662117\n",
      "iteration 6083: loss: 0.23725572228431702\n",
      "iteration 6084: loss: 0.23725466430187225\n",
      "iteration 6085: loss: 0.23725339770317078\n",
      "iteration 6086: loss: 0.2372523993253708\n",
      "iteration 6087: loss: 0.23725132644176483\n",
      "iteration 6088: loss: 0.23725028336048126\n",
      "iteration 6089: loss: 0.2372492104768753\n",
      "iteration 6090: loss: 0.23724813759326935\n",
      "iteration 6091: loss: 0.23724694550037384\n",
      "iteration 6092: loss: 0.23724588751792908\n",
      "iteration 6093: loss: 0.2372448444366455\n",
      "iteration 6094: loss: 0.23724360764026642\n",
      "iteration 6095: loss: 0.23724257946014404\n",
      "iteration 6096: loss: 0.23724135756492615\n",
      "iteration 6097: loss: 0.2372404783964157\n",
      "iteration 6098: loss: 0.23723943531513214\n",
      "iteration 6099: loss: 0.23723819851875305\n",
      "iteration 6100: loss: 0.23723717033863068\n",
      "iteration 6101: loss: 0.2372361421585083\n",
      "iteration 6102: loss: 0.2372349202632904\n",
      "iteration 6103: loss: 0.23723387718200684\n",
      "iteration 6104: loss: 0.23723283410072327\n",
      "iteration 6105: loss: 0.23723164200782776\n",
      "iteration 6106: loss: 0.2372305691242218\n",
      "iteration 6107: loss: 0.23722967505455017\n",
      "iteration 6108: loss: 0.23722846806049347\n",
      "iteration 6109: loss: 0.2372274100780487\n",
      "iteration 6110: loss: 0.237226203083992\n",
      "iteration 6111: loss: 0.23722514510154724\n",
      "iteration 6112: loss: 0.23722414672374725\n",
      "iteration 6113: loss: 0.23722290992736816\n",
      "iteration 6114: loss: 0.23722180724143982\n",
      "iteration 6115: loss: 0.23722076416015625\n",
      "iteration 6116: loss: 0.23721957206726074\n",
      "iteration 6117: loss: 0.23721866309642792\n",
      "iteration 6118: loss: 0.23721762001514435\n",
      "iteration 6119: loss: 0.23721642792224884\n",
      "iteration 6120: loss: 0.23721535503864288\n",
      "iteration 6121: loss: 0.23721416294574738\n",
      "iteration 6122: loss: 0.2372131049633026\n",
      "iteration 6123: loss: 0.23721209168434143\n",
      "iteration 6124: loss: 0.23721083998680115\n",
      "iteration 6125: loss: 0.23720979690551758\n",
      "iteration 6126: loss: 0.237208753824234\n",
      "iteration 6127: loss: 0.23720774054527283\n",
      "iteration 6128: loss: 0.23720669746398926\n",
      "iteration 6129: loss: 0.2372056543827057\n",
      "iteration 6130: loss: 0.23720446228981018\n",
      "iteration 6131: loss: 0.23720340430736542\n",
      "iteration 6132: loss: 0.2372022122144699\n",
      "iteration 6133: loss: 0.23720116913318634\n",
      "iteration 6134: loss: 0.23720017075538635\n",
      "iteration 6135: loss: 0.23719894886016846\n",
      "iteration 6136: loss: 0.23719792068004608\n",
      "iteration 6137: loss: 0.2371968775987625\n",
      "iteration 6138: loss: 0.237195685505867\n",
      "iteration 6139: loss: 0.23719465732574463\n",
      "iteration 6140: loss: 0.23719358444213867\n",
      "iteration 6141: loss: 0.2371925562620163\n",
      "iteration 6142: loss: 0.23719152808189392\n",
      "iteration 6143: loss: 0.2371903359889984\n",
      "iteration 6144: loss: 0.23718932271003723\n",
      "iteration 6145: loss: 0.23718824982643127\n",
      "iteration 6146: loss: 0.23718702793121338\n",
      "iteration 6147: loss: 0.23718610405921936\n",
      "iteration 6148: loss: 0.23718507587909698\n",
      "iteration 6149: loss: 0.23718388378620148\n",
      "iteration 6150: loss: 0.2371828258037567\n",
      "iteration 6151: loss: 0.2371816188097\n",
      "iteration 6152: loss: 0.23718078434467316\n",
      "iteration 6153: loss: 0.2371797263622284\n",
      "iteration 6154: loss: 0.23717853426933289\n",
      "iteration 6155: loss: 0.2371775209903717\n",
      "iteration 6156: loss: 0.23717650771141052\n",
      "iteration 6157: loss: 0.23717525601387024\n",
      "iteration 6158: loss: 0.23717424273490906\n",
      "iteration 6159: loss: 0.23717303574085236\n",
      "iteration 6160: loss: 0.23717203736305237\n",
      "iteration 6161: loss: 0.2371709793806076\n",
      "iteration 6162: loss: 0.2371697872877121\n",
      "iteration 6163: loss: 0.2371687889099121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6164: loss: 0.23716774582862854\n",
      "iteration 6165: loss: 0.23716656863689423\n",
      "iteration 6166: loss: 0.23716554045677185\n",
      "iteration 6167: loss: 0.23716433346271515\n",
      "iteration 6168: loss: 0.2371634691953659\n",
      "iteration 6169: loss: 0.23716244101524353\n",
      "iteration 6170: loss: 0.23716124892234802\n",
      "iteration 6171: loss: 0.23716020584106445\n",
      "iteration 6172: loss: 0.23715904355049133\n",
      "iteration 6173: loss: 0.23715801537036896\n",
      "iteration 6174: loss: 0.23715698719024658\n",
      "iteration 6175: loss: 0.23715579509735107\n",
      "iteration 6176: loss: 0.2371547669172287\n",
      "iteration 6177: loss: 0.23715360462665558\n",
      "iteration 6178: loss: 0.2371525764465332\n",
      "iteration 6179: loss: 0.23715154826641083\n",
      "iteration 6180: loss: 0.23715026676654816\n",
      "iteration 6181: loss: 0.2371492087841034\n",
      "iteration 6182: loss: 0.2371482104063034\n",
      "iteration 6183: loss: 0.2371470183134079\n",
      "iteration 6184: loss: 0.23714616894721985\n",
      "iteration 6185: loss: 0.23714497685432434\n",
      "iteration 6186: loss: 0.23714396357536316\n",
      "iteration 6187: loss: 0.23714296519756317\n",
      "iteration 6188: loss: 0.23714175820350647\n",
      "iteration 6189: loss: 0.2371407449245453\n",
      "iteration 6190: loss: 0.2371395379304886\n",
      "iteration 6191: loss: 0.2371385544538498\n",
      "iteration 6192: loss: 0.23713752627372742\n",
      "iteration 6193: loss: 0.2371363341808319\n",
      "iteration 6194: loss: 0.23713532090187073\n",
      "iteration 6195: loss: 0.23713412880897522\n",
      "iteration 6196: loss: 0.23713310062885284\n",
      "iteration 6197: loss: 0.23713207244873047\n",
      "iteration 6198: loss: 0.23713091015815735\n",
      "iteration 6199: loss: 0.23712985217571259\n",
      "iteration 6200: loss: 0.23712868988513947\n",
      "iteration 6201: loss: 0.23712769150733948\n",
      "iteration 6202: loss: 0.2371266782283783\n",
      "iteration 6203: loss: 0.2371254861354828\n",
      "iteration 6204: loss: 0.2371244728565216\n",
      "iteration 6205: loss: 0.23712344467639923\n",
      "iteration 6206: loss: 0.23712241649627686\n",
      "iteration 6207: loss: 0.23712141811847687\n",
      "iteration 6208: loss: 0.23712024092674255\n",
      "iteration 6209: loss: 0.23711922764778137\n",
      "iteration 6210: loss: 0.23711805045604706\n",
      "iteration 6211: loss: 0.23711708188056946\n",
      "iteration 6212: loss: 0.23711593449115753\n",
      "iteration 6213: loss: 0.23711493611335754\n",
      "iteration 6214: loss: 0.23711390793323517\n",
      "iteration 6215: loss: 0.23711273074150085\n",
      "iteration 6216: loss: 0.23711173236370087\n",
      "iteration 6217: loss: 0.23711052536964417\n",
      "iteration 6218: loss: 0.23710951209068298\n",
      "iteration 6219: loss: 0.237108513712883\n",
      "iteration 6220: loss: 0.2371072769165039\n",
      "iteration 6221: loss: 0.2371063530445099\n",
      "iteration 6222: loss: 0.237105131149292\n",
      "iteration 6223: loss: 0.2371041476726532\n",
      "iteration 6224: loss: 0.23710307478904724\n",
      "iteration 6225: loss: 0.2371019423007965\n",
      "iteration 6226: loss: 0.23710091412067413\n",
      "iteration 6227: loss: 0.23709973692893982\n",
      "iteration 6228: loss: 0.23709873855113983\n",
      "iteration 6229: loss: 0.23709776997566223\n",
      "iteration 6230: loss: 0.23709671199321747\n",
      "iteration 6231: loss: 0.23709571361541748\n",
      "iteration 6232: loss: 0.23709452152252197\n",
      "iteration 6233: loss: 0.23709353804588318\n",
      "iteration 6234: loss: 0.23709234595298767\n",
      "iteration 6235: loss: 0.23709134757518768\n",
      "iteration 6236: loss: 0.2370903491973877\n",
      "iteration 6237: loss: 0.2370891571044922\n",
      "iteration 6238: loss: 0.2370881736278534\n",
      "iteration 6239: loss: 0.23708698153495789\n",
      "iteration 6240: loss: 0.2370859682559967\n",
      "iteration 6241: loss: 0.2370850145816803\n",
      "iteration 6242: loss: 0.2370838224887848\n",
      "iteration 6243: loss: 0.2370828092098236\n",
      "iteration 6244: loss: 0.2370816469192505\n",
      "iteration 6245: loss: 0.23708060383796692\n",
      "iteration 6246: loss: 0.23707962036132812\n",
      "iteration 6247: loss: 0.237078458070755\n",
      "iteration 6248: loss: 0.23707738518714905\n",
      "iteration 6249: loss: 0.23707619309425354\n",
      "iteration 6250: loss: 0.23707520961761475\n",
      "iteration 6251: loss: 0.23707398772239685\n",
      "iteration 6252: loss: 0.23707301914691925\n",
      "iteration 6253: loss: 0.23707203567028046\n",
      "iteration 6254: loss: 0.23707084357738495\n",
      "iteration 6255: loss: 0.23706984519958496\n",
      "iteration 6256: loss: 0.23706869781017303\n",
      "iteration 6257: loss: 0.23706766963005066\n",
      "iteration 6258: loss: 0.23706650733947754\n",
      "iteration 6259: loss: 0.23706550896167755\n",
      "iteration 6260: loss: 0.23706452548503876\n",
      "iteration 6261: loss: 0.23706336319446564\n",
      "iteration 6262: loss: 0.23706237971782684\n",
      "iteration 6263: loss: 0.23706118762493134\n",
      "iteration 6264: loss: 0.23706018924713135\n",
      "iteration 6265: loss: 0.23705902695655823\n",
      "iteration 6266: loss: 0.23705804347991943\n",
      "iteration 6267: loss: 0.23705701529979706\n",
      "iteration 6268: loss: 0.23705586791038513\n",
      "iteration 6269: loss: 0.23705486953258514\n",
      "iteration 6270: loss: 0.23705370724201202\n",
      "iteration 6271: loss: 0.23705272376537323\n",
      "iteration 6272: loss: 0.2370515614748001\n",
      "iteration 6273: loss: 0.23705056309700012\n",
      "iteration 6274: loss: 0.2370496541261673\n",
      "iteration 6275: loss: 0.2370484620332718\n",
      "iteration 6276: loss: 0.237047478556633\n",
      "iteration 6277: loss: 0.23704631626605988\n",
      "iteration 6278: loss: 0.2370453178882599\n",
      "iteration 6279: loss: 0.23704417049884796\n",
      "iteration 6280: loss: 0.23704314231872559\n",
      "iteration 6281: loss: 0.2370421588420868\n",
      "iteration 6282: loss: 0.23704102635383606\n",
      "iteration 6283: loss: 0.23704001307487488\n",
      "iteration 6284: loss: 0.23703885078430176\n",
      "iteration 6285: loss: 0.23703785240650177\n",
      "iteration 6286: loss: 0.23703670501708984\n",
      "iteration 6287: loss: 0.23703570663928986\n",
      "iteration 6288: loss: 0.23703472316265106\n",
      "iteration 6289: loss: 0.23703356087207794\n",
      "iteration 6290: loss: 0.23703256249427795\n",
      "iteration 6291: loss: 0.23703138530254364\n",
      "iteration 6292: loss: 0.23703041672706604\n",
      "iteration 6293: loss: 0.2370292693376541\n",
      "iteration 6294: loss: 0.23702828586101532\n",
      "iteration 6295: loss: 0.23702728748321533\n",
      "iteration 6296: loss: 0.2370261400938034\n",
      "iteration 6297: loss: 0.23702514171600342\n",
      "iteration 6298: loss: 0.2370239943265915\n",
      "iteration 6299: loss: 0.2370230257511139\n",
      "iteration 6300: loss: 0.23702184855937958\n",
      "iteration 6301: loss: 0.2370208501815796\n",
      "iteration 6302: loss: 0.23701970279216766\n",
      "iteration 6303: loss: 0.23701873421669006\n",
      "iteration 6304: loss: 0.23701772093772888\n",
      "iteration 6305: loss: 0.23701658844947815\n",
      "iteration 6306: loss: 0.23701557517051697\n",
      "iteration 6307: loss: 0.23701444268226624\n",
      "iteration 6308: loss: 0.23701342940330505\n",
      "iteration 6309: loss: 0.23701226711273193\n",
      "iteration 6310: loss: 0.23701128363609314\n",
      "iteration 6311: loss: 0.23701031506061554\n",
      "iteration 6312: loss: 0.237009197473526\n",
      "iteration 6313: loss: 0.23700816929340363\n",
      "iteration 6314: loss: 0.2370069921016693\n",
      "iteration 6315: loss: 0.2370060384273529\n",
      "iteration 6316: loss: 0.2370048463344574\n",
      "iteration 6317: loss: 0.237003892660141\n",
      "iteration 6318: loss: 0.2370026856660843\n",
      "iteration 6319: loss: 0.2370016872882843\n",
      "iteration 6320: loss: 0.23700053989887238\n",
      "iteration 6321: loss: 0.23699955642223358\n",
      "iteration 6322: loss: 0.2369985580444336\n",
      "iteration 6323: loss: 0.23699745535850525\n",
      "iteration 6324: loss: 0.23699645698070526\n",
      "iteration 6325: loss: 0.23699529469013214\n",
      "iteration 6326: loss: 0.23699431121349335\n",
      "iteration 6327: loss: 0.23699316382408142\n",
      "iteration 6328: loss: 0.23699221014976501\n",
      "iteration 6329: loss: 0.2369910478591919\n",
      "iteration 6330: loss: 0.23698990046977997\n",
      "iteration 6331: loss: 0.23698875308036804\n",
      "iteration 6332: loss: 0.23698775470256805\n",
      "iteration 6333: loss: 0.23698678612709045\n",
      "iteration 6334: loss: 0.23698565363883972\n",
      "iteration 6335: loss: 0.2369847595691681\n",
      "iteration 6336: loss: 0.23698362708091736\n",
      "iteration 6337: loss: 0.23698265850543976\n",
      "iteration 6338: loss: 0.23698146641254425\n",
      "iteration 6339: loss: 0.23698051273822784\n",
      "iteration 6340: loss: 0.23697936534881592\n",
      "iteration 6341: loss: 0.23697838187217712\n",
      "iteration 6342: loss: 0.236977219581604\n",
      "iteration 6343: loss: 0.2369762659072876\n",
      "iteration 6344: loss: 0.2369752824306488\n",
      "iteration 6345: loss: 0.23697414994239807\n",
      "iteration 6346: loss: 0.23697319626808167\n",
      "iteration 6347: loss: 0.23697204887866974\n",
      "iteration 6348: loss: 0.23697105050086975\n",
      "iteration 6349: loss: 0.23696991801261902\n",
      "iteration 6350: loss: 0.23696894943714142\n",
      "iteration 6351: loss: 0.2369678020477295\n",
      "iteration 6352: loss: 0.2369668036699295\n",
      "iteration 6353: loss: 0.2369658201932907\n",
      "iteration 6354: loss: 0.23696473240852356\n",
      "iteration 6355: loss: 0.23696374893188477\n",
      "iteration 6356: loss: 0.23696260154247284\n",
      "iteration 6357: loss: 0.2369614839553833\n",
      "iteration 6358: loss: 0.23696032166481018\n",
      "iteration 6359: loss: 0.23695936799049377\n",
      "iteration 6360: loss: 0.23695823550224304\n",
      "iteration 6361: loss: 0.23695728182792664\n",
      "iteration 6362: loss: 0.23695611953735352\n",
      "iteration 6363: loss: 0.2369551658630371\n",
      "iteration 6364: loss: 0.23695404827594757\n",
      "iteration 6365: loss: 0.23695304989814758\n",
      "iteration 6366: loss: 0.23695191740989685\n",
      "iteration 6367: loss: 0.23695096373558044\n",
      "iteration 6368: loss: 0.23694996535778046\n",
      "iteration 6369: loss: 0.23694881796836853\n",
      "iteration 6370: loss: 0.23694786429405212\n",
      "iteration 6371: loss: 0.2369467318058014\n",
      "iteration 6372: loss: 0.23694577813148499\n",
      "iteration 6373: loss: 0.23694463074207306\n",
      "iteration 6374: loss: 0.23694364726543427\n",
      "iteration 6375: loss: 0.23694248497486115\n",
      "iteration 6376: loss: 0.2369414120912552\n",
      "iteration 6377: loss: 0.23694029450416565\n",
      "iteration 6378: loss: 0.23693926632404327\n",
      "iteration 6379: loss: 0.23693814873695374\n",
      "iteration 6380: loss: 0.23693720996379852\n",
      "iteration 6381: loss: 0.23693609237670898\n",
      "iteration 6382: loss: 0.2369351089000702\n",
      "iteration 6383: loss: 0.23693399131298065\n",
      "iteration 6384: loss: 0.23693306744098663\n",
      "iteration 6385: loss: 0.23693208396434784\n",
      "iteration 6386: loss: 0.23693092167377472\n",
      "iteration 6387: loss: 0.2369299829006195\n",
      "iteration 6388: loss: 0.23692885041236877\n",
      "iteration 6389: loss: 0.23692789673805237\n",
      "iteration 6390: loss: 0.23692667484283447\n",
      "iteration 6391: loss: 0.23692569136619568\n",
      "iteration 6392: loss: 0.23692438006401062\n",
      "iteration 6393: loss: 0.2369234561920166\n",
      "iteration 6394: loss: 0.23692230880260468\n",
      "iteration 6395: loss: 0.23692139983177185\n",
      "iteration 6396: loss: 0.2369202822446823\n",
      "iteration 6397: loss: 0.2369193583726883\n",
      "iteration 6398: loss: 0.23691824078559875\n",
      "iteration 6399: loss: 0.23691725730895996\n",
      "iteration 6400: loss: 0.23691610991954803\n",
      "iteration 6401: loss: 0.2369152009487152\n",
      "iteration 6402: loss: 0.2369140386581421\n",
      "iteration 6403: loss: 0.23691308498382568\n",
      "iteration 6404: loss: 0.23691198229789734\n",
      "iteration 6405: loss: 0.23691101372241974\n",
      "iteration 6406: loss: 0.23690971732139587\n",
      "iteration 6407: loss: 0.23690874874591827\n",
      "iteration 6408: loss: 0.23690779507160187\n",
      "iteration 6409: loss: 0.23690664768218994\n",
      "iteration 6410: loss: 0.23690572381019592\n",
      "iteration 6411: loss: 0.236904576420784\n",
      "iteration 6412: loss: 0.2369035929441452\n",
      "iteration 6413: loss: 0.23690252006053925\n",
      "iteration 6414: loss: 0.23690152168273926\n",
      "iteration 6415: loss: 0.23690040409564972\n",
      "iteration 6416: loss: 0.2368994504213333\n",
      "iteration 6417: loss: 0.23689834773540497\n",
      "iteration 6418: loss: 0.23689737915992737\n",
      "iteration 6419: loss: 0.23689627647399902\n",
      "iteration 6420: loss: 0.2368951290845871\n",
      "iteration 6421: loss: 0.23689401149749756\n",
      "iteration 6422: loss: 0.23689305782318115\n",
      "iteration 6423: loss: 0.2368919551372528\n",
      "iteration 6424: loss: 0.2368910014629364\n",
      "iteration 6425: loss: 0.23688988387584686\n",
      "iteration 6426: loss: 0.23688893020153046\n",
      "iteration 6427: loss: 0.23688781261444092\n",
      "iteration 6428: loss: 0.2368868887424469\n",
      "iteration 6429: loss: 0.23688575625419617\n",
      "iteration 6430: loss: 0.23688462376594543\n",
      "iteration 6431: loss: 0.2368834912776947\n",
      "iteration 6432: loss: 0.23688256740570068\n",
      "iteration 6433: loss: 0.23688142001628876\n",
      "iteration 6434: loss: 0.23688049614429474\n",
      "iteration 6435: loss: 0.2368793785572052\n",
      "iteration 6436: loss: 0.2368784248828888\n",
      "iteration 6437: loss: 0.2368774712085724\n",
      "iteration 6438: loss: 0.23687633872032166\n",
      "iteration 6439: loss: 0.23687538504600525\n",
      "iteration 6440: loss: 0.23687422275543213\n",
      "iteration 6441: loss: 0.2368733435869217\n",
      "iteration 6442: loss: 0.23687200248241425\n",
      "iteration 6443: loss: 0.23687109351158142\n",
      "iteration 6444: loss: 0.23686997592449188\n",
      "iteration 6445: loss: 0.23686905205249786\n",
      "iteration 6446: loss: 0.23686794936656952\n",
      "iteration 6447: loss: 0.2368669956922531\n",
      "iteration 6448: loss: 0.2368658483028412\n",
      "iteration 6449: loss: 0.23686492443084717\n",
      "iteration 6450: loss: 0.23686380684375763\n",
      "iteration 6451: loss: 0.23686270415782928\n",
      "iteration 6452: loss: 0.23686161637306213\n",
      "iteration 6453: loss: 0.23686063289642334\n",
      "iteration 6454: loss: 0.23685960471630096\n",
      "iteration 6455: loss: 0.23685868084430695\n",
      "iteration 6456: loss: 0.2368575632572174\n",
      "iteration 6457: loss: 0.236856609582901\n",
      "iteration 6458: loss: 0.23685550689697266\n",
      "iteration 6459: loss: 0.2368544042110443\n",
      "iteration 6460: loss: 0.23685327172279358\n",
      "iteration 6461: loss: 0.23685233294963837\n",
      "iteration 6462: loss: 0.23685124516487122\n",
      "iteration 6463: loss: 0.23685026168823242\n",
      "iteration 6464: loss: 0.23684903979301453\n",
      "iteration 6465: loss: 0.2368481606245041\n",
      "iteration 6466: loss: 0.23684704303741455\n",
      "iteration 6467: loss: 0.23684611916542053\n",
      "iteration 6468: loss: 0.2368449866771698\n",
      "iteration 6469: loss: 0.23684386909008026\n",
      "iteration 6470: loss: 0.2368428260087967\n",
      "iteration 6471: loss: 0.2368418276309967\n",
      "iteration 6472: loss: 0.23684072494506836\n",
      "iteration 6473: loss: 0.23683980107307434\n",
      "iteration 6474: loss: 0.2368386685848236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6475: loss: 0.2368377447128296\n",
      "iteration 6476: loss: 0.23683662712574005\n",
      "iteration 6477: loss: 0.23683571815490723\n",
      "iteration 6478: loss: 0.23683440685272217\n",
      "iteration 6479: loss: 0.23683349788188934\n",
      "iteration 6480: loss: 0.23683235049247742\n",
      "iteration 6481: loss: 0.2368314564228058\n",
      "iteration 6482: loss: 0.23683035373687744\n",
      "iteration 6483: loss: 0.23682942986488342\n",
      "iteration 6484: loss: 0.23682832717895508\n",
      "iteration 6485: loss: 0.23682737350463867\n",
      "iteration 6486: loss: 0.236826092004776\n",
      "iteration 6487: loss: 0.23682518303394318\n",
      "iteration 6488: loss: 0.23682408034801483\n",
      "iteration 6489: loss: 0.2368231564760208\n",
      "iteration 6490: loss: 0.23682203888893127\n",
      "iteration 6491: loss: 0.23682110011577606\n",
      "iteration 6492: loss: 0.23681998252868652\n",
      "iteration 6493: loss: 0.23681895434856415\n",
      "iteration 6494: loss: 0.2368178367614746\n",
      "iteration 6495: loss: 0.2368168830871582\n",
      "iteration 6496: loss: 0.23681576550006866\n",
      "iteration 6497: loss: 0.23681466281414032\n",
      "iteration 6498: loss: 0.2368137389421463\n",
      "iteration 6499: loss: 0.23681282997131348\n",
      "iteration 6500: loss: 0.23681168258190155\n",
      "iteration 6501: loss: 0.2368105947971344\n",
      "iteration 6502: loss: 0.23680949211120605\n",
      "iteration 6503: loss: 0.2368083894252777\n",
      "iteration 6504: loss: 0.2368074357509613\n",
      "iteration 6505: loss: 0.23680636286735535\n",
      "iteration 6506: loss: 0.23680540919303894\n",
      "iteration 6507: loss: 0.23680433630943298\n",
      "iteration 6508: loss: 0.23680341243743896\n",
      "iteration 6509: loss: 0.23680217564105988\n",
      "iteration 6510: loss: 0.23680123686790466\n",
      "iteration 6511: loss: 0.2368001490831375\n",
      "iteration 6512: loss: 0.2367992401123047\n",
      "iteration 6513: loss: 0.23679819703102112\n",
      "iteration 6514: loss: 0.2367972433567047\n",
      "iteration 6515: loss: 0.23679614067077637\n",
      "iteration 6516: loss: 0.2367950677871704\n",
      "iteration 6517: loss: 0.23679396510124207\n",
      "iteration 6518: loss: 0.23679304122924805\n",
      "iteration 6519: loss: 0.2367919385433197\n",
      "iteration 6520: loss: 0.23679104447364807\n",
      "iteration 6521: loss: 0.23678991198539734\n",
      "iteration 6522: loss: 0.23678883910179138\n",
      "iteration 6523: loss: 0.23678776621818542\n",
      "iteration 6524: loss: 0.23678681254386902\n",
      "iteration 6525: loss: 0.23678572475910187\n",
      "iteration 6526: loss: 0.23678477108478546\n",
      "iteration 6527: loss: 0.2367836982011795\n",
      "iteration 6528: loss: 0.23678278923034668\n",
      "iteration 6529: loss: 0.2367815226316452\n",
      "iteration 6530: loss: 0.23678059875965118\n",
      "iteration 6531: loss: 0.23677949607372284\n",
      "iteration 6532: loss: 0.2367786169052124\n",
      "iteration 6533: loss: 0.23677749931812286\n",
      "iteration 6534: loss: 0.23677654564380646\n",
      "iteration 6535: loss: 0.23677532374858856\n",
      "iteration 6536: loss: 0.2367742508649826\n",
      "iteration 6537: loss: 0.2367732971906662\n",
      "iteration 6538: loss: 0.23677220940589905\n",
      "iteration 6539: loss: 0.23677130043506622\n",
      "iteration 6540: loss: 0.23677019774913788\n",
      "iteration 6541: loss: 0.23676924407482147\n",
      "iteration 6542: loss: 0.23676797747612\n",
      "iteration 6543: loss: 0.2367670238018036\n",
      "iteration 6544: loss: 0.23676595091819763\n",
      "iteration 6545: loss: 0.23676499724388123\n",
      "iteration 6546: loss: 0.23676392436027527\n",
      "iteration 6547: loss: 0.23676300048828125\n",
      "iteration 6548: loss: 0.23676176369190216\n",
      "iteration 6549: loss: 0.23676085472106934\n",
      "iteration 6550: loss: 0.236759752035141\n",
      "iteration 6551: loss: 0.23675885796546936\n",
      "iteration 6552: loss: 0.23675775527954102\n",
      "iteration 6553: loss: 0.2367568463087082\n",
      "iteration 6554: loss: 0.2367555797100067\n",
      "iteration 6555: loss: 0.2367546558380127\n",
      "iteration 6556: loss: 0.23675358295440674\n",
      "iteration 6557: loss: 0.23675251007080078\n",
      "iteration 6558: loss: 0.23675160109996796\n",
      "iteration 6559: loss: 0.23675048351287842\n",
      "iteration 6560: loss: 0.23674944043159485\n",
      "iteration 6561: loss: 0.23674830794334412\n",
      "iteration 6562: loss: 0.23674741387367249\n",
      "iteration 6563: loss: 0.23674634099006653\n",
      "iteration 6564: loss: 0.2367454320192337\n",
      "iteration 6565: loss: 0.23674419522285461\n",
      "iteration 6566: loss: 0.2367432564496994\n",
      "iteration 6567: loss: 0.23674218356609344\n",
      "iteration 6568: loss: 0.23674125969409943\n",
      "iteration 6569: loss: 0.23674026131629944\n",
      "iteration 6570: loss: 0.2367393672466278\n",
      "iteration 6571: loss: 0.23673805594444275\n",
      "iteration 6572: loss: 0.2367372065782547\n",
      "iteration 6573: loss: 0.23673610389232635\n",
      "iteration 6574: loss: 0.23673506081104279\n",
      "iteration 6575: loss: 0.23673410713672638\n",
      "iteration 6576: loss: 0.23673303425312042\n",
      "iteration 6577: loss: 0.23673197627067566\n",
      "iteration 6578: loss: 0.2367308884859085\n",
      "iteration 6579: loss: 0.23673000931739807\n",
      "iteration 6580: loss: 0.23672887682914734\n",
      "iteration 6581: loss: 0.2367279827594757\n",
      "iteration 6582: loss: 0.23672673106193542\n",
      "iteration 6583: loss: 0.236725851893425\n",
      "iteration 6584: loss: 0.23672473430633545\n",
      "iteration 6585: loss: 0.236723855137825\n",
      "iteration 6586: loss: 0.23672275245189667\n",
      "iteration 6587: loss: 0.2367216795682907\n",
      "iteration 6588: loss: 0.23672063648700714\n",
      "iteration 6589: loss: 0.2367195338010788\n",
      "iteration 6590: loss: 0.23671865463256836\n",
      "iteration 6591: loss: 0.2367175817489624\n",
      "iteration 6592: loss: 0.23671650886535645\n",
      "iteration 6593: loss: 0.23671546578407288\n",
      "iteration 6594: loss: 0.23671451210975647\n",
      "iteration 6595: loss: 0.2367134392261505\n",
      "iteration 6596: loss: 0.23671254515647888\n",
      "iteration 6597: loss: 0.23671147227287292\n",
      "iteration 6598: loss: 0.2367103546857834\n",
      "iteration 6599: loss: 0.236709326505661\n",
      "iteration 6600: loss: 0.23670825362205505\n",
      "iteration 6601: loss: 0.23670735955238342\n",
      "iteration 6602: loss: 0.23670628666877747\n",
      "iteration 6603: loss: 0.23670518398284912\n",
      "iteration 6604: loss: 0.23670411109924316\n",
      "iteration 6605: loss: 0.23670320212841034\n",
      "iteration 6606: loss: 0.23670215904712677\n",
      "iteration 6607: loss: 0.23670125007629395\n",
      "iteration 6608: loss: 0.23670001327991486\n",
      "iteration 6609: loss: 0.23669910430908203\n",
      "iteration 6610: loss: 0.23669803142547607\n",
      "iteration 6611: loss: 0.23669712245464325\n",
      "iteration 6612: loss: 0.23669609427452087\n",
      "iteration 6613: loss: 0.2366948127746582\n",
      "iteration 6614: loss: 0.23669390380382538\n",
      "iteration 6615: loss: 0.2366929054260254\n",
      "iteration 6616: loss: 0.23669195175170898\n",
      "iteration 6617: loss: 0.2366907149553299\n",
      "iteration 6618: loss: 0.23668983578681946\n",
      "iteration 6619: loss: 0.2366887629032135\n",
      "iteration 6620: loss: 0.23668785393238068\n",
      "iteration 6621: loss: 0.23668670654296875\n",
      "iteration 6622: loss: 0.2366856038570404\n",
      "iteration 6623: loss: 0.23668456077575684\n",
      "iteration 6624: loss: 0.23668353259563446\n",
      "iteration 6625: loss: 0.2366826981306076\n",
      "iteration 6626: loss: 0.23668161034584045\n",
      "iteration 6627: loss: 0.2366805523633957\n",
      "iteration 6628: loss: 0.23667947947978973\n",
      "iteration 6629: loss: 0.2366785705089569\n",
      "iteration 6630: loss: 0.23667752742767334\n",
      "iteration 6631: loss: 0.23667661845684052\n",
      "iteration 6632: loss: 0.2366754114627838\n",
      "iteration 6633: loss: 0.23667451739311218\n",
      "iteration 6634: loss: 0.23667344450950623\n",
      "iteration 6635: loss: 0.2366725504398346\n",
      "iteration 6636: loss: 0.2366713285446167\n",
      "iteration 6637: loss: 0.23667025566101074\n",
      "iteration 6638: loss: 0.23666934669017792\n",
      "iteration 6639: loss: 0.23666830360889435\n",
      "iteration 6640: loss: 0.2366672307252884\n",
      "iteration 6641: loss: 0.23666620254516602\n",
      "iteration 6642: loss: 0.2366652935743332\n",
      "iteration 6643: loss: 0.23666425049304962\n",
      "iteration 6644: loss: 0.23666319251060486\n",
      "iteration 6645: loss: 0.2366621047258377\n",
      "iteration 6646: loss: 0.23666107654571533\n",
      "iteration 6647: loss: 0.2366601973772049\n",
      "iteration 6648: loss: 0.23665909469127655\n",
      "iteration 6649: loss: 0.23665805160999298\n",
      "iteration 6650: loss: 0.23665694892406464\n",
      "iteration 6651: loss: 0.2366560697555542\n",
      "iteration 6652: loss: 0.23665502667427063\n",
      "iteration 6653: loss: 0.2366541177034378\n",
      "iteration 6654: loss: 0.23665288090705872\n",
      "iteration 6655: loss: 0.23665182292461395\n",
      "iteration 6656: loss: 0.23665091395378113\n",
      "iteration 6657: loss: 0.23664990067481995\n",
      "iteration 6658: loss: 0.23664884269237518\n",
      "iteration 6659: loss: 0.2366477996110916\n",
      "iteration 6660: loss: 0.2366468906402588\n",
      "iteration 6661: loss: 0.23664584755897522\n",
      "iteration 6662: loss: 0.2366446554660797\n",
      "iteration 6663: loss: 0.2366437464952469\n",
      "iteration 6664: loss: 0.23664268851280212\n",
      "iteration 6665: loss: 0.2366417944431305\n",
      "iteration 6666: loss: 0.23664073646068573\n",
      "iteration 6667: loss: 0.23663969337940216\n",
      "iteration 6668: loss: 0.2366386353969574\n",
      "iteration 6669: loss: 0.23663774132728577\n",
      "iteration 6670: loss: 0.236636683344841\n",
      "iteration 6671: loss: 0.23663544654846191\n",
      "iteration 6672: loss: 0.23663456737995148\n",
      "iteration 6673: loss: 0.2366335093975067\n",
      "iteration 6674: loss: 0.23663266003131866\n",
      "iteration 6675: loss: 0.23663143813610077\n",
      "iteration 6676: loss: 0.23663051426410675\n",
      "iteration 6677: loss: 0.23662948608398438\n",
      "iteration 6678: loss: 0.23662857711315155\n",
      "iteration 6679: loss: 0.23662734031677246\n",
      "iteration 6680: loss: 0.23662646114826202\n",
      "iteration 6681: loss: 0.2366255223751068\n",
      "iteration 6682: loss: 0.23662447929382324\n",
      "iteration 6683: loss: 0.2366233766078949\n",
      "iteration 6684: loss: 0.2366223782300949\n",
      "iteration 6685: loss: 0.23662146925926208\n",
      "iteration 6686: loss: 0.23662027716636658\n",
      "iteration 6687: loss: 0.23661942780017853\n",
      "iteration 6688: loss: 0.23661836981773376\n",
      "iteration 6689: loss: 0.2366172969341278\n",
      "iteration 6690: loss: 0.23661625385284424\n",
      "iteration 6691: loss: 0.23661518096923828\n",
      "iteration 6692: loss: 0.23661431670188904\n",
      "iteration 6693: loss: 0.23661327362060547\n",
      "iteration 6694: loss: 0.2366122305393219\n",
      "iteration 6695: loss: 0.23661117255687714\n",
      "iteration 6696: loss: 0.2366102635860443\n",
      "iteration 6697: loss: 0.23660926520824432\n",
      "iteration 6698: loss: 0.23660802841186523\n",
      "iteration 6699: loss: 0.23660710453987122\n",
      "iteration 6700: loss: 0.23660609126091003\n",
      "iteration 6701: loss: 0.2366051971912384\n",
      "iteration 6702: loss: 0.2366039752960205\n",
      "iteration 6703: loss: 0.23660311102867126\n",
      "iteration 6704: loss: 0.2366020679473877\n",
      "iteration 6705: loss: 0.23660095036029816\n",
      "iteration 6706: loss: 0.2365998923778534\n",
      "iteration 6707: loss: 0.23659884929656982\n",
      "iteration 6708: loss: 0.23659798502922058\n",
      "iteration 6709: loss: 0.2365969717502594\n",
      "iteration 6710: loss: 0.2365957796573639\n",
      "iteration 6711: loss: 0.23659491539001465\n",
      "iteration 6712: loss: 0.2365938425064087\n",
      "iteration 6713: loss: 0.23659279942512512\n",
      "iteration 6714: loss: 0.23659174144268036\n",
      "iteration 6715: loss: 0.23659086227416992\n",
      "iteration 6716: loss: 0.23658983409404755\n",
      "iteration 6717: loss: 0.23658862709999084\n",
      "iteration 6718: loss: 0.2365877628326416\n",
      "iteration 6719: loss: 0.23658668994903564\n",
      "iteration 6720: loss: 0.23658564686775208\n",
      "iteration 6721: loss: 0.2365846186876297\n",
      "iteration 6722: loss: 0.23658373951911926\n",
      "iteration 6723: loss: 0.2365826815366745\n",
      "iteration 6724: loss: 0.23658163845539093\n",
      "iteration 6725: loss: 0.23658064007759094\n",
      "iteration 6726: loss: 0.23657956719398499\n",
      "iteration 6727: loss: 0.23657867312431335\n",
      "iteration 6728: loss: 0.23657746613025665\n",
      "iteration 6729: loss: 0.23657658696174622\n",
      "iteration 6730: loss: 0.23657557368278503\n",
      "iteration 6731: loss: 0.23657450079917908\n",
      "iteration 6732: loss: 0.2365734875202179\n",
      "iteration 6733: loss: 0.2365724742412567\n",
      "iteration 6734: loss: 0.23657158017158508\n",
      "iteration 6735: loss: 0.23657044768333435\n",
      "iteration 6736: loss: 0.2365695983171463\n",
      "iteration 6737: loss: 0.23656854033470154\n",
      "iteration 6738: loss: 0.23656752705574036\n",
      "iteration 6739: loss: 0.23656649887561798\n",
      "iteration 6740: loss: 0.23656544089317322\n",
      "iteration 6741: loss: 0.23656459152698517\n",
      "iteration 6742: loss: 0.23656339943408966\n",
      "iteration 6743: loss: 0.23656252026557922\n",
      "iteration 6744: loss: 0.23656149208545685\n",
      "iteration 6745: loss: 0.23656046390533447\n",
      "iteration 6746: loss: 0.2365594208240509\n",
      "iteration 6747: loss: 0.23655839264392853\n",
      "iteration 6748: loss: 0.23655733466148376\n",
      "iteration 6749: loss: 0.23655632138252258\n",
      "iteration 6750: loss: 0.23655545711517334\n",
      "iteration 6751: loss: 0.23655441403388977\n",
      "iteration 6752: loss: 0.236553356051445\n",
      "iteration 6753: loss: 0.2365523874759674\n",
      "iteration 6754: loss: 0.23655135929584503\n",
      "iteration 6755: loss: 0.23655028641223907\n",
      "iteration 6756: loss: 0.2365492880344391\n",
      "iteration 6757: loss: 0.23654842376708984\n",
      "iteration 6758: loss: 0.23654720187187195\n",
      "iteration 6759: loss: 0.23654618859291077\n",
      "iteration 6760: loss: 0.23654527962207794\n",
      "iteration 6761: loss: 0.23654425144195557\n",
      "iteration 6762: loss: 0.2365432232618332\n",
      "iteration 6763: loss: 0.23654219508171082\n",
      "iteration 6764: loss: 0.23654134571552277\n",
      "iteration 6765: loss: 0.23654010891914368\n",
      "iteration 6766: loss: 0.2365390956401825\n",
      "iteration 6767: loss: 0.23653826117515564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6768: loss: 0.23653721809387207\n",
      "iteration 6769: loss: 0.2365361750125885\n",
      "iteration 6770: loss: 0.23653516173362732\n",
      "iteration 6771: loss: 0.23653411865234375\n",
      "iteration 6772: loss: 0.23653307557106018\n",
      "iteration 6773: loss: 0.236532062292099\n",
      "iteration 6774: loss: 0.23653122782707214\n",
      "iteration 6775: loss: 0.23653002083301544\n",
      "iteration 6776: loss: 0.2365291565656662\n",
      "iteration 6777: loss: 0.2365281581878662\n",
      "iteration 6778: loss: 0.2365269660949707\n",
      "iteration 6779: loss: 0.23652610182762146\n",
      "iteration 6780: loss: 0.23652508854866028\n",
      "iteration 6781: loss: 0.23652389645576477\n",
      "iteration 6782: loss: 0.23652303218841553\n",
      "iteration 6783: loss: 0.23652204871177673\n",
      "iteration 6784: loss: 0.23652096092700958\n",
      "iteration 6785: loss: 0.2365199774503708\n",
      "iteration 6786: loss: 0.23651909828186035\n",
      "iteration 6787: loss: 0.23651790618896484\n",
      "iteration 6788: loss: 0.23651699721813202\n",
      "iteration 6789: loss: 0.23651614785194397\n",
      "iteration 6790: loss: 0.23651492595672607\n",
      "iteration 6791: loss: 0.23651406168937683\n",
      "iteration 6792: loss: 0.23651298880577087\n",
      "iteration 6793: loss: 0.2365119755268097\n",
      "iteration 6794: loss: 0.23651091754436493\n",
      "iteration 6795: loss: 0.23650988936424255\n",
      "iteration 6796: loss: 0.23650884628295898\n",
      "iteration 6797: loss: 0.2365078181028366\n",
      "iteration 6798: loss: 0.23650698363780975\n",
      "iteration 6799: loss: 0.23650594055652618\n",
      "iteration 6800: loss: 0.2365049123764038\n",
      "iteration 6801: loss: 0.23650388419628143\n",
      "iteration 6802: loss: 0.23650285601615906\n",
      "iteration 6803: loss: 0.23650185763835907\n",
      "iteration 6804: loss: 0.2365008294582367\n",
      "iteration 6805: loss: 0.2364998310804367\n",
      "iteration 6806: loss: 0.23649880290031433\n",
      "iteration 6807: loss: 0.23649780452251434\n",
      "iteration 6808: loss: 0.23649676144123077\n",
      "iteration 6809: loss: 0.23649577796459198\n",
      "iteration 6810: loss: 0.23649492859840393\n",
      "iteration 6811: loss: 0.23649373650550842\n",
      "iteration 6812: loss: 0.23649272322654724\n",
      "iteration 6813: loss: 0.2364918738603592\n",
      "iteration 6814: loss: 0.23649068176746368\n",
      "iteration 6815: loss: 0.23648984730243683\n",
      "iteration 6816: loss: 0.23648878931999207\n",
      "iteration 6817: loss: 0.23648762702941895\n",
      "iteration 6818: loss: 0.2364867925643921\n",
      "iteration 6819: loss: 0.23648574948310852\n",
      "iteration 6820: loss: 0.23648473620414734\n",
      "iteration 6821: loss: 0.23648373782634735\n",
      "iteration 6822: loss: 0.23648270964622498\n",
      "iteration 6823: loss: 0.236481711268425\n",
      "iteration 6824: loss: 0.23648066818714142\n",
      "iteration 6825: loss: 0.23647980391979218\n",
      "iteration 6826: loss: 0.23647864162921906\n",
      "iteration 6827: loss: 0.23647765815258026\n",
      "iteration 6828: loss: 0.2364768087863922\n",
      "iteration 6829: loss: 0.2364756166934967\n",
      "iteration 6830: loss: 0.23647478222846985\n",
      "iteration 6831: loss: 0.23647375404834747\n",
      "iteration 6832: loss: 0.23647257685661316\n",
      "iteration 6833: loss: 0.2364717423915863\n",
      "iteration 6834: loss: 0.2364705502986908\n",
      "iteration 6835: loss: 0.23646970093250275\n",
      "iteration 6836: loss: 0.23646870255470276\n",
      "iteration 6837: loss: 0.23646751046180725\n",
      "iteration 6838: loss: 0.2364666759967804\n",
      "iteration 6839: loss: 0.2364656925201416\n",
      "iteration 6840: loss: 0.23646464943885803\n",
      "iteration 6841: loss: 0.23646371066570282\n",
      "iteration 6842: loss: 0.23646286129951477\n",
      "iteration 6843: loss: 0.23646168410778046\n",
      "iteration 6844: loss: 0.23646068572998047\n",
      "iteration 6845: loss: 0.23645982146263123\n",
      "iteration 6846: loss: 0.2364586591720581\n",
      "iteration 6847: loss: 0.23645778000354767\n",
      "iteration 6848: loss: 0.23645679652690887\n",
      "iteration 6849: loss: 0.23645563423633575\n",
      "iteration 6850: loss: 0.2364547997713089\n",
      "iteration 6851: loss: 0.236453577876091\n",
      "iteration 6852: loss: 0.23645277321338654\n",
      "iteration 6853: loss: 0.23645177483558655\n",
      "iteration 6854: loss: 0.23645058274269104\n",
      "iteration 6855: loss: 0.23644976317882538\n",
      "iteration 6856: loss: 0.2364487648010254\n",
      "iteration 6857: loss: 0.23644772171974182\n",
      "iteration 6858: loss: 0.23644676804542542\n",
      "iteration 6859: loss: 0.23644554615020752\n",
      "iteration 6860: loss: 0.23644474148750305\n",
      "iteration 6861: loss: 0.23644371330738068\n",
      "iteration 6862: loss: 0.23644275963306427\n",
      "iteration 6863: loss: 0.2364417314529419\n",
      "iteration 6864: loss: 0.2364407330751419\n",
      "iteration 6865: loss: 0.23643970489501953\n",
      "iteration 6866: loss: 0.23643870651721954\n",
      "iteration 6867: loss: 0.23643770813941956\n",
      "iteration 6868: loss: 0.23643667995929718\n",
      "iteration 6869: loss: 0.2364356964826584\n",
      "iteration 6870: loss: 0.236434668302536\n",
      "iteration 6871: loss: 0.23643365502357483\n",
      "iteration 6872: loss: 0.23643283545970917\n",
      "iteration 6873: loss: 0.23643167316913605\n",
      "iteration 6874: loss: 0.23643064498901367\n",
      "iteration 6875: loss: 0.2364296019077301\n",
      "iteration 6876: loss: 0.2364286482334137\n",
      "iteration 6877: loss: 0.23642782866954803\n",
      "iteration 6878: loss: 0.23642663657665253\n",
      "iteration 6879: loss: 0.23642563819885254\n",
      "iteration 6880: loss: 0.23642483353614807\n",
      "iteration 6881: loss: 0.23642364144325256\n",
      "iteration 6882: loss: 0.23642270267009735\n",
      "iteration 6883: loss: 0.23642155528068542\n",
      "iteration 6884: loss: 0.23642058670520782\n",
      "iteration 6885: loss: 0.23641972243785858\n",
      "iteration 6886: loss: 0.23641856014728546\n",
      "iteration 6887: loss: 0.23641756176948547\n",
      "iteration 6888: loss: 0.23641672730445862\n",
      "iteration 6889: loss: 0.2364155799150467\n",
      "iteration 6890: loss: 0.23641471564769745\n",
      "iteration 6891: loss: 0.23641355335712433\n",
      "iteration 6892: loss: 0.23641259968280792\n",
      "iteration 6893: loss: 0.23641173541545868\n",
      "iteration 6894: loss: 0.23641064763069153\n",
      "iteration 6895: loss: 0.23640981316566467\n",
      "iteration 6896: loss: 0.23640866577625275\n",
      "iteration 6897: loss: 0.23640763759613037\n",
      "iteration 6898: loss: 0.23640663921833038\n",
      "iteration 6899: loss: 0.2364056557416916\n",
      "iteration 6900: loss: 0.23640485107898712\n",
      "iteration 6901: loss: 0.2364036738872528\n",
      "iteration 6902: loss: 0.23640267550945282\n",
      "iteration 6903: loss: 0.23640164732933044\n",
      "iteration 6904: loss: 0.23640069365501404\n",
      "iteration 6905: loss: 0.23639985918998718\n",
      "iteration 6906: loss: 0.23639869689941406\n",
      "iteration 6907: loss: 0.23639769852161407\n",
      "iteration 6908: loss: 0.23639686405658722\n",
      "iteration 6909: loss: 0.2363956868648529\n",
      "iteration 6910: loss: 0.23639485239982605\n",
      "iteration 6911: loss: 0.23639371991157532\n",
      "iteration 6912: loss: 0.23639273643493652\n",
      "iteration 6913: loss: 0.23639170825481415\n",
      "iteration 6914: loss: 0.23639073967933655\n",
      "iteration 6915: loss: 0.23638994991779327\n",
      "iteration 6916: loss: 0.23638877272605896\n",
      "iteration 6917: loss: 0.23638775944709778\n",
      "iteration 6918: loss: 0.2363867461681366\n",
      "iteration 6919: loss: 0.236385777592659\n",
      "iteration 6920: loss: 0.23638494312763214\n",
      "iteration 6921: loss: 0.23638379573822021\n",
      "iteration 6922: loss: 0.23638281226158142\n",
      "iteration 6923: loss: 0.23638179898262024\n",
      "iteration 6924: loss: 0.23638081550598145\n",
      "iteration 6925: loss: 0.23637983202934265\n",
      "iteration 6926: loss: 0.23637881875038147\n",
      "iteration 6927: loss: 0.23637786507606506\n",
      "iteration 6928: loss: 0.23637685179710388\n",
      "iteration 6929: loss: 0.23637588322162628\n",
      "iteration 6930: loss: 0.2363748848438263\n",
      "iteration 6931: loss: 0.23637385666370392\n",
      "iteration 6932: loss: 0.2363729029893875\n",
      "iteration 6933: loss: 0.23637190461158752\n",
      "iteration 6934: loss: 0.23637089133262634\n",
      "iteration 6935: loss: 0.2363697588443756\n",
      "iteration 6936: loss: 0.23636893928050995\n",
      "iteration 6937: loss: 0.23636797070503235\n",
      "iteration 6938: loss: 0.23636682331562042\n",
      "iteration 6939: loss: 0.23636600375175476\n",
      "iteration 6940: loss: 0.23636487126350403\n",
      "iteration 6941: loss: 0.23636400699615479\n",
      "iteration 6942: loss: 0.23636284470558167\n",
      "iteration 6943: loss: 0.23636186122894287\n",
      "iteration 6944: loss: 0.2363610714673996\n",
      "iteration 6945: loss: 0.23635998368263245\n",
      "iteration 6946: loss: 0.2363591492176056\n",
      "iteration 6947: loss: 0.23635800182819366\n",
      "iteration 6948: loss: 0.23635700345039368\n",
      "iteration 6949: loss: 0.23635601997375488\n",
      "iteration 6950: loss: 0.23635506629943848\n",
      "iteration 6951: loss: 0.2363540679216385\n",
      "iteration 6952: loss: 0.2363530695438385\n",
      "iteration 6953: loss: 0.2363521158695221\n",
      "iteration 6954: loss: 0.2363511025905609\n",
      "iteration 6955: loss: 0.23635010421276093\n",
      "iteration 6956: loss: 0.2363489866256714\n",
      "iteration 6957: loss: 0.23634815216064453\n",
      "iteration 6958: loss: 0.2363470047712326\n",
      "iteration 6959: loss: 0.23634620010852814\n",
      "iteration 6960: loss: 0.23634520173072815\n",
      "iteration 6961: loss: 0.23634400963783264\n",
      "iteration 6962: loss: 0.23634323477745056\n",
      "iteration 6963: loss: 0.23634204268455505\n",
      "iteration 6964: loss: 0.23634108901023865\n",
      "iteration 6965: loss: 0.23634031414985657\n",
      "iteration 6966: loss: 0.23633913695812225\n",
      "iteration 6967: loss: 0.23633833229541779\n",
      "iteration 6968: loss: 0.23633718490600586\n",
      "iteration 6969: loss: 0.23633620142936707\n",
      "iteration 6970: loss: 0.23633524775505066\n",
      "iteration 6971: loss: 0.23633427917957306\n",
      "iteration 6972: loss: 0.23633325099945068\n",
      "iteration 6973: loss: 0.23633232712745667\n",
      "iteration 6974: loss: 0.23633110523223877\n",
      "iteration 6975: loss: 0.23633034527301788\n",
      "iteration 6976: loss: 0.2363293617963791\n",
      "iteration 6977: loss: 0.23632819950580597\n",
      "iteration 6978: loss: 0.23632729053497314\n",
      "iteration 6979: loss: 0.2363261729478836\n",
      "iteration 6980: loss: 0.23632530868053436\n",
      "iteration 6981: loss: 0.2363242208957672\n",
      "iteration 6982: loss: 0.23632323741912842\n",
      "iteration 6983: loss: 0.23632223904132843\n",
      "iteration 6984: loss: 0.23632125556468964\n",
      "iteration 6985: loss: 0.2363201379776001\n",
      "iteration 6986: loss: 0.23631930351257324\n",
      "iteration 6987: loss: 0.23631832003593445\n",
      "iteration 6988: loss: 0.23631735146045685\n",
      "iteration 6989: loss: 0.23631635308265686\n",
      "iteration 6990: loss: 0.23631525039672852\n",
      "iteration 6991: loss: 0.23631444573402405\n",
      "iteration 6992: loss: 0.23631326854228973\n",
      "iteration 6993: loss: 0.23631231486797333\n",
      "iteration 6994: loss: 0.23631134629249573\n",
      "iteration 6995: loss: 0.23631036281585693\n",
      "iteration 6996: loss: 0.2363094836473465\n",
      "iteration 6997: loss: 0.2363085001707077\n",
      "iteration 6998: loss: 0.23630733788013458\n",
      "iteration 6999: loss: 0.23630651831626892\n",
      "iteration 7000: loss: 0.2363055944442749\n",
      "iteration 7001: loss: 0.23630444705486298\n",
      "iteration 7002: loss: 0.2363036423921585\n",
      "iteration 7003: loss: 0.23630249500274658\n",
      "iteration 7004: loss: 0.2363017052412033\n",
      "iteration 7005: loss: 0.2363005429506302\n",
      "iteration 7006: loss: 0.23629958927631378\n",
      "iteration 7007: loss: 0.236298605799675\n",
      "iteration 7008: loss: 0.23629765212535858\n",
      "iteration 7009: loss: 0.2362966537475586\n",
      "iteration 7010: loss: 0.2362957000732422\n",
      "iteration 7011: loss: 0.23629455268383026\n",
      "iteration 7012: loss: 0.2362937480211258\n",
      "iteration 7013: loss: 0.23629260063171387\n",
      "iteration 7014: loss: 0.23629164695739746\n",
      "iteration 7015: loss: 0.23629064857959747\n",
      "iteration 7016: loss: 0.23628970980644226\n",
      "iteration 7017: loss: 0.2362888753414154\n",
      "iteration 7018: loss: 0.23628771305084229\n",
      "iteration 7019: loss: 0.23628680408000946\n",
      "iteration 7020: loss: 0.23628577589988708\n",
      "iteration 7021: loss: 0.23628482222557068\n",
      "iteration 7022: loss: 0.23628370463848114\n",
      "iteration 7023: loss: 0.23628294467926025\n",
      "iteration 7024: loss: 0.23628179728984833\n",
      "iteration 7025: loss: 0.23628099262714386\n",
      "iteration 7026: loss: 0.23627984523773193\n",
      "iteration 7027: loss: 0.23627886176109314\n",
      "iteration 7028: loss: 0.23627789318561554\n",
      "iteration 7029: loss: 0.23627695441246033\n",
      "iteration 7030: loss: 0.2362758219242096\n",
      "iteration 7031: loss: 0.23627500236034393\n",
      "iteration 7032: loss: 0.23627391457557678\n",
      "iteration 7033: loss: 0.2362729012966156\n",
      "iteration 7034: loss: 0.2362719476222992\n",
      "iteration 7035: loss: 0.2362709790468216\n",
      "iteration 7036: loss: 0.2362699955701828\n",
      "iteration 7037: loss: 0.2362690418958664\n",
      "iteration 7038: loss: 0.23626789450645447\n",
      "iteration 7039: loss: 0.2362671196460724\n",
      "iteration 7040: loss: 0.23626597225666046\n",
      "iteration 7041: loss: 0.2362651824951172\n",
      "iteration 7042: loss: 0.23626422882080078\n",
      "iteration 7043: loss: 0.23626306653022766\n",
      "iteration 7044: loss: 0.2362622767686844\n",
      "iteration 7045: loss: 0.23626115918159485\n",
      "iteration 7046: loss: 0.23626026511192322\n",
      "iteration 7047: loss: 0.2362593114376068\n",
      "iteration 7048: loss: 0.23625817894935608\n",
      "iteration 7049: loss: 0.23625735938549042\n",
      "iteration 7050: loss: 0.23625624179840088\n",
      "iteration 7051: loss: 0.23625531792640686\n",
      "iteration 7052: loss: 0.23625430464744568\n",
      "iteration 7053: loss: 0.23625341057777405\n",
      "iteration 7054: loss: 0.23625223338603973\n",
      "iteration 7055: loss: 0.23625144362449646\n",
      "iteration 7056: loss: 0.23625031113624573\n",
      "iteration 7057: loss: 0.2362494170665741\n",
      "iteration 7058: loss: 0.2362484186887741\n",
      "iteration 7059: loss: 0.2362474650144577\n",
      "iteration 7060: loss: 0.23624646663665771\n",
      "iteration 7061: loss: 0.2362455427646637\n",
      "iteration 7062: loss: 0.23624441027641296\n",
      "iteration 7063: loss: 0.2362436056137085\n",
      "iteration 7064: loss: 0.23624248802661896\n",
      "iteration 7065: loss: 0.23624153435230255\n",
      "iteration 7066: loss: 0.23624058067798615\n",
      "iteration 7067: loss: 0.23623962700366974\n",
      "iteration 7068: loss: 0.23623862862586975\n",
      "iteration 7069: loss: 0.23623767495155334\n",
      "iteration 7070: loss: 0.236236572265625\n",
      "iteration 7071: loss: 0.23623573780059814\n",
      "iteration 7072: loss: 0.2362346202135086\n",
      "iteration 7073: loss: 0.2362336665391922\n",
      "iteration 7074: loss: 0.2362327128648758\n",
      "iteration 7075: loss: 0.2362317591905594\n",
      "iteration 7076: loss: 0.23623065650463104\n",
      "iteration 7077: loss: 0.23622985184192657\n",
      "iteration 7078: loss: 0.23622874915599823\n",
      "iteration 7079: loss: 0.2362278401851654\n",
      "iteration 7080: loss: 0.23622672259807587\n",
      "iteration 7081: loss: 0.23622579872608185\n",
      "iteration 7082: loss: 0.23622481524944305\n",
      "iteration 7083: loss: 0.23622386157512665\n",
      "iteration 7084: loss: 0.2362227737903595\n",
      "iteration 7085: loss: 0.23622199892997742\n",
      "iteration 7086: loss: 0.2362208068370819\n",
      "iteration 7087: loss: 0.2362198829650879\n",
      "iteration 7088: loss: 0.23621892929077148\n",
      "iteration 7089: loss: 0.23621781170368195\n",
      "iteration 7090: loss: 0.23621702194213867\n",
      "iteration 7091: loss: 0.23621587455272675\n",
      "iteration 7092: loss: 0.23621492087841034\n",
      "iteration 7093: loss: 0.23621395230293274\n",
      "iteration 7094: loss: 0.23621304333209991\n",
      "iteration 7095: loss: 0.23621205985546112\n",
      "iteration 7096: loss: 0.2362111359834671\n",
      "iteration 7097: loss: 0.23621010780334473\n",
      "iteration 7098: loss: 0.23620930314064026\n",
      "iteration 7099: loss: 0.23620820045471191\n",
      "iteration 7100: loss: 0.2362072467803955\n",
      "iteration 7101: loss: 0.2362062633037567\n",
      "iteration 7102: loss: 0.23620514571666718\n",
      "iteration 7103: loss: 0.2362043857574463\n",
      "iteration 7104: loss: 0.23620326817035675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7105: loss: 0.23620232939720154\n",
      "iteration 7106: loss: 0.23620137572288513\n",
      "iteration 7107: loss: 0.2362004518508911\n",
      "iteration 7108: loss: 0.23619933426380157\n",
      "iteration 7109: loss: 0.23619849979877472\n",
      "iteration 7110: loss: 0.23619742691516876\n",
      "iteration 7111: loss: 0.23619647324085236\n",
      "iteration 7112: loss: 0.23619548976421356\n",
      "iteration 7113: loss: 0.23619458079338074\n",
      "iteration 7114: loss: 0.23619344830513\n",
      "iteration 7115: loss: 0.23619265854358673\n",
      "iteration 7116: loss: 0.2361915558576584\n",
      "iteration 7117: loss: 0.23619043827056885\n",
      "iteration 7118: loss: 0.23618964850902557\n",
      "iteration 7119: loss: 0.23618850111961365\n",
      "iteration 7120: loss: 0.23618774116039276\n",
      "iteration 7121: loss: 0.23618659377098083\n",
      "iteration 7122: loss: 0.2361856997013092\n",
      "iteration 7123: loss: 0.2361847460269928\n",
      "iteration 7124: loss: 0.23618380725383759\n",
      "iteration 7125: loss: 0.23618265986442566\n",
      "iteration 7126: loss: 0.23618188500404358\n",
      "iteration 7127: loss: 0.23618081212043762\n",
      "iteration 7128: loss: 0.23617997765541077\n",
      "iteration 7129: loss: 0.23617887496948242\n",
      "iteration 7130: loss: 0.23617777228355408\n",
      "iteration 7131: loss: 0.2361770123243332\n",
      "iteration 7132: loss: 0.23617586493492126\n",
      "iteration 7133: loss: 0.23617494106292725\n",
      "iteration 7134: loss: 0.23617401719093323\n",
      "iteration 7135: loss: 0.23617291450500488\n",
      "iteration 7136: loss: 0.23617196083068848\n",
      "iteration 7137: loss: 0.23617100715637207\n",
      "iteration 7138: loss: 0.23617008328437805\n",
      "iteration 7139: loss: 0.2361689805984497\n",
      "iteration 7140: loss: 0.23616822063922882\n",
      "iteration 7141: loss: 0.23616710305213928\n",
      "iteration 7142: loss: 0.23616616427898407\n",
      "iteration 7143: loss: 0.23616519570350647\n",
      "iteration 7144: loss: 0.2361641228199005\n",
      "iteration 7145: loss: 0.23616333305835724\n",
      "iteration 7146: loss: 0.23616230487823486\n",
      "iteration 7147: loss: 0.23616138100624084\n",
      "iteration 7148: loss: 0.23616036772727966\n",
      "iteration 7149: loss: 0.23615944385528564\n",
      "iteration 7150: loss: 0.23615851998329163\n",
      "iteration 7151: loss: 0.2361573874950409\n",
      "iteration 7152: loss: 0.23615646362304688\n",
      "iteration 7153: loss: 0.23615550994873047\n",
      "iteration 7154: loss: 0.23615458607673645\n",
      "iteration 7155: loss: 0.23615345358848572\n",
      "iteration 7156: loss: 0.23615269362926483\n",
      "iteration 7157: loss: 0.2361515760421753\n",
      "iteration 7158: loss: 0.23615065217018127\n",
      "iteration 7159: loss: 0.23614971339702606\n",
      "iteration 7160: loss: 0.2361486405134201\n",
      "iteration 7161: loss: 0.2361476868391037\n",
      "iteration 7162: loss: 0.23614676296710968\n",
      "iteration 7163: loss: 0.23614582419395447\n",
      "iteration 7164: loss: 0.23614473640918732\n",
      "iteration 7165: loss: 0.23614397644996643\n",
      "iteration 7166: loss: 0.23614279925823212\n",
      "iteration 7167: loss: 0.2361418753862381\n",
      "iteration 7168: loss: 0.23614093661308289\n",
      "iteration 7169: loss: 0.23613986372947693\n",
      "iteration 7170: loss: 0.23613908886909485\n",
      "iteration 7171: loss: 0.23613795638084412\n",
      "iteration 7172: loss: 0.23613688349723816\n",
      "iteration 7173: loss: 0.23613610863685608\n",
      "iteration 7174: loss: 0.23613500595092773\n",
      "iteration 7175: loss: 0.2361340969800949\n",
      "iteration 7176: loss: 0.2361331284046173\n",
      "iteration 7177: loss: 0.23613202571868896\n",
      "iteration 7178: loss: 0.2361312359571457\n",
      "iteration 7179: loss: 0.23613011837005615\n",
      "iteration 7180: loss: 0.23612920939922333\n",
      "iteration 7181: loss: 0.2361282855272293\n",
      "iteration 7182: loss: 0.23612718284130096\n",
      "iteration 7183: loss: 0.23612625896930695\n",
      "iteration 7184: loss: 0.23612532019615173\n",
      "iteration 7185: loss: 0.2361244261264801\n",
      "iteration 7186: loss: 0.23612336814403534\n",
      "iteration 7187: loss: 0.2361222803592682\n",
      "iteration 7188: loss: 0.23612137138843536\n",
      "iteration 7189: loss: 0.23612038791179657\n",
      "iteration 7190: loss: 0.23611947894096375\n",
      "iteration 7191: loss: 0.2361183613538742\n",
      "iteration 7192: loss: 0.23611760139465332\n",
      "iteration 7193: loss: 0.23611649870872498\n",
      "iteration 7194: loss: 0.23611541092395782\n",
      "iteration 7195: loss: 0.2361147403717041\n",
      "iteration 7196: loss: 0.23611362278461456\n",
      "iteration 7197: loss: 0.23611271381378174\n",
      "iteration 7198: loss: 0.23611176013946533\n",
      "iteration 7199: loss: 0.2361106425523758\n",
      "iteration 7200: loss: 0.23610973358154297\n",
      "iteration 7201: loss: 0.23610877990722656\n",
      "iteration 7202: loss: 0.23610785603523254\n",
      "iteration 7203: loss: 0.2361067831516266\n",
      "iteration 7204: loss: 0.2361057996749878\n",
      "iteration 7205: loss: 0.23610489070415497\n",
      "iteration 7206: loss: 0.23610396683216095\n",
      "iteration 7207: loss: 0.23610302805900574\n",
      "iteration 7208: loss: 0.2361019402742386\n",
      "iteration 7209: loss: 0.23610100150108337\n",
      "iteration 7210: loss: 0.23610007762908936\n",
      "iteration 7211: loss: 0.2360990047454834\n",
      "iteration 7212: loss: 0.2360982447862625\n",
      "iteration 7213: loss: 0.23609712719917297\n",
      "iteration 7214: loss: 0.23609605431556702\n",
      "iteration 7215: loss: 0.2360953390598297\n",
      "iteration 7216: loss: 0.23609423637390137\n",
      "iteration 7217: loss: 0.23609313368797302\n",
      "iteration 7218: loss: 0.23609237372875214\n",
      "iteration 7219: loss: 0.23609130084514618\n",
      "iteration 7220: loss: 0.23609021306037903\n",
      "iteration 7221: loss: 0.23608942329883575\n",
      "iteration 7222: loss: 0.2360883504152298\n",
      "iteration 7223: loss: 0.2360875904560089\n",
      "iteration 7224: loss: 0.23608651757240295\n",
      "iteration 7225: loss: 0.2360854148864746\n",
      "iteration 7226: loss: 0.23608461022377014\n",
      "iteration 7227: loss: 0.23608353734016418\n",
      "iteration 7228: loss: 0.23608264327049255\n",
      "iteration 7229: loss: 0.23608168959617615\n",
      "iteration 7230: loss: 0.236080601811409\n",
      "iteration 7231: loss: 0.23607969284057617\n",
      "iteration 7232: loss: 0.23607873916625977\n",
      "iteration 7233: loss: 0.23607763648033142\n",
      "iteration 7234: loss: 0.23607692122459412\n",
      "iteration 7235: loss: 0.23607580363750458\n",
      "iteration 7236: loss: 0.23607487976551056\n",
      "iteration 7237: loss: 0.23607397079467773\n",
      "iteration 7238: loss: 0.2360728681087494\n",
      "iteration 7239: loss: 0.23607197403907776\n",
      "iteration 7240: loss: 0.23607102036476135\n",
      "iteration 7241: loss: 0.236069917678833\n",
      "iteration 7242: loss: 0.23606903851032257\n",
      "iteration 7243: loss: 0.23606809973716736\n",
      "iteration 7244: loss: 0.2360672950744629\n",
      "iteration 7245: loss: 0.23606617748737335\n",
      "iteration 7246: loss: 0.23606523871421814\n",
      "iteration 7247: loss: 0.2360643446445465\n",
      "iteration 7248: loss: 0.23606328666210175\n",
      "iteration 7249: loss: 0.23606233298778534\n",
      "iteration 7250: loss: 0.23606140911579132\n",
      "iteration 7251: loss: 0.23606033623218536\n",
      "iteration 7252: loss: 0.23605942726135254\n",
      "iteration 7253: loss: 0.2360585480928421\n",
      "iteration 7254: loss: 0.23605743050575256\n",
      "iteration 7255: loss: 0.23605652153491974\n",
      "iteration 7256: loss: 0.23605561256408691\n",
      "iteration 7257: loss: 0.2360546588897705\n",
      "iteration 7258: loss: 0.23605358600616455\n",
      "iteration 7259: loss: 0.23605267703533173\n",
      "iteration 7260: loss: 0.2360517531633377\n",
      "iteration 7261: loss: 0.23605084419250488\n",
      "iteration 7262: loss: 0.23604972660541534\n",
      "iteration 7263: loss: 0.23604878783226013\n",
      "iteration 7264: loss: 0.2360478937625885\n",
      "iteration 7265: loss: 0.23604698479175568\n",
      "iteration 7266: loss: 0.23604591190814972\n",
      "iteration 7267: loss: 0.23604503273963928\n",
      "iteration 7268: loss: 0.2360440492630005\n",
      "iteration 7269: loss: 0.23604300618171692\n",
      "iteration 7270: loss: 0.2360420674085617\n",
      "iteration 7271: loss: 0.2360411435365677\n",
      "iteration 7272: loss: 0.23604023456573486\n",
      "iteration 7273: loss: 0.23603913187980652\n",
      "iteration 7274: loss: 0.2360382378101349\n",
      "iteration 7275: loss: 0.23603732883930206\n",
      "iteration 7276: loss: 0.2360362708568573\n",
      "iteration 7277: loss: 0.2360353171825409\n",
      "iteration 7278: loss: 0.23603442311286926\n",
      "iteration 7279: loss: 0.2360333502292633\n",
      "iteration 7280: loss: 0.2360323965549469\n",
      "iteration 7281: loss: 0.23603153228759766\n",
      "iteration 7282: loss: 0.2360304594039917\n",
      "iteration 7283: loss: 0.2360294759273529\n",
      "iteration 7284: loss: 0.23602862656116486\n",
      "iteration 7285: loss: 0.2360275238752365\n",
      "iteration 7286: loss: 0.2360266000032425\n",
      "iteration 7287: loss: 0.23602572083473206\n",
      "iteration 7288: loss: 0.23602476716041565\n",
      "iteration 7289: loss: 0.2360236942768097\n",
      "iteration 7290: loss: 0.23602280020713806\n",
      "iteration 7291: loss: 0.23602187633514404\n",
      "iteration 7292: loss: 0.23602084815502167\n",
      "iteration 7293: loss: 0.23601999878883362\n",
      "iteration 7294: loss: 0.2360190600156784\n",
      "iteration 7295: loss: 0.23601797223091125\n",
      "iteration 7296: loss: 0.2360168993473053\n",
      "iteration 7297: loss: 0.236016184091568\n",
      "iteration 7298: loss: 0.23601511120796204\n",
      "iteration 7299: loss: 0.23601403832435608\n",
      "iteration 7300: loss: 0.2360132932662964\n",
      "iteration 7301: loss: 0.23601210117340088\n",
      "iteration 7302: loss: 0.23601102828979492\n",
      "iteration 7303: loss: 0.23601028323173523\n",
      "iteration 7304: loss: 0.23600921034812927\n",
      "iteration 7305: loss: 0.23600825667381287\n",
      "iteration 7306: loss: 0.23600737750530243\n",
      "iteration 7307: loss: 0.23600630462169647\n",
      "iteration 7308: loss: 0.23600538074970245\n",
      "iteration 7309: loss: 0.23600450158119202\n",
      "iteration 7310: loss: 0.23600342869758606\n",
      "iteration 7311: loss: 0.23600248992443085\n",
      "iteration 7312: loss: 0.23600158095359802\n",
      "iteration 7313: loss: 0.23600050806999207\n",
      "iteration 7314: loss: 0.23599961400032043\n",
      "iteration 7315: loss: 0.2359987497329712\n",
      "iteration 7316: loss: 0.23599763214588165\n",
      "iteration 7317: loss: 0.23599672317504883\n",
      "iteration 7318: loss: 0.2359958440065384\n",
      "iteration 7319: loss: 0.23599477112293243\n",
      "iteration 7320: loss: 0.23599383234977722\n",
      "iteration 7321: loss: 0.23599298298358917\n",
      "iteration 7322: loss: 0.23599188029766083\n",
      "iteration 7323: loss: 0.2359909564256668\n",
      "iteration 7324: loss: 0.23599004745483398\n",
      "iteration 7325: loss: 0.23598900437355042\n",
      "iteration 7326: loss: 0.2359880656003952\n",
      "iteration 7327: loss: 0.23598702251911163\n",
      "iteration 7328: loss: 0.23598630726337433\n",
      "iteration 7329: loss: 0.2359851896762848\n",
      "iteration 7330: loss: 0.23598413169384003\n",
      "iteration 7331: loss: 0.23598341643810272\n",
      "iteration 7332: loss: 0.23598232865333557\n",
      "iteration 7333: loss: 0.23598125576972961\n",
      "iteration 7334: loss: 0.2359805405139923\n",
      "iteration 7335: loss: 0.23597946763038635\n",
      "iteration 7336: loss: 0.2359783947467804\n",
      "iteration 7337: loss: 0.23597745597362518\n",
      "iteration 7338: loss: 0.23597660660743713\n",
      "iteration 7339: loss: 0.23597559332847595\n",
      "iteration 7340: loss: 0.23597469925880432\n",
      "iteration 7341: loss: 0.2359738051891327\n",
      "iteration 7342: loss: 0.23597276210784912\n",
      "iteration 7343: loss: 0.2359718382358551\n",
      "iteration 7344: loss: 0.23597092926502228\n",
      "iteration 7345: loss: 0.23597002029418945\n",
      "iteration 7346: loss: 0.2359689474105835\n",
      "iteration 7347: loss: 0.23596790432929993\n",
      "iteration 7348: loss: 0.23596712946891785\n",
      "iteration 7349: loss: 0.2359660565853119\n",
      "iteration 7350: loss: 0.23596501350402832\n",
      "iteration 7351: loss: 0.23596426844596863\n",
      "iteration 7352: loss: 0.23596319556236267\n",
      "iteration 7353: loss: 0.2359621524810791\n",
      "iteration 7354: loss: 0.2359614074230194\n",
      "iteration 7355: loss: 0.23596033453941345\n",
      "iteration 7356: loss: 0.23595929145812988\n",
      "iteration 7357: loss: 0.23595836758613586\n",
      "iteration 7358: loss: 0.23595750331878662\n",
      "iteration 7359: loss: 0.23595640063285828\n",
      "iteration 7360: loss: 0.23595552146434784\n",
      "iteration 7361: loss: 0.23595461249351501\n",
      "iteration 7362: loss: 0.23595356941223145\n",
      "iteration 7363: loss: 0.23595266044139862\n",
      "iteration 7364: loss: 0.23595158755779266\n",
      "iteration 7365: loss: 0.23595085740089417\n",
      "iteration 7366: loss: 0.2359497994184494\n",
      "iteration 7367: loss: 0.23594872653484344\n",
      "iteration 7368: loss: 0.235947847366333\n",
      "iteration 7369: loss: 0.23594693839550018\n",
      "iteration 7370: loss: 0.2359458953142166\n",
      "iteration 7371: loss: 0.2359449863433838\n",
      "iteration 7372: loss: 0.23594407737255096\n",
      "iteration 7373: loss: 0.2359430491924286\n",
      "iteration 7374: loss: 0.23594212532043457\n",
      "iteration 7375: loss: 0.235941082239151\n",
      "iteration 7376: loss: 0.23594018816947937\n",
      "iteration 7377: loss: 0.23593929409980774\n",
      "iteration 7378: loss: 0.23593822121620178\n",
      "iteration 7379: loss: 0.23593716323375702\n",
      "iteration 7380: loss: 0.23593643307685852\n",
      "iteration 7381: loss: 0.23593537509441376\n",
      "iteration 7382: loss: 0.23593434691429138\n",
      "iteration 7383: loss: 0.2359335869550705\n",
      "iteration 7384: loss: 0.23593254387378693\n",
      "iteration 7385: loss: 0.2359316349029541\n",
      "iteration 7386: loss: 0.2359306365251541\n",
      "iteration 7387: loss: 0.23592977225780487\n",
      "iteration 7388: loss: 0.23592884838581085\n",
      "iteration 7389: loss: 0.2359277755022049\n",
      "iteration 7390: loss: 0.23592694103717804\n",
      "iteration 7391: loss: 0.23592598736286163\n",
      "iteration 7392: loss: 0.23592492938041687\n",
      "iteration 7393: loss: 0.2359238862991333\n",
      "iteration 7394: loss: 0.23592312633991241\n",
      "iteration 7395: loss: 0.23592209815979004\n",
      "iteration 7396: loss: 0.23592102527618408\n",
      "iteration 7397: loss: 0.23592014610767365\n",
      "iteration 7398: loss: 0.2359192818403244\n",
      "iteration 7399: loss: 0.23591820895671844\n",
      "iteration 7400: loss: 0.235917329788208\n",
      "iteration 7401: loss: 0.23591625690460205\n",
      "iteration 7402: loss: 0.2359153777360916\n",
      "iteration 7403: loss: 0.2359144687652588\n",
      "iteration 7404: loss: 0.2359134405851364\n",
      "iteration 7405: loss: 0.23591239750385284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7406: loss: 0.23591165244579315\n",
      "iteration 7407: loss: 0.23591060936450958\n",
      "iteration 7408: loss: 0.23590955138206482\n",
      "iteration 7409: loss: 0.2359086573123932\n",
      "iteration 7410: loss: 0.23590782284736633\n",
      "iteration 7411: loss: 0.23590688407421112\n",
      "iteration 7412: loss: 0.23590584099292755\n",
      "iteration 7413: loss: 0.2359047830104828\n",
      "iteration 7414: loss: 0.2359040230512619\n",
      "iteration 7415: loss: 0.23590299487113953\n",
      "iteration 7416: loss: 0.23590195178985596\n",
      "iteration 7417: loss: 0.23590107262134552\n",
      "iteration 7418: loss: 0.23590019345283508\n",
      "iteration 7419: loss: 0.23589912056922913\n",
      "iteration 7420: loss: 0.2358982115983963\n",
      "iteration 7421: loss: 0.23589718341827393\n",
      "iteration 7422: loss: 0.2358963042497635\n",
      "iteration 7423: loss: 0.23589539527893066\n",
      "iteration 7424: loss: 0.23589424788951874\n",
      "iteration 7425: loss: 0.2358933389186859\n",
      "iteration 7426: loss: 0.23589229583740234\n",
      "iteration 7427: loss: 0.2358914315700531\n",
      "iteration 7428: loss: 0.23589053750038147\n",
      "iteration 7429: loss: 0.2358894795179367\n",
      "iteration 7430: loss: 0.23588843643665314\n",
      "iteration 7431: loss: 0.23588772118091583\n",
      "iteration 7432: loss: 0.23588666319847107\n",
      "iteration 7433: loss: 0.23588569462299347\n",
      "iteration 7434: loss: 0.23588499426841736\n",
      "iteration 7435: loss: 0.2358839064836502\n",
      "iteration 7436: loss: 0.23588290810585022\n",
      "iteration 7437: loss: 0.235881969332695\n",
      "iteration 7438: loss: 0.23588092625141144\n",
      "iteration 7439: loss: 0.2358800619840622\n",
      "iteration 7440: loss: 0.23587913811206818\n",
      "iteration 7441: loss: 0.2358781099319458\n",
      "iteration 7442: loss: 0.23587706685066223\n",
      "iteration 7443: loss: 0.23587636649608612\n",
      "iteration 7444: loss: 0.23587532341480255\n",
      "iteration 7445: loss: 0.2358742654323578\n",
      "iteration 7446: loss: 0.23587338626384735\n",
      "iteration 7447: loss: 0.23587234318256378\n",
      "iteration 7448: loss: 0.23587146401405334\n",
      "iteration 7449: loss: 0.2358705699443817\n",
      "iteration 7450: loss: 0.23586949706077576\n",
      "iteration 7451: loss: 0.23586861789226532\n",
      "iteration 7452: loss: 0.23586776852607727\n",
      "iteration 7453: loss: 0.2358667105436325\n",
      "iteration 7454: loss: 0.23586580157279968\n",
      "iteration 7455: loss: 0.2358647584915161\n",
      "iteration 7456: loss: 0.23586371541023254\n",
      "iteration 7457: loss: 0.23586300015449524\n",
      "iteration 7458: loss: 0.23586194217205048\n",
      "iteration 7459: loss: 0.2358609139919281\n",
      "iteration 7460: loss: 0.23586001992225647\n",
      "iteration 7461: loss: 0.23585915565490723\n",
      "iteration 7462: loss: 0.23585817217826843\n",
      "iteration 7463: loss: 0.23585724830627441\n",
      "iteration 7464: loss: 0.23585622012615204\n",
      "iteration 7465: loss: 0.23585517704486847\n",
      "iteration 7466: loss: 0.23585446178913116\n",
      "iteration 7467: loss: 0.2358533889055252\n",
      "iteration 7468: loss: 0.23585239052772522\n",
      "iteration 7469: loss: 0.2358514815568924\n",
      "iteration 7470: loss: 0.23585042357444763\n",
      "iteration 7471: loss: 0.23584957420825958\n",
      "iteration 7472: loss: 0.23584866523742676\n",
      "iteration 7473: loss: 0.23584763705730438\n",
      "iteration 7474: loss: 0.2358465939760208\n",
      "iteration 7475: loss: 0.23584571480751038\n",
      "iteration 7476: loss: 0.23584482073783875\n",
      "iteration 7477: loss: 0.2358439415693283\n",
      "iteration 7478: loss: 0.23584289848804474\n",
      "iteration 7479: loss: 0.23584187030792236\n",
      "iteration 7480: loss: 0.23584122955799103\n",
      "iteration 7481: loss: 0.23584017157554626\n",
      "iteration 7482: loss: 0.23583915829658508\n",
      "iteration 7483: loss: 0.23583821952342987\n",
      "iteration 7484: loss: 0.23583722114562988\n",
      "iteration 7485: loss: 0.2358362227678299\n",
      "iteration 7486: loss: 0.2358355075120926\n",
      "iteration 7487: loss: 0.23583444952964783\n",
      "iteration 7488: loss: 0.2358335554599762\n",
      "iteration 7489: loss: 0.23583249747753143\n",
      "iteration 7490: loss: 0.23583142459392548\n",
      "iteration 7491: loss: 0.23583078384399414\n",
      "iteration 7492: loss: 0.23582971096038818\n",
      "iteration 7493: loss: 0.23582866787910461\n",
      "iteration 7494: loss: 0.23582783341407776\n",
      "iteration 7495: loss: 0.235826775431633\n",
      "iteration 7496: loss: 0.23582592606544495\n",
      "iteration 7497: loss: 0.23582501709461212\n",
      "iteration 7498: loss: 0.23582398891448975\n",
      "iteration 7499: loss: 0.23582296073436737\n",
      "iteration 7500: loss: 0.23582205176353455\n",
      "iteration 7501: loss: 0.2358212172985077\n",
      "iteration 7502: loss: 0.2358202040195465\n",
      "iteration 7503: loss: 0.23581929504871368\n",
      "iteration 7504: loss: 0.2358182668685913\n",
      "iteration 7505: loss: 0.23581723868846893\n",
      "iteration 7506: loss: 0.23581652343273163\n",
      "iteration 7507: loss: 0.23581549525260925\n",
      "iteration 7508: loss: 0.23581445217132568\n",
      "iteration 7509: loss: 0.23581357300281525\n",
      "iteration 7510: loss: 0.23581251502037048\n",
      "iteration 7511: loss: 0.23581147193908691\n",
      "iteration 7512: loss: 0.235810786485672\n",
      "iteration 7513: loss: 0.23580975830554962\n",
      "iteration 7514: loss: 0.23580887913703918\n",
      "iteration 7515: loss: 0.23580782115459442\n",
      "iteration 7516: loss: 0.23580679297447205\n",
      "iteration 7517: loss: 0.23580607771873474\n",
      "iteration 7518: loss: 0.23580506443977356\n",
      "iteration 7519: loss: 0.23580403625965118\n",
      "iteration 7520: loss: 0.23580315709114075\n",
      "iteration 7521: loss: 0.23580212891101837\n",
      "iteration 7522: loss: 0.235801100730896\n",
      "iteration 7523: loss: 0.2358003854751587\n",
      "iteration 7524: loss: 0.23579935729503632\n",
      "iteration 7525: loss: 0.23579835891723633\n",
      "iteration 7526: loss: 0.23579749464988708\n",
      "iteration 7527: loss: 0.2357964813709259\n",
      "iteration 7528: loss: 0.23579545319080353\n",
      "iteration 7529: loss: 0.2357947826385498\n",
      "iteration 7530: loss: 0.23579370975494385\n",
      "iteration 7531: loss: 0.23579271137714386\n",
      "iteration 7532: loss: 0.23579183220863342\n",
      "iteration 7533: loss: 0.23579077422618866\n",
      "iteration 7534: loss: 0.2357899248600006\n",
      "iteration 7535: loss: 0.23578903079032898\n",
      "iteration 7536: loss: 0.235788032412529\n",
      "iteration 7537: loss: 0.23578715324401855\n",
      "iteration 7538: loss: 0.23578611016273499\n",
      "iteration 7539: loss: 0.2357850819826126\n",
      "iteration 7540: loss: 0.23578421771526337\n",
      "iteration 7541: loss: 0.23578333854675293\n",
      "iteration 7542: loss: 0.23578231036663055\n",
      "iteration 7543: loss: 0.2357814610004425\n",
      "iteration 7544: loss: 0.23578040301799774\n",
      "iteration 7545: loss: 0.23577937483787537\n",
      "iteration 7546: loss: 0.23577852547168732\n",
      "iteration 7547: loss: 0.23577749729156494\n",
      "iteration 7548: loss: 0.23577675223350525\n",
      "iteration 7549: loss: 0.23577578365802765\n",
      "iteration 7550: loss: 0.23577472567558289\n",
      "iteration 7551: loss: 0.23577383160591125\n",
      "iteration 7552: loss: 0.23577281832695007\n",
      "iteration 7553: loss: 0.23577181994915009\n",
      "iteration 7554: loss: 0.23577097058296204\n",
      "iteration 7555: loss: 0.23577013611793518\n",
      "iteration 7556: loss: 0.23576907813549042\n",
      "iteration 7557: loss: 0.23576819896697998\n",
      "iteration 7558: loss: 0.23576708137989044\n",
      "iteration 7559: loss: 0.23576602339744568\n",
      "iteration 7560: loss: 0.23576517403125763\n",
      "iteration 7561: loss: 0.23576433956623077\n",
      "iteration 7562: loss: 0.2357633113861084\n",
      "iteration 7563: loss: 0.23576238751411438\n",
      "iteration 7564: loss: 0.2357613742351532\n",
      "iteration 7565: loss: 0.2357603758573532\n",
      "iteration 7566: loss: 0.23575949668884277\n",
      "iteration 7567: loss: 0.23575863242149353\n",
      "iteration 7568: loss: 0.23575763404369354\n",
      "iteration 7569: loss: 0.2357567846775055\n",
      "iteration 7570: loss: 0.23575572669506073\n",
      "iteration 7571: loss: 0.23575472831726074\n",
      "iteration 7572: loss: 0.2357538938522339\n",
      "iteration 7573: loss: 0.2357528656721115\n",
      "iteration 7574: loss: 0.2357521802186966\n",
      "iteration 7575: loss: 0.23575115203857422\n",
      "iteration 7576: loss: 0.23575010895729065\n",
      "iteration 7577: loss: 0.2357492446899414\n",
      "iteration 7578: loss: 0.23574824631214142\n",
      "iteration 7579: loss: 0.23574721813201904\n",
      "iteration 7580: loss: 0.2357463538646698\n",
      "iteration 7581: loss: 0.23574528098106384\n",
      "iteration 7582: loss: 0.2357446253299713\n",
      "iteration 7583: loss: 0.23574361205101013\n",
      "iteration 7584: loss: 0.23574261367321014\n",
      "iteration 7585: loss: 0.2357417643070221\n",
      "iteration 7586: loss: 0.2357407510280609\n",
      "iteration 7587: loss: 0.23573973774909973\n",
      "iteration 7588: loss: 0.2357388436794281\n",
      "iteration 7589: loss: 0.23573783040046692\n",
      "iteration 7590: loss: 0.23573696613311768\n",
      "iteration 7591: loss: 0.23573613166809082\n",
      "iteration 7592: loss: 0.23573508858680725\n",
      "iteration 7593: loss: 0.23573406040668488\n",
      "iteration 7594: loss: 0.23573315143585205\n",
      "iteration 7595: loss: 0.23573216795921326\n",
      "iteration 7596: loss: 0.23573115468025208\n",
      "iteration 7597: loss: 0.23573040962219238\n",
      "iteration 7598: loss: 0.2357294112443924\n",
      "iteration 7599: loss: 0.2357284277677536\n",
      "iteration 7600: loss: 0.23572754859924316\n",
      "iteration 7601: loss: 0.23572655022144318\n",
      "iteration 7602: loss: 0.2357255220413208\n",
      "iteration 7603: loss: 0.23572464287281036\n",
      "iteration 7604: loss: 0.235723614692688\n",
      "iteration 7605: loss: 0.23572292923927307\n",
      "iteration 7606: loss: 0.2357219010591507\n",
      "iteration 7607: loss: 0.2357209026813507\n",
      "iteration 7608: loss: 0.23572000861167908\n",
      "iteration 7609: loss: 0.2357190102338791\n",
      "iteration 7610: loss: 0.2357179820537567\n",
      "iteration 7611: loss: 0.23571713268756866\n",
      "iteration 7612: loss: 0.2357161045074463\n",
      "iteration 7613: loss: 0.2357151210308075\n",
      "iteration 7614: loss: 0.23571422696113586\n",
      "iteration 7615: loss: 0.2357134073972702\n",
      "iteration 7616: loss: 0.23571237921714783\n",
      "iteration 7617: loss: 0.23571154475212097\n",
      "iteration 7618: loss: 0.23571065068244934\n",
      "iteration 7619: loss: 0.23570962250232697\n",
      "iteration 7620: loss: 0.2357087880373001\n",
      "iteration 7621: loss: 0.23570775985717773\n",
      "iteration 7622: loss: 0.2357068508863449\n",
      "iteration 7623: loss: 0.23570585250854492\n",
      "iteration 7624: loss: 0.23570482432842255\n",
      "iteration 7625: loss: 0.23570415377616882\n",
      "iteration 7626: loss: 0.23570314049720764\n",
      "iteration 7627: loss: 0.23570208251476288\n",
      "iteration 7628: loss: 0.23570124804973602\n",
      "iteration 7629: loss: 0.23570021986961365\n",
      "iteration 7630: loss: 0.23569920659065247\n",
      "iteration 7631: loss: 0.23569831252098083\n",
      "iteration 7632: loss: 0.23569731414318085\n",
      "iteration 7633: loss: 0.23569631576538086\n",
      "iteration 7634: loss: 0.23569564521312714\n",
      "iteration 7635: loss: 0.23569461703300476\n",
      "iteration 7636: loss: 0.2356937676668167\n",
      "iteration 7637: loss: 0.23569273948669434\n",
      "iteration 7638: loss: 0.23569171130657196\n",
      "iteration 7639: loss: 0.2356908768415451\n",
      "iteration 7640: loss: 0.23568984866142273\n",
      "iteration 7641: loss: 0.23568885028362274\n",
      "iteration 7642: loss: 0.2356880009174347\n",
      "iteration 7643: loss: 0.2356869876384735\n",
      "iteration 7644: loss: 0.23568598926067352\n",
      "iteration 7645: loss: 0.2356853187084198\n",
      "iteration 7646: loss: 0.2356843203306198\n",
      "iteration 7647: loss: 0.23568329215049744\n",
      "iteration 7648: loss: 0.2356824427843094\n",
      "iteration 7649: loss: 0.2356814444065094\n",
      "iteration 7650: loss: 0.23568038642406464\n",
      "iteration 7651: loss: 0.23567955195903778\n",
      "iteration 7652: loss: 0.23567862808704376\n",
      "iteration 7653: loss: 0.23567771911621094\n",
      "iteration 7654: loss: 0.23567669093608856\n",
      "iteration 7655: loss: 0.23567572236061096\n",
      "iteration 7656: loss: 0.23567482829093933\n",
      "iteration 7657: loss: 0.23567385971546173\n",
      "iteration 7658: loss: 0.2356729805469513\n",
      "iteration 7659: loss: 0.23567216098308563\n",
      "iteration 7660: loss: 0.23567108809947968\n",
      "iteration 7661: loss: 0.23567011952400208\n",
      "iteration 7662: loss: 0.23566925525665283\n",
      "iteration 7663: loss: 0.23566830158233643\n",
      "iteration 7664: loss: 0.23566730320453644\n",
      "iteration 7665: loss: 0.2356664389371872\n",
      "iteration 7666: loss: 0.2356654405593872\n",
      "iteration 7667: loss: 0.23566457629203796\n",
      "iteration 7668: loss: 0.23566357791423798\n",
      "iteration 7669: loss: 0.235662579536438\n",
      "iteration 7670: loss: 0.23566189408302307\n",
      "iteration 7671: loss: 0.2356608808040619\n",
      "iteration 7672: loss: 0.2356598824262619\n",
      "iteration 7673: loss: 0.23565903306007385\n",
      "iteration 7674: loss: 0.2356579750776291\n",
      "iteration 7675: loss: 0.23565702140331268\n",
      "iteration 7676: loss: 0.23565614223480225\n",
      "iteration 7677: loss: 0.23565511405467987\n",
      "iteration 7678: loss: 0.23565427958965302\n",
      "iteration 7679: loss: 0.23565331101417542\n",
      "iteration 7680: loss: 0.23565228283405304\n",
      "iteration 7681: loss: 0.23565144836902618\n",
      "iteration 7682: loss: 0.2356504499912262\n",
      "iteration 7683: loss: 0.23564943671226501\n",
      "iteration 7684: loss: 0.23564858734607697\n",
      "iteration 7685: loss: 0.23564758896827698\n",
      "iteration 7686: loss: 0.23564660549163818\n",
      "iteration 7687: loss: 0.23564593493938446\n",
      "iteration 7688: loss: 0.23564493656158447\n",
      "iteration 7689: loss: 0.23564393818378448\n",
      "iteration 7690: loss: 0.23564305901527405\n",
      "iteration 7691: loss: 0.23564204573631287\n",
      "iteration 7692: loss: 0.23564109206199646\n",
      "iteration 7693: loss: 0.2356402426958084\n",
      "iteration 7694: loss: 0.23563918471336365\n",
      "iteration 7695: loss: 0.2356383353471756\n",
      "iteration 7696: loss: 0.2356373369693756\n",
      "iteration 7697: loss: 0.23563632369041443\n",
      "iteration 7698: loss: 0.23563547432422638\n",
      "iteration 7699: loss: 0.2356344759464264\n",
      "iteration 7700: loss: 0.2356334626674652\n",
      "iteration 7701: loss: 0.23563261330127716\n",
      "iteration 7702: loss: 0.23563161492347717\n",
      "iteration 7703: loss: 0.23563094437122345\n",
      "iteration 7704: loss: 0.23562991619110107\n",
      "iteration 7705: loss: 0.23562884330749512\n",
      "iteration 7706: loss: 0.23562797904014587\n",
      "iteration 7707: loss: 0.23562702536582947\n",
      "iteration 7708: loss: 0.23562605679035187\n",
      "iteration 7709: loss: 0.235625222325325\n",
      "iteration 7710: loss: 0.23562422394752502\n",
      "iteration 7711: loss: 0.23562324047088623\n",
      "iteration 7712: loss: 0.23562240600585938\n",
      "iteration 7713: loss: 0.2356214076280594\n",
      "iteration 7714: loss: 0.2356204092502594\n",
      "iteration 7715: loss: 0.23561954498291016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7716: loss: 0.23561859130859375\n",
      "iteration 7717: loss: 0.23561759293079376\n",
      "iteration 7718: loss: 0.23561672866344452\n",
      "iteration 7719: loss: 0.23561573028564453\n",
      "iteration 7720: loss: 0.2356148660182953\n",
      "iteration 7721: loss: 0.2356138974428177\n",
      "iteration 7722: loss: 0.2356129139661789\n",
      "iteration 7723: loss: 0.23561206459999084\n",
      "iteration 7724: loss: 0.23561105132102966\n",
      "iteration 7725: loss: 0.23561005294322968\n",
      "iteration 7726: loss: 0.23560920357704163\n",
      "iteration 7727: loss: 0.23560822010040283\n",
      "iteration 7728: loss: 0.23560723662376404\n",
      "iteration 7729: loss: 0.2356063574552536\n",
      "iteration 7730: loss: 0.235605388879776\n",
      "iteration 7731: loss: 0.23560452461242676\n",
      "iteration 7732: loss: 0.23560354113578796\n",
      "iteration 7733: loss: 0.23560269176959991\n",
      "iteration 7734: loss: 0.23560182750225067\n",
      "iteration 7735: loss: 0.23560085892677307\n",
      "iteration 7736: loss: 0.23559987545013428\n",
      "iteration 7737: loss: 0.23559901118278503\n",
      "iteration 7738: loss: 0.23559801280498505\n",
      "iteration 7739: loss: 0.23559704422950745\n",
      "iteration 7740: loss: 0.2355961799621582\n",
      "iteration 7741: loss: 0.23559518158435822\n",
      "iteration 7742: loss: 0.23559436202049255\n",
      "iteration 7743: loss: 0.23559336364269257\n",
      "iteration 7744: loss: 0.23559236526489258\n",
      "iteration 7745: loss: 0.23559153079986572\n",
      "iteration 7746: loss: 0.23559050261974335\n",
      "iteration 7747: loss: 0.23558954894542694\n",
      "iteration 7748: loss: 0.2355886995792389\n",
      "iteration 7749: loss: 0.2355877161026001\n",
      "iteration 7750: loss: 0.2355867326259613\n",
      "iteration 7751: loss: 0.23558588325977325\n",
      "iteration 7752: loss: 0.23558489978313446\n",
      "iteration 7753: loss: 0.2355840504169464\n",
      "iteration 7754: loss: 0.2355831116437912\n",
      "iteration 7755: loss: 0.2355821132659912\n",
      "iteration 7756: loss: 0.23558130860328674\n",
      "iteration 7757: loss: 0.23558028042316437\n",
      "iteration 7758: loss: 0.23557929694652557\n",
      "iteration 7759: loss: 0.23557844758033752\n",
      "iteration 7760: loss: 0.23557743430137634\n",
      "iteration 7761: loss: 0.23557646572589874\n",
      "iteration 7762: loss: 0.2355756312608719\n",
      "iteration 7763: loss: 0.23557467758655548\n",
      "iteration 7764: loss: 0.23557376861572266\n",
      "iteration 7765: loss: 0.23557281494140625\n",
      "iteration 7766: loss: 0.23557183146476746\n",
      "iteration 7767: loss: 0.2355709969997406\n",
      "iteration 7768: loss: 0.235570028424263\n",
      "iteration 7769: loss: 0.235569030046463\n",
      "iteration 7770: loss: 0.23556816577911377\n",
      "iteration 7771: loss: 0.23556718230247498\n",
      "iteration 7772: loss: 0.23556634783744812\n",
      "iteration 7773: loss: 0.23556537926197052\n",
      "iteration 7774: loss: 0.23556438088417053\n",
      "iteration 7775: loss: 0.2355635166168213\n",
      "iteration 7776: loss: 0.23556256294250488\n",
      "iteration 7777: loss: 0.2355615347623825\n",
      "iteration 7778: loss: 0.23556068539619446\n",
      "iteration 7779: loss: 0.23555970191955566\n",
      "iteration 7780: loss: 0.23555874824523926\n",
      "iteration 7781: loss: 0.23555786907672882\n",
      "iteration 7782: loss: 0.23555691540241241\n",
      "iteration 7783: loss: 0.23555591702461243\n",
      "iteration 7784: loss: 0.23555508255958557\n",
      "iteration 7785: loss: 0.23555409908294678\n",
      "iteration 7786: loss: 0.23555326461791992\n",
      "iteration 7787: loss: 0.23555226624011993\n",
      "iteration 7788: loss: 0.23555131256580353\n",
      "iteration 7789: loss: 0.23555044829845428\n",
      "iteration 7790: loss: 0.23554947972297668\n",
      "iteration 7791: loss: 0.2355484664440155\n",
      "iteration 7792: loss: 0.23554766178131104\n",
      "iteration 7793: loss: 0.23554667830467224\n",
      "iteration 7794: loss: 0.2355458289384842\n",
      "iteration 7795: loss: 0.2355448454618454\n",
      "iteration 7796: loss: 0.2355438470840454\n",
      "iteration 7797: loss: 0.23554301261901855\n",
      "iteration 7798: loss: 0.23554210364818573\n",
      "iteration 7799: loss: 0.23554114997386932\n",
      "iteration 7800: loss: 0.23554030060768127\n",
      "iteration 7801: loss: 0.23553934693336487\n",
      "iteration 7802: loss: 0.2355383336544037\n",
      "iteration 7803: loss: 0.23553752899169922\n",
      "iteration 7804: loss: 0.2355365753173828\n",
      "iteration 7805: loss: 0.23553569614887238\n",
      "iteration 7806: loss: 0.23553471267223358\n",
      "iteration 7807: loss: 0.2355337142944336\n",
      "iteration 7808: loss: 0.23553287982940674\n",
      "iteration 7809: loss: 0.23553171753883362\n",
      "iteration 7810: loss: 0.2355307638645172\n",
      "iteration 7811: loss: 0.23552989959716797\n",
      "iteration 7812: loss: 0.23552891612052917\n",
      "iteration 7813: loss: 0.23552806675434113\n",
      "iteration 7814: loss: 0.23552706837654114\n",
      "iteration 7815: loss: 0.23552608489990234\n",
      "iteration 7816: loss: 0.23552528023719788\n",
      "iteration 7817: loss: 0.23552429676055908\n",
      "iteration 7818: loss: 0.2355233132839203\n",
      "iteration 7819: loss: 0.23552246391773224\n",
      "iteration 7820: loss: 0.23552151024341583\n",
      "iteration 7821: loss: 0.23552051186561584\n",
      "iteration 7822: loss: 0.23551972210407257\n",
      "iteration 7823: loss: 0.2355186939239502\n",
      "iteration 7824: loss: 0.23551785945892334\n",
      "iteration 7825: loss: 0.23551690578460693\n",
      "iteration 7826: loss: 0.23551592230796814\n",
      "iteration 7827: loss: 0.2355150729417801\n",
      "iteration 7828: loss: 0.2355140894651413\n",
      "iteration 7829: loss: 0.2355131357908249\n",
      "iteration 7830: loss: 0.23551230132579803\n",
      "iteration 7831: loss: 0.23551133275032043\n",
      "iteration 7832: loss: 0.23551049828529358\n",
      "iteration 7833: loss: 0.23550951480865479\n",
      "iteration 7834: loss: 0.235508531332016\n",
      "iteration 7835: loss: 0.23550769686698914\n",
      "iteration 7836: loss: 0.23550672829151154\n",
      "iteration 7837: loss: 0.2355055809020996\n",
      "iteration 7838: loss: 0.23550474643707275\n",
      "iteration 7839: loss: 0.23550376296043396\n",
      "iteration 7840: loss: 0.2355029135942459\n",
      "iteration 7841: loss: 0.2355019599199295\n",
      "iteration 7842: loss: 0.23550105094909668\n",
      "iteration 7843: loss: 0.23550021648406982\n",
      "iteration 7844: loss: 0.23549926280975342\n",
      "iteration 7845: loss: 0.23549826443195343\n",
      "iteration 7846: loss: 0.23549742996692657\n",
      "iteration 7847: loss: 0.23549644649028778\n",
      "iteration 7848: loss: 0.23549561202526093\n",
      "iteration 7849: loss: 0.23549464344978333\n",
      "iteration 7850: loss: 0.2354937046766281\n",
      "iteration 7851: loss: 0.23549285531044006\n",
      "iteration 7852: loss: 0.23549187183380127\n",
      "iteration 7853: loss: 0.23549091815948486\n",
      "iteration 7854: loss: 0.23549005389213562\n",
      "iteration 7855: loss: 0.23548908531665802\n",
      "iteration 7856: loss: 0.23548826575279236\n",
      "iteration 7857: loss: 0.23548729717731476\n",
      "iteration 7858: loss: 0.23548611998558044\n",
      "iteration 7859: loss: 0.2354852706193924\n",
      "iteration 7860: loss: 0.235484316945076\n",
      "iteration 7861: loss: 0.235483318567276\n",
      "iteration 7862: loss: 0.23548249900341034\n",
      "iteration 7863: loss: 0.23548157513141632\n",
      "iteration 7864: loss: 0.23548059165477753\n",
      "iteration 7865: loss: 0.23547978699207306\n",
      "iteration 7866: loss: 0.23547883331775665\n",
      "iteration 7867: loss: 0.2354779690504074\n",
      "iteration 7868: loss: 0.2354770004749298\n",
      "iteration 7869: loss: 0.2354760468006134\n",
      "iteration 7870: loss: 0.23547521233558655\n",
      "iteration 7871: loss: 0.2354741394519806\n",
      "iteration 7872: loss: 0.23547334969043732\n",
      "iteration 7873: loss: 0.2354721575975418\n",
      "iteration 7874: loss: 0.2354712039232254\n",
      "iteration 7875: loss: 0.23547033965587616\n",
      "iteration 7876: loss: 0.23546937108039856\n",
      "iteration 7877: loss: 0.23546841740608215\n",
      "iteration 7878: loss: 0.2354675829410553\n",
      "iteration 7879: loss: 0.2354665994644165\n",
      "iteration 7880: loss: 0.23546576499938965\n",
      "iteration 7881: loss: 0.23546481132507324\n",
      "iteration 7882: loss: 0.23546382784843445\n",
      "iteration 7883: loss: 0.2354629933834076\n",
      "iteration 7884: loss: 0.2354620397090912\n",
      "iteration 7885: loss: 0.23546108603477478\n",
      "iteration 7886: loss: 0.2354602813720703\n",
      "iteration 7887: loss: 0.23545917868614197\n",
      "iteration 7888: loss: 0.23545832931995392\n",
      "iteration 7889: loss: 0.2354573756456375\n",
      "iteration 7890: loss: 0.2354564219713211\n",
      "iteration 7891: loss: 0.23545558750629425\n",
      "iteration 7892: loss: 0.23545460402965546\n",
      "iteration 7893: loss: 0.23545363545417786\n",
      "iteration 7894: loss: 0.23545284569263458\n",
      "iteration 7895: loss: 0.2354518473148346\n",
      "iteration 7896: loss: 0.23545102775096893\n",
      "iteration 7897: loss: 0.23545002937316895\n",
      "iteration 7898: loss: 0.23544912040233612\n",
      "iteration 7899: loss: 0.23544809222221375\n",
      "iteration 7900: loss: 0.23544713854789734\n",
      "iteration 7901: loss: 0.23544616997241974\n",
      "iteration 7902: loss: 0.23544533550739288\n",
      "iteration 7903: loss: 0.23544438183307648\n",
      "iteration 7904: loss: 0.23544354736804962\n",
      "iteration 7905: loss: 0.23544259369373322\n",
      "iteration 7906: loss: 0.2354416400194168\n",
      "iteration 7907: loss: 0.23544080555438995\n",
      "iteration 7908: loss: 0.23543982207775116\n",
      "iteration 7909: loss: 0.2354390174150467\n",
      "iteration 7910: loss: 0.23543787002563477\n",
      "iteration 7911: loss: 0.23543687164783478\n",
      "iteration 7912: loss: 0.23543603718280792\n",
      "iteration 7913: loss: 0.23543508350849152\n",
      "iteration 7914: loss: 0.2354341447353363\n",
      "iteration 7915: loss: 0.23543331027030945\n",
      "iteration 7916: loss: 0.23543235659599304\n",
      "iteration 7917: loss: 0.2354315221309662\n",
      "iteration 7918: loss: 0.23543059825897217\n",
      "iteration 7919: loss: 0.23542961478233337\n",
      "iteration 7920: loss: 0.2354286164045334\n",
      "iteration 7921: loss: 0.23542766273021698\n",
      "iteration 7922: loss: 0.23542681336402893\n",
      "iteration 7923: loss: 0.23542585968971252\n",
      "iteration 7924: loss: 0.23542490601539612\n",
      "iteration 7925: loss: 0.23542407155036926\n",
      "iteration 7926: loss: 0.23542313277721405\n",
      "iteration 7927: loss: 0.23542216420173645\n",
      "iteration 7928: loss: 0.2354212999343872\n",
      "iteration 7929: loss: 0.2354203760623932\n",
      "iteration 7930: loss: 0.235419362783432\n",
      "iteration 7931: loss: 0.23541846871376038\n",
      "iteration 7932: loss: 0.23541751503944397\n",
      "iteration 7933: loss: 0.2354166954755783\n",
      "iteration 7934: loss: 0.2354157269001007\n",
      "iteration 7935: loss: 0.2354147881269455\n",
      "iteration 7936: loss: 0.23541395366191864\n",
      "iteration 7937: loss: 0.23541302978992462\n",
      "iteration 7938: loss: 0.23541200160980225\n",
      "iteration 7939: loss: 0.23541101813316345\n",
      "iteration 7940: loss: 0.23541006445884705\n",
      "iteration 7941: loss: 0.23540925979614258\n",
      "iteration 7942: loss: 0.23540827631950378\n",
      "iteration 7943: loss: 0.2354075014591217\n",
      "iteration 7944: loss: 0.23540648818016052\n",
      "iteration 7945: loss: 0.23540553450584412\n",
      "iteration 7946: loss: 0.23540470004081726\n",
      "iteration 7947: loss: 0.23540377616882324\n",
      "iteration 7948: loss: 0.23540277779102325\n",
      "iteration 7949: loss: 0.23540179431438446\n",
      "iteration 7950: loss: 0.23540084064006805\n",
      "iteration 7951: loss: 0.23540005087852478\n",
      "iteration 7952: loss: 0.23539908230304718\n",
      "iteration 7953: loss: 0.23539814352989197\n",
      "iteration 7954: loss: 0.2353973090648651\n",
      "iteration 7955: loss: 0.23539617657661438\n",
      "iteration 7956: loss: 0.23539535701274872\n",
      "iteration 7957: loss: 0.2353944331407547\n",
      "iteration 7958: loss: 0.2353934347629547\n",
      "iteration 7959: loss: 0.23539265990257263\n",
      "iteration 7960: loss: 0.23539166152477264\n",
      "iteration 7961: loss: 0.23539087176322937\n",
      "iteration 7962: loss: 0.23538990318775177\n",
      "iteration 7963: loss: 0.23538878560066223\n",
      "iteration 7964: loss: 0.2353878915309906\n",
      "iteration 7965: loss: 0.23538701236248016\n",
      "iteration 7966: loss: 0.23538604378700256\n",
      "iteration 7967: loss: 0.2353852093219757\n",
      "iteration 7968: loss: 0.2353842556476593\n",
      "iteration 7969: loss: 0.23538346588611603\n",
      "iteration 7970: loss: 0.23538248240947723\n",
      "iteration 7971: loss: 0.23538139462471008\n",
      "iteration 7972: loss: 0.23538056015968323\n",
      "iteration 7973: loss: 0.23537960648536682\n",
      "iteration 7974: loss: 0.23537877202033997\n",
      "iteration 7975: loss: 0.23537786304950714\n",
      "iteration 7976: loss: 0.23537692427635193\n",
      "iteration 7977: loss: 0.23537611961364746\n",
      "iteration 7978: loss: 0.23537500202655792\n",
      "iteration 7979: loss: 0.23537416756153107\n",
      "iteration 7980: loss: 0.23537321388721466\n",
      "iteration 7981: loss: 0.23537221550941467\n",
      "iteration 7982: loss: 0.2353714406490326\n",
      "iteration 7983: loss: 0.235370472073555\n",
      "iteration 7984: loss: 0.23536968231201172\n",
      "iteration 7985: loss: 0.23536869883537292\n",
      "iteration 7986: loss: 0.2353675663471222\n",
      "iteration 7987: loss: 0.23536674678325653\n",
      "iteration 7988: loss: 0.2353658229112625\n",
      "iteration 7989: loss: 0.2353648692369461\n",
      "iteration 7990: loss: 0.23536403477191925\n",
      "iteration 7991: loss: 0.23536312580108643\n",
      "iteration 7992: loss: 0.23536209762096405\n",
      "iteration 7993: loss: 0.23536117374897003\n",
      "iteration 7994: loss: 0.23536019027233124\n",
      "iteration 7995: loss: 0.23535940051078796\n",
      "iteration 7996: loss: 0.23535847663879395\n",
      "iteration 7997: loss: 0.23535767197608948\n",
      "iteration 7998: loss: 0.23535649478435516\n",
      "iteration 7999: loss: 0.23535557091236115\n",
      "iteration 8000: loss: 0.23535475134849548\n",
      "iteration 8001: loss: 0.23535379767417908\n",
      "iteration 8002: loss: 0.23535284399986267\n",
      "iteration 8003: loss: 0.2353520691394806\n",
      "iteration 8004: loss: 0.23535092175006866\n",
      "iteration 8005: loss: 0.2353500872850418\n",
      "iteration 8006: loss: 0.23534910380840302\n",
      "iteration 8007: loss: 0.235348179936409\n",
      "iteration 8008: loss: 0.23534736037254333\n",
      "iteration 8009: loss: 0.23534640669822693\n",
      "iteration 8010: loss: 0.23534560203552246\n",
      "iteration 8011: loss: 0.23534467816352844\n",
      "iteration 8012: loss: 0.2353435456752777\n",
      "iteration 8013: loss: 0.23534271121025085\n",
      "iteration 8014: loss: 0.23534174263477325\n",
      "iteration 8015: loss: 0.23534095287322998\n",
      "iteration 8016: loss: 0.23534002900123596\n",
      "iteration 8017: loss: 0.23533907532691956\n",
      "iteration 8018: loss: 0.23533806204795837\n",
      "iteration 8019: loss: 0.23533718287944794\n",
      "iteration 8020: loss: 0.23533639311790466\n",
      "iteration 8021: loss: 0.23533539474010468\n",
      "iteration 8022: loss: 0.23533447086811066\n",
      "iteration 8023: loss: 0.23533344268798828\n",
      "iteration 8024: loss: 0.23533253371715546\n",
      "iteration 8025: loss: 0.235331729054451\n",
      "iteration 8026: loss: 0.23533082008361816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8027: loss: 0.23532986640930176\n",
      "iteration 8028: loss: 0.23532888293266296\n",
      "iteration 8029: loss: 0.23532792925834656\n",
      "iteration 8030: loss: 0.2353270947933197\n",
      "iteration 8031: loss: 0.23532620072364807\n",
      "iteration 8032: loss: 0.23532524704933167\n",
      "iteration 8033: loss: 0.2353244572877884\n",
      "iteration 8034: loss: 0.23532328009605408\n",
      "iteration 8035: loss: 0.235322505235672\n",
      "iteration 8036: loss: 0.23532156646251678\n",
      "iteration 8037: loss: 0.23532059788703918\n",
      "iteration 8038: loss: 0.2353198081254959\n",
      "iteration 8039: loss: 0.2353188544511795\n",
      "iteration 8040: loss: 0.23531779646873474\n",
      "iteration 8041: loss: 0.2353169023990631\n",
      "iteration 8042: loss: 0.2353159487247467\n",
      "iteration 8043: loss: 0.23531515896320343\n",
      "iteration 8044: loss: 0.23531422019004822\n",
      "iteration 8045: loss: 0.23531341552734375\n",
      "iteration 8046: loss: 0.2353122979402542\n",
      "iteration 8047: loss: 0.23531131446361542\n",
      "iteration 8048: loss: 0.23531052470207214\n",
      "iteration 8049: loss: 0.23530957102775574\n",
      "iteration 8050: loss: 0.23530876636505127\n",
      "iteration 8051: loss: 0.23530766367912292\n",
      "iteration 8052: loss: 0.23530669510364532\n",
      "iteration 8053: loss: 0.23530590534210205\n",
      "iteration 8054: loss: 0.23530498147010803\n",
      "iteration 8055: loss: 0.23530414700508118\n",
      "iteration 8056: loss: 0.23530299961566925\n",
      "iteration 8057: loss: 0.23530206084251404\n",
      "iteration 8058: loss: 0.23530128598213196\n",
      "iteration 8059: loss: 0.23530033230781555\n",
      "iteration 8060: loss: 0.23529954254627228\n",
      "iteration 8061: loss: 0.23529839515686035\n",
      "iteration 8062: loss: 0.2352975308895111\n",
      "iteration 8063: loss: 0.23529675602912903\n",
      "iteration 8064: loss: 0.23529572784900665\n",
      "iteration 8065: loss: 0.235294908285141\n",
      "iteration 8066: loss: 0.23529379069805145\n",
      "iteration 8067: loss: 0.23529283702373505\n",
      "iteration 8068: loss: 0.23529204726219177\n",
      "iteration 8069: loss: 0.23529109358787537\n",
      "iteration 8070: loss: 0.2352903187274933\n",
      "iteration 8071: loss: 0.23528918623924255\n",
      "iteration 8072: loss: 0.23528823256492615\n",
      "iteration 8073: loss: 0.23528742790222168\n",
      "iteration 8074: loss: 0.23528651893138885\n",
      "iteration 8075: loss: 0.235285684466362\n",
      "iteration 8076: loss: 0.23528456687927246\n",
      "iteration 8077: loss: 0.23528364300727844\n",
      "iteration 8078: loss: 0.23528282344341278\n",
      "iteration 8079: loss: 0.23528186976909637\n",
      "iteration 8080: loss: 0.2352810800075531\n",
      "iteration 8081: loss: 0.23527996242046356\n",
      "iteration 8082: loss: 0.23527900874614716\n",
      "iteration 8083: loss: 0.2352781742811203\n",
      "iteration 8084: loss: 0.23527726531028748\n",
      "iteration 8085: loss: 0.23527629673480988\n",
      "iteration 8086: loss: 0.23527534306049347\n",
      "iteration 8087: loss: 0.23527458310127258\n",
      "iteration 8088: loss: 0.23527362942695618\n",
      "iteration 8089: loss: 0.23527269065380096\n",
      "iteration 8090: loss: 0.23527169227600098\n",
      "iteration 8091: loss: 0.23527073860168457\n",
      "iteration 8092: loss: 0.2352699488401413\n",
      "iteration 8093: loss: 0.23526902496814728\n",
      "iteration 8094: loss: 0.23526808619499207\n",
      "iteration 8095: loss: 0.23526708781719208\n",
      "iteration 8096: loss: 0.23526617884635925\n",
      "iteration 8097: loss: 0.23526540398597717\n",
      "iteration 8098: loss: 0.23526449501514435\n",
      "iteration 8099: loss: 0.23526331782341003\n",
      "iteration 8100: loss: 0.23526254296302795\n",
      "iteration 8101: loss: 0.23526160418987274\n",
      "iteration 8102: loss: 0.23526084423065186\n",
      "iteration 8103: loss: 0.23525969684123993\n",
      "iteration 8104: loss: 0.2352587729692459\n",
      "iteration 8105: loss: 0.23525795340538025\n",
      "iteration 8106: loss: 0.2352571040391922\n",
      "iteration 8107: loss: 0.23525628447532654\n",
      "iteration 8108: loss: 0.2352551519870758\n",
      "iteration 8109: loss: 0.23525424301624298\n",
      "iteration 8110: loss: 0.2352534532546997\n",
      "iteration 8111: loss: 0.2352524995803833\n",
      "iteration 8112: loss: 0.2352515012025833\n",
      "iteration 8113: loss: 0.2352505624294281\n",
      "iteration 8114: loss: 0.23524980247020721\n",
      "iteration 8115: loss: 0.2352488785982132\n",
      "iteration 8116: loss: 0.23524773120880127\n",
      "iteration 8117: loss: 0.235246941447258\n",
      "iteration 8118: loss: 0.23524601757526398\n",
      "iteration 8119: loss: 0.23524519801139832\n",
      "iteration 8120: loss: 0.23524406552314758\n",
      "iteration 8121: loss: 0.23524312674999237\n",
      "iteration 8122: loss: 0.23524236679077148\n",
      "iteration 8123: loss: 0.23524144291877747\n",
      "iteration 8124: loss: 0.2352406233549118\n",
      "iteration 8125: loss: 0.23523950576782227\n",
      "iteration 8126: loss: 0.2352387011051178\n",
      "iteration 8127: loss: 0.23523776233196259\n",
      "iteration 8128: loss: 0.23523685336112976\n",
      "iteration 8129: loss: 0.23523588478565216\n",
      "iteration 8130: loss: 0.23523493111133575\n",
      "iteration 8131: loss: 0.23523414134979248\n",
      "iteration 8132: loss: 0.23523321747779846\n",
      "iteration 8133: loss: 0.23523211479187012\n",
      "iteration 8134: loss: 0.23523132503032684\n",
      "iteration 8135: loss: 0.23523037135601044\n",
      "iteration 8136: loss: 0.23522964119911194\n",
      "iteration 8137: loss: 0.2352285087108612\n",
      "iteration 8138: loss: 0.2352275550365448\n",
      "iteration 8139: loss: 0.23522678017616272\n",
      "iteration 8140: loss: 0.2352258712053299\n",
      "iteration 8141: loss: 0.23522484302520752\n",
      "iteration 8142: loss: 0.2352239340543747\n",
      "iteration 8143: loss: 0.23522312939167023\n",
      "iteration 8144: loss: 0.2352222204208374\n",
      "iteration 8145: loss: 0.23522107303142548\n",
      "iteration 8146: loss: 0.2352202832698822\n",
      "iteration 8147: loss: 0.23521938920021057\n",
      "iteration 8148: loss: 0.2352185696363449\n",
      "iteration 8149: loss: 0.23521749675273895\n",
      "iteration 8150: loss: 0.23521658778190613\n",
      "iteration 8151: loss: 0.23521578311920166\n",
      "iteration 8152: loss: 0.23521487414836884\n",
      "iteration 8153: loss: 0.23521387577056885\n",
      "iteration 8154: loss: 0.23521296679973602\n",
      "iteration 8155: loss: 0.23521201312541962\n",
      "iteration 8156: loss: 0.23521122336387634\n",
      "iteration 8157: loss: 0.2352101355791092\n",
      "iteration 8158: loss: 0.23520931601524353\n",
      "iteration 8159: loss: 0.2352084219455719\n",
      "iteration 8160: loss: 0.23520763218402863\n",
      "iteration 8161: loss: 0.2352064847946167\n",
      "iteration 8162: loss: 0.23520568013191223\n",
      "iteration 8163: loss: 0.2352047711610794\n",
      "iteration 8164: loss: 0.23520365357398987\n",
      "iteration 8165: loss: 0.2352028787136078\n",
      "iteration 8166: loss: 0.23520192503929138\n",
      "iteration 8167: loss: 0.23520112037658691\n",
      "iteration 8168: loss: 0.23520001769065857\n",
      "iteration 8169: loss: 0.23519928753376007\n",
      "iteration 8170: loss: 0.23519830405712128\n",
      "iteration 8171: loss: 0.23519723117351532\n",
      "iteration 8172: loss: 0.23519644141197205\n",
      "iteration 8173: loss: 0.23519548773765564\n",
      "iteration 8174: loss: 0.23519475758075714\n",
      "iteration 8175: loss: 0.23519361019134521\n",
      "iteration 8176: loss: 0.2351927012205124\n",
      "iteration 8177: loss: 0.23519191145896912\n",
      "iteration 8178: loss: 0.2351909875869751\n",
      "iteration 8179: loss: 0.2351899892091751\n",
      "iteration 8180: loss: 0.23518908023834229\n",
      "iteration 8181: loss: 0.23518827557563782\n",
      "iteration 8182: loss: 0.2351873815059662\n",
      "iteration 8183: loss: 0.23518629372119904\n",
      "iteration 8184: loss: 0.23518545925617218\n",
      "iteration 8185: loss: 0.23518452048301697\n",
      "iteration 8186: loss: 0.23518356680870056\n",
      "iteration 8187: loss: 0.23518261313438416\n",
      "iteration 8188: loss: 0.23518171906471252\n",
      "iteration 8189: loss: 0.23518089950084686\n",
      "iteration 8190: loss: 0.2351798117160797\n",
      "iteration 8191: loss: 0.23517903685569763\n",
      "iteration 8192: loss: 0.23517818748950958\n",
      "iteration 8193: loss: 0.23517736792564392\n",
      "iteration 8194: loss: 0.23517625033855438\n",
      "iteration 8195: loss: 0.23517537117004395\n",
      "iteration 8196: loss: 0.23517458140850067\n",
      "iteration 8197: loss: 0.23517346382141113\n",
      "iteration 8198: loss: 0.23517270386219025\n",
      "iteration 8199: loss: 0.23517175018787384\n",
      "iteration 8200: loss: 0.23517076671123505\n",
      "iteration 8201: loss: 0.2351698875427246\n",
      "iteration 8202: loss: 0.23516909778118134\n",
      "iteration 8203: loss: 0.2351679801940918\n",
      "iteration 8204: loss: 0.23516705632209778\n",
      "iteration 8205: loss: 0.2351662814617157\n",
      "iteration 8206: loss: 0.23516535758972168\n",
      "iteration 8207: loss: 0.23516437411308289\n",
      "iteration 8208: loss: 0.23516348004341125\n",
      "iteration 8209: loss: 0.2351626604795456\n",
      "iteration 8210: loss: 0.23516154289245605\n",
      "iteration 8211: loss: 0.23516063392162323\n",
      "iteration 8212: loss: 0.23515987396240234\n",
      "iteration 8213: loss: 0.23515892028808594\n",
      "iteration 8214: loss: 0.23515793681144714\n",
      "iteration 8215: loss: 0.2351570427417755\n",
      "iteration 8216: loss: 0.23515626788139343\n",
      "iteration 8217: loss: 0.2351551502943039\n",
      "iteration 8218: loss: 0.23515422642230988\n",
      "iteration 8219: loss: 0.235153466463089\n",
      "iteration 8220: loss: 0.23515233397483826\n",
      "iteration 8221: loss: 0.23515157401561737\n",
      "iteration 8222: loss: 0.23515066504478455\n",
      "iteration 8223: loss: 0.23514977097511292\n",
      "iteration 8224: loss: 0.23514875769615173\n",
      "iteration 8225: loss: 0.2351478636264801\n",
      "iteration 8226: loss: 0.23514704406261444\n",
      "iteration 8227: loss: 0.2351459264755249\n",
      "iteration 8228: loss: 0.2351452112197876\n",
      "iteration 8229: loss: 0.23514428734779358\n",
      "iteration 8230: loss: 0.23514330387115479\n",
      "iteration 8231: loss: 0.23514237999916077\n",
      "iteration 8232: loss: 0.23514147102832794\n",
      "iteration 8233: loss: 0.23514047265052795\n",
      "iteration 8234: loss: 0.23513956367969513\n",
      "iteration 8235: loss: 0.23513884842395782\n",
      "iteration 8236: loss: 0.23513777554035187\n",
      "iteration 8237: loss: 0.2351369857788086\n",
      "iteration 8238: loss: 0.2351360321044922\n",
      "iteration 8239: loss: 0.23513507843017578\n",
      "iteration 8240: loss: 0.23513415455818176\n",
      "iteration 8241: loss: 0.23513321578502655\n",
      "iteration 8242: loss: 0.23513245582580566\n",
      "iteration 8243: loss: 0.23513135313987732\n",
      "iteration 8244: loss: 0.23513057827949524\n",
      "iteration 8245: loss: 0.2351296842098236\n",
      "iteration 8246: loss: 0.23512868583202362\n",
      "iteration 8247: loss: 0.2351277768611908\n",
      "iteration 8248: loss: 0.23512688279151917\n",
      "iteration 8249: loss: 0.23512589931488037\n",
      "iteration 8250: loss: 0.23512502014636993\n",
      "iteration 8251: loss: 0.23512423038482666\n",
      "iteration 8252: loss: 0.23512311279773712\n",
      "iteration 8253: loss: 0.23512235283851624\n",
      "iteration 8254: loss: 0.23512141406536102\n",
      "iteration 8255: loss: 0.23512034118175507\n",
      "iteration 8256: loss: 0.2351195365190506\n",
      "iteration 8257: loss: 0.23511862754821777\n",
      "iteration 8258: loss: 0.23511767387390137\n",
      "iteration 8259: loss: 0.23511675000190735\n",
      "iteration 8260: loss: 0.23511597514152527\n",
      "iteration 8261: loss: 0.23511485755443573\n",
      "iteration 8262: loss: 0.23511409759521484\n",
      "iteration 8263: loss: 0.23511318862438202\n",
      "iteration 8264: loss: 0.23511207103729248\n",
      "iteration 8265: loss: 0.2351112812757492\n",
      "iteration 8266: loss: 0.23511040210723877\n",
      "iteration 8267: loss: 0.23510941863059998\n",
      "iteration 8268: loss: 0.23510852456092834\n",
      "iteration 8269: loss: 0.23510774970054626\n",
      "iteration 8270: loss: 0.23510661721229553\n",
      "iteration 8271: loss: 0.2351057082414627\n",
      "iteration 8272: loss: 0.23510491847991943\n",
      "iteration 8273: loss: 0.2351038157939911\n",
      "iteration 8274: loss: 0.235103040933609\n",
      "iteration 8275: loss: 0.23510214686393738\n",
      "iteration 8276: loss: 0.23510117828845978\n",
      "iteration 8277: loss: 0.23510026931762695\n",
      "iteration 8278: loss: 0.23509955406188965\n",
      "iteration 8279: loss: 0.2350984513759613\n",
      "iteration 8280: loss: 0.23509769141674042\n",
      "iteration 8281: loss: 0.2350967675447464\n",
      "iteration 8282: loss: 0.23509566485881805\n",
      "iteration 8283: loss: 0.23509490489959717\n",
      "iteration 8284: loss: 0.23509378731250763\n",
      "iteration 8285: loss: 0.23509302735328674\n",
      "iteration 8286: loss: 0.23509211838245392\n",
      "iteration 8287: loss: 0.23509113490581512\n",
      "iteration 8288: loss: 0.2350902259349823\n",
      "iteration 8289: loss: 0.23508933186531067\n",
      "iteration 8290: loss: 0.23508834838867188\n",
      "iteration 8291: loss: 0.23508743941783905\n",
      "iteration 8292: loss: 0.23508664965629578\n",
      "iteration 8293: loss: 0.23508557677268982\n",
      "iteration 8294: loss: 0.23508481681346893\n",
      "iteration 8295: loss: 0.2350839078426361\n",
      "iteration 8296: loss: 0.23508282005786896\n",
      "iteration 8297: loss: 0.23508194088935852\n",
      "iteration 8298: loss: 0.2350810468196869\n",
      "iteration 8299: loss: 0.2350800484418869\n",
      "iteration 8300: loss: 0.23507916927337646\n",
      "iteration 8301: loss: 0.2350783795118332\n",
      "iteration 8302: loss: 0.23507730662822723\n",
      "iteration 8303: loss: 0.23507651686668396\n",
      "iteration 8304: loss: 0.2350754290819168\n",
      "iteration 8305: loss: 0.23507463932037354\n",
      "iteration 8306: loss: 0.2350737303495407\n",
      "iteration 8307: loss: 0.23507261276245117\n",
      "iteration 8308: loss: 0.23507185280323029\n",
      "iteration 8309: loss: 0.23507098853588104\n",
      "iteration 8310: loss: 0.23506999015808105\n",
      "iteration 8311: loss: 0.23506908118724823\n",
      "iteration 8312: loss: 0.23506812751293182\n",
      "iteration 8313: loss: 0.235067218542099\n",
      "iteration 8314: loss: 0.23506644368171692\n",
      "iteration 8315: loss: 0.23506537079811096\n",
      "iteration 8316: loss: 0.2350645810365677\n",
      "iteration 8317: loss: 0.23506364226341248\n",
      "iteration 8318: loss: 0.23506267368793488\n",
      "iteration 8319: loss: 0.23506176471710205\n",
      "iteration 8320: loss: 0.2350608855485916\n",
      "iteration 8321: loss: 0.23506000638008118\n",
      "iteration 8322: loss: 0.23505909740924835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8323: loss: 0.23505833745002747\n",
      "iteration 8324: loss: 0.23505723476409912\n",
      "iteration 8325: loss: 0.23505644500255585\n",
      "iteration 8326: loss: 0.2350553721189499\n",
      "iteration 8327: loss: 0.23505452275276184\n",
      "iteration 8328: loss: 0.23505373299121857\n",
      "iteration 8329: loss: 0.23505263030529022\n",
      "iteration 8330: loss: 0.23505182564258575\n",
      "iteration 8331: loss: 0.2350507527589798\n",
      "iteration 8332: loss: 0.2350500077009201\n",
      "iteration 8333: loss: 0.23504909873008728\n",
      "iteration 8334: loss: 0.23504813015460968\n",
      "iteration 8335: loss: 0.23504722118377686\n",
      "iteration 8336: loss: 0.23504647612571716\n",
      "iteration 8337: loss: 0.23504535853862762\n",
      "iteration 8338: loss: 0.2350444793701172\n",
      "iteration 8339: loss: 0.2350434958934784\n",
      "iteration 8340: loss: 0.23504261672496796\n",
      "iteration 8341: loss: 0.23504182696342468\n",
      "iteration 8342: loss: 0.23504075407981873\n",
      "iteration 8343: loss: 0.23503997921943665\n",
      "iteration 8344: loss: 0.2350388467311859\n",
      "iteration 8345: loss: 0.23503808677196503\n",
      "iteration 8346: loss: 0.2350372076034546\n",
      "iteration 8347: loss: 0.2350362241268158\n",
      "iteration 8348: loss: 0.23503530025482178\n",
      "iteration 8349: loss: 0.23503455519676208\n",
      "iteration 8350: loss: 0.23503348231315613\n",
      "iteration 8351: loss: 0.2350325882434845\n",
      "iteration 8352: loss: 0.2350316047668457\n",
      "iteration 8353: loss: 0.23503072559833527\n",
      "iteration 8354: loss: 0.235029935836792\n",
      "iteration 8355: loss: 0.23502889275550842\n",
      "iteration 8356: loss: 0.23502810299396515\n",
      "iteration 8357: loss: 0.2350270301103592\n",
      "iteration 8358: loss: 0.23502621054649353\n",
      "iteration 8359: loss: 0.2350253313779831\n",
      "iteration 8360: loss: 0.2350243628025055\n",
      "iteration 8361: loss: 0.23502346873283386\n",
      "iteration 8362: loss: 0.23502254486083984\n",
      "iteration 8363: loss: 0.23502163589000702\n",
      "iteration 8364: loss: 0.23502078652381897\n",
      "iteration 8365: loss: 0.23501983284950256\n",
      "iteration 8366: loss: 0.23501893877983093\n",
      "iteration 8367: loss: 0.23501797020435333\n",
      "iteration 8368: loss: 0.2350170910358429\n",
      "iteration 8369: loss: 0.23501631617546082\n",
      "iteration 8370: loss: 0.23501524329185486\n",
      "iteration 8371: loss: 0.2350144386291504\n",
      "iteration 8372: loss: 0.23501339554786682\n",
      "iteration 8373: loss: 0.23501260578632355\n",
      "iteration 8374: loss: 0.23501169681549072\n",
      "iteration 8375: loss: 0.23501062393188477\n",
      "iteration 8376: loss: 0.23500986397266388\n",
      "iteration 8377: loss: 0.23500876128673553\n",
      "iteration 8378: loss: 0.23500800132751465\n",
      "iteration 8379: loss: 0.2350069284439087\n",
      "iteration 8380: loss: 0.23500613868236542\n",
      "iteration 8381: loss: 0.23500525951385498\n",
      "iteration 8382: loss: 0.235004261136055\n",
      "iteration 8383: loss: 0.23500338196754456\n",
      "iteration 8384: loss: 0.23500242829322815\n",
      "iteration 8385: loss: 0.23500153422355652\n",
      "iteration 8386: loss: 0.23500077426433563\n",
      "iteration 8387: loss: 0.23499970138072968\n",
      "iteration 8388: loss: 0.2349989414215088\n",
      "iteration 8389: loss: 0.23499783873558044\n",
      "iteration 8390: loss: 0.2349969446659088\n",
      "iteration 8391: loss: 0.23499619960784912\n",
      "iteration 8392: loss: 0.23499512672424316\n",
      "iteration 8393: loss: 0.2349943369626999\n",
      "iteration 8394: loss: 0.23499329388141632\n",
      "iteration 8395: loss: 0.23499250411987305\n",
      "iteration 8396: loss: 0.23499146103858948\n",
      "iteration 8397: loss: 0.23499064147472382\n",
      "iteration 8398: loss: 0.234989732503891\n",
      "iteration 8399: loss: 0.23498883843421936\n",
      "iteration 8400: loss: 0.23498789966106415\n",
      "iteration 8401: loss: 0.23498697578907013\n",
      "iteration 8402: loss: 0.23498603701591492\n",
      "iteration 8403: loss: 0.2349850833415985\n",
      "iteration 8404: loss: 0.23498420417308807\n",
      "iteration 8405: loss: 0.2349834442138672\n",
      "iteration 8406: loss: 0.23498237133026123\n",
      "iteration 8407: loss: 0.234981507062912\n",
      "iteration 8408: loss: 0.23498058319091797\n",
      "iteration 8409: loss: 0.23497965931892395\n",
      "iteration 8410: loss: 0.23497872054576874\n",
      "iteration 8411: loss: 0.2349778711795807\n",
      "iteration 8412: loss: 0.23497705161571503\n",
      "iteration 8413: loss: 0.23497600853443146\n",
      "iteration 8414: loss: 0.23497524857521057\n",
      "iteration 8415: loss: 0.23497414588928223\n",
      "iteration 8416: loss: 0.23497338593006134\n",
      "iteration 8417: loss: 0.23497232794761658\n",
      "iteration 8418: loss: 0.2349715530872345\n",
      "iteration 8419: loss: 0.23497048020362854\n",
      "iteration 8420: loss: 0.23496970534324646\n",
      "iteration 8421: loss: 0.23496882617473602\n",
      "iteration 8422: loss: 0.23496785759925842\n",
      "iteration 8423: loss: 0.2349669188261032\n",
      "iteration 8424: loss: 0.23496600985527039\n",
      "iteration 8425: loss: 0.23496513068675995\n",
      "iteration 8426: loss: 0.2349640429019928\n",
      "iteration 8427: loss: 0.23496326804161072\n",
      "iteration 8428: loss: 0.23496238887310028\n",
      "iteration 8429: loss: 0.23496143519878387\n",
      "iteration 8430: loss: 0.23496052622795105\n",
      "iteration 8431: loss: 0.23495957255363464\n",
      "iteration 8432: loss: 0.2349587231874466\n",
      "iteration 8433: loss: 0.2349577695131302\n",
      "iteration 8434: loss: 0.23495686054229736\n",
      "iteration 8435: loss: 0.23495611548423767\n",
      "iteration 8436: loss: 0.23495498299598694\n",
      "iteration 8437: loss: 0.23495426774024963\n",
      "iteration 8438: loss: 0.2349531650543213\n",
      "iteration 8439: loss: 0.2349524050951004\n",
      "iteration 8440: loss: 0.23495133221149445\n",
      "iteration 8441: loss: 0.23495057225227356\n",
      "iteration 8442: loss: 0.2349495142698288\n",
      "iteration 8443: loss: 0.2349487543106079\n",
      "iteration 8444: loss: 0.23494765162467957\n",
      "iteration 8445: loss: 0.23494680225849152\n",
      "iteration 8446: loss: 0.2349458485841751\n",
      "iteration 8447: loss: 0.23494496941566467\n",
      "iteration 8448: loss: 0.2349441945552826\n",
      "iteration 8449: loss: 0.2349431961774826\n",
      "iteration 8450: loss: 0.23494240641593933\n",
      "iteration 8451: loss: 0.23494131863117218\n",
      "iteration 8452: loss: 0.23494060337543488\n",
      "iteration 8453: loss: 0.23493953049182892\n",
      "iteration 8454: loss: 0.23493877053260803\n",
      "iteration 8455: loss: 0.2349376678466797\n",
      "iteration 8456: loss: 0.2349369078874588\n",
      "iteration 8457: loss: 0.23493580520153046\n",
      "iteration 8458: loss: 0.23493508994579315\n",
      "iteration 8459: loss: 0.23493418097496033\n",
      "iteration 8460: loss: 0.23493321239948273\n",
      "iteration 8461: loss: 0.23493234813213348\n",
      "iteration 8462: loss: 0.23493139445781708\n",
      "iteration 8463: loss: 0.23493051528930664\n",
      "iteration 8464: loss: 0.23492956161499023\n",
      "iteration 8465: loss: 0.234928697347641\n",
      "iteration 8466: loss: 0.234927698969841\n",
      "iteration 8467: loss: 0.23492684960365295\n",
      "iteration 8468: loss: 0.2349257916212082\n",
      "iteration 8469: loss: 0.23492500185966492\n",
      "iteration 8470: loss: 0.23492395877838135\n",
      "iteration 8471: loss: 0.23492319881916046\n",
      "iteration 8472: loss: 0.23492209613323212\n",
      "iteration 8473: loss: 0.23492136597633362\n",
      "iteration 8474: loss: 0.23492026329040527\n",
      "iteration 8475: loss: 0.23491954803466797\n",
      "iteration 8476: loss: 0.23491863906383514\n",
      "iteration 8477: loss: 0.23491768538951874\n",
      "iteration 8478: loss: 0.2349168062210083\n",
      "iteration 8479: loss: 0.23491588234901428\n",
      "iteration 8480: loss: 0.23491498827934265\n",
      "iteration 8481: loss: 0.23491401970386505\n",
      "iteration 8482: loss: 0.23491314053535461\n",
      "iteration 8483: loss: 0.2349122017621994\n",
      "iteration 8484: loss: 0.23491132259368896\n",
      "iteration 8485: loss: 0.23491036891937256\n",
      "iteration 8486: loss: 0.23490950465202332\n",
      "iteration 8487: loss: 0.23490853607654572\n",
      "iteration 8488: loss: 0.23490767180919647\n",
      "iteration 8489: loss: 0.23490670323371887\n",
      "iteration 8490: loss: 0.23490580916404724\n",
      "iteration 8491: loss: 0.23490488529205322\n",
      "iteration 8492: loss: 0.23490405082702637\n",
      "iteration 8493: loss: 0.23490312695503235\n",
      "iteration 8494: loss: 0.23490223288536072\n",
      "iteration 8495: loss: 0.2349012792110443\n",
      "iteration 8496: loss: 0.23490042984485626\n",
      "iteration 8497: loss: 0.23489944636821747\n",
      "iteration 8498: loss: 0.23489859700202942\n",
      "iteration 8499: loss: 0.23489761352539062\n",
      "iteration 8500: loss: 0.23489677906036377\n",
      "iteration 8501: loss: 0.23489582538604736\n",
      "iteration 8502: loss: 0.23489496111869812\n",
      "iteration 8503: loss: 0.23489396274089813\n",
      "iteration 8504: loss: 0.23489311337471008\n",
      "iteration 8505: loss: 0.23489217460155487\n",
      "iteration 8506: loss: 0.23489126563072205\n",
      "iteration 8507: loss: 0.23489034175872803\n",
      "iteration 8508: loss: 0.23488947749137878\n",
      "iteration 8509: loss: 0.23488855361938477\n",
      "iteration 8510: loss: 0.23488764464855194\n",
      "iteration 8511: loss: 0.23488672077655792\n",
      "iteration 8512: loss: 0.23488584160804749\n",
      "iteration 8513: loss: 0.23488476872444153\n",
      "iteration 8514: loss: 0.23488399386405945\n",
      "iteration 8515: loss: 0.234883114695549\n",
      "iteration 8516: loss: 0.234882190823555\n",
      "iteration 8517: loss: 0.23488128185272217\n",
      "iteration 8518: loss: 0.23488035798072815\n",
      "iteration 8519: loss: 0.23487958312034607\n",
      "iteration 8520: loss: 0.2348785400390625\n",
      "iteration 8521: loss: 0.2348777949810028\n",
      "iteration 8522: loss: 0.23487670719623566\n",
      "iteration 8523: loss: 0.23487596213817596\n",
      "iteration 8524: loss: 0.2348748743534088\n",
      "iteration 8525: loss: 0.23487401008605957\n",
      "iteration 8526: loss: 0.23487302660942078\n",
      "iteration 8527: loss: 0.23487210273742676\n",
      "iteration 8528: loss: 0.23487123847007751\n",
      "iteration 8529: loss: 0.23487034440040588\n",
      "iteration 8530: loss: 0.23486940562725067\n",
      "iteration 8531: loss: 0.23486848175525665\n",
      "iteration 8532: loss: 0.2348676174879074\n",
      "iteration 8533: loss: 0.23486654460430145\n",
      "iteration 8534: loss: 0.23486585915088654\n",
      "iteration 8535: loss: 0.23486492037773132\n",
      "iteration 8536: loss: 0.2348640412092209\n",
      "iteration 8537: loss: 0.23486299812793732\n",
      "iteration 8538: loss: 0.2348622828722\n",
      "iteration 8539: loss: 0.23486116528511047\n",
      "iteration 8540: loss: 0.2348603904247284\n",
      "iteration 8541: loss: 0.2348593771457672\n",
      "iteration 8542: loss: 0.23485860228538513\n",
      "iteration 8543: loss: 0.23485755920410156\n",
      "iteration 8544: loss: 0.23485681414604187\n",
      "iteration 8545: loss: 0.23485572636127472\n",
      "iteration 8546: loss: 0.23485498130321503\n",
      "iteration 8547: loss: 0.23485390841960907\n",
      "iteration 8548: loss: 0.23485317826271057\n",
      "iteration 8549: loss: 0.23485207557678223\n",
      "iteration 8550: loss: 0.23485136032104492\n",
      "iteration 8551: loss: 0.23485025763511658\n",
      "iteration 8552: loss: 0.2348494976758957\n",
      "iteration 8553: loss: 0.23484845459461212\n",
      "iteration 8554: loss: 0.23484769463539124\n",
      "iteration 8555: loss: 0.23484662175178528\n",
      "iteration 8556: loss: 0.23484568297863007\n",
      "iteration 8557: loss: 0.23484496772289276\n",
      "iteration 8558: loss: 0.2348438799381256\n",
      "iteration 8559: loss: 0.23484313488006592\n",
      "iteration 8560: loss: 0.23484206199645996\n",
      "iteration 8561: loss: 0.23484130203723907\n",
      "iteration 8562: loss: 0.23484022915363312\n",
      "iteration 8563: loss: 0.23483948409557343\n",
      "iteration 8564: loss: 0.23483844101428986\n",
      "iteration 8565: loss: 0.23483769595623016\n",
      "iteration 8566: loss: 0.2348366528749466\n",
      "iteration 8567: loss: 0.2348359078168869\n",
      "iteration 8568: loss: 0.23483483493328094\n",
      "iteration 8569: loss: 0.23483386635780334\n",
      "iteration 8570: loss: 0.2348330020904541\n",
      "iteration 8571: loss: 0.23483207821846008\n",
      "iteration 8572: loss: 0.23483124375343323\n",
      "iteration 8573: loss: 0.23483029007911682\n",
      "iteration 8574: loss: 0.2348293960094452\n",
      "iteration 8575: loss: 0.23482847213745117\n",
      "iteration 8576: loss: 0.2348276674747467\n",
      "iteration 8577: loss: 0.2348267138004303\n",
      "iteration 8578: loss: 0.23482584953308105\n",
      "iteration 8579: loss: 0.23482489585876465\n",
      "iteration 8580: loss: 0.2348240315914154\n",
      "iteration 8581: loss: 0.234823077917099\n",
      "iteration 8582: loss: 0.23482222855091095\n",
      "iteration 8583: loss: 0.23482127487659454\n",
      "iteration 8584: loss: 0.23482021689414978\n",
      "iteration 8585: loss: 0.2348194569349289\n",
      "iteration 8586: loss: 0.23481841385364532\n",
      "iteration 8587: loss: 0.23481765389442444\n",
      "iteration 8588: loss: 0.23481659591197968\n",
      "iteration 8589: loss: 0.23481586575508118\n",
      "iteration 8590: loss: 0.23481476306915283\n",
      "iteration 8591: loss: 0.23481404781341553\n",
      "iteration 8592: loss: 0.23481300473213196\n",
      "iteration 8593: loss: 0.23481225967407227\n",
      "iteration 8594: loss: 0.2348111867904663\n",
      "iteration 8595: loss: 0.23481020331382751\n",
      "iteration 8596: loss: 0.23480939865112305\n",
      "iteration 8597: loss: 0.23480844497680664\n",
      "iteration 8598: loss: 0.2348075807094574\n",
      "iteration 8599: loss: 0.234806627035141\n",
      "iteration 8600: loss: 0.23480577766895294\n",
      "iteration 8601: loss: 0.23480482399463654\n",
      "iteration 8602: loss: 0.23480376601219177\n",
      "iteration 8603: loss: 0.23480303585529327\n",
      "iteration 8604: loss: 0.23480208218097687\n",
      "iteration 8605: loss: 0.23480124771595\n",
      "iteration 8606: loss: 0.2348002940416336\n",
      "iteration 8607: loss: 0.23479941487312317\n",
      "iteration 8608: loss: 0.23479847609996796\n",
      "iteration 8609: loss: 0.234797403216362\n",
      "iteration 8610: loss: 0.2347966730594635\n",
      "iteration 8611: loss: 0.23479561507701874\n",
      "iteration 8612: loss: 0.23479488492012024\n",
      "iteration 8613: loss: 0.23479381203651428\n",
      "iteration 8614: loss: 0.23479297757148743\n",
      "iteration 8615: loss: 0.23479190468788147\n",
      "iteration 8616: loss: 0.23479115962982178\n",
      "iteration 8617: loss: 0.23479008674621582\n",
      "iteration 8618: loss: 0.23478934168815613\n",
      "iteration 8619: loss: 0.23478837311267853\n",
      "iteration 8620: loss: 0.23478741943836212\n",
      "iteration 8621: loss: 0.23478655517101288\n",
      "iteration 8622: loss: 0.23478563129901886\n",
      "iteration 8623: loss: 0.2347848117351532\n",
      "iteration 8624: loss: 0.2347838580608368\n",
      "iteration 8625: loss: 0.23478281497955322\n",
      "iteration 8626: loss: 0.23478202521800995\n",
      "iteration 8627: loss: 0.23478099703788757\n",
      "iteration 8628: loss: 0.2347802370786667\n",
      "iteration 8629: loss: 0.2347792088985443\n",
      "iteration 8630: loss: 0.23477844893932343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8631: loss: 0.23477740585803986\n",
      "iteration 8632: loss: 0.23477646708488464\n",
      "iteration 8633: loss: 0.2347756177186966\n",
      "iteration 8634: loss: 0.23477467894554138\n",
      "iteration 8635: loss: 0.2347739189863205\n",
      "iteration 8636: loss: 0.2347729206085205\n",
      "iteration 8637: loss: 0.23477213084697723\n",
      "iteration 8638: loss: 0.23477105796337128\n",
      "iteration 8639: loss: 0.23477013409137726\n",
      "iteration 8640: loss: 0.23476926982402802\n",
      "iteration 8641: loss: 0.234768345952034\n",
      "iteration 8642: loss: 0.23476746678352356\n",
      "iteration 8643: loss: 0.23476652801036835\n",
      "iteration 8644: loss: 0.2347656488418579\n",
      "iteration 8645: loss: 0.2347647249698639\n",
      "iteration 8646: loss: 0.23476366698741913\n",
      "iteration 8647: loss: 0.23476290702819824\n",
      "iteration 8648: loss: 0.23476186394691467\n",
      "iteration 8649: loss: 0.23476114869117737\n",
      "iteration 8650: loss: 0.23476020991802216\n",
      "iteration 8651: loss: 0.2347591370344162\n",
      "iteration 8652: loss: 0.2347584068775177\n",
      "iteration 8653: loss: 0.23475737869739532\n",
      "iteration 8654: loss: 0.23475661873817444\n",
      "iteration 8655: loss: 0.23475559055805206\n",
      "iteration 8656: loss: 0.23475465178489685\n",
      "iteration 8657: loss: 0.2347538024187088\n",
      "iteration 8658: loss: 0.2347528636455536\n",
      "iteration 8659: loss: 0.23475201427936554\n",
      "iteration 8660: loss: 0.23475107550621033\n",
      "iteration 8661: loss: 0.23475010693073273\n",
      "iteration 8662: loss: 0.23474934697151184\n",
      "iteration 8663: loss: 0.23474831879138947\n",
      "iteration 8664: loss: 0.23474760353565216\n",
      "iteration 8665: loss: 0.2347465306520462\n",
      "iteration 8666: loss: 0.2347455471754074\n",
      "iteration 8667: loss: 0.23474469780921936\n",
      "iteration 8668: loss: 0.23474378883838654\n",
      "iteration 8669: loss: 0.2347429245710373\n",
      "iteration 8670: loss: 0.23474200069904327\n",
      "iteration 8671: loss: 0.23474092781543732\n",
      "iteration 8672: loss: 0.23474018275737762\n",
      "iteration 8673: loss: 0.2347392588853836\n",
      "iteration 8674: loss: 0.23473842442035675\n",
      "iteration 8675: loss: 0.23473744094371796\n",
      "iteration 8676: loss: 0.2347363978624344\n",
      "iteration 8677: loss: 0.23473568260669708\n",
      "iteration 8678: loss: 0.23473462462425232\n",
      "iteration 8679: loss: 0.23473386466503143\n",
      "iteration 8680: loss: 0.23473282158374786\n",
      "iteration 8681: loss: 0.23473188281059265\n",
      "iteration 8682: loss: 0.2347310483455658\n",
      "iteration 8683: loss: 0.23473012447357178\n",
      "iteration 8684: loss: 0.23472937941551208\n",
      "iteration 8685: loss: 0.23472833633422852\n",
      "iteration 8686: loss: 0.2347274124622345\n",
      "iteration 8687: loss: 0.23472654819488525\n",
      "iteration 8688: loss: 0.23472562432289124\n",
      "iteration 8689: loss: 0.2347247302532196\n",
      "iteration 8690: loss: 0.23472380638122559\n",
      "iteration 8691: loss: 0.2347227782011032\n",
      "iteration 8692: loss: 0.2347220480442047\n",
      "iteration 8693: loss: 0.23472099006175995\n",
      "iteration 8694: loss: 0.23472006618976593\n",
      "iteration 8695: loss: 0.2347192019224167\n",
      "iteration 8696: loss: 0.23471824824810028\n",
      "iteration 8697: loss: 0.23471741378307343\n",
      "iteration 8698: loss: 0.2347165048122406\n",
      "iteration 8699: loss: 0.2347155511379242\n",
      "iteration 8700: loss: 0.23471477627754211\n",
      "iteration 8701: loss: 0.2347138375043869\n",
      "iteration 8702: loss: 0.23471274971961975\n",
      "iteration 8703: loss: 0.23471209406852722\n",
      "iteration 8704: loss: 0.23471102118492126\n",
      "iteration 8705: loss: 0.23471029102802277\n",
      "iteration 8706: loss: 0.234709233045578\n",
      "iteration 8707: loss: 0.23470830917358398\n",
      "iteration 8708: loss: 0.23470747470855713\n",
      "iteration 8709: loss: 0.2347065508365631\n",
      "iteration 8710: loss: 0.23470547795295715\n",
      "iteration 8711: loss: 0.23470473289489746\n",
      "iteration 8712: loss: 0.23470382392406464\n",
      "iteration 8713: loss: 0.2347029745578766\n",
      "iteration 8714: loss: 0.23470203578472137\n",
      "iteration 8715: loss: 0.234701007604599\n",
      "iteration 8716: loss: 0.2347002476453781\n",
      "iteration 8717: loss: 0.23469920456409454\n",
      "iteration 8718: loss: 0.23469826579093933\n",
      "iteration 8719: loss: 0.23469741642475128\n",
      "iteration 8720: loss: 0.23469650745391846\n",
      "iteration 8721: loss: 0.2346956431865692\n",
      "iteration 8722: loss: 0.2346947193145752\n",
      "iteration 8723: loss: 0.23469364643096924\n",
      "iteration 8724: loss: 0.23469293117523193\n",
      "iteration 8725: loss: 0.23469200730323792\n",
      "iteration 8726: loss: 0.23469097912311554\n",
      "iteration 8727: loss: 0.23469023406505585\n",
      "iteration 8728: loss: 0.23468919098377228\n",
      "iteration 8729: loss: 0.23468828201293945\n",
      "iteration 8730: loss: 0.2346874177455902\n",
      "iteration 8731: loss: 0.2346864640712738\n",
      "iteration 8732: loss: 0.23468557000160217\n",
      "iteration 8733: loss: 0.23468470573425293\n",
      "iteration 8734: loss: 0.2346837818622589\n",
      "iteration 8735: loss: 0.23468291759490967\n",
      "iteration 8736: loss: 0.23468193411827087\n",
      "iteration 8737: loss: 0.2346809208393097\n",
      "iteration 8738: loss: 0.2346802055835724\n",
      "iteration 8739: loss: 0.23467917740345\n",
      "iteration 8740: loss: 0.2346782237291336\n",
      "iteration 8741: loss: 0.2346775084733963\n",
      "iteration 8742: loss: 0.23467645049095154\n",
      "iteration 8743: loss: 0.2346755564212799\n",
      "iteration 8744: loss: 0.23467469215393066\n",
      "iteration 8745: loss: 0.23467373847961426\n",
      "iteration 8746: loss: 0.23467299342155457\n",
      "iteration 8747: loss: 0.23467206954956055\n",
      "iteration 8748: loss: 0.2346709668636322\n",
      "iteration 8749: loss: 0.23467028141021729\n",
      "iteration 8750: loss: 0.23466923832893372\n",
      "iteration 8751: loss: 0.2346682995557785\n",
      "iteration 8752: loss: 0.23466746509075165\n",
      "iteration 8753: loss: 0.23466654121875763\n",
      "iteration 8754: loss: 0.23466551303863525\n",
      "iteration 8755: loss: 0.23466476798057556\n",
      "iteration 8756: loss: 0.23466384410858154\n",
      "iteration 8757: loss: 0.23466280102729797\n",
      "iteration 8758: loss: 0.23466208577156067\n",
      "iteration 8759: loss: 0.2346610277891159\n",
      "iteration 8760: loss: 0.23466011881828308\n",
      "iteration 8761: loss: 0.23465926945209503\n",
      "iteration 8762: loss: 0.23465833067893982\n",
      "iteration 8763: loss: 0.2346574068069458\n",
      "iteration 8764: loss: 0.23465654253959656\n",
      "iteration 8765: loss: 0.23465566337108612\n",
      "iteration 8766: loss: 0.23465459048748016\n",
      "iteration 8767: loss: 0.23465386033058167\n",
      "iteration 8768: loss: 0.2346528321504593\n",
      "iteration 8769: loss: 0.2346518486738205\n",
      "iteration 8770: loss: 0.23465116322040558\n",
      "iteration 8771: loss: 0.23465010523796082\n",
      "iteration 8772: loss: 0.2346491515636444\n",
      "iteration 8773: loss: 0.23464831709861755\n",
      "iteration 8774: loss: 0.23464742302894592\n",
      "iteration 8775: loss: 0.23464635014533997\n",
      "iteration 8776: loss: 0.23464563488960266\n",
      "iteration 8777: loss: 0.23464469611644745\n",
      "iteration 8778: loss: 0.23464366793632507\n",
      "iteration 8779: loss: 0.23464293777942657\n",
      "iteration 8780: loss: 0.2346418797969818\n",
      "iteration 8781: loss: 0.23464098572731018\n",
      "iteration 8782: loss: 0.23464015126228333\n",
      "iteration 8783: loss: 0.2346392124891281\n",
      "iteration 8784: loss: 0.23463818430900574\n",
      "iteration 8785: loss: 0.23463749885559082\n",
      "iteration 8786: loss: 0.23463642597198486\n",
      "iteration 8787: loss: 0.23463550209999084\n",
      "iteration 8788: loss: 0.23463483154773712\n",
      "iteration 8789: loss: 0.23463377356529236\n",
      "iteration 8790: loss: 0.23463284969329834\n",
      "iteration 8791: loss: 0.23463204503059387\n",
      "iteration 8792: loss: 0.23463110625743866\n",
      "iteration 8793: loss: 0.2346300631761551\n",
      "iteration 8794: loss: 0.23462934792041779\n",
      "iteration 8795: loss: 0.23462840914726257\n",
      "iteration 8796: loss: 0.234627366065979\n",
      "iteration 8797: loss: 0.23462645709514618\n",
      "iteration 8798: loss: 0.23462560772895813\n",
      "iteration 8799: loss: 0.23462466895580292\n",
      "iteration 8800: loss: 0.23462383449077606\n",
      "iteration 8801: loss: 0.23462291061878204\n",
      "iteration 8802: loss: 0.23462188243865967\n",
      "iteration 8803: loss: 0.23462097346782684\n",
      "iteration 8804: loss: 0.23462024331092834\n",
      "iteration 8805: loss: 0.23461918532848358\n",
      "iteration 8806: loss: 0.23461826145648956\n",
      "iteration 8807: loss: 0.2346174269914627\n",
      "iteration 8808: loss: 0.23461651802062988\n",
      "iteration 8809: loss: 0.2346154749393463\n",
      "iteration 8810: loss: 0.23461472988128662\n",
      "iteration 8811: loss: 0.2346138209104538\n",
      "iteration 8812: loss: 0.23461277782917023\n",
      "iteration 8813: loss: 0.2346118688583374\n",
      "iteration 8814: loss: 0.23461103439331055\n",
      "iteration 8815: loss: 0.23461011052131653\n",
      "iteration 8816: loss: 0.23460917174816132\n",
      "iteration 8817: loss: 0.23460833728313446\n",
      "iteration 8818: loss: 0.23460742831230164\n",
      "iteration 8819: loss: 0.23460635542869568\n",
      "iteration 8820: loss: 0.23460564017295837\n",
      "iteration 8821: loss: 0.23460468649864197\n",
      "iteration 8822: loss: 0.23460368812084198\n",
      "iteration 8823: loss: 0.23460295796394348\n",
      "iteration 8824: loss: 0.23460189998149872\n",
      "iteration 8825: loss: 0.2346009910106659\n",
      "iteration 8826: loss: 0.2345999777317047\n",
      "iteration 8827: loss: 0.2345992624759674\n",
      "iteration 8828: loss: 0.23459820449352264\n",
      "iteration 8829: loss: 0.23459728062152863\n",
      "iteration 8830: loss: 0.2345966398715973\n",
      "iteration 8831: loss: 0.23459558188915253\n",
      "iteration 8832: loss: 0.23459473252296448\n",
      "iteration 8833: loss: 0.23459367454051971\n",
      "iteration 8834: loss: 0.23459294438362122\n",
      "iteration 8835: loss: 0.23459191620349884\n",
      "iteration 8836: loss: 0.23459096252918243\n",
      "iteration 8837: loss: 0.23459024727344513\n",
      "iteration 8838: loss: 0.23458924889564514\n",
      "iteration 8839: loss: 0.23458833992481232\n",
      "iteration 8840: loss: 0.23458728194236755\n",
      "iteration 8841: loss: 0.23458652198314667\n",
      "iteration 8842: loss: 0.23458552360534668\n",
      "iteration 8843: loss: 0.23458456993103027\n",
      "iteration 8844: loss: 0.23458388447761536\n",
      "iteration 8845: loss: 0.23458285629749298\n",
      "iteration 8846: loss: 0.23458190262317657\n",
      "iteration 8847: loss: 0.23458106815814972\n",
      "iteration 8848: loss: 0.2345801591873169\n",
      "iteration 8849: loss: 0.23457925021648407\n",
      "iteration 8850: loss: 0.2345782220363617\n",
      "iteration 8851: loss: 0.2345774918794632\n",
      "iteration 8852: loss: 0.23457643389701843\n",
      "iteration 8853: loss: 0.23457551002502441\n",
      "iteration 8854: loss: 0.23457467555999756\n",
      "iteration 8855: loss: 0.23457375168800354\n",
      "iteration 8856: loss: 0.2345728576183319\n",
      "iteration 8857: loss: 0.23457178473472595\n",
      "iteration 8858: loss: 0.23457112908363342\n",
      "iteration 8859: loss: 0.23457005620002747\n",
      "iteration 8860: loss: 0.23456914722919464\n",
      "iteration 8861: loss: 0.23456831276416779\n",
      "iteration 8862: loss: 0.23456737399101257\n",
      "iteration 8863: loss: 0.23456647992134094\n",
      "iteration 8864: loss: 0.23456545174121857\n",
      "iteration 8865: loss: 0.23456470668315887\n",
      "iteration 8866: loss: 0.23456379771232605\n",
      "iteration 8867: loss: 0.23456275463104248\n",
      "iteration 8868: loss: 0.23456183075904846\n",
      "iteration 8869: loss: 0.2345610111951828\n",
      "iteration 8870: loss: 0.2345600575208664\n",
      "iteration 8871: loss: 0.23455920815467834\n",
      "iteration 8872: loss: 0.23455822467803955\n",
      "iteration 8873: loss: 0.23455747961997986\n",
      "iteration 8874: loss: 0.23455646634101868\n",
      "iteration 8875: loss: 0.23455552756786346\n",
      "iteration 8876: loss: 0.2345546931028366\n",
      "iteration 8877: loss: 0.23455378413200378\n",
      "iteration 8878: loss: 0.23455289006233215\n",
      "iteration 8879: loss: 0.23455186188220978\n",
      "iteration 8880: loss: 0.23455111682415009\n",
      "iteration 8881: loss: 0.23455007374286652\n",
      "iteration 8882: loss: 0.23454919457435608\n",
      "iteration 8883: loss: 0.23454837501049042\n",
      "iteration 8884: loss: 0.2345474511384964\n",
      "iteration 8885: loss: 0.2345465123653412\n",
      "iteration 8886: loss: 0.23454549908638\n",
      "iteration 8887: loss: 0.2345447838306427\n",
      "iteration 8888: loss: 0.23454372584819794\n",
      "iteration 8889: loss: 0.2345428466796875\n",
      "iteration 8890: loss: 0.23454180359840393\n",
      "iteration 8891: loss: 0.23454108834266663\n",
      "iteration 8892: loss: 0.234540194272995\n",
      "iteration 8893: loss: 0.23453912138938904\n",
      "iteration 8894: loss: 0.2345382273197174\n",
      "iteration 8895: loss: 0.23453739285469055\n",
      "iteration 8896: loss: 0.23453648388385773\n",
      "iteration 8897: loss: 0.2345355749130249\n",
      "iteration 8898: loss: 0.23453457653522491\n",
      "iteration 8899: loss: 0.2345336228609085\n",
      "iteration 8900: loss: 0.2345329076051712\n",
      "iteration 8901: loss: 0.23453187942504883\n",
      "iteration 8902: loss: 0.2345309555530548\n",
      "iteration 8903: loss: 0.23453012108802795\n",
      "iteration 8904: loss: 0.23452918231487274\n",
      "iteration 8905: loss: 0.23452825844287872\n",
      "iteration 8906: loss: 0.23452723026275635\n",
      "iteration 8907: loss: 0.23452651500701904\n",
      "iteration 8908: loss: 0.23452548682689667\n",
      "iteration 8909: loss: 0.23452457785606384\n",
      "iteration 8910: loss: 0.2345236986875534\n",
      "iteration 8911: loss: 0.23452281951904297\n",
      "iteration 8912: loss: 0.23452191054821014\n",
      "iteration 8913: loss: 0.23452088236808777\n",
      "iteration 8914: loss: 0.23452000319957733\n",
      "iteration 8915: loss: 0.23451919853687286\n",
      "iteration 8916: loss: 0.23451828956604004\n",
      "iteration 8917: loss: 0.23451738059520721\n",
      "iteration 8918: loss: 0.23451638221740723\n",
      "iteration 8919: loss: 0.23451566696166992\n",
      "iteration 8920: loss: 0.23451463878154755\n",
      "iteration 8921: loss: 0.23451371490955353\n",
      "iteration 8922: loss: 0.23451285064220428\n",
      "iteration 8923: loss: 0.23451177775859833\n",
      "iteration 8924: loss: 0.2345110923051834\n",
      "iteration 8925: loss: 0.23451009392738342\n",
      "iteration 8926: loss: 0.2345091551542282\n",
      "iteration 8927: loss: 0.2345082312822342\n",
      "iteration 8928: loss: 0.23450741171836853\n",
      "iteration 8929: loss: 0.2345065176486969\n",
      "iteration 8930: loss: 0.23450545966625214\n",
      "iteration 8931: loss: 0.2345045506954193\n",
      "iteration 8932: loss: 0.2345038205385208\n",
      "iteration 8933: loss: 0.23450279235839844\n",
      "iteration 8934: loss: 0.2345018833875656\n",
      "iteration 8935: loss: 0.23450085520744324\n",
      "iteration 8936: loss: 0.23450012505054474\n",
      "iteration 8937: loss: 0.23449921607971191\n",
      "iteration 8938: loss: 0.23449817299842834\n",
      "iteration 8939: loss: 0.2344972789287567\n",
      "iteration 8940: loss: 0.23449644446372986\n",
      "iteration 8941: loss: 0.23449555039405823\n",
      "iteration 8942: loss: 0.2344946414232254\n",
      "iteration 8943: loss: 0.23449361324310303\n",
      "iteration 8944: loss: 0.234492689371109\n",
      "iteration 8945: loss: 0.23449185490608215\n",
      "iteration 8946: loss: 0.23449094593524933\n",
      "iteration 8947: loss: 0.2344900667667389\n",
      "iteration 8948: loss: 0.2344890534877777\n",
      "iteration 8949: loss: 0.2344880998134613\n",
      "iteration 8950: loss: 0.2344874143600464\n",
      "iteration 8951: loss: 0.234486386179924\n",
      "iteration 8952: loss: 0.2344854772090912\n",
      "iteration 8953: loss: 0.2344844788312912\n",
      "iteration 8954: loss: 0.23448355495929718\n",
      "iteration 8955: loss: 0.23448285460472107\n",
      "iteration 8956: loss: 0.2344818115234375\n",
      "iteration 8957: loss: 0.23448097705841064\n",
      "iteration 8958: loss: 0.23448002338409424\n",
      "iteration 8959: loss: 0.23447918891906738\n",
      "iteration 8960: loss: 0.23447832465171814\n",
      "iteration 8961: loss: 0.23447728157043457\n",
      "iteration 8962: loss: 0.23447637259960175\n",
      "iteration 8963: loss: 0.23447568714618683\n",
      "iteration 8964: loss: 0.23447461426258087\n",
      "iteration 8965: loss: 0.23447370529174805\n",
      "iteration 8966: loss: 0.23447267711162567\n",
      "iteration 8967: loss: 0.23447179794311523\n",
      "iteration 8968: loss: 0.23447105288505554\n",
      "iteration 8969: loss: 0.23447003960609436\n",
      "iteration 8970: loss: 0.23446917533874512\n",
      "iteration 8971: loss: 0.23446813225746155\n",
      "iteration 8972: loss: 0.23446722328662872\n",
      "iteration 8973: loss: 0.2344665229320526\n",
      "iteration 8974: loss: 0.23446550965309143\n",
      "iteration 8975: loss: 0.2344646006822586\n",
      "iteration 8976: loss: 0.23446357250213623\n",
      "iteration 8977: loss: 0.2344626933336258\n",
      "iteration 8978: loss: 0.2344619333744049\n",
      "iteration 8979: loss: 0.2344609498977661\n",
      "iteration 8980: loss: 0.23446007072925568\n",
      "iteration 8981: loss: 0.23445916175842285\n",
      "iteration 8982: loss: 0.23445811867713928\n",
      "iteration 8983: loss: 0.23445740342140198\n",
      "iteration 8984: loss: 0.2344563752412796\n",
      "iteration 8985: loss: 0.2344554364681244\n",
      "iteration 8986: loss: 0.23445454239845276\n",
      "iteration 8987: loss: 0.23445352911949158\n",
      "iteration 8988: loss: 0.23445279896259308\n",
      "iteration 8989: loss: 0.2344517707824707\n",
      "iteration 8990: loss: 0.23445086181163788\n",
      "iteration 8991: loss: 0.23444995284080505\n",
      "iteration 8992: loss: 0.23444895446300507\n",
      "iteration 8993: loss: 0.23444822430610657\n",
      "iteration 8994: loss: 0.23444733023643494\n",
      "iteration 8995: loss: 0.23444628715515137\n",
      "iteration 8996: loss: 0.23444536328315735\n",
      "iteration 8997: loss: 0.23444435000419617\n",
      "iteration 8998: loss: 0.23444366455078125\n",
      "iteration 8999: loss: 0.2344428300857544\n",
      "iteration 9000: loss: 0.23444178700447083\n",
      "iteration 9001: loss: 0.234440878033638\n",
      "iteration 9002: loss: 0.23443999886512756\n",
      "iteration 9003: loss: 0.2344389706850052\n",
      "iteration 9004: loss: 0.23443827033042908\n",
      "iteration 9005: loss: 0.2344372570514679\n",
      "iteration 9006: loss: 0.23443634808063507\n",
      "iteration 9007: loss: 0.23443546891212463\n",
      "iteration 9008: loss: 0.23443444073200226\n",
      "iteration 9009: loss: 0.23443372547626495\n",
      "iteration 9010: loss: 0.23443278670310974\n",
      "iteration 9011: loss: 0.23443178832530975\n",
      "iteration 9012: loss: 0.23443086445331573\n",
      "iteration 9013: loss: 0.23442986607551575\n",
      "iteration 9014: loss: 0.23442915081977844\n",
      "iteration 9015: loss: 0.23442824184894562\n",
      "iteration 9016: loss: 0.23442721366882324\n",
      "iteration 9017: loss: 0.23442628979682922\n",
      "iteration 9018: loss: 0.2344253957271576\n",
      "iteration 9019: loss: 0.2344243824481964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9020: loss: 0.2344236820936203\n",
      "iteration 9021: loss: 0.2344226837158203\n",
      "iteration 9022: loss: 0.2344217747449875\n",
      "iteration 9023: loss: 0.23442086577415466\n",
      "iteration 9024: loss: 0.2344198226928711\n",
      "iteration 9025: loss: 0.23441915214061737\n",
      "iteration 9026: loss: 0.234418123960495\n",
      "iteration 9027: loss: 0.23441722989082336\n",
      "iteration 9028: loss: 0.23441633582115173\n",
      "iteration 9029: loss: 0.23441529273986816\n",
      "iteration 9030: loss: 0.23441441357135773\n",
      "iteration 9031: loss: 0.23441369831562042\n",
      "iteration 9032: loss: 0.23441267013549805\n",
      "iteration 9033: loss: 0.2344117909669876\n",
      "iteration 9034: loss: 0.23441091179847717\n",
      "iteration 9035: loss: 0.2344098538160324\n",
      "iteration 9036: loss: 0.23440897464752197\n",
      "iteration 9037: loss: 0.23440806567668915\n",
      "iteration 9038: loss: 0.2344072163105011\n",
      "iteration 9039: loss: 0.23440635204315186\n",
      "iteration 9040: loss: 0.2344052791595459\n",
      "iteration 9041: loss: 0.23440447449684143\n",
      "iteration 9042: loss: 0.23440352082252502\n",
      "iteration 9043: loss: 0.23440270125865936\n",
      "iteration 9044: loss: 0.23440179228782654\n",
      "iteration 9045: loss: 0.2344008982181549\n",
      "iteration 9046: loss: 0.23439988493919373\n",
      "iteration 9047: loss: 0.2343990057706833\n",
      "iteration 9048: loss: 0.23439793288707733\n",
      "iteration 9049: loss: 0.2343972623348236\n",
      "iteration 9050: loss: 0.23439636826515198\n",
      "iteration 9051: loss: 0.2343953549861908\n",
      "iteration 9052: loss: 0.23439447581768036\n",
      "iteration 9053: loss: 0.23439355194568634\n",
      "iteration 9054: loss: 0.23439255356788635\n",
      "iteration 9055: loss: 0.23439161479473114\n",
      "iteration 9056: loss: 0.23439092934131622\n",
      "iteration 9057: loss: 0.23438993096351624\n",
      "iteration 9058: loss: 0.2343890368938446\n",
      "iteration 9059: loss: 0.23438802361488342\n",
      "iteration 9060: loss: 0.2343870848417282\n",
      "iteration 9061: loss: 0.23438625037670135\n",
      "iteration 9062: loss: 0.23438520729541779\n",
      "iteration 9063: loss: 0.23438449203968048\n",
      "iteration 9064: loss: 0.23438361287117004\n",
      "iteration 9065: loss: 0.23438259959220886\n",
      "iteration 9066: loss: 0.23438167572021484\n",
      "iteration 9067: loss: 0.2343807965517044\n",
      "iteration 9068: loss: 0.23437976837158203\n",
      "iteration 9069: loss: 0.2343788594007492\n",
      "iteration 9070: loss: 0.23437809944152832\n",
      "iteration 9071: loss: 0.23437714576721191\n",
      "iteration 9072: loss: 0.2343762367963791\n",
      "iteration 9073: loss: 0.2343752384185791\n",
      "iteration 9074: loss: 0.23437431454658508\n",
      "iteration 9075: loss: 0.23437342047691345\n",
      "iteration 9076: loss: 0.23437261581420898\n",
      "iteration 9077: loss: 0.23437170684337616\n",
      "iteration 9078: loss: 0.23437079787254333\n",
      "iteration 9079: loss: 0.23436979949474335\n",
      "iteration 9080: loss: 0.23436889052391052\n",
      "iteration 9081: loss: 0.23436789214611053\n",
      "iteration 9082: loss: 0.23436696827411652\n",
      "iteration 9083: loss: 0.23436634242534637\n",
      "iteration 9084: loss: 0.234365314245224\n",
      "iteration 9085: loss: 0.23436442017555237\n",
      "iteration 9086: loss: 0.23436352610588074\n",
      "iteration 9087: loss: 0.23436251282691956\n",
      "iteration 9088: loss: 0.23436161875724792\n",
      "iteration 9089: loss: 0.2343607246875763\n",
      "iteration 9090: loss: 0.23435969650745392\n",
      "iteration 9091: loss: 0.2343590259552002\n",
      "iteration 9092: loss: 0.23435810208320618\n",
      "iteration 9093: loss: 0.23435711860656738\n",
      "iteration 9094: loss: 0.23435620963573456\n",
      "iteration 9095: loss: 0.23435530066490173\n",
      "iteration 9096: loss: 0.23435428738594055\n",
      "iteration 9097: loss: 0.23435337841510773\n",
      "iteration 9098: loss: 0.2343524694442749\n",
      "iteration 9099: loss: 0.23435166478157043\n",
      "iteration 9100: loss: 0.2343507707118988\n",
      "iteration 9101: loss: 0.23434977233409882\n",
      "iteration 9102: loss: 0.234348863363266\n",
      "iteration 9103: loss: 0.23434793949127197\n",
      "iteration 9104: loss: 0.23434695601463318\n",
      "iteration 9105: loss: 0.23434606194496155\n",
      "iteration 9106: loss: 0.23434534668922424\n",
      "iteration 9107: loss: 0.23434433341026306\n",
      "iteration 9108: loss: 0.23434343934059143\n",
      "iteration 9109: loss: 0.2343425303697586\n",
      "iteration 9110: loss: 0.23434153199195862\n",
      "iteration 9111: loss: 0.234340637922287\n",
      "iteration 9112: loss: 0.23433971405029297\n",
      "iteration 9113: loss: 0.2343386858701706\n",
      "iteration 9114: loss: 0.23433800041675568\n",
      "iteration 9115: loss: 0.23433713614940643\n",
      "iteration 9116: loss: 0.23433610796928406\n",
      "iteration 9117: loss: 0.23433522880077362\n",
      "iteration 9118: loss: 0.23433434963226318\n",
      "iteration 9119: loss: 0.2343333214521408\n",
      "iteration 9120: loss: 0.23433244228363037\n",
      "iteration 9121: loss: 0.234331414103508\n",
      "iteration 9122: loss: 0.23433050513267517\n",
      "iteration 9123: loss: 0.23432962596416473\n",
      "iteration 9124: loss: 0.23432883620262146\n",
      "iteration 9125: loss: 0.23432794213294983\n",
      "iteration 9126: loss: 0.23432707786560059\n",
      "iteration 9127: loss: 0.2343260496854782\n",
      "iteration 9128: loss: 0.23432517051696777\n",
      "iteration 9129: loss: 0.23432426154613495\n",
      "iteration 9130: loss: 0.23432323336601257\n",
      "iteration 9131: loss: 0.23432235419750214\n",
      "iteration 9132: loss: 0.23432166874408722\n",
      "iteration 9133: loss: 0.23432064056396484\n",
      "iteration 9134: loss: 0.23431973159313202\n",
      "iteration 9135: loss: 0.2343188226222992\n",
      "iteration 9136: loss: 0.2343178242444992\n",
      "iteration 9137: loss: 0.23431697487831116\n",
      "iteration 9138: loss: 0.23431606590747833\n",
      "iteration 9139: loss: 0.23431503772735596\n",
      "iteration 9140: loss: 0.23431411385536194\n",
      "iteration 9141: loss: 0.23431344330310822\n",
      "iteration 9142: loss: 0.23431244492530823\n",
      "iteration 9143: loss: 0.2343115359544754\n",
      "iteration 9144: loss: 0.23431067168712616\n",
      "iteration 9145: loss: 0.23430964350700378\n",
      "iteration 9146: loss: 0.23430876433849335\n",
      "iteration 9147: loss: 0.2343078851699829\n",
      "iteration 9148: loss: 0.23430688679218292\n",
      "iteration 9149: loss: 0.23430600762367249\n",
      "iteration 9150: loss: 0.23430505394935608\n",
      "iteration 9151: loss: 0.23430407047271729\n",
      "iteration 9152: loss: 0.23430316150188446\n",
      "iteration 9153: loss: 0.23430247604846954\n",
      "iteration 9154: loss: 0.23430147767066956\n",
      "iteration 9155: loss: 0.2343006134033203\n",
      "iteration 9156: loss: 0.2342996597290039\n",
      "iteration 9157: loss: 0.23429866135120392\n",
      "iteration 9158: loss: 0.2342977523803711\n",
      "iteration 9159: loss: 0.23429687321186066\n",
      "iteration 9160: loss: 0.23429587483406067\n",
      "iteration 9161: loss: 0.23429493606090546\n",
      "iteration 9162: loss: 0.23429426550865173\n",
      "iteration 9163: loss: 0.23429325222969055\n",
      "iteration 9164: loss: 0.23429234325885773\n",
      "iteration 9165: loss: 0.2342914640903473\n",
      "iteration 9166: loss: 0.2342904508113861\n",
      "iteration 9167: loss: 0.23428960144519806\n",
      "iteration 9168: loss: 0.23428873717784882\n",
      "iteration 9169: loss: 0.23428770899772644\n",
      "iteration 9170: loss: 0.23428687453269958\n",
      "iteration 9171: loss: 0.23428595066070557\n",
      "iteration 9172: loss: 0.23428496718406677\n",
      "iteration 9173: loss: 0.23428404331207275\n",
      "iteration 9174: loss: 0.23428316414356232\n",
      "iteration 9175: loss: 0.23428216576576233\n",
      "iteration 9176: loss: 0.23428146541118622\n",
      "iteration 9177: loss: 0.2342805564403534\n",
      "iteration 9178: loss: 0.2342795580625534\n",
      "iteration 9179: loss: 0.23427867889404297\n",
      "iteration 9180: loss: 0.23427775502204895\n",
      "iteration 9181: loss: 0.23427677154541016\n",
      "iteration 9182: loss: 0.23427586257457733\n",
      "iteration 9183: loss: 0.2342749834060669\n",
      "iteration 9184: loss: 0.23427395522594452\n",
      "iteration 9185: loss: 0.23427310585975647\n",
      "iteration 9186: loss: 0.23427219688892365\n",
      "iteration 9187: loss: 0.23427137732505798\n",
      "iteration 9188: loss: 0.23427045345306396\n",
      "iteration 9189: loss: 0.23426958918571472\n",
      "iteration 9190: loss: 0.23426857590675354\n",
      "iteration 9191: loss: 0.2342677116394043\n",
      "iteration 9192: loss: 0.23426678776741028\n",
      "iteration 9193: loss: 0.23426580429077148\n",
      "iteration 9194: loss: 0.23426489531993866\n",
      "iteration 9195: loss: 0.23426401615142822\n",
      "iteration 9196: loss: 0.23426301777362823\n",
      "iteration 9197: loss: 0.2342621386051178\n",
      "iteration 9198: loss: 0.23426124453544617\n",
      "iteration 9199: loss: 0.2342602014541626\n",
      "iteration 9200: loss: 0.23425929248332977\n",
      "iteration 9201: loss: 0.23425844311714172\n",
      "iteration 9202: loss: 0.23425745964050293\n",
      "iteration 9203: loss: 0.234256774187088\n",
      "iteration 9204: loss: 0.2342558652162552\n",
      "iteration 9205: loss: 0.23425500094890594\n",
      "iteration 9206: loss: 0.23425397276878357\n",
      "iteration 9207: loss: 0.23425304889678955\n",
      "iteration 9208: loss: 0.2342521697282791\n",
      "iteration 9209: loss: 0.23425117135047913\n",
      "iteration 9210: loss: 0.23425033688545227\n",
      "iteration 9211: loss: 0.23424942791461945\n",
      "iteration 9212: loss: 0.23424844443798065\n",
      "iteration 9213: loss: 0.23424753546714783\n",
      "iteration 9214: loss: 0.2342466413974762\n",
      "iteration 9215: loss: 0.2342456579208374\n",
      "iteration 9216: loss: 0.23424477875232697\n",
      "iteration 9217: loss: 0.23424386978149414\n",
      "iteration 9218: loss: 0.23424308001995087\n",
      "iteration 9219: loss: 0.23424215614795685\n",
      "iteration 9220: loss: 0.2342412918806076\n",
      "iteration 9221: loss: 0.23424032330513\n",
      "iteration 9222: loss: 0.2342393845319748\n",
      "iteration 9223: loss: 0.23423853516578674\n",
      "iteration 9224: loss: 0.23423752188682556\n",
      "iteration 9225: loss: 0.23423659801483154\n",
      "iteration 9226: loss: 0.2342357337474823\n",
      "iteration 9227: loss: 0.23423485457897186\n",
      "iteration 9228: loss: 0.2342338263988495\n",
      "iteration 9229: loss: 0.23423294723033905\n",
      "iteration 9230: loss: 0.2342320680618286\n",
      "iteration 9231: loss: 0.23423106968402863\n",
      "iteration 9232: loss: 0.2342301607131958\n",
      "iteration 9233: loss: 0.23422929644584656\n",
      "iteration 9234: loss: 0.23422828316688538\n",
      "iteration 9235: loss: 0.23422738909721375\n",
      "iteration 9236: loss: 0.23422649502754211\n",
      "iteration 9237: loss: 0.23422551155090332\n",
      "iteration 9238: loss: 0.23422479629516602\n",
      "iteration 9239: loss: 0.2342238873243332\n",
      "iteration 9240: loss: 0.23422303795814514\n",
      "iteration 9241: loss: 0.23422202467918396\n",
      "iteration 9242: loss: 0.23422114551067352\n",
      "iteration 9243: loss: 0.2342202216386795\n",
      "iteration 9244: loss: 0.2342192679643631\n",
      "iteration 9245: loss: 0.23421835899353027\n",
      "iteration 9246: loss: 0.23421747982501984\n",
      "iteration 9247: loss: 0.23421648144721985\n",
      "iteration 9248: loss: 0.23421558737754822\n",
      "iteration 9249: loss: 0.2342146933078766\n",
      "iteration 9250: loss: 0.2342137098312378\n",
      "iteration 9251: loss: 0.23421280086040497\n",
      "iteration 9252: loss: 0.23421192169189453\n",
      "iteration 9253: loss: 0.2342109978199005\n",
      "iteration 9254: loss: 0.23421010375022888\n",
      "iteration 9255: loss: 0.23420922458171844\n",
      "iteration 9256: loss: 0.23420831561088562\n",
      "iteration 9257: loss: 0.23420731723308563\n",
      "iteration 9258: loss: 0.2342064082622528\n",
      "iteration 9259: loss: 0.23420552909374237\n",
      "iteration 9260: loss: 0.23420456051826477\n",
      "iteration 9261: loss: 0.23420365154743195\n",
      "iteration 9262: loss: 0.2342027723789215\n",
      "iteration 9263: loss: 0.23420175909996033\n",
      "iteration 9264: loss: 0.2342008799314499\n",
      "iteration 9265: loss: 0.23419997096061707\n",
      "iteration 9266: loss: 0.23419909179210663\n",
      "iteration 9267: loss: 0.23419830203056335\n",
      "iteration 9268: loss: 0.23419740796089172\n",
      "iteration 9269: loss: 0.2341964989900589\n",
      "iteration 9270: loss: 0.2341955155134201\n",
      "iteration 9271: loss: 0.23419463634490967\n",
      "iteration 9272: loss: 0.23419375717639923\n",
      "iteration 9273: loss: 0.23419272899627686\n",
      "iteration 9274: loss: 0.2341918647289276\n",
      "iteration 9275: loss: 0.23419098556041718\n",
      "iteration 9276: loss: 0.23419010639190674\n",
      "iteration 9277: loss: 0.23418912291526794\n",
      "iteration 9278: loss: 0.2341882437467575\n",
      "iteration 9279: loss: 0.2341873198747635\n",
      "iteration 9280: loss: 0.23418636620044708\n",
      "iteration 9281: loss: 0.23418545722961426\n",
      "iteration 9282: loss: 0.23418457806110382\n",
      "iteration 9283: loss: 0.23418359458446503\n",
      "iteration 9284: loss: 0.2341826856136322\n",
      "iteration 9285: loss: 0.23418179154396057\n",
      "iteration 9286: loss: 0.23418089747428894\n",
      "iteration 9287: loss: 0.23417992889881134\n",
      "iteration 9288: loss: 0.23417901992797852\n",
      "iteration 9289: loss: 0.23417814075946808\n",
      "iteration 9290: loss: 0.2341771125793457\n",
      "iteration 9291: loss: 0.23417624831199646\n",
      "iteration 9292: loss: 0.23417536914348602\n",
      "iteration 9293: loss: 0.23417437076568604\n",
      "iteration 9294: loss: 0.2341735064983368\n",
      "iteration 9295: loss: 0.23417265713214874\n",
      "iteration 9296: loss: 0.23417174816131592\n",
      "iteration 9297: loss: 0.2341707944869995\n",
      "iteration 9298: loss: 0.2341698855161667\n",
      "iteration 9299: loss: 0.23416900634765625\n",
      "iteration 9300: loss: 0.23416805267333984\n",
      "iteration 9301: loss: 0.23416714370250702\n",
      "iteration 9302: loss: 0.23416626453399658\n",
      "iteration 9303: loss: 0.2341652363538742\n",
      "iteration 9304: loss: 0.23416435718536377\n",
      "iteration 9305: loss: 0.23416347801685333\n",
      "iteration 9306: loss: 0.2341625988483429\n",
      "iteration 9307: loss: 0.2341616451740265\n",
      "iteration 9308: loss: 0.23416073620319366\n",
      "iteration 9309: loss: 0.23415982723236084\n",
      "iteration 9310: loss: 0.23415882885456085\n",
      "iteration 9311: loss: 0.23415794968605042\n",
      "iteration 9312: loss: 0.23415708541870117\n",
      "iteration 9313: loss: 0.23415620625019073\n",
      "iteration 9314: loss: 0.23415517807006836\n",
      "iteration 9315: loss: 0.23415429890155792\n",
      "iteration 9316: loss: 0.2341533899307251\n",
      "iteration 9317: loss: 0.2341523915529251\n",
      "iteration 9318: loss: 0.23415151238441467\n",
      "iteration 9319: loss: 0.23415064811706543\n",
      "iteration 9320: loss: 0.2341497391462326\n",
      "iteration 9321: loss: 0.234148770570755\n",
      "iteration 9322: loss: 0.23414787650108337\n",
      "iteration 9323: loss: 0.23414699733257294\n",
      "iteration 9324: loss: 0.23414602875709534\n",
      "iteration 9325: loss: 0.2341451346874237\n",
      "iteration 9326: loss: 0.23414428532123566\n",
      "iteration 9327: loss: 0.23414337635040283\n",
      "iteration 9328: loss: 0.23414234817028046\n",
      "iteration 9329: loss: 0.2341414988040924\n",
      "iteration 9330: loss: 0.23414063453674316\n",
      "iteration 9331: loss: 0.23413963615894318\n",
      "iteration 9332: loss: 0.23413875699043274\n",
      "iteration 9333: loss: 0.2341378629207611\n",
      "iteration 9334: loss: 0.23413698375225067\n",
      "iteration 9335: loss: 0.23413598537445068\n",
      "iteration 9336: loss: 0.23413510620594025\n",
      "iteration 9337: loss: 0.2341342717409134\n",
      "iteration 9338: loss: 0.2341332733631134\n",
      "iteration 9339: loss: 0.23413237929344177\n",
      "iteration 9340: loss: 0.23413148522377014\n",
      "iteration 9341: loss: 0.2341306209564209\n",
      "iteration 9342: loss: 0.2341296374797821\n",
      "iteration 9343: loss: 0.23412871360778809\n",
      "iteration 9344: loss: 0.23412787914276123\n",
      "iteration 9345: loss: 0.23412683606147766\n",
      "iteration 9346: loss: 0.2341260015964508\n",
      "iteration 9347: loss: 0.23412509262561798\n",
      "iteration 9348: loss: 0.23412425816059113\n",
      "iteration 9349: loss: 0.23412322998046875\n",
      "iteration 9350: loss: 0.2341223657131195\n",
      "iteration 9351: loss: 0.23412151634693146\n",
      "iteration 9352: loss: 0.23412027955055237\n",
      "iteration 9353: loss: 0.23411941528320312\n",
      "iteration 9354: loss: 0.2341185063123703\n",
      "iteration 9355: loss: 0.23411765694618225\n",
      "iteration 9356: loss: 0.23411667346954346\n",
      "iteration 9357: loss: 0.23411579430103302\n",
      "iteration 9358: loss: 0.2341148853302002\n",
      "iteration 9359: loss: 0.2341138869524002\n",
      "iteration 9360: loss: 0.23411302268505096\n",
      "iteration 9361: loss: 0.23411214351654053\n",
      "iteration 9362: loss: 0.23411127924919128\n",
      "iteration 9363: loss: 0.2341102808713913\n",
      "iteration 9364: loss: 0.23410940170288086\n",
      "iteration 9365: loss: 0.23410852253437042\n",
      "iteration 9366: loss: 0.2341076135635376\n",
      "iteration 9367: loss: 0.2341066151857376\n",
      "iteration 9368: loss: 0.23410573601722717\n",
      "iteration 9369: loss: 0.23410484194755554\n",
      "iteration 9370: loss: 0.2341039925813675\n",
      "iteration 9371: loss: 0.23410293459892273\n",
      "iteration 9372: loss: 0.2341020554304123\n",
      "iteration 9373: loss: 0.23410120606422424\n",
      "iteration 9374: loss: 0.23410019278526306\n",
      "iteration 9375: loss: 0.23409931361675262\n",
      "iteration 9376: loss: 0.23409846425056458\n",
      "iteration 9377: loss: 0.23409757018089294\n",
      "iteration 9378: loss: 0.23409657180309296\n",
      "iteration 9379: loss: 0.2340957224369049\n",
      "iteration 9380: loss: 0.23409490287303925\n",
      "iteration 9381: loss: 0.23409371078014374\n",
      "iteration 9382: loss: 0.2340928614139557\n",
      "iteration 9383: loss: 0.23409196734428406\n",
      "iteration 9384: loss: 0.23409108817577362\n",
      "iteration 9385: loss: 0.23409008979797363\n",
      "iteration 9386: loss: 0.23408925533294678\n",
      "iteration 9387: loss: 0.23408834636211395\n",
      "iteration 9388: loss: 0.23408746719360352\n",
      "iteration 9389: loss: 0.23408646881580353\n",
      "iteration 9390: loss: 0.23408560454845428\n",
      "iteration 9391: loss: 0.23408469557762146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9392: loss: 0.2340838462114334\n",
      "iteration 9393: loss: 0.23408281803131104\n",
      "iteration 9394: loss: 0.2340819537639618\n",
      "iteration 9395: loss: 0.23408105969429016\n",
      "iteration 9396: loss: 0.23408007621765137\n",
      "iteration 9397: loss: 0.23407921195030212\n",
      "iteration 9398: loss: 0.2340783178806305\n",
      "iteration 9399: loss: 0.23407742381095886\n",
      "iteration 9400: loss: 0.23407645523548126\n",
      "iteration 9401: loss: 0.23407559096813202\n",
      "iteration 9402: loss: 0.23407471179962158\n",
      "iteration 9403: loss: 0.23407351970672607\n",
      "iteration 9404: loss: 0.23407264053821564\n",
      "iteration 9405: loss: 0.2340717762708664\n",
      "iteration 9406: loss: 0.23407086730003357\n",
      "iteration 9407: loss: 0.23406989872455597\n",
      "iteration 9408: loss: 0.2340690642595291\n",
      "iteration 9409: loss: 0.2340681254863739\n",
      "iteration 9410: loss: 0.23406729102134705\n",
      "iteration 9411: loss: 0.23406629264354706\n",
      "iteration 9412: loss: 0.23406541347503662\n",
      "iteration 9413: loss: 0.23406454920768738\n",
      "iteration 9414: loss: 0.23406365513801575\n",
      "iteration 9415: loss: 0.23406267166137695\n",
      "iteration 9416: loss: 0.2340618073940277\n",
      "iteration 9417: loss: 0.23406091332435608\n",
      "iteration 9418: loss: 0.23405981063842773\n",
      "iteration 9419: loss: 0.23405882716178894\n",
      "iteration 9420: loss: 0.2340579330921173\n",
      "iteration 9421: loss: 0.23405706882476807\n",
      "iteration 9422: loss: 0.2340562343597412\n",
      "iteration 9423: loss: 0.2340552806854248\n",
      "iteration 9424: loss: 0.23405437171459198\n",
      "iteration 9425: loss: 0.23405352234840393\n",
      "iteration 9426: loss: 0.2340526282787323\n",
      "iteration 9427: loss: 0.2340516299009323\n",
      "iteration 9428: loss: 0.23405075073242188\n",
      "iteration 9429: loss: 0.23404984176158905\n",
      "iteration 9430: loss: 0.2340490072965622\n",
      "iteration 9431: loss: 0.2340480089187622\n",
      "iteration 9432: loss: 0.23404712975025177\n",
      "iteration 9433: loss: 0.23404626548290253\n",
      "iteration 9434: loss: 0.23404507339000702\n",
      "iteration 9435: loss: 0.23404422402381897\n",
      "iteration 9436: loss: 0.23404331505298615\n",
      "iteration 9437: loss: 0.2340424805879593\n",
      "iteration 9438: loss: 0.2340414822101593\n",
      "iteration 9439: loss: 0.23404058814048767\n",
      "iteration 9440: loss: 0.23403973877429962\n",
      "iteration 9441: loss: 0.23403887450695038\n",
      "iteration 9442: loss: 0.2340378761291504\n",
      "iteration 9443: loss: 0.23403699696063995\n",
      "iteration 9444: loss: 0.2340361326932907\n",
      "iteration 9445: loss: 0.23403505980968475\n",
      "iteration 9446: loss: 0.23403418064117432\n",
      "iteration 9447: loss: 0.23403315246105194\n",
      "iteration 9448: loss: 0.2340323030948639\n",
      "iteration 9449: loss: 0.23403139412403107\n",
      "iteration 9450: loss: 0.23403044044971466\n",
      "iteration 9451: loss: 0.23402956128120422\n",
      "iteration 9452: loss: 0.2340286672115326\n",
      "iteration 9453: loss: 0.23402781784534454\n",
      "iteration 9454: loss: 0.23402683436870575\n",
      "iteration 9455: loss: 0.2340259850025177\n",
      "iteration 9456: loss: 0.23402509093284607\n",
      "iteration 9457: loss: 0.2340240776538849\n",
      "iteration 9458: loss: 0.23402301967144012\n",
      "iteration 9459: loss: 0.23402217030525208\n",
      "iteration 9460: loss: 0.23402127623558044\n",
      "iteration 9461: loss: 0.23402030766010284\n",
      "iteration 9462: loss: 0.2340194284915924\n",
      "iteration 9463: loss: 0.23401853442192078\n",
      "iteration 9464: loss: 0.2340177297592163\n",
      "iteration 9465: loss: 0.2340168058872223\n",
      "iteration 9466: loss: 0.23401586711406708\n",
      "iteration 9467: loss: 0.23401498794555664\n",
      "iteration 9468: loss: 0.23401394486427307\n",
      "iteration 9469: loss: 0.23401308059692383\n",
      "iteration 9470: loss: 0.23401208221912384\n",
      "iteration 9471: loss: 0.2340112030506134\n",
      "iteration 9472: loss: 0.23401030898094177\n",
      "iteration 9473: loss: 0.23400945961475372\n",
      "iteration 9474: loss: 0.23400846123695374\n",
      "iteration 9475: loss: 0.23400762677192688\n",
      "iteration 9476: loss: 0.23400668799877167\n",
      "iteration 9477: loss: 0.23400583863258362\n",
      "iteration 9478: loss: 0.234004944562912\n",
      "iteration 9479: loss: 0.23400375247001648\n",
      "iteration 9480: loss: 0.23400287330150604\n",
      "iteration 9481: loss: 0.234002023935318\n",
      "iteration 9482: loss: 0.23400115966796875\n",
      "iteration 9483: loss: 0.23400017619132996\n",
      "iteration 9484: loss: 0.23399928212165833\n",
      "iteration 9485: loss: 0.23399841785430908\n",
      "iteration 9486: loss: 0.23399750888347626\n",
      "iteration 9487: loss: 0.23399658501148224\n",
      "iteration 9488: loss: 0.23399564623832703\n",
      "iteration 9489: loss: 0.23399457335472107\n",
      "iteration 9490: loss: 0.23399372398853302\n",
      "iteration 9491: loss: 0.2339927703142166\n",
      "iteration 9492: loss: 0.23399189114570618\n",
      "iteration 9493: loss: 0.23399099707603455\n",
      "iteration 9494: loss: 0.2339901477098465\n",
      "iteration 9495: loss: 0.2339891642332077\n",
      "iteration 9496: loss: 0.23398828506469727\n",
      "iteration 9497: loss: 0.2339872568845749\n",
      "iteration 9498: loss: 0.23398637771606445\n",
      "iteration 9499: loss: 0.23398546874523163\n",
      "iteration 9500: loss: 0.23398450016975403\n",
      "iteration 9501: loss: 0.23398363590240479\n",
      "iteration 9502: loss: 0.23398272693157196\n",
      "iteration 9503: loss: 0.2339818775653839\n",
      "iteration 9504: loss: 0.23398086428642273\n",
      "iteration 9505: loss: 0.23398001492023468\n",
      "iteration 9506: loss: 0.23397913575172424\n",
      "iteration 9507: loss: 0.2339780330657959\n",
      "iteration 9508: loss: 0.23397712409496307\n",
      "iteration 9509: loss: 0.2339763194322586\n",
      "iteration 9510: loss: 0.23397544026374817\n",
      "iteration 9511: loss: 0.23397445678710938\n",
      "iteration 9512: loss: 0.23397357761859894\n",
      "iteration 9513: loss: 0.2339726984500885\n",
      "iteration 9514: loss: 0.23397180438041687\n",
      "iteration 9515: loss: 0.23397064208984375\n",
      "iteration 9516: loss: 0.2339698076248169\n",
      "iteration 9517: loss: 0.23396892845630646\n",
      "iteration 9518: loss: 0.23396806418895721\n",
      "iteration 9519: loss: 0.23396706581115723\n",
      "iteration 9520: loss: 0.2339661866426468\n",
      "iteration 9521: loss: 0.23396532237529755\n",
      "iteration 9522: loss: 0.2339644432067871\n",
      "iteration 9523: loss: 0.23396340012550354\n",
      "iteration 9524: loss: 0.23396238684654236\n",
      "iteration 9525: loss: 0.23396150767803192\n",
      "iteration 9526: loss: 0.23396062850952148\n",
      "iteration 9527: loss: 0.23395976424217224\n",
      "iteration 9528: loss: 0.2339589148759842\n",
      "iteration 9529: loss: 0.2339579164981842\n",
      "iteration 9530: loss: 0.23395684361457825\n",
      "iteration 9531: loss: 0.2339560091495514\n",
      "iteration 9532: loss: 0.23395510017871857\n",
      "iteration 9533: loss: 0.23395423591136932\n",
      "iteration 9534: loss: 0.23395326733589172\n",
      "iteration 9535: loss: 0.23395240306854248\n",
      "iteration 9536: loss: 0.23395147919654846\n",
      "iteration 9537: loss: 0.23395061492919922\n",
      "iteration 9538: loss: 0.2339494675397873\n",
      "iteration 9539: loss: 0.23394858837127686\n",
      "iteration 9540: loss: 0.23394770920276642\n",
      "iteration 9541: loss: 0.23394683003425598\n",
      "iteration 9542: loss: 0.23394593596458435\n",
      "iteration 9543: loss: 0.23394496738910675\n",
      "iteration 9544: loss: 0.2339441329240799\n",
      "iteration 9545: loss: 0.23394326865673065\n",
      "iteration 9546: loss: 0.2339421808719635\n",
      "iteration 9547: loss: 0.23394116759300232\n",
      "iteration 9548: loss: 0.23394033312797546\n",
      "iteration 9549: loss: 0.23393940925598145\n",
      "iteration 9550: loss: 0.23393864929676056\n",
      "iteration 9551: loss: 0.23393777012825012\n",
      "iteration 9552: loss: 0.2339365929365158\n",
      "iteration 9553: loss: 0.23393568396568298\n",
      "iteration 9554: loss: 0.23393487930297852\n",
      "iteration 9555: loss: 0.23393401503562927\n",
      "iteration 9556: loss: 0.23393301665782928\n",
      "iteration 9557: loss: 0.23393213748931885\n",
      "iteration 9558: loss: 0.2339312732219696\n",
      "iteration 9559: loss: 0.23393018543720245\n",
      "iteration 9560: loss: 0.2339293509721756\n",
      "iteration 9561: loss: 0.2339283674955368\n",
      "iteration 9562: loss: 0.23392748832702637\n",
      "iteration 9563: loss: 0.23392662405967712\n",
      "iteration 9564: loss: 0.23392577469348907\n",
      "iteration 9565: loss: 0.23392479121685028\n",
      "iteration 9566: loss: 0.23392370343208313\n",
      "iteration 9567: loss: 0.23392286896705627\n",
      "iteration 9568: loss: 0.23392200469970703\n",
      "iteration 9569: loss: 0.23392102122306824\n",
      "iteration 9570: loss: 0.2339201718568802\n",
      "iteration 9571: loss: 0.23391926288604736\n",
      "iteration 9572: loss: 0.23391838371753693\n",
      "iteration 9573: loss: 0.233917236328125\n",
      "iteration 9574: loss: 0.23391637206077576\n",
      "iteration 9575: loss: 0.23391547799110413\n",
      "iteration 9576: loss: 0.23391461372375488\n",
      "iteration 9577: loss: 0.23391374945640564\n",
      "iteration 9578: loss: 0.23391275107860565\n",
      "iteration 9579: loss: 0.2339116781949997\n",
      "iteration 9580: loss: 0.23391082882881165\n",
      "iteration 9581: loss: 0.2339099943637848\n",
      "iteration 9582: loss: 0.23390908539295197\n",
      "iteration 9583: loss: 0.23390813171863556\n",
      "iteration 9584: loss: 0.23390725255012512\n",
      "iteration 9585: loss: 0.23390617966651917\n",
      "iteration 9586: loss: 0.2339053452014923\n",
      "iteration 9587: loss: 0.23390448093414307\n",
      "iteration 9588: loss: 0.23390349745750427\n",
      "iteration 9589: loss: 0.23390261828899384\n",
      "iteration 9590: loss: 0.2339017391204834\n",
      "iteration 9591: loss: 0.23390069603919983\n",
      "iteration 9592: loss: 0.23389987647533417\n",
      "iteration 9593: loss: 0.23389890789985657\n",
      "iteration 9594: loss: 0.23389801383018494\n",
      "iteration 9595: loss: 0.2338971644639969\n",
      "iteration 9596: loss: 0.23389630019664764\n",
      "iteration 9597: loss: 0.2338952124118805\n",
      "iteration 9598: loss: 0.2338942289352417\n",
      "iteration 9599: loss: 0.23389336466789246\n",
      "iteration 9600: loss: 0.23389248549938202\n",
      "iteration 9601: loss: 0.23389168083667755\n",
      "iteration 9602: loss: 0.23389074206352234\n",
      "iteration 9603: loss: 0.23388954997062683\n",
      "iteration 9604: loss: 0.23388871550559998\n",
      "iteration 9605: loss: 0.23388783633708954\n",
      "iteration 9606: loss: 0.2338869869709015\n",
      "iteration 9607: loss: 0.23388609290122986\n",
      "iteration 9608: loss: 0.23388509452342987\n",
      "iteration 9609: loss: 0.2338840663433075\n",
      "iteration 9610: loss: 0.23388318717479706\n",
      "iteration 9611: loss: 0.233882337808609\n",
      "iteration 9612: loss: 0.23388147354125977\n",
      "iteration 9613: loss: 0.23388047516345978\n",
      "iteration 9614: loss: 0.23387941718101501\n",
      "iteration 9615: loss: 0.23387856781482697\n",
      "iteration 9616: loss: 0.23387770354747772\n",
      "iteration 9617: loss: 0.23387674987316132\n",
      "iteration 9618: loss: 0.23387590050697327\n",
      "iteration 9619: loss: 0.23387479782104492\n",
      "iteration 9620: loss: 0.23387399315834045\n",
      "iteration 9621: loss: 0.23387309908866882\n",
      "iteration 9622: loss: 0.23387213051319122\n",
      "iteration 9623: loss: 0.23387129604816437\n",
      "iteration 9624: loss: 0.23387038707733154\n",
      "iteration 9625: loss: 0.23386934399604797\n",
      "iteration 9626: loss: 0.2338683158159256\n",
      "iteration 9627: loss: 0.23386748135089874\n",
      "iteration 9628: loss: 0.2338666170835495\n",
      "iteration 9629: loss: 0.23386570811271667\n",
      "iteration 9630: loss: 0.23386487364768982\n",
      "iteration 9631: loss: 0.2338637113571167\n",
      "iteration 9632: loss: 0.23386287689208984\n",
      "iteration 9633: loss: 0.2338619977235794\n",
      "iteration 9634: loss: 0.23386113345623016\n",
      "iteration 9635: loss: 0.2338602989912033\n",
      "iteration 9636: loss: 0.2338591367006302\n",
      "iteration 9637: loss: 0.23385825753211975\n",
      "iteration 9638: loss: 0.2338573932647705\n",
      "iteration 9639: loss: 0.23385652899742126\n",
      "iteration 9640: loss: 0.23385564982891083\n",
      "iteration 9641: loss: 0.2338545024394989\n",
      "iteration 9642: loss: 0.23385365307331085\n",
      "iteration 9643: loss: 0.23385277390480042\n",
      "iteration 9644: loss: 0.23385190963745117\n",
      "iteration 9645: loss: 0.23385104537010193\n",
      "iteration 9646: loss: 0.2338498830795288\n",
      "iteration 9647: loss: 0.23384901881217957\n",
      "iteration 9648: loss: 0.23384813964366913\n",
      "iteration 9649: loss: 0.2338472604751587\n",
      "iteration 9650: loss: 0.23384645581245422\n",
      "iteration 9651: loss: 0.23384538292884827\n",
      "iteration 9652: loss: 0.23384436964988708\n",
      "iteration 9653: loss: 0.23384353518486023\n",
      "iteration 9654: loss: 0.2338426560163498\n",
      "iteration 9655: loss: 0.23384162783622742\n",
      "iteration 9656: loss: 0.2338407337665558\n",
      "iteration 9657: loss: 0.23383978009223938\n",
      "iteration 9658: loss: 0.23383888602256775\n",
      "iteration 9659: loss: 0.2338380068540573\n",
      "iteration 9660: loss: 0.23383715748786926\n",
      "iteration 9661: loss: 0.2338360846042633\n",
      "iteration 9662: loss: 0.23383519053459167\n",
      "iteration 9663: loss: 0.23383423686027527\n",
      "iteration 9664: loss: 0.23383335769176483\n",
      "iteration 9665: loss: 0.23383250832557678\n",
      "iteration 9666: loss: 0.23383143544197083\n",
      "iteration 9667: loss: 0.23383057117462158\n",
      "iteration 9668: loss: 0.23382961750030518\n",
      "iteration 9669: loss: 0.23382873833179474\n",
      "iteration 9670: loss: 0.23382790386676788\n",
      "iteration 9671: loss: 0.23382680118083954\n",
      "iteration 9672: loss: 0.2338259220123291\n",
      "iteration 9673: loss: 0.23382499814033508\n",
      "iteration 9674: loss: 0.23382416367530823\n",
      "iteration 9675: loss: 0.23382309079170227\n",
      "iteration 9676: loss: 0.23382222652435303\n",
      "iteration 9677: loss: 0.23382136225700378\n",
      "iteration 9678: loss: 0.23382048308849335\n",
      "iteration 9679: loss: 0.2338194102048874\n",
      "iteration 9680: loss: 0.23381853103637695\n",
      "iteration 9681: loss: 0.2338176667690277\n",
      "iteration 9682: loss: 0.23381681740283966\n",
      "iteration 9683: loss: 0.23381583392620087\n",
      "iteration 9684: loss: 0.23381483554840088\n",
      "iteration 9685: loss: 0.23381391167640686\n",
      "iteration 9686: loss: 0.23381304740905762\n",
      "iteration 9687: loss: 0.23381221294403076\n",
      "iteration 9688: loss: 0.23381122946739197\n",
      "iteration 9689: loss: 0.23381014168262482\n",
      "iteration 9690: loss: 0.23380927741527557\n",
      "iteration 9691: loss: 0.23380839824676514\n",
      "iteration 9692: loss: 0.23380760848522186\n",
      "iteration 9693: loss: 0.2338065356016159\n",
      "iteration 9694: loss: 0.2338055670261383\n",
      "iteration 9695: loss: 0.23380470275878906\n",
      "iteration 9696: loss: 0.23380383849143982\n",
      "iteration 9697: loss: 0.23380295932292938\n",
      "iteration 9698: loss: 0.233801931142807\n",
      "iteration 9699: loss: 0.23380093276500702\n",
      "iteration 9700: loss: 0.23380009829998016\n",
      "iteration 9701: loss: 0.23379918932914734\n",
      "iteration 9702: loss: 0.23379814624786377\n",
      "iteration 9703: loss: 0.23379731178283691\n",
      "iteration 9704: loss: 0.23379631340503693\n",
      "iteration 9705: loss: 0.23379547894001007\n",
      "iteration 9706: loss: 0.2337944060564041\n",
      "iteration 9707: loss: 0.23379354178905487\n",
      "iteration 9708: loss: 0.23379269242286682\n",
      "iteration 9709: loss: 0.23379182815551758\n",
      "iteration 9710: loss: 0.23379066586494446\n",
      "iteration 9711: loss: 0.23378977179527283\n",
      "iteration 9712: loss: 0.23378892242908478\n",
      "iteration 9713: loss: 0.23378808796405792\n",
      "iteration 9714: loss: 0.23378701508045197\n",
      "iteration 9715: loss: 0.23378615081310272\n",
      "iteration 9716: loss: 0.23378518223762512\n",
      "iteration 9717: loss: 0.23378431797027588\n",
      "iteration 9718: loss: 0.23378348350524902\n",
      "iteration 9719: loss: 0.23378241062164307\n",
      "iteration 9720: loss: 0.2337815761566162\n",
      "iteration 9721: loss: 0.23378069698810577\n",
      "iteration 9722: loss: 0.23377975821495056\n",
      "iteration 9723: loss: 0.23377864062786102\n",
      "iteration 9724: loss: 0.23377785086631775\n",
      "iteration 9725: loss: 0.23377692699432373\n",
      "iteration 9726: loss: 0.23377609252929688\n",
      "iteration 9727: loss: 0.23377522826194763\n",
      "iteration 9728: loss: 0.2337740957736969\n",
      "iteration 9729: loss: 0.23377320170402527\n",
      "iteration 9730: loss: 0.23377232253551483\n",
      "iteration 9731: loss: 0.23377132415771484\n",
      "iteration 9732: loss: 0.23377041518688202\n",
      "iteration 9733: loss: 0.23376956582069397\n",
      "iteration 9734: loss: 0.23376858234405518\n",
      "iteration 9735: loss: 0.23376774787902832\n",
      "iteration 9736: loss: 0.23376670479774475\n",
      "iteration 9737: loss: 0.2337658405303955\n",
      "iteration 9738: loss: 0.23376496136188507\n",
      "iteration 9739: loss: 0.2337639331817627\n",
      "iteration 9740: loss: 0.2337629795074463\n",
      "iteration 9741: loss: 0.23376207053661346\n",
      "iteration 9742: loss: 0.233761265873909\n",
      "iteration 9743: loss: 0.23376019299030304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9744: loss: 0.2337593138217926\n",
      "iteration 9745: loss: 0.23375847935676575\n",
      "iteration 9746: loss: 0.23375749588012695\n",
      "iteration 9747: loss: 0.23375646770000458\n",
      "iteration 9748: loss: 0.23375558853149414\n",
      "iteration 9749: loss: 0.23375478386878967\n",
      "iteration 9750: loss: 0.23375387489795685\n",
      "iteration 9751: loss: 0.23375269770622253\n",
      "iteration 9752: loss: 0.23375186324119568\n",
      "iteration 9753: loss: 0.23375098407268524\n",
      "iteration 9754: loss: 0.2337501496076584\n",
      "iteration 9755: loss: 0.23374906182289124\n",
      "iteration 9756: loss: 0.23374812304973602\n",
      "iteration 9757: loss: 0.23374727368354797\n",
      "iteration 9758: loss: 0.23374640941619873\n",
      "iteration 9759: loss: 0.23374533653259277\n",
      "iteration 9760: loss: 0.23374450206756592\n",
      "iteration 9761: loss: 0.23374363780021667\n",
      "iteration 9762: loss: 0.23374250531196594\n",
      "iteration 9763: loss: 0.2337416708469391\n",
      "iteration 9764: loss: 0.23374083638191223\n",
      "iteration 9765: loss: 0.2337399423122406\n",
      "iteration 9766: loss: 0.23373889923095703\n",
      "iteration 9767: loss: 0.2337380349636078\n",
      "iteration 9768: loss: 0.23373708128929138\n",
      "iteration 9769: loss: 0.23373620212078094\n",
      "iteration 9770: loss: 0.23373517394065857\n",
      "iteration 9771: loss: 0.23373429477214813\n",
      "iteration 9772: loss: 0.23373346030712128\n",
      "iteration 9773: loss: 0.23373238742351532\n",
      "iteration 9774: loss: 0.23373155295848846\n",
      "iteration 9775: loss: 0.23373059928417206\n",
      "iteration 9776: loss: 0.233729749917984\n",
      "iteration 9777: loss: 0.23372864723205566\n",
      "iteration 9778: loss: 0.23372778296470642\n",
      "iteration 9779: loss: 0.23372693359851837\n",
      "iteration 9780: loss: 0.23372606933116913\n",
      "iteration 9781: loss: 0.233724907040596\n",
      "iteration 9782: loss: 0.23372404277324677\n",
      "iteration 9783: loss: 0.23372319340705872\n",
      "iteration 9784: loss: 0.23372213542461395\n",
      "iteration 9785: loss: 0.23372125625610352\n",
      "iteration 9786: loss: 0.23372042179107666\n",
      "iteration 9787: loss: 0.23371955752372742\n",
      "iteration 9788: loss: 0.2337183952331543\n",
      "iteration 9789: loss: 0.23371756076812744\n",
      "iteration 9790: loss: 0.233716681599617\n",
      "iteration 9791: loss: 0.23371565341949463\n",
      "iteration 9792: loss: 0.2337147444486618\n",
      "iteration 9793: loss: 0.23371390998363495\n",
      "iteration 9794: loss: 0.23371294140815735\n",
      "iteration 9795: loss: 0.23371188342571259\n",
      "iteration 9796: loss: 0.23371100425720215\n",
      "iteration 9797: loss: 0.23371019959449768\n",
      "iteration 9798: loss: 0.23370930552482605\n",
      "iteration 9799: loss: 0.23370826244354248\n",
      "iteration 9800: loss: 0.23370739817619324\n",
      "iteration 9801: loss: 0.23370642960071564\n",
      "iteration 9802: loss: 0.23370540142059326\n",
      "iteration 9803: loss: 0.23370453715324402\n",
      "iteration 9804: loss: 0.23370368778705597\n",
      "iteration 9805: loss: 0.2337028533220291\n",
      "iteration 9806: loss: 0.23370185494422913\n",
      "iteration 9807: loss: 0.23370087146759033\n",
      "iteration 9808: loss: 0.2337000072002411\n",
      "iteration 9809: loss: 0.2336989939212799\n",
      "iteration 9810: loss: 0.23369812965393066\n",
      "iteration 9811: loss: 0.23369726538658142\n",
      "iteration 9812: loss: 0.23369619250297546\n",
      "iteration 9813: loss: 0.23369523882865906\n",
      "iteration 9814: loss: 0.233694389462471\n",
      "iteration 9815: loss: 0.23369356989860535\n",
      "iteration 9816: loss: 0.2336924970149994\n",
      "iteration 9817: loss: 0.23369161784648895\n",
      "iteration 9818: loss: 0.2336907833814621\n",
      "iteration 9819: loss: 0.23368971049785614\n",
      "iteration 9820: loss: 0.23368878662586212\n",
      "iteration 9821: loss: 0.2336878776550293\n",
      "iteration 9822: loss: 0.23368707299232483\n",
      "iteration 9823: loss: 0.23368600010871887\n",
      "iteration 9824: loss: 0.23368510603904724\n",
      "iteration 9825: loss: 0.2336842566728592\n",
      "iteration 9826: loss: 0.23368313908576965\n",
      "iteration 9827: loss: 0.2336822748184204\n",
      "iteration 9828: loss: 0.23368139564990997\n",
      "iteration 9829: loss: 0.2336803376674652\n",
      "iteration 9830: loss: 0.23367950320243835\n",
      "iteration 9831: loss: 0.2336786538362503\n",
      "iteration 9832: loss: 0.2336776703596115\n",
      "iteration 9833: loss: 0.23367664217948914\n",
      "iteration 9834: loss: 0.2336757928133011\n",
      "iteration 9835: loss: 0.23367492854595184\n",
      "iteration 9836: loss: 0.2336738556623459\n",
      "iteration 9837: loss: 0.23367305099964142\n",
      "iteration 9838: loss: 0.23367217183113098\n",
      "iteration 9839: loss: 0.23367111384868622\n",
      "iteration 9840: loss: 0.2336701601743698\n",
      "iteration 9841: loss: 0.23366932570934296\n",
      "iteration 9842: loss: 0.2336682826280594\n",
      "iteration 9843: loss: 0.23366741836071014\n",
      "iteration 9844: loss: 0.2336665391921997\n",
      "iteration 9845: loss: 0.23366549611091614\n",
      "iteration 9846: loss: 0.2336646318435669\n",
      "iteration 9847: loss: 0.2336636781692505\n",
      "iteration 9848: loss: 0.23366279900074005\n",
      "iteration 9849: loss: 0.23366181552410126\n",
      "iteration 9850: loss: 0.23366093635559082\n",
      "iteration 9851: loss: 0.23366007208824158\n",
      "iteration 9852: loss: 0.233659029006958\n",
      "iteration 9853: loss: 0.23365822434425354\n",
      "iteration 9854: loss: 0.23365724086761475\n",
      "iteration 9855: loss: 0.23365619778633118\n",
      "iteration 9856: loss: 0.23365530371665955\n",
      "iteration 9857: loss: 0.2336544543504715\n",
      "iteration 9858: loss: 0.23365342617034912\n",
      "iteration 9859: loss: 0.23365254700183868\n",
      "iteration 9860: loss: 0.23365171253681183\n",
      "iteration 9861: loss: 0.23365052044391632\n",
      "iteration 9862: loss: 0.23364968597888947\n",
      "iteration 9863: loss: 0.2336488515138626\n",
      "iteration 9864: loss: 0.23364782333374023\n",
      "iteration 9865: loss: 0.233646959066391\n",
      "iteration 9866: loss: 0.23364612460136414\n",
      "iteration 9867: loss: 0.23364505171775818\n",
      "iteration 9868: loss: 0.23364409804344177\n",
      "iteration 9869: loss: 0.23364326357841492\n",
      "iteration 9870: loss: 0.23364219069480896\n",
      "iteration 9871: loss: 0.2336413562297821\n",
      "iteration 9872: loss: 0.23364047706127167\n",
      "iteration 9873: loss: 0.2336394339799881\n",
      "iteration 9874: loss: 0.23363856971263885\n",
      "iteration 9875: loss: 0.23363761603832245\n",
      "iteration 9876: loss: 0.2336367815732956\n",
      "iteration 9877: loss: 0.23363569378852844\n",
      "iteration 9878: loss: 0.2336348593235016\n",
      "iteration 9879: loss: 0.23363399505615234\n",
      "iteration 9880: loss: 0.23363296687602997\n",
      "iteration 9881: loss: 0.23363208770751953\n",
      "iteration 9882: loss: 0.23363113403320312\n",
      "iteration 9883: loss: 0.23363010585308075\n",
      "iteration 9884: loss: 0.2336292713880539\n",
      "iteration 9885: loss: 0.23362822830677032\n",
      "iteration 9886: loss: 0.23362739384174347\n",
      "iteration 9887: loss: 0.23362648487091064\n",
      "iteration 9888: loss: 0.23362545669078827\n",
      "iteration 9889: loss: 0.2336246222257614\n",
      "iteration 9890: loss: 0.233623668551445\n",
      "iteration 9891: loss: 0.23362262547016144\n",
      "iteration 9892: loss: 0.23362179100513458\n",
      "iteration 9893: loss: 0.23362097144126892\n",
      "iteration 9894: loss: 0.23361992835998535\n",
      "iteration 9895: loss: 0.2336190938949585\n",
      "iteration 9896: loss: 0.23361818492412567\n",
      "iteration 9897: loss: 0.23361703753471375\n",
      "iteration 9898: loss: 0.2336161881685257\n",
      "iteration 9899: loss: 0.23361532390117645\n",
      "iteration 9900: loss: 0.23361428081989288\n",
      "iteration 9901: loss: 0.23361344635486603\n",
      "iteration 9902: loss: 0.23361258208751678\n",
      "iteration 9903: loss: 0.23361153900623322\n",
      "iteration 9904: loss: 0.23361070454120636\n",
      "iteration 9905: loss: 0.23360970616340637\n",
      "iteration 9906: loss: 0.2336086928844452\n",
      "iteration 9907: loss: 0.23360785841941833\n",
      "iteration 9908: loss: 0.2336069643497467\n",
      "iteration 9909: loss: 0.23360590636730194\n",
      "iteration 9910: loss: 0.23360507190227509\n",
      "iteration 9911: loss: 0.23360422253608704\n",
      "iteration 9912: loss: 0.23360319435596466\n",
      "iteration 9913: loss: 0.23360221087932587\n",
      "iteration 9914: loss: 0.233601376414299\n",
      "iteration 9915: loss: 0.23360033333301544\n",
      "iteration 9916: loss: 0.2335994690656662\n",
      "iteration 9917: loss: 0.23359844088554382\n",
      "iteration 9918: loss: 0.2335975617170334\n",
      "iteration 9919: loss: 0.23359672725200653\n",
      "iteration 9920: loss: 0.23359568417072296\n",
      "iteration 9921: loss: 0.23359473049640656\n",
      "iteration 9922: loss: 0.2335938960313797\n",
      "iteration 9923: loss: 0.23359282314777374\n",
      "iteration 9924: loss: 0.2335919588804245\n",
      "iteration 9925: loss: 0.23359112441539764\n",
      "iteration 9926: loss: 0.2335900515317917\n",
      "iteration 9927: loss: 0.23358920216560364\n",
      "iteration 9928: loss: 0.2335883378982544\n",
      "iteration 9929: loss: 0.23358729481697083\n",
      "iteration 9930: loss: 0.2335863560438156\n",
      "iteration 9931: loss: 0.23358547687530518\n",
      "iteration 9932: loss: 0.2335844486951828\n",
      "iteration 9933: loss: 0.23358359932899475\n",
      "iteration 9934: loss: 0.23358254134655\n",
      "iteration 9935: loss: 0.2335817813873291\n",
      "iteration 9936: loss: 0.23358091711997986\n",
      "iteration 9937: loss: 0.23357990384101868\n",
      "iteration 9938: loss: 0.23357892036437988\n",
      "iteration 9939: loss: 0.23357805609703064\n",
      "iteration 9940: loss: 0.23357704281806946\n",
      "iteration 9941: loss: 0.23357614874839783\n",
      "iteration 9942: loss: 0.23357513546943665\n",
      "iteration 9943: loss: 0.2335743010044098\n",
      "iteration 9944: loss: 0.23357343673706055\n",
      "iteration 9945: loss: 0.23357240855693817\n",
      "iteration 9946: loss: 0.23357143998146057\n",
      "iteration 9947: loss: 0.23357060551643372\n",
      "iteration 9948: loss: 0.23356957733631134\n",
      "iteration 9949: loss: 0.2335686981678009\n",
      "iteration 9950: loss: 0.23356786370277405\n",
      "iteration 9951: loss: 0.2335667610168457\n",
      "iteration 9952: loss: 0.23356592655181885\n",
      "iteration 9953: loss: 0.23356488347053528\n",
      "iteration 9954: loss: 0.23356406390666962\n",
      "iteration 9955: loss: 0.23356309533119202\n",
      "iteration 9956: loss: 0.23356206715106964\n",
      "iteration 9957: loss: 0.2335612028837204\n",
      "iteration 9958: loss: 0.23356035351753235\n",
      "iteration 9959: loss: 0.23355929553508759\n",
      "iteration 9960: loss: 0.23355846107006073\n",
      "iteration 9961: loss: 0.23355741798877716\n",
      "iteration 9962: loss: 0.23355650901794434\n",
      "iteration 9963: loss: 0.2335556000471115\n",
      "iteration 9964: loss: 0.23355457186698914\n",
      "iteration 9965: loss: 0.23355376720428467\n",
      "iteration 9966: loss: 0.2335526943206787\n",
      "iteration 9967: loss: 0.23355183005332947\n",
      "iteration 9968: loss: 0.23355098068714142\n",
      "iteration 9969: loss: 0.23354990780353546\n",
      "iteration 9970: loss: 0.2335490882396698\n",
      "iteration 9971: loss: 0.23354816436767578\n",
      "iteration 9972: loss: 0.23354706168174744\n",
      "iteration 9973: loss: 0.23354622721672058\n",
      "iteration 9974: loss: 0.2335452139377594\n",
      "iteration 9975: loss: 0.23354434967041016\n",
      "iteration 9976: loss: 0.2335434854030609\n",
      "iteration 9977: loss: 0.23354244232177734\n",
      "iteration 9978: loss: 0.23354168236255646\n",
      "iteration 9979: loss: 0.23354080319404602\n",
      "iteration 9980: loss: 0.2335396558046341\n",
      "iteration 9981: loss: 0.23353877663612366\n",
      "iteration 9982: loss: 0.23353776335716248\n",
      "iteration 9983: loss: 0.23353692889213562\n",
      "iteration 9984: loss: 0.23353607952594757\n",
      "iteration 9985: loss: 0.2335350215435028\n",
      "iteration 9986: loss: 0.23353421688079834\n",
      "iteration 9987: loss: 0.23353317379951477\n",
      "iteration 9988: loss: 0.23353227972984314\n",
      "iteration 9989: loss: 0.2335314303636551\n",
      "iteration 9990: loss: 0.23353028297424316\n",
      "iteration 9991: loss: 0.23352941870689392\n",
      "iteration 9992: loss: 0.23352840542793274\n",
      "iteration 9993: loss: 0.2335275411605835\n",
      "iteration 9994: loss: 0.23352670669555664\n",
      "iteration 9995: loss: 0.23352566361427307\n",
      "iteration 9996: loss: 0.23352482914924622\n",
      "iteration 9997: loss: 0.23352372646331787\n",
      "iteration 9998: loss: 0.2335229367017746\n",
      "iteration 9999: loss: 0.23352208733558655\n",
      "iteration 10000: loss: 0.23352089524269104\n",
      "iteration 10001: loss: 0.2335200309753418\n",
      "iteration 10002: loss: 0.23351922631263733\n",
      "iteration 10003: loss: 0.23351816833019257\n",
      "iteration 10004: loss: 0.23351731896400452\n",
      "iteration 10005: loss: 0.23351626098155975\n",
      "iteration 10006: loss: 0.2335154265165329\n",
      "iteration 10007: loss: 0.23351459205150604\n",
      "iteration 10008: loss: 0.23351351916790009\n",
      "iteration 10009: loss: 0.23351259529590607\n",
      "iteration 10010: loss: 0.2335115224123001\n",
      "iteration 10011: loss: 0.23351070284843445\n",
      "iteration 10012: loss: 0.2335098683834076\n",
      "iteration 10013: loss: 0.23350879549980164\n",
      "iteration 10014: loss: 0.23350796103477478\n",
      "iteration 10015: loss: 0.2335069179534912\n",
      "iteration 10016: loss: 0.23350608348846436\n",
      "iteration 10017: loss: 0.2335050404071808\n",
      "iteration 10018: loss: 0.23350408673286438\n",
      "iteration 10019: loss: 0.23350325226783752\n",
      "iteration 10020: loss: 0.23350222408771515\n",
      "iteration 10021: loss: 0.2335014045238495\n",
      "iteration 10022: loss: 0.23350036144256592\n",
      "iteration 10023: loss: 0.23349952697753906\n",
      "iteration 10024: loss: 0.2334986925125122\n",
      "iteration 10025: loss: 0.2334977388381958\n",
      "iteration 10026: loss: 0.23349687457084656\n",
      "iteration 10027: loss: 0.23349586129188538\n",
      "iteration 10028: loss: 0.23349499702453613\n",
      "iteration 10029: loss: 0.23349395394325256\n",
      "iteration 10030: loss: 0.23349300026893616\n",
      "iteration 10031: loss: 0.2334921658039093\n",
      "iteration 10032: loss: 0.23349113762378693\n",
      "iteration 10033: loss: 0.23349027335643768\n",
      "iteration 10034: loss: 0.2334892302751541\n",
      "iteration 10035: loss: 0.23348839581012726\n",
      "iteration 10036: loss: 0.2334875613451004\n",
      "iteration 10037: loss: 0.23348648846149445\n",
      "iteration 10038: loss: 0.23348554968833923\n",
      "iteration 10039: loss: 0.23348450660705566\n",
      "iteration 10040: loss: 0.23348364233970642\n",
      "iteration 10041: loss: 0.23348283767700195\n",
      "iteration 10042: loss: 0.23348179459571838\n",
      "iteration 10043: loss: 0.23348093032836914\n",
      "iteration 10044: loss: 0.23347988724708557\n",
      "iteration 10045: loss: 0.23347902297973633\n",
      "iteration 10046: loss: 0.23347799479961395\n",
      "iteration 10047: loss: 0.2334771454334259\n",
      "iteration 10048: loss: 0.23347623646259308\n",
      "iteration 10049: loss: 0.23347516357898712\n",
      "iteration 10050: loss: 0.23347434401512146\n",
      "iteration 10051: loss: 0.2334733009338379\n",
      "iteration 10052: loss: 0.23347243666648865\n",
      "iteration 10053: loss: 0.23347139358520508\n",
      "iteration 10054: loss: 0.23347052931785583\n",
      "iteration 10055: loss: 0.23346967995166779\n",
      "iteration 10056: loss: 0.2334686815738678\n",
      "iteration 10057: loss: 0.23346781730651855\n",
      "iteration 10058: loss: 0.23346677422523499\n",
      "iteration 10059: loss: 0.23346590995788574\n",
      "iteration 10060: loss: 0.23346491158008575\n",
      "iteration 10061: loss: 0.23346395790576935\n",
      "iteration 10062: loss: 0.23346289992332458\n",
      "iteration 10063: loss: 0.23346206545829773\n",
      "iteration 10064: loss: 0.23346126079559326\n",
      "iteration 10065: loss: 0.2334602326154709\n",
      "iteration 10066: loss: 0.23345938324928284\n",
      "iteration 10067: loss: 0.23345832526683807\n",
      "iteration 10068: loss: 0.23345749080181122\n",
      "iteration 10069: loss: 0.23345646262168884\n",
      "iteration 10070: loss: 0.2334555834531784\n",
      "iteration 10071: loss: 0.23345474898815155\n",
      "iteration 10072: loss: 0.23345370590686798\n",
      "iteration 10073: loss: 0.2334527224302292\n",
      "iteration 10074: loss: 0.2334517538547516\n",
      "iteration 10075: loss: 0.23345085978507996\n",
      "iteration 10076: loss: 0.23344984650611877\n",
      "iteration 10077: loss: 0.23344898223876953\n",
      "iteration 10078: loss: 0.2334481179714203\n",
      "iteration 10079: loss: 0.2334471195936203\n",
      "iteration 10080: loss: 0.23344627022743225\n",
      "iteration 10081: loss: 0.2334452122449875\n",
      "iteration 10082: loss: 0.23344440758228302\n",
      "iteration 10083: loss: 0.23344333469867706\n",
      "iteration 10084: loss: 0.23344239592552185\n",
      "iteration 10085: loss: 0.23344138264656067\n",
      "iteration 10086: loss: 0.2334405481815338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10087: loss: 0.23343965411186218\n",
      "iteration 10088: loss: 0.2334386557340622\n",
      "iteration 10089: loss: 0.23343777656555176\n",
      "iteration 10090: loss: 0.23343677818775177\n",
      "iteration 10091: loss: 0.23343589901924133\n",
      "iteration 10092: loss: 0.23343487083911896\n",
      "iteration 10093: loss: 0.2334340363740921\n",
      "iteration 10094: loss: 0.23343300819396973\n",
      "iteration 10095: loss: 0.23343214392662048\n",
      "iteration 10096: loss: 0.23343110084533691\n",
      "iteration 10097: loss: 0.23343022167682648\n",
      "iteration 10098: loss: 0.23342931270599365\n",
      "iteration 10099: loss: 0.23342826962471008\n",
      "iteration 10100: loss: 0.23342743515968323\n",
      "iteration 10101: loss: 0.23342640697956085\n",
      "iteration 10102: loss: 0.23342552781105042\n",
      "iteration 10103: loss: 0.23342449963092804\n",
      "iteration 10104: loss: 0.23342366516590118\n",
      "iteration 10105: loss: 0.23342280089855194\n",
      "iteration 10106: loss: 0.23342175781726837\n",
      "iteration 10107: loss: 0.2334209382534027\n",
      "iteration 10108: loss: 0.2334199696779251\n",
      "iteration 10109: loss: 0.23341898620128632\n",
      "iteration 10110: loss: 0.23341795802116394\n",
      "iteration 10111: loss: 0.23341712355613708\n",
      "iteration 10112: loss: 0.2334161102771759\n",
      "iteration 10113: loss: 0.23341524600982666\n",
      "iteration 10114: loss: 0.2334141731262207\n",
      "iteration 10115: loss: 0.23341333866119385\n",
      "iteration 10116: loss: 0.23341234028339386\n",
      "iteration 10117: loss: 0.233411505818367\n",
      "iteration 10118: loss: 0.23341067135334015\n",
      "iteration 10119: loss: 0.2334095984697342\n",
      "iteration 10120: loss: 0.23340877890586853\n",
      "iteration 10121: loss: 0.233407661318779\n",
      "iteration 10122: loss: 0.23340681195259094\n",
      "iteration 10123: loss: 0.23340573906898499\n",
      "iteration 10124: loss: 0.23340491950511932\n",
      "iteration 10125: loss: 0.23340389132499695\n",
      "iteration 10126: loss: 0.2334030568599701\n",
      "iteration 10127: loss: 0.2334020435810089\n",
      "iteration 10128: loss: 0.23340117931365967\n",
      "iteration 10129: loss: 0.2334001064300537\n",
      "iteration 10130: loss: 0.23339927196502686\n",
      "iteration 10131: loss: 0.23339824378490448\n",
      "iteration 10132: loss: 0.23339739441871643\n",
      "iteration 10133: loss: 0.23339636623859406\n",
      "iteration 10134: loss: 0.2333955317735672\n",
      "iteration 10135: loss: 0.23339450359344482\n",
      "iteration 10136: loss: 0.23339354991912842\n",
      "iteration 10137: loss: 0.23339268565177917\n",
      "iteration 10138: loss: 0.2333916425704956\n",
      "iteration 10139: loss: 0.23339080810546875\n",
      "iteration 10140: loss: 0.23338976502418518\n",
      "iteration 10141: loss: 0.23338893055915833\n",
      "iteration 10142: loss: 0.23338787257671356\n",
      "iteration 10143: loss: 0.2333870232105255\n",
      "iteration 10144: loss: 0.23338599503040314\n",
      "iteration 10145: loss: 0.23338517546653748\n",
      "iteration 10146: loss: 0.2333841323852539\n",
      "iteration 10147: loss: 0.23338326811790466\n",
      "iteration 10148: loss: 0.23338225483894348\n",
      "iteration 10149: loss: 0.23338130116462708\n",
      "iteration 10150: loss: 0.2333802878856659\n",
      "iteration 10151: loss: 0.23337948322296143\n",
      "iteration 10152: loss: 0.23337845504283905\n",
      "iteration 10153: loss: 0.2333776205778122\n",
      "iteration 10154: loss: 0.23337657749652863\n",
      "iteration 10155: loss: 0.23337574303150177\n",
      "iteration 10156: loss: 0.23337487876415253\n",
      "iteration 10157: loss: 0.23337383568286896\n",
      "iteration 10158: loss: 0.23337297141551971\n",
      "iteration 10159: loss: 0.23337197303771973\n",
      "iteration 10160: loss: 0.23337110877037048\n",
      "iteration 10161: loss: 0.23337006568908691\n",
      "iteration 10162: loss: 0.23336923122406006\n",
      "iteration 10163: loss: 0.23336823284626007\n",
      "iteration 10164: loss: 0.23336736857891083\n",
      "iteration 10165: loss: 0.23336632549762726\n",
      "iteration 10166: loss: 0.23336541652679443\n",
      "iteration 10167: loss: 0.23336437344551086\n",
      "iteration 10168: loss: 0.2333635538816452\n",
      "iteration 10169: loss: 0.23336248099803925\n",
      "iteration 10170: loss: 0.23336167633533478\n",
      "iteration 10171: loss: 0.23336061835289001\n",
      "iteration 10172: loss: 0.23335978388786316\n",
      "iteration 10173: loss: 0.2333587110042572\n",
      "iteration 10174: loss: 0.23335790634155273\n",
      "iteration 10175: loss: 0.23335687816143036\n",
      "iteration 10176: loss: 0.2333560287952423\n",
      "iteration 10177: loss: 0.23335497081279755\n",
      "iteration 10178: loss: 0.23335416615009308\n",
      "iteration 10179: loss: 0.2333531379699707\n",
      "iteration 10180: loss: 0.2333521842956543\n",
      "iteration 10181: loss: 0.23335114121437073\n",
      "iteration 10182: loss: 0.23335030674934387\n",
      "iteration 10183: loss: 0.2333492785692215\n",
      "iteration 10184: loss: 0.23334841430187225\n",
      "iteration 10185: loss: 0.23334737122058868\n",
      "iteration 10186: loss: 0.23334655165672302\n",
      "iteration 10187: loss: 0.23334550857543945\n",
      "iteration 10188: loss: 0.2333446741104126\n",
      "iteration 10189: loss: 0.23334363102912903\n",
      "iteration 10190: loss: 0.23334284126758575\n",
      "iteration 10191: loss: 0.2333417683839798\n",
      "iteration 10192: loss: 0.23334093391895294\n",
      "iteration 10193: loss: 0.23333990573883057\n",
      "iteration 10194: loss: 0.2333391159772873\n",
      "iteration 10195: loss: 0.23333796858787537\n",
      "iteration 10196: loss: 0.23333711922168732\n",
      "iteration 10197: loss: 0.23333612084388733\n",
      "iteration 10198: loss: 0.23333525657653809\n",
      "iteration 10199: loss: 0.2333342283964157\n",
      "iteration 10200: loss: 0.23333339393138885\n",
      "iteration 10201: loss: 0.23333236575126648\n",
      "iteration 10202: loss: 0.233331561088562\n",
      "iteration 10203: loss: 0.23333051800727844\n",
      "iteration 10204: loss: 0.2333296537399292\n",
      "iteration 10205: loss: 0.23332858085632324\n",
      "iteration 10206: loss: 0.23332779109477997\n",
      "iteration 10207: loss: 0.2333267480134964\n",
      "iteration 10208: loss: 0.23332591354846954\n",
      "iteration 10209: loss: 0.23332488536834717\n",
      "iteration 10210: loss: 0.2333238571882248\n",
      "iteration 10211: loss: 0.23332300782203674\n",
      "iteration 10212: loss: 0.23332194983959198\n",
      "iteration 10213: loss: 0.23332110047340393\n",
      "iteration 10214: loss: 0.23331999778747559\n",
      "iteration 10215: loss: 0.23331913352012634\n",
      "iteration 10216: loss: 0.23331813514232635\n",
      "iteration 10217: loss: 0.2333173006772995\n",
      "iteration 10218: loss: 0.23331622779369354\n",
      "iteration 10219: loss: 0.23331542313098907\n",
      "iteration 10220: loss: 0.2333143651485443\n",
      "iteration 10221: loss: 0.23331353068351746\n",
      "iteration 10222: loss: 0.23331251740455627\n",
      "iteration 10223: loss: 0.2333116978406906\n",
      "iteration 10224: loss: 0.23331065475940704\n",
      "iteration 10225: loss: 0.2333098202943802\n",
      "iteration 10226: loss: 0.23330876231193542\n",
      "iteration 10227: loss: 0.23330791294574738\n",
      "iteration 10228: loss: 0.233306884765625\n",
      "iteration 10229: loss: 0.23330602049827576\n",
      "iteration 10230: loss: 0.23330502212047577\n",
      "iteration 10231: loss: 0.2333041876554489\n",
      "iteration 10232: loss: 0.23330314457416534\n",
      "iteration 10233: loss: 0.23330223560333252\n",
      "iteration 10234: loss: 0.23330116271972656\n",
      "iteration 10235: loss: 0.2333003282546997\n",
      "iteration 10236: loss: 0.23329930007457733\n",
      "iteration 10237: loss: 0.23329851031303406\n",
      "iteration 10238: loss: 0.23329749703407288\n",
      "iteration 10239: loss: 0.2332964688539505\n",
      "iteration 10240: loss: 0.23329563438892365\n",
      "iteration 10241: loss: 0.23329457640647888\n",
      "iteration 10242: loss: 0.2332938015460968\n",
      "iteration 10243: loss: 0.23329274356365204\n",
      "iteration 10244: loss: 0.23329193890094757\n",
      "iteration 10245: loss: 0.2332908809185028\n",
      "iteration 10246: loss: 0.23329003155231476\n",
      "iteration 10247: loss: 0.23328900337219238\n",
      "iteration 10248: loss: 0.23328813910484314\n",
      "iteration 10249: loss: 0.23328712582588196\n",
      "iteration 10250: loss: 0.2332863062620163\n",
      "iteration 10251: loss: 0.23328523337841034\n",
      "iteration 10252: loss: 0.2332841157913208\n",
      "iteration 10253: loss: 0.2332833707332611\n",
      "iteration 10254: loss: 0.23328225314617157\n",
      "iteration 10255: loss: 0.23328140377998352\n",
      "iteration 10256: loss: 0.23328039050102234\n",
      "iteration 10257: loss: 0.23327955603599548\n",
      "iteration 10258: loss: 0.23327851295471191\n",
      "iteration 10259: loss: 0.23327770829200745\n",
      "iteration 10260: loss: 0.23327665030956268\n",
      "iteration 10261: loss: 0.23327580094337463\n",
      "iteration 10262: loss: 0.23327478766441345\n",
      "iteration 10263: loss: 0.23327398300170898\n",
      "iteration 10264: loss: 0.23327293992042542\n",
      "iteration 10265: loss: 0.23327210545539856\n",
      "iteration 10266: loss: 0.2332710474729538\n",
      "iteration 10267: loss: 0.23327001929283142\n",
      "iteration 10268: loss: 0.23326921463012695\n",
      "iteration 10269: loss: 0.23326817154884338\n",
      "iteration 10270: loss: 0.23326735198497772\n",
      "iteration 10271: loss: 0.23326632380485535\n",
      "iteration 10272: loss: 0.2332654893398285\n",
      "iteration 10273: loss: 0.23326441645622253\n",
      "iteration 10274: loss: 0.23326349258422852\n",
      "iteration 10275: loss: 0.23326249420642853\n",
      "iteration 10276: loss: 0.23326143622398376\n",
      "iteration 10277: loss: 0.2332606017589569\n",
      "iteration 10278: loss: 0.23325955867767334\n",
      "iteration 10279: loss: 0.23325872421264648\n",
      "iteration 10280: loss: 0.23325768113136292\n",
      "iteration 10281: loss: 0.23325693607330322\n",
      "iteration 10282: loss: 0.23325589299201965\n",
      "iteration 10283: loss: 0.2332550585269928\n",
      "iteration 10284: loss: 0.23325404524803162\n",
      "iteration 10285: loss: 0.23325300216674805\n",
      "iteration 10286: loss: 0.2332521378993988\n",
      "iteration 10287: loss: 0.2332511693239212\n",
      "iteration 10288: loss: 0.23325033485889435\n",
      "iteration 10289: loss: 0.23324927687644958\n",
      "iteration 10290: loss: 0.23324847221374512\n",
      "iteration 10291: loss: 0.23324744403362274\n",
      "iteration 10292: loss: 0.2332465946674347\n",
      "iteration 10293: loss: 0.2332455813884735\n",
      "iteration 10294: loss: 0.23324453830718994\n",
      "iteration 10295: loss: 0.23324370384216309\n",
      "iteration 10296: loss: 0.23324266076087952\n",
      "iteration 10297: loss: 0.2332417219877243\n",
      "iteration 10298: loss: 0.23324067890644073\n",
      "iteration 10299: loss: 0.23323984444141388\n",
      "iteration 10300: loss: 0.2332388460636139\n",
      "iteration 10301: loss: 0.23323801159858704\n",
      "iteration 10302: loss: 0.23323695361614227\n",
      "iteration 10303: loss: 0.2332359254360199\n",
      "iteration 10304: loss: 0.23323512077331543\n",
      "iteration 10305: loss: 0.23323407769203186\n",
      "iteration 10306: loss: 0.23323321342468262\n",
      "iteration 10307: loss: 0.23323218524456024\n",
      "iteration 10308: loss: 0.2332313507795334\n",
      "iteration 10309: loss: 0.2332303524017334\n",
      "iteration 10310: loss: 0.23322954773902893\n",
      "iteration 10311: loss: 0.23322848975658417\n",
      "iteration 10312: loss: 0.2332274615764618\n",
      "iteration 10313: loss: 0.23322661221027374\n",
      "iteration 10314: loss: 0.23322562873363495\n",
      "iteration 10315: loss: 0.2332247793674469\n",
      "iteration 10316: loss: 0.23322375118732452\n",
      "iteration 10317: loss: 0.23322269320487976\n",
      "iteration 10318: loss: 0.23322191834449768\n",
      "iteration 10319: loss: 0.23322086036205292\n",
      "iteration 10320: loss: 0.23322002589702606\n",
      "iteration 10321: loss: 0.2332189828157425\n",
      "iteration 10322: loss: 0.23321814835071564\n",
      "iteration 10323: loss: 0.23321712017059326\n",
      "iteration 10324: loss: 0.23321613669395447\n",
      "iteration 10325: loss: 0.23321533203125\n",
      "iteration 10326: loss: 0.23321425914764404\n",
      "iteration 10327: loss: 0.23321345448493958\n",
      "iteration 10328: loss: 0.2332124263048172\n",
      "iteration 10329: loss: 0.2332114726305008\n",
      "iteration 10330: loss: 0.23321044445037842\n",
      "iteration 10331: loss: 0.23320944607257843\n",
      "iteration 10332: loss: 0.23320861160755157\n",
      "iteration 10333: loss: 0.2332076132297516\n",
      "iteration 10334: loss: 0.23320674896240234\n",
      "iteration 10335: loss: 0.23320572078227997\n",
      "iteration 10336: loss: 0.2332046926021576\n",
      "iteration 10337: loss: 0.23320385813713074\n",
      "iteration 10338: loss: 0.23320285975933075\n",
      "iteration 10339: loss: 0.2332020252943039\n",
      "iteration 10340: loss: 0.23320098221302032\n",
      "iteration 10341: loss: 0.23320014774799347\n",
      "iteration 10342: loss: 0.2331990897655487\n",
      "iteration 10343: loss: 0.2331981211900711\n",
      "iteration 10344: loss: 0.23319725692272186\n",
      "iteration 10345: loss: 0.2331962287425995\n",
      "iteration 10346: loss: 0.23319537937641144\n",
      "iteration 10347: loss: 0.23319438099861145\n",
      "iteration 10348: loss: 0.2331935465335846\n",
      "iteration 10349: loss: 0.23319251835346222\n",
      "iteration 10350: loss: 0.23319146037101746\n",
      "iteration 10351: loss: 0.233190655708313\n",
      "iteration 10352: loss: 0.2331896275281906\n",
      "iteration 10353: loss: 0.23318879306316376\n",
      "iteration 10354: loss: 0.23318776488304138\n",
      "iteration 10355: loss: 0.2331867516040802\n",
      "iteration 10356: loss: 0.23318591713905334\n",
      "iteration 10357: loss: 0.23318485915660858\n",
      "iteration 10358: loss: 0.2331840544939041\n",
      "iteration 10359: loss: 0.23318299651145935\n",
      "iteration 10360: loss: 0.23318199813365936\n",
      "iteration 10361: loss: 0.2331811934709549\n",
      "iteration 10362: loss: 0.23318013548851013\n",
      "iteration 10363: loss: 0.2331792563199997\n",
      "iteration 10364: loss: 0.23317837715148926\n",
      "iteration 10365: loss: 0.23317721486091614\n",
      "iteration 10366: loss: 0.23317642509937286\n",
      "iteration 10367: loss: 0.23317542672157288\n",
      "iteration 10368: loss: 0.23317459225654602\n",
      "iteration 10369: loss: 0.23317356407642365\n",
      "iteration 10370: loss: 0.23317253589630127\n",
      "iteration 10371: loss: 0.2331717312335968\n",
      "iteration 10372: loss: 0.23317067325115204\n",
      "iteration 10373: loss: 0.23316983878612518\n",
      "iteration 10374: loss: 0.2331688404083252\n",
      "iteration 10375: loss: 0.23316781222820282\n",
      "iteration 10376: loss: 0.23316696286201477\n",
      "iteration 10377: loss: 0.23316597938537598\n",
      "iteration 10378: loss: 0.23316514492034912\n",
      "iteration 10379: loss: 0.23316411674022675\n",
      "iteration 10380: loss: 0.23316308856010437\n",
      "iteration 10381: loss: 0.23316225409507751\n",
      "iteration 10382: loss: 0.23316121101379395\n",
      "iteration 10383: loss: 0.23316040635108948\n",
      "iteration 10384: loss: 0.2331593781709671\n",
      "iteration 10385: loss: 0.23315837979316711\n",
      "iteration 10386: loss: 0.23315754532814026\n",
      "iteration 10387: loss: 0.2331564724445343\n",
      "iteration 10388: loss: 0.23315568268299103\n",
      "iteration 10389: loss: 0.23315462470054626\n",
      "iteration 10390: loss: 0.23315362632274628\n",
      "iteration 10391: loss: 0.23315277695655823\n",
      "iteration 10392: loss: 0.23315174877643585\n",
      "iteration 10393: loss: 0.2331509292125702\n",
      "iteration 10394: loss: 0.233149915933609\n",
      "iteration 10395: loss: 0.23314885795116425\n",
      "iteration 10396: loss: 0.23314806818962097\n",
      "iteration 10397: loss: 0.2331470251083374\n",
      "iteration 10398: loss: 0.23314619064331055\n",
      "iteration 10399: loss: 0.23314516246318817\n",
      "iteration 10400: loss: 0.2331441342830658\n",
      "iteration 10401: loss: 0.23314329981803894\n",
      "iteration 10402: loss: 0.23314228653907776\n",
      "iteration 10403: loss: 0.233141228556633\n",
      "iteration 10404: loss: 0.23314043879508972\n",
      "iteration 10405: loss: 0.23313939571380615\n",
      "iteration 10406: loss: 0.23313859105110168\n",
      "iteration 10407: loss: 0.23313751816749573\n",
      "iteration 10408: loss: 0.23313653469085693\n",
      "iteration 10409: loss: 0.23313570022583008\n",
      "iteration 10410: loss: 0.2331346720457077\n",
      "iteration 10411: loss: 0.2331337034702301\n",
      "iteration 10412: loss: 0.2331327646970749\n",
      "iteration 10413: loss: 0.23313184082508087\n",
      "iteration 10414: loss: 0.2331310212612152\n",
      "iteration 10415: loss: 0.23312994837760925\n",
      "iteration 10416: loss: 0.23312894999980927\n",
      "iteration 10417: loss: 0.23312802612781525\n",
      "iteration 10418: loss: 0.23312711715698242\n",
      "iteration 10419: loss: 0.2331259697675705\n",
      "iteration 10420: loss: 0.23312518000602722\n",
      "iteration 10421: loss: 0.23312413692474365\n",
      "iteration 10422: loss: 0.23312333226203918\n",
      "iteration 10423: loss: 0.2331223040819168\n",
      "iteration 10424: loss: 0.23312124609947205\n",
      "iteration 10425: loss: 0.2331204116344452\n",
      "iteration 10426: loss: 0.233119398355484\n",
      "iteration 10427: loss: 0.23311838507652283\n",
      "iteration 10428: loss: 0.23311758041381836\n",
      "iteration 10429: loss: 0.2331165373325348\n",
      "iteration 10430: loss: 0.23311570286750793\n",
      "iteration 10431: loss: 0.23311467468738556\n",
      "iteration 10432: loss: 0.23311364650726318\n",
      "iteration 10433: loss: 0.23311281204223633\n",
      "iteration 10434: loss: 0.23311181366443634\n",
      "iteration 10435: loss: 0.23311078548431396\n",
      "iteration 10436: loss: 0.2331099510192871\n",
      "iteration 10437: loss: 0.23310892283916473\n",
      "iteration 10438: loss: 0.23310811817646027\n",
      "iteration 10439: loss: 0.2331070601940155\n",
      "iteration 10440: loss: 0.23310604691505432\n",
      "iteration 10441: loss: 0.23310522735118866\n",
      "iteration 10442: loss: 0.23310422897338867\n",
      "iteration 10443: loss: 0.2331031858921051\n",
      "iteration 10444: loss: 0.23310235142707825\n",
      "iteration 10445: loss: 0.23310132324695587\n",
      "iteration 10446: loss: 0.2331002950668335\n",
      "iteration 10447: loss: 0.23309946060180664\n",
      "iteration 10448: loss: 0.23309846222400665\n",
      "iteration 10449: loss: 0.2330976277589798\n",
      "iteration 10450: loss: 0.23309656977653503\n",
      "iteration 10451: loss: 0.23309557139873505\n",
      "iteration 10452: loss: 0.23309476673603058\n",
      "iteration 10453: loss: 0.2330937385559082\n",
      "iteration 10454: loss: 0.2330927550792694\n",
      "iteration 10455: loss: 0.23309192061424255\n",
      "iteration 10456: loss: 0.23309092223644257\n",
      "iteration 10457: loss: 0.233089879155159\n",
      "iteration 10458: loss: 0.23308904469013214\n",
      "iteration 10459: loss: 0.23308804631233215\n",
      "iteration 10460: loss: 0.23308701813220978\n",
      "iteration 10461: loss: 0.23308618366718292\n",
      "iteration 10462: loss: 0.23308515548706055\n",
      "iteration 10463: loss: 0.23308435082435608\n",
      "iteration 10464: loss: 0.2330833226442337\n",
      "iteration 10465: loss: 0.23308232426643372\n",
      "iteration 10466: loss: 0.23308148980140686\n",
      "iteration 10467: loss: 0.2330804318189621\n",
      "iteration 10468: loss: 0.23307940363883972\n",
      "iteration 10469: loss: 0.23307859897613525\n",
      "iteration 10470: loss: 0.23307757079601288\n",
      "iteration 10471: loss: 0.2330765277147293\n",
      "iteration 10472: loss: 0.23307573795318604\n",
      "iteration 10473: loss: 0.23307470977306366\n",
      "iteration 10474: loss: 0.23307368159294128\n",
      "iteration 10475: loss: 0.23307287693023682\n",
      "iteration 10476: loss: 0.23307180404663086\n",
      "iteration 10477: loss: 0.23307080566883087\n",
      "iteration 10478: loss: 0.23306997120380402\n",
      "iteration 10479: loss: 0.23306897282600403\n",
      "iteration 10480: loss: 0.23306794464588165\n",
      "iteration 10481: loss: 0.233067125082016\n",
      "iteration 10482: loss: 0.2330661118030548\n",
      "iteration 10483: loss: 0.23306505382061005\n",
      "iteration 10484: loss: 0.23306424915790558\n",
      "iteration 10485: loss: 0.23306326568126678\n",
      "iteration 10486: loss: 0.23306235671043396\n",
      "iteration 10487: loss: 0.23306135833263397\n",
      "iteration 10488: loss: 0.2330603301525116\n",
      "iteration 10489: loss: 0.23305952548980713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10490: loss: 0.23305854201316833\n",
      "iteration 10491: loss: 0.23305749893188477\n",
      "iteration 10492: loss: 0.2330566942691803\n",
      "iteration 10493: loss: 0.23305563628673553\n",
      "iteration 10494: loss: 0.23305463790893555\n",
      "iteration 10495: loss: 0.2330538034439087\n",
      "iteration 10496: loss: 0.23305277526378632\n",
      "iteration 10497: loss: 0.23305177688598633\n",
      "iteration 10498: loss: 0.23305097222328186\n",
      "iteration 10499: loss: 0.23305001854896545\n",
      "iteration 10500: loss: 0.2330489456653595\n",
      "iteration 10501: loss: 0.23304812610149384\n",
      "iteration 10502: loss: 0.23304708302021027\n",
      "iteration 10503: loss: 0.2330460548400879\n",
      "iteration 10504: loss: 0.23304525017738342\n",
      "iteration 10505: loss: 0.23304419219493866\n",
      "iteration 10506: loss: 0.23304322361946106\n",
      "iteration 10507: loss: 0.2330423891544342\n",
      "iteration 10508: loss: 0.23304136097431183\n",
      "iteration 10509: loss: 0.23304033279418945\n",
      "iteration 10510: loss: 0.23303930461406708\n",
      "iteration 10511: loss: 0.2330385148525238\n",
      "iteration 10512: loss: 0.23303747177124023\n",
      "iteration 10513: loss: 0.23303647339344025\n",
      "iteration 10514: loss: 0.23303565382957458\n",
      "iteration 10515: loss: 0.23303461074829102\n",
      "iteration 10516: loss: 0.23303361237049103\n",
      "iteration 10517: loss: 0.23303279280662537\n",
      "iteration 10518: loss: 0.23303177952766418\n",
      "iteration 10519: loss: 0.23303072154521942\n",
      "iteration 10520: loss: 0.23302991688251495\n",
      "iteration 10521: loss: 0.23302888870239258\n",
      "iteration 10522: loss: 0.2330278605222702\n",
      "iteration 10523: loss: 0.23302705585956573\n",
      "iteration 10524: loss: 0.23302602767944336\n",
      "iteration 10525: loss: 0.2330249845981598\n",
      "iteration 10526: loss: 0.23302419483661652\n",
      "iteration 10527: loss: 0.23302316665649414\n",
      "iteration 10528: loss: 0.23302216827869415\n",
      "iteration 10529: loss: 0.2330213338136673\n",
      "iteration 10530: loss: 0.23302030563354492\n",
      "iteration 10531: loss: 0.23301927745342255\n",
      "iteration 10532: loss: 0.2330184429883957\n",
      "iteration 10533: loss: 0.2330174446105957\n",
      "iteration 10534: loss: 0.23301640152931213\n",
      "iteration 10535: loss: 0.23301561176776886\n",
      "iteration 10536: loss: 0.23301458358764648\n",
      "iteration 10537: loss: 0.2330135554075241\n",
      "iteration 10538: loss: 0.2330125868320465\n",
      "iteration 10539: loss: 0.23301172256469727\n",
      "iteration 10540: loss: 0.23301072418689728\n",
      "iteration 10541: loss: 0.23300977051258087\n",
      "iteration 10542: loss: 0.23300890624523163\n",
      "iteration 10543: loss: 0.23300786316394806\n",
      "iteration 10544: loss: 0.23300686478614807\n",
      "iteration 10545: loss: 0.2330060452222824\n",
      "iteration 10546: loss: 0.23300501704216003\n",
      "iteration 10547: loss: 0.23300401866436005\n",
      "iteration 10548: loss: 0.233003169298172\n",
      "iteration 10549: loss: 0.23300214111804962\n",
      "iteration 10550: loss: 0.23300114274024963\n",
      "iteration 10551: loss: 0.23300012946128845\n",
      "iteration 10552: loss: 0.2329992949962616\n",
      "iteration 10553: loss: 0.2329982966184616\n",
      "iteration 10554: loss: 0.23299725353717804\n",
      "iteration 10555: loss: 0.23299643397331238\n",
      "iteration 10556: loss: 0.2329954355955124\n",
      "iteration 10557: loss: 0.23299439251422882\n",
      "iteration 10558: loss: 0.23299360275268555\n",
      "iteration 10559: loss: 0.23299257457256317\n",
      "iteration 10560: loss: 0.23299157619476318\n",
      "iteration 10561: loss: 0.2329905778169632\n",
      "iteration 10562: loss: 0.23298971354961395\n",
      "iteration 10563: loss: 0.23298871517181396\n",
      "iteration 10564: loss: 0.2329876720905304\n",
      "iteration 10565: loss: 0.23298683762550354\n",
      "iteration 10566: loss: 0.23298589885234833\n",
      "iteration 10567: loss: 0.23298487067222595\n",
      "iteration 10568: loss: 0.23298406600952148\n",
      "iteration 10569: loss: 0.2329830676317215\n",
      "iteration 10570: loss: 0.23298200964927673\n",
      "iteration 10571: loss: 0.23298120498657227\n",
      "iteration 10572: loss: 0.2329801768064499\n",
      "iteration 10573: loss: 0.2329792082309723\n",
      "iteration 10574: loss: 0.23297818005084991\n",
      "iteration 10575: loss: 0.23297734558582306\n",
      "iteration 10576: loss: 0.23297634720802307\n",
      "iteration 10577: loss: 0.2329753190279007\n",
      "iteration 10578: loss: 0.23297448456287384\n",
      "iteration 10579: loss: 0.23297348618507385\n",
      "iteration 10580: loss: 0.2329724282026291\n",
      "iteration 10581: loss: 0.23297163844108582\n",
      "iteration 10582: loss: 0.23297062516212463\n",
      "iteration 10583: loss: 0.23296961188316345\n",
      "iteration 10584: loss: 0.23296861350536346\n",
      "iteration 10585: loss: 0.23296785354614258\n",
      "iteration 10586: loss: 0.232966810464859\n",
      "iteration 10587: loss: 0.23296579718589783\n",
      "iteration 10588: loss: 0.23296479880809784\n",
      "iteration 10589: loss: 0.2329639494419098\n",
      "iteration 10590: loss: 0.2329629361629486\n",
      "iteration 10591: loss: 0.23296192288398743\n",
      "iteration 10592: loss: 0.23296110332012177\n",
      "iteration 10593: loss: 0.2329600751399994\n",
      "iteration 10594: loss: 0.2329590618610382\n",
      "iteration 10595: loss: 0.23295824229717255\n",
      "iteration 10596: loss: 0.23295721411705017\n",
      "iteration 10597: loss: 0.2329561710357666\n",
      "iteration 10598: loss: 0.2329551875591278\n",
      "iteration 10599: loss: 0.23295435309410095\n",
      "iteration 10600: loss: 0.23295339941978455\n",
      "iteration 10601: loss: 0.23295235633850098\n",
      "iteration 10602: loss: 0.232951357960701\n",
      "iteration 10603: loss: 0.23295052349567413\n",
      "iteration 10604: loss: 0.23294949531555176\n",
      "iteration 10605: loss: 0.23294849693775177\n",
      "iteration 10606: loss: 0.23294749855995178\n",
      "iteration 10607: loss: 0.23294663429260254\n",
      "iteration 10608: loss: 0.23294560611248016\n",
      "iteration 10609: loss: 0.23294463753700256\n",
      "iteration 10610: loss: 0.23294377326965332\n",
      "iteration 10611: loss: 0.23294274508953094\n",
      "iteration 10612: loss: 0.23294177651405334\n",
      "iteration 10613: loss: 0.2329409122467041\n",
      "iteration 10614: loss: 0.2329399138689041\n",
      "iteration 10615: loss: 0.23293891549110413\n",
      "iteration 10616: loss: 0.23293788731098175\n",
      "iteration 10617: loss: 0.2329370528459549\n",
      "iteration 10618: loss: 0.23293614387512207\n",
      "iteration 10619: loss: 0.23293516039848328\n",
      "iteration 10620: loss: 0.2329341471195221\n",
      "iteration 10621: loss: 0.23293331265449524\n",
      "iteration 10622: loss: 0.23293228447437286\n",
      "iteration 10623: loss: 0.23293128609657288\n",
      "iteration 10624: loss: 0.2329304963350296\n",
      "iteration 10625: loss: 0.23292943835258484\n",
      "iteration 10626: loss: 0.23292843997478485\n",
      "iteration 10627: loss: 0.23292741179466248\n",
      "iteration 10628: loss: 0.2329266518354416\n",
      "iteration 10629: loss: 0.2329256534576416\n",
      "iteration 10630: loss: 0.23292462527751923\n",
      "iteration 10631: loss: 0.2329237163066864\n",
      "iteration 10632: loss: 0.23292291164398193\n",
      "iteration 10633: loss: 0.23292188346385956\n",
      "iteration 10634: loss: 0.23292088508605957\n",
      "iteration 10635: loss: 0.2329198569059372\n",
      "iteration 10636: loss: 0.23291902244091034\n",
      "iteration 10637: loss: 0.23291799426078796\n",
      "iteration 10638: loss: 0.23291698098182678\n",
      "iteration 10639: loss: 0.23291616141796112\n",
      "iteration 10640: loss: 0.23291511833667755\n",
      "iteration 10641: loss: 0.23291413486003876\n",
      "iteration 10642: loss: 0.23291313648223877\n",
      "iteration 10643: loss: 0.23291230201721191\n",
      "iteration 10644: loss: 0.23291125893592834\n",
      "iteration 10645: loss: 0.23291024565696716\n",
      "iteration 10646: loss: 0.23290927708148956\n",
      "iteration 10647: loss: 0.2329084426164627\n",
      "iteration 10648: loss: 0.23290744423866272\n",
      "iteration 10649: loss: 0.23290643095970154\n",
      "iteration 10650: loss: 0.23290543258190155\n",
      "iteration 10651: loss: 0.23290443420410156\n",
      "iteration 10652: loss: 0.23290356993675232\n",
      "iteration 10653: loss: 0.23290260136127472\n",
      "iteration 10654: loss: 0.23290154337882996\n",
      "iteration 10655: loss: 0.23290054500102997\n",
      "iteration 10656: loss: 0.2328997403383255\n",
      "iteration 10657: loss: 0.23289871215820312\n",
      "iteration 10658: loss: 0.23289766907691956\n",
      "iteration 10659: loss: 0.23289687931537628\n",
      "iteration 10660: loss: 0.23289594054222107\n",
      "iteration 10661: loss: 0.23289494216442108\n",
      "iteration 10662: loss: 0.2328939139842987\n",
      "iteration 10663: loss: 0.23289307951927185\n",
      "iteration 10664: loss: 0.23289206624031067\n",
      "iteration 10665: loss: 0.2328910529613495\n",
      "iteration 10666: loss: 0.2328900843858719\n",
      "iteration 10667: loss: 0.23288922011852264\n",
      "iteration 10668: loss: 0.23288825154304504\n",
      "iteration 10669: loss: 0.23288723826408386\n",
      "iteration 10670: loss: 0.2328861951828003\n",
      "iteration 10671: loss: 0.23288539052009583\n",
      "iteration 10672: loss: 0.23288440704345703\n",
      "iteration 10673: loss: 0.23288340866565704\n",
      "iteration 10674: loss: 0.23288241028785706\n",
      "iteration 10675: loss: 0.2328815758228302\n",
      "iteration 10676: loss: 0.2328805923461914\n",
      "iteration 10677: loss: 0.23287959396839142\n",
      "iteration 10678: loss: 0.23287853598594666\n",
      "iteration 10679: loss: 0.2328777313232422\n",
      "iteration 10680: loss: 0.2328767329454422\n",
      "iteration 10681: loss: 0.23287570476531982\n",
      "iteration 10682: loss: 0.23287467658519745\n",
      "iteration 10683: loss: 0.2328738421201706\n",
      "iteration 10684: loss: 0.232872873544693\n",
      "iteration 10685: loss: 0.23287181556224823\n",
      "iteration 10686: loss: 0.23287081718444824\n",
      "iteration 10687: loss: 0.23286978900432587\n",
      "iteration 10688: loss: 0.232868954539299\n",
      "iteration 10689: loss: 0.2328679859638214\n",
      "iteration 10690: loss: 0.23286697268486023\n",
      "iteration 10691: loss: 0.23286597430706024\n",
      "iteration 10692: loss: 0.2328651398420334\n",
      "iteration 10693: loss: 0.23286423087120056\n",
      "iteration 10694: loss: 0.23286326229572296\n",
      "iteration 10695: loss: 0.23286223411560059\n",
      "iteration 10696: loss: 0.23286142945289612\n",
      "iteration 10697: loss: 0.23286040127277374\n",
      "iteration 10698: loss: 0.23285937309265137\n",
      "iteration 10699: loss: 0.23285837471485138\n",
      "iteration 10700: loss: 0.2328573763370514\n",
      "iteration 10701: loss: 0.23285655677318573\n",
      "iteration 10702: loss: 0.23285551369190216\n",
      "iteration 10703: loss: 0.23285453021526337\n",
      "iteration 10704: loss: 0.232853502035141\n",
      "iteration 10705: loss: 0.23285266757011414\n",
      "iteration 10706: loss: 0.23285166919231415\n",
      "iteration 10707: loss: 0.23285070061683655\n",
      "iteration 10708: loss: 0.23284967243671417\n",
      "iteration 10709: loss: 0.23284883797168732\n",
      "iteration 10710: loss: 0.23284780979156494\n",
      "iteration 10711: loss: 0.23284682631492615\n",
      "iteration 10712: loss: 0.23284581303596497\n",
      "iteration 10713: loss: 0.23284482955932617\n",
      "iteration 10714: loss: 0.23284396529197693\n",
      "iteration 10715: loss: 0.23284301161766052\n",
      "iteration 10716: loss: 0.23284201323986053\n",
      "iteration 10717: loss: 0.23284098505973816\n",
      "iteration 10718: loss: 0.23284001648426056\n",
      "iteration 10719: loss: 0.2328391820192337\n",
      "iteration 10720: loss: 0.23283818364143372\n",
      "iteration 10721: loss: 0.23283717036247253\n",
      "iteration 10722: loss: 0.23283624649047852\n",
      "iteration 10723: loss: 0.23283544182777405\n",
      "iteration 10724: loss: 0.23283442854881287\n",
      "iteration 10725: loss: 0.23283343017101288\n",
      "iteration 10726: loss: 0.2328324019908905\n",
      "iteration 10727: loss: 0.23283155262470245\n",
      "iteration 10728: loss: 0.23283056914806366\n",
      "iteration 10729: loss: 0.23282954096794128\n",
      "iteration 10730: loss: 0.2328285425901413\n",
      "iteration 10731: loss: 0.2328275442123413\n",
      "iteration 10732: loss: 0.23282670974731445\n",
      "iteration 10733: loss: 0.23282568156719208\n",
      "iteration 10734: loss: 0.23282471299171448\n",
      "iteration 10735: loss: 0.2328236848115921\n",
      "iteration 10736: loss: 0.23282289505004883\n",
      "iteration 10737: loss: 0.23282186686992645\n",
      "iteration 10738: loss: 0.23282083868980408\n",
      "iteration 10739: loss: 0.23281988501548767\n",
      "iteration 10740: loss: 0.2328188419342041\n",
      "iteration 10741: loss: 0.23281803727149963\n",
      "iteration 10742: loss: 0.23281700909137726\n",
      "iteration 10743: loss: 0.23281602561473846\n",
      "iteration 10744: loss: 0.23281502723693848\n",
      "iteration 10745: loss: 0.2328139990568161\n",
      "iteration 10746: loss: 0.23281319439411163\n",
      "iteration 10747: loss: 0.23281219601631165\n",
      "iteration 10748: loss: 0.23281116783618927\n",
      "iteration 10749: loss: 0.23281022906303406\n",
      "iteration 10750: loss: 0.23280926048755646\n",
      "iteration 10751: loss: 0.2328084260225296\n",
      "iteration 10752: loss: 0.23280739784240723\n",
      "iteration 10753: loss: 0.23280641436576843\n",
      "iteration 10754: loss: 0.23280540108680725\n",
      "iteration 10755: loss: 0.23280438780784607\n",
      "iteration 10756: loss: 0.2328035831451416\n",
      "iteration 10757: loss: 0.23280255496501923\n",
      "iteration 10758: loss: 0.23280158638954163\n",
      "iteration 10759: loss: 0.23280060291290283\n",
      "iteration 10760: loss: 0.23279960453510284\n",
      "iteration 10761: loss: 0.23279878497123718\n",
      "iteration 10762: loss: 0.232797771692276\n",
      "iteration 10763: loss: 0.23279674351215363\n",
      "iteration 10764: loss: 0.23279574513435364\n",
      "iteration 10765: loss: 0.23279473185539246\n",
      "iteration 10766: loss: 0.23279395699501038\n",
      "iteration 10767: loss: 0.2327929437160492\n",
      "iteration 10768: loss: 0.232791930437088\n",
      "iteration 10769: loss: 0.23279091715812683\n",
      "iteration 10770: loss: 0.23278990387916565\n",
      "iteration 10771: loss: 0.2327890843153\n",
      "iteration 10772: loss: 0.23278813064098358\n",
      "iteration 10773: loss: 0.2327871322631836\n",
      "iteration 10774: loss: 0.2327861338853836\n",
      "iteration 10775: loss: 0.2327851504087448\n",
      "iteration 10776: loss: 0.23278434574604034\n",
      "iteration 10777: loss: 0.23278331756591797\n",
      "iteration 10778: loss: 0.23278231918811798\n",
      "iteration 10779: loss: 0.232781320810318\n",
      "iteration 10780: loss: 0.2327803373336792\n",
      "iteration 10781: loss: 0.23277930915355682\n",
      "iteration 10782: loss: 0.23277847468852997\n",
      "iteration 10783: loss: 0.23277747631072998\n",
      "iteration 10784: loss: 0.23277647793293\n",
      "iteration 10785: loss: 0.23277544975280762\n",
      "iteration 10786: loss: 0.23277445137500763\n",
      "iteration 10787: loss: 0.23277363181114197\n",
      "iteration 10788: loss: 0.2327726185321808\n",
      "iteration 10789: loss: 0.232771635055542\n",
      "iteration 10790: loss: 0.23277060687541962\n",
      "iteration 10791: loss: 0.23276957869529724\n",
      "iteration 10792: loss: 0.23276874423027039\n",
      "iteration 10793: loss: 0.23276789486408234\n",
      "iteration 10794: loss: 0.23276686668395996\n",
      "iteration 10795: loss: 0.23276586830615997\n",
      "iteration 10796: loss: 0.23276486992835999\n",
      "iteration 10797: loss: 0.2327638566493988\n",
      "iteration 10798: loss: 0.23276305198669434\n",
      "iteration 10799: loss: 0.23276205360889435\n",
      "iteration 10800: loss: 0.23276105523109436\n",
      "iteration 10801: loss: 0.2327599972486496\n",
      "iteration 10802: loss: 0.2327589988708496\n",
      "iteration 10803: loss: 0.23275825381278992\n",
      "iteration 10804: loss: 0.23275724053382874\n",
      "iteration 10805: loss: 0.23275622725486755\n",
      "iteration 10806: loss: 0.23275521397590637\n",
      "iteration 10807: loss: 0.2327542006969452\n",
      "iteration 10808: loss: 0.2327532023191452\n",
      "iteration 10809: loss: 0.23275239765644073\n",
      "iteration 10810: loss: 0.23275139927864075\n",
      "iteration 10811: loss: 0.23275046050548553\n",
      "iteration 10812: loss: 0.23274949193000793\n",
      "iteration 10813: loss: 0.23274843394756317\n",
      "iteration 10814: loss: 0.2327476441860199\n",
      "iteration 10815: loss: 0.2327466458082199\n",
      "iteration 10816: loss: 0.23274564743041992\n",
      "iteration 10817: loss: 0.23274464905261993\n",
      "iteration 10818: loss: 0.23274365067481995\n",
      "iteration 10819: loss: 0.23274263739585876\n",
      "iteration 10820: loss: 0.23274186253547668\n",
      "iteration 10821: loss: 0.2327408343553543\n",
      "iteration 10822: loss: 0.23273983597755432\n",
      "iteration 10823: loss: 0.23273877799510956\n",
      "iteration 10824: loss: 0.23273780941963196\n",
      "iteration 10825: loss: 0.23273678123950958\n",
      "iteration 10826: loss: 0.2327357828617096\n",
      "iteration 10827: loss: 0.23273496329784393\n",
      "iteration 10828: loss: 0.23273396492004395\n",
      "iteration 10829: loss: 0.23273296654224396\n",
      "iteration 10830: loss: 0.23273196816444397\n",
      "iteration 10831: loss: 0.2327311486005783\n",
      "iteration 10832: loss: 0.23273015022277832\n",
      "iteration 10833: loss: 0.23272931575775146\n",
      "iteration 10834: loss: 0.23272833228111267\n",
      "iteration 10835: loss: 0.23272733390331268\n",
      "iteration 10836: loss: 0.2327263057231903\n",
      "iteration 10837: loss: 0.2327253371477127\n",
      "iteration 10838: loss: 0.23272433876991272\n",
      "iteration 10839: loss: 0.23272351920604706\n",
      "iteration 10840: loss: 0.2327224761247635\n",
      "iteration 10841: loss: 0.23272152245044708\n",
      "iteration 10842: loss: 0.2327204942703247\n",
      "iteration 10843: loss: 0.23271949589252472\n",
      "iteration 10844: loss: 0.23271849751472473\n",
      "iteration 10845: loss: 0.23271766304969788\n",
      "iteration 10846: loss: 0.23271670937538147\n",
      "iteration 10847: loss: 0.2327156960964203\n",
      "iteration 10848: loss: 0.2327146977186203\n",
      "iteration 10849: loss: 0.2327136993408203\n",
      "iteration 10850: loss: 0.23271270096302032\n",
      "iteration 10851: loss: 0.23271198570728302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10852: loss: 0.23271098732948303\n",
      "iteration 10853: loss: 0.23270997405052185\n",
      "iteration 10854: loss: 0.23270897567272186\n",
      "iteration 10855: loss: 0.23270797729492188\n",
      "iteration 10856: loss: 0.2327069491147995\n",
      "iteration 10857: loss: 0.2327059805393219\n",
      "iteration 10858: loss: 0.23270516097545624\n",
      "iteration 10859: loss: 0.23270413279533386\n",
      "iteration 10860: loss: 0.23270316421985626\n",
      "iteration 10861: loss: 0.23270216584205627\n",
      "iteration 10862: loss: 0.2327011376619339\n",
      "iteration 10863: loss: 0.2327001392841339\n",
      "iteration 10864: loss: 0.23269911110401154\n",
      "iteration 10865: loss: 0.23269835114479065\n",
      "iteration 10866: loss: 0.23269732296466827\n",
      "iteration 10867: loss: 0.2326963245868683\n",
      "iteration 10868: loss: 0.2326952964067459\n",
      "iteration 10869: loss: 0.23269441723823547\n",
      "iteration 10870: loss: 0.2326933890581131\n",
      "iteration 10871: loss: 0.2326924055814743\n",
      "iteration 10872: loss: 0.23269157111644745\n",
      "iteration 10873: loss: 0.23269057273864746\n",
      "iteration 10874: loss: 0.23268957436084747\n",
      "iteration 10875: loss: 0.23268857598304749\n",
      "iteration 10876: loss: 0.2326875627040863\n",
      "iteration 10877: loss: 0.23268656432628632\n",
      "iteration 10878: loss: 0.23268572986125946\n",
      "iteration 10879: loss: 0.23268476128578186\n",
      "iteration 10880: loss: 0.23268373310565948\n",
      "iteration 10881: loss: 0.2326827496290207\n",
      "iteration 10882: loss: 0.2326817512512207\n",
      "iteration 10883: loss: 0.2326807677745819\n",
      "iteration 10884: loss: 0.23267975449562073\n",
      "iteration 10885: loss: 0.23267877101898193\n",
      "iteration 10886: loss: 0.23267793655395508\n",
      "iteration 10887: loss: 0.23267695307731628\n",
      "iteration 10888: loss: 0.23267599940299988\n",
      "iteration 10889: loss: 0.23267504572868347\n",
      "iteration 10890: loss: 0.23267407715320587\n",
      "iteration 10891: loss: 0.23267309367656708\n",
      "iteration 10892: loss: 0.2326720654964447\n",
      "iteration 10893: loss: 0.23267123103141785\n",
      "iteration 10894: loss: 0.23267026245594025\n",
      "iteration 10895: loss: 0.23266923427581787\n",
      "iteration 10896: loss: 0.23266825079917908\n",
      "iteration 10897: loss: 0.2326672375202179\n",
      "iteration 10898: loss: 0.2326662540435791\n",
      "iteration 10899: loss: 0.2326652556657791\n",
      "iteration 10900: loss: 0.23266425728797913\n",
      "iteration 10901: loss: 0.23266324400901794\n",
      "iteration 10902: loss: 0.2326624095439911\n",
      "iteration 10903: loss: 0.2326614409685135\n",
      "iteration 10904: loss: 0.23266053199768066\n",
      "iteration 10905: loss: 0.23265954852104187\n",
      "iteration 10906: loss: 0.23265855014324188\n",
      "iteration 10907: loss: 0.2326575219631195\n",
      "iteration 10908: loss: 0.23265652358531952\n",
      "iteration 10909: loss: 0.23265571892261505\n",
      "iteration 10910: loss: 0.23265472054481506\n",
      "iteration 10911: loss: 0.23265373706817627\n",
      "iteration 10912: loss: 0.2326527088880539\n",
      "iteration 10913: loss: 0.23265168070793152\n",
      "iteration 10914: loss: 0.23265068233013153\n",
      "iteration 10915: loss: 0.23264968395233154\n",
      "iteration 10916: loss: 0.23264892399311066\n",
      "iteration 10917: loss: 0.23264789581298828\n",
      "iteration 10918: loss: 0.2326468974351883\n",
      "iteration 10919: loss: 0.2326458990573883\n",
      "iteration 10920: loss: 0.23264499008655548\n",
      "iteration 10921: loss: 0.2326439917087555\n",
      "iteration 10922: loss: 0.2326429784297943\n",
      "iteration 10923: loss: 0.23264198005199432\n",
      "iteration 10924: loss: 0.23264098167419434\n",
      "iteration 10925: loss: 0.23264019191265106\n",
      "iteration 10926: loss: 0.2326391190290451\n",
      "iteration 10927: loss: 0.23263812065124512\n",
      "iteration 10928: loss: 0.2326371669769287\n",
      "iteration 10929: loss: 0.23263616859912872\n",
      "iteration 10930: loss: 0.23263518512248993\n",
      "iteration 10931: loss: 0.23263415694236755\n",
      "iteration 10932: loss: 0.23263315856456757\n",
      "iteration 10933: loss: 0.23263220489025116\n",
      "iteration 10934: loss: 0.2326314002275467\n",
      "iteration 10935: loss: 0.2326304167509079\n",
      "iteration 10936: loss: 0.23262949287891388\n",
      "iteration 10937: loss: 0.2326284945011139\n",
      "iteration 10938: loss: 0.2326275110244751\n",
      "iteration 10939: loss: 0.2326265126466751\n",
      "iteration 10940: loss: 0.23262551426887512\n",
      "iteration 10941: loss: 0.23262448608875275\n",
      "iteration 10942: loss: 0.23262366652488708\n",
      "iteration 10943: loss: 0.2326226681470871\n",
      "iteration 10944: loss: 0.2326216697692871\n",
      "iteration 10945: loss: 0.23262067139148712\n",
      "iteration 10946: loss: 0.23261967301368713\n",
      "iteration 10947: loss: 0.23261868953704834\n",
      "iteration 10948: loss: 0.23261766135692596\n",
      "iteration 10949: loss: 0.23261675238609314\n",
      "iteration 10950: loss: 0.23261579871177673\n",
      "iteration 10951: loss: 0.23261483013629913\n",
      "iteration 10952: loss: 0.23261380195617676\n",
      "iteration 10953: loss: 0.23261301219463348\n",
      "iteration 10954: loss: 0.2326119840145111\n",
      "iteration 10955: loss: 0.2326110154390335\n",
      "iteration 10956: loss: 0.23260998725891113\n",
      "iteration 10957: loss: 0.23260898888111115\n",
      "iteration 10958: loss: 0.23260799050331116\n",
      "iteration 10959: loss: 0.23260697722434998\n",
      "iteration 10960: loss: 0.23260597884655\n",
      "iteration 10961: loss: 0.23260517418384552\n",
      "iteration 10962: loss: 0.23260417580604553\n",
      "iteration 10963: loss: 0.23260319232940674\n",
      "iteration 10964: loss: 0.23260216414928436\n",
      "iteration 10965: loss: 0.23260119557380676\n",
      "iteration 10966: loss: 0.23260030150413513\n",
      "iteration 10967: loss: 0.23259928822517395\n",
      "iteration 10968: loss: 0.23259827494621277\n",
      "iteration 10969: loss: 0.23259730637073517\n",
      "iteration 10970: loss: 0.2325962781906128\n",
      "iteration 10971: loss: 0.232595294713974\n",
      "iteration 10972: loss: 0.23259451985359192\n",
      "iteration 10973: loss: 0.23259349167346954\n",
      "iteration 10974: loss: 0.23259250819683075\n",
      "iteration 10975: loss: 0.23259148001670837\n",
      "iteration 10976: loss: 0.23259055614471436\n",
      "iteration 10977: loss: 0.23258955776691437\n",
      "iteration 10978: loss: 0.23258855938911438\n",
      "iteration 10979: loss: 0.232587531208992\n",
      "iteration 10980: loss: 0.2325865477323532\n",
      "iteration 10981: loss: 0.23258551955223083\n",
      "iteration 10982: loss: 0.2325848639011383\n",
      "iteration 10983: loss: 0.23258383572101593\n",
      "iteration 10984: loss: 0.23258285224437714\n",
      "iteration 10985: loss: 0.23258185386657715\n",
      "iteration 10986: loss: 0.23258085548877716\n",
      "iteration 10987: loss: 0.23257982730865479\n",
      "iteration 10988: loss: 0.232578843832016\n",
      "iteration 10989: loss: 0.2325778752565384\n",
      "iteration 10990: loss: 0.23257684707641602\n",
      "iteration 10991: loss: 0.23257586359977722\n",
      "iteration 10992: loss: 0.23257486522197723\n",
      "iteration 10993: loss: 0.23257403075695038\n",
      "iteration 10994: loss: 0.23257306218147278\n",
      "iteration 10995: loss: 0.23257207870483398\n",
      "iteration 10996: loss: 0.23257103562355042\n",
      "iteration 10997: loss: 0.23257014155387878\n",
      "iteration 10998: loss: 0.23256917297840118\n",
      "iteration 10999: loss: 0.2325681746006012\n",
      "iteration 11000: loss: 0.2325671911239624\n",
      "iteration 11001: loss: 0.23256619274616241\n",
      "iteration 11002: loss: 0.23256519436836243\n",
      "iteration 11003: loss: 0.23256418108940125\n",
      "iteration 11004: loss: 0.23256318271160126\n",
      "iteration 11005: loss: 0.23256215453147888\n",
      "iteration 11006: loss: 0.2325613796710968\n",
      "iteration 11007: loss: 0.232560396194458\n",
      "iteration 11008: loss: 0.23255936801433563\n",
      "iteration 11009: loss: 0.23255836963653564\n",
      "iteration 11010: loss: 0.2325575053691864\n",
      "iteration 11011: loss: 0.2325565069913864\n",
      "iteration 11012: loss: 0.23255550861358643\n",
      "iteration 11013: loss: 0.23255451023578644\n",
      "iteration 11014: loss: 0.23255351185798645\n",
      "iteration 11015: loss: 0.23255249857902527\n",
      "iteration 11016: loss: 0.23255153000354767\n",
      "iteration 11017: loss: 0.23255062103271484\n",
      "iteration 11018: loss: 0.23254962265491486\n",
      "iteration 11019: loss: 0.23254866898059845\n",
      "iteration 11020: loss: 0.23254768550395966\n",
      "iteration 11021: loss: 0.2325468808412552\n",
      "iteration 11022: loss: 0.2325458824634552\n",
      "iteration 11023: loss: 0.23254485428333282\n",
      "iteration 11024: loss: 0.2325439751148224\n",
      "iteration 11025: loss: 0.2325429916381836\n",
      "iteration 11026: loss: 0.2325419932603836\n",
      "iteration 11027: loss: 0.2325410097837448\n",
      "iteration 11028: loss: 0.23253998160362244\n",
      "iteration 11029: loss: 0.23253901302814484\n",
      "iteration 11030: loss: 0.23253801465034485\n",
      "iteration 11031: loss: 0.23253703117370605\n",
      "iteration 11032: loss: 0.23253603279590607\n",
      "iteration 11033: loss: 0.23253503441810608\n",
      "iteration 11034: loss: 0.23253405094146729\n",
      "iteration 11035: loss: 0.2325330525636673\n",
      "iteration 11036: loss: 0.2325320541858673\n",
      "iteration 11037: loss: 0.23253126442432404\n",
      "iteration 11038: loss: 0.23253020644187927\n",
      "iteration 11039: loss: 0.23252935707569122\n",
      "iteration 11040: loss: 0.23252837359905243\n",
      "iteration 11041: loss: 0.23252734541893005\n",
      "iteration 11042: loss: 0.23252637684345245\n",
      "iteration 11043: loss: 0.23252539336681366\n",
      "iteration 11044: loss: 0.23252439498901367\n",
      "iteration 11045: loss: 0.23252339661121368\n",
      "iteration 11046: loss: 0.2325223982334137\n",
      "iteration 11047: loss: 0.23252137005329132\n",
      "iteration 11048: loss: 0.23252038657665253\n",
      "iteration 11049: loss: 0.23251938819885254\n",
      "iteration 11050: loss: 0.23251838982105255\n",
      "iteration 11051: loss: 0.23251740634441376\n",
      "iteration 11052: loss: 0.2325166016817093\n",
      "iteration 11053: loss: 0.23251572251319885\n",
      "iteration 11054: loss: 0.23251470923423767\n",
      "iteration 11055: loss: 0.23251371085643768\n",
      "iteration 11056: loss: 0.2325127124786377\n",
      "iteration 11057: loss: 0.2325117588043213\n",
      "iteration 11058: loss: 0.2325107604265213\n",
      "iteration 11059: loss: 0.2325097620487213\n",
      "iteration 11060: loss: 0.23250873386859894\n",
      "iteration 11061: loss: 0.23250775039196014\n",
      "iteration 11062: loss: 0.23250675201416016\n",
      "iteration 11063: loss: 0.23250579833984375\n",
      "iteration 11064: loss: 0.23250481486320496\n",
      "iteration 11065: loss: 0.23250380158424377\n",
      "iteration 11066: loss: 0.23250284790992737\n",
      "iteration 11067: loss: 0.23250193893909454\n",
      "iteration 11068: loss: 0.23250094056129456\n",
      "iteration 11069: loss: 0.23249992728233337\n",
      "iteration 11070: loss: 0.2324989289045334\n",
      "iteration 11071: loss: 0.2324979603290558\n",
      "iteration 11072: loss: 0.23249714076519012\n",
      "iteration 11073: loss: 0.23249617218971252\n",
      "iteration 11074: loss: 0.23249514400959015\n",
      "iteration 11075: loss: 0.23249420523643494\n",
      "iteration 11076: loss: 0.23249316215515137\n",
      "iteration 11077: loss: 0.23249216377735138\n",
      "iteration 11078: loss: 0.23249118030071259\n",
      "iteration 11079: loss: 0.23249027132987976\n",
      "iteration 11080: loss: 0.23248930275440216\n",
      "iteration 11081: loss: 0.23248831927776337\n",
      "iteration 11082: loss: 0.23248732089996338\n",
      "iteration 11083: loss: 0.2324863225221634\n",
      "iteration 11084: loss: 0.2324853390455246\n",
      "iteration 11085: loss: 0.2324843406677246\n",
      "iteration 11086: loss: 0.23248334228992462\n",
      "iteration 11087: loss: 0.23248238861560822\n",
      "iteration 11088: loss: 0.23248133063316345\n",
      "iteration 11089: loss: 0.23248037695884705\n",
      "iteration 11090: loss: 0.23247948288917542\n",
      "iteration 11091: loss: 0.23247846961021423\n",
      "iteration 11092: loss: 0.23247747123241425\n",
      "iteration 11093: loss: 0.23247647285461426\n",
      "iteration 11094: loss: 0.23247547447681427\n",
      "iteration 11095: loss: 0.23247449100017548\n",
      "iteration 11096: loss: 0.2324734926223755\n",
      "iteration 11097: loss: 0.2324724942445755\n",
      "iteration 11098: loss: 0.2324715107679367\n",
      "iteration 11099: loss: 0.23247070610523224\n",
      "iteration 11100: loss: 0.23246970772743225\n",
      "iteration 11101: loss: 0.23246872425079346\n",
      "iteration 11102: loss: 0.23246769607067108\n",
      "iteration 11103: loss: 0.23246684670448303\n",
      "iteration 11104: loss: 0.23246583342552185\n",
      "iteration 11105: loss: 0.23246486485004425\n",
      "iteration 11106: loss: 0.23246391117572784\n",
      "iteration 11107: loss: 0.23246291279792786\n",
      "iteration 11108: loss: 0.23246192932128906\n",
      "iteration 11109: loss: 0.2324609011411667\n",
      "iteration 11110: loss: 0.23245994746685028\n",
      "iteration 11111: loss: 0.2324589192867279\n",
      "iteration 11112: loss: 0.23245792090892792\n",
      "iteration 11113: loss: 0.23245695233345032\n",
      "iteration 11114: loss: 0.23245593905448914\n",
      "iteration 11115: loss: 0.23245497047901154\n",
      "iteration 11116: loss: 0.2324540913105011\n",
      "iteration 11117: loss: 0.23245307803153992\n",
      "iteration 11118: loss: 0.23245207965373993\n",
      "iteration 11119: loss: 0.23245111107826233\n",
      "iteration 11120: loss: 0.23245009779930115\n",
      "iteration 11121: loss: 0.23244914412498474\n",
      "iteration 11122: loss: 0.23244813084602356\n",
      "iteration 11123: loss: 0.23244711756706238\n",
      "iteration 11124: loss: 0.2324461191892624\n",
      "iteration 11125: loss: 0.2324451506137848\n",
      "iteration 11126: loss: 0.232444167137146\n",
      "iteration 11127: loss: 0.2324431836605072\n",
      "iteration 11128: loss: 0.23244225978851318\n",
      "iteration 11129: loss: 0.2324412763118744\n",
      "iteration 11130: loss: 0.2324402779340744\n",
      "iteration 11131: loss: 0.2324393093585968\n",
      "iteration 11132: loss: 0.232438325881958\n",
      "iteration 11133: loss: 0.23243749141693115\n",
      "iteration 11134: loss: 0.23243653774261475\n",
      "iteration 11135: loss: 0.23243550956249237\n",
      "iteration 11136: loss: 0.23243454098701477\n",
      "iteration 11137: loss: 0.2324335277080536\n",
      "iteration 11138: loss: 0.2324325293302536\n",
      "iteration 11139: loss: 0.23243150115013123\n",
      "iteration 11140: loss: 0.23243066668510437\n",
      "iteration 11141: loss: 0.23242966830730438\n",
      "iteration 11142: loss: 0.2324286699295044\n",
      "iteration 11143: loss: 0.2324276864528656\n",
      "iteration 11144: loss: 0.2324266880750656\n",
      "iteration 11145: loss: 0.23242568969726562\n",
      "iteration 11146: loss: 0.23242469131946564\n",
      "iteration 11147: loss: 0.23242370784282684\n",
      "iteration 11148: loss: 0.23242273926734924\n",
      "iteration 11149: loss: 0.23242178559303284\n",
      "iteration 11150: loss: 0.23242077231407166\n",
      "iteration 11151: loss: 0.23241980373859406\n",
      "iteration 11152: loss: 0.23241892457008362\n",
      "iteration 11153: loss: 0.23241794109344482\n",
      "iteration 11154: loss: 0.23241695761680603\n",
      "iteration 11155: loss: 0.23241595923900604\n",
      "iteration 11156: loss: 0.23241496086120605\n",
      "iteration 11157: loss: 0.23241397738456726\n",
      "iteration 11158: loss: 0.23241300880908966\n",
      "iteration 11159: loss: 0.23241202533245087\n",
      "iteration 11160: loss: 0.2324109971523285\n",
      "iteration 11161: loss: 0.23241004347801208\n",
      "iteration 11162: loss: 0.2324090301990509\n",
      "iteration 11163: loss: 0.2324080765247345\n",
      "iteration 11164: loss: 0.23240713775157928\n",
      "iteration 11165: loss: 0.23240616917610168\n",
      "iteration 11166: loss: 0.2324051409959793\n",
      "iteration 11167: loss: 0.23240415751934052\n",
      "iteration 11168: loss: 0.23240318894386292\n",
      "iteration 11169: loss: 0.2324022352695465\n",
      "iteration 11170: loss: 0.23240122199058533\n",
      "iteration 11171: loss: 0.23240025341510773\n",
      "iteration 11172: loss: 0.23239922523498535\n",
      "iteration 11173: loss: 0.23239827156066895\n",
      "iteration 11174: loss: 0.23239727318286896\n",
      "iteration 11175: loss: 0.23239627480506897\n",
      "iteration 11176: loss: 0.23239532113075256\n",
      "iteration 11177: loss: 0.2323942929506302\n",
      "iteration 11178: loss: 0.23239342868328094\n",
      "iteration 11179: loss: 0.23239243030548096\n",
      "iteration 11180: loss: 0.23239140212535858\n",
      "iteration 11181: loss: 0.23239044845104218\n",
      "iteration 11182: loss: 0.2323894500732422\n",
      "iteration 11183: loss: 0.2323884665966034\n",
      "iteration 11184: loss: 0.2323874682188034\n",
      "iteration 11185: loss: 0.23238658905029297\n",
      "iteration 11186: loss: 0.23238559067249298\n",
      "iteration 11187: loss: 0.2323846071958542\n",
      "iteration 11188: loss: 0.2323836088180542\n",
      "iteration 11189: loss: 0.23238272964954376\n",
      "iteration 11190: loss: 0.23238173127174377\n",
      "iteration 11191: loss: 0.2323807179927826\n",
      "iteration 11192: loss: 0.23237979412078857\n",
      "iteration 11193: loss: 0.23237881064414978\n",
      "iteration 11194: loss: 0.23237784206867218\n",
      "iteration 11195: loss: 0.2323768138885498\n",
      "iteration 11196: loss: 0.2323758602142334\n",
      "iteration 11197: loss: 0.2323748618364334\n",
      "iteration 11198: loss: 0.232373908162117\n",
      "iteration 11199: loss: 0.2323727309703827\n",
      "iteration 11200: loss: 0.2323717325925827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11201: loss: 0.2323707640171051\n",
      "iteration 11202: loss: 0.23236985504627228\n",
      "iteration 11203: loss: 0.2323688566684723\n",
      "iteration 11204: loss: 0.2323678433895111\n",
      "iteration 11205: loss: 0.23236684501171112\n",
      "iteration 11206: loss: 0.23236589133739471\n",
      "iteration 11207: loss: 0.23236489295959473\n",
      "iteration 11208: loss: 0.23236390948295593\n",
      "iteration 11209: loss: 0.23236291110515594\n",
      "iteration 11210: loss: 0.23236194252967834\n",
      "iteration 11211: loss: 0.23236092925071716\n",
      "iteration 11212: loss: 0.23235997557640076\n",
      "iteration 11213: loss: 0.23235897719860077\n",
      "iteration 11214: loss: 0.23235812783241272\n",
      "iteration 11215: loss: 0.23235711455345154\n",
      "iteration 11216: loss: 0.23235616087913513\n",
      "iteration 11217: loss: 0.23235514760017395\n",
      "iteration 11218: loss: 0.23235416412353516\n",
      "iteration 11219: loss: 0.23235316574573517\n",
      "iteration 11220: loss: 0.23235218226909637\n",
      "iteration 11221: loss: 0.23235122859477997\n",
      "iteration 11222: loss: 0.2323502004146576\n",
      "iteration 11223: loss: 0.2323492020368576\n",
      "iteration 11224: loss: 0.23234823346138\n",
      "iteration 11225: loss: 0.2323472797870636\n",
      "iteration 11226: loss: 0.23234626650810242\n",
      "iteration 11227: loss: 0.2323453724384308\n",
      "iteration 11228: loss: 0.232344388961792\n",
      "iteration 11229: loss: 0.232343390583992\n",
      "iteration 11230: loss: 0.2323424071073532\n",
      "iteration 11231: loss: 0.2323414385318756\n",
      "iteration 11232: loss: 0.23234042525291443\n",
      "iteration 11233: loss: 0.23233942687511444\n",
      "iteration 11234: loss: 0.23233845829963684\n",
      "iteration 11235: loss: 0.23233751952648163\n",
      "iteration 11236: loss: 0.23233655095100403\n",
      "iteration 11237: loss: 0.23233537375926971\n",
      "iteration 11238: loss: 0.23233449459075928\n",
      "iteration 11239: loss: 0.2323334962129593\n",
      "iteration 11240: loss: 0.2323325127363205\n",
      "iteration 11241: loss: 0.2323315441608429\n",
      "iteration 11242: loss: 0.2323305606842041\n",
      "iteration 11243: loss: 0.2323295623064041\n",
      "iteration 11244: loss: 0.23232856392860413\n",
      "iteration 11245: loss: 0.23232761025428772\n",
      "iteration 11246: loss: 0.23232659697532654\n",
      "iteration 11247: loss: 0.23232559859752655\n",
      "iteration 11248: loss: 0.23232463002204895\n",
      "iteration 11249: loss: 0.23232373595237732\n",
      "iteration 11250: loss: 0.23232276737689972\n",
      "iteration 11251: loss: 0.23232178390026093\n",
      "iteration 11252: loss: 0.23232078552246094\n",
      "iteration 11253: loss: 0.23231978714466095\n",
      "iteration 11254: loss: 0.23231878876686096\n",
      "iteration 11255: loss: 0.23231783509254456\n",
      "iteration 11256: loss: 0.23231685161590576\n",
      "iteration 11257: loss: 0.23231586813926697\n",
      "iteration 11258: loss: 0.23231486976146698\n",
      "iteration 11259: loss: 0.23231390118598938\n",
      "iteration 11260: loss: 0.23231291770935059\n",
      "iteration 11261: loss: 0.23231203854084015\n",
      "iteration 11262: loss: 0.23231104016304016\n",
      "iteration 11263: loss: 0.23230986297130585\n",
      "iteration 11264: loss: 0.23230890929698944\n",
      "iteration 11265: loss: 0.23230791091918945\n",
      "iteration 11266: loss: 0.23230692744255066\n",
      "iteration 11267: loss: 0.23230592906475067\n",
      "iteration 11268: loss: 0.23230496048927307\n",
      "iteration 11269: loss: 0.2323039472103119\n",
      "iteration 11270: loss: 0.2323029488325119\n",
      "iteration 11271: loss: 0.23230195045471191\n",
      "iteration 11272: loss: 0.2323009967803955\n",
      "iteration 11273: loss: 0.23230008780956268\n",
      "iteration 11274: loss: 0.2322990894317627\n",
      "iteration 11275: loss: 0.2322981059551239\n",
      "iteration 11276: loss: 0.2322971522808075\n",
      "iteration 11277: loss: 0.2322961539030075\n",
      "iteration 11278: loss: 0.2322952300310135\n",
      "iteration 11279: loss: 0.2322942316532135\n",
      "iteration 11280: loss: 0.2322932779788971\n",
      "iteration 11281: loss: 0.2322922646999359\n",
      "iteration 11282: loss: 0.2322912961244583\n",
      "iteration 11283: loss: 0.23229031264781952\n",
      "iteration 11284: loss: 0.23228923976421356\n",
      "iteration 11285: loss: 0.23228824138641357\n",
      "iteration 11286: loss: 0.23228728771209717\n",
      "iteration 11287: loss: 0.23228628933429718\n",
      "iteration 11288: loss: 0.2322853058576584\n",
      "iteration 11289: loss: 0.2322843372821808\n",
      "iteration 11290: loss: 0.2322833240032196\n",
      "iteration 11291: loss: 0.23228232562541962\n",
      "iteration 11292: loss: 0.2322813719511032\n",
      "iteration 11293: loss: 0.23228037357330322\n",
      "iteration 11294: loss: 0.23227949440479279\n",
      "iteration 11295: loss: 0.232278510928154\n",
      "iteration 11296: loss: 0.2322775423526764\n",
      "iteration 11297: loss: 0.2322765290737152\n",
      "iteration 11298: loss: 0.2322755604982376\n",
      "iteration 11299: loss: 0.2322743833065033\n",
      "iteration 11300: loss: 0.2322733849287033\n",
      "iteration 11301: loss: 0.2322724312543869\n",
      "iteration 11302: loss: 0.23227143287658691\n",
      "iteration 11303: loss: 0.23227056860923767\n",
      "iteration 11304: loss: 0.23226957023143768\n",
      "iteration 11305: loss: 0.23226860165596008\n",
      "iteration 11306: loss: 0.2322676181793213\n",
      "iteration 11307: loss: 0.2322666198015213\n",
      "iteration 11308: loss: 0.2322656661272049\n",
      "iteration 11309: loss: 0.2322646826505661\n",
      "iteration 11310: loss: 0.2322636842727661\n",
      "iteration 11311: loss: 0.23226265609264374\n",
      "iteration 11312: loss: 0.23226170241832733\n",
      "iteration 11313: loss: 0.23226074874401093\n",
      "iteration 11314: loss: 0.23225955665111542\n",
      "iteration 11315: loss: 0.2322586476802826\n",
      "iteration 11316: loss: 0.2322576940059662\n",
      "iteration 11317: loss: 0.2322566956281662\n",
      "iteration 11318: loss: 0.23225566744804382\n",
      "iteration 11319: loss: 0.23225469887256622\n",
      "iteration 11320: loss: 0.23225374519824982\n",
      "iteration 11321: loss: 0.2322528064250946\n",
      "iteration 11322: loss: 0.232251837849617\n",
      "iteration 11323: loss: 0.2322508990764618\n",
      "iteration 11324: loss: 0.2322499006986618\n",
      "iteration 11325: loss: 0.2322489321231842\n",
      "iteration 11326: loss: 0.23224791884422302\n",
      "iteration 11327: loss: 0.23224692046642303\n",
      "iteration 11328: loss: 0.23224587738513947\n",
      "iteration 11329: loss: 0.23224487900733948\n",
      "iteration 11330: loss: 0.23224392533302307\n",
      "iteration 11331: loss: 0.2322429120540619\n",
      "iteration 11332: loss: 0.2322419434785843\n",
      "iteration 11333: loss: 0.2322409600019455\n",
      "iteration 11334: loss: 0.2322399914264679\n",
      "iteration 11335: loss: 0.2322390079498291\n",
      "iteration 11336: loss: 0.2322380244731903\n",
      "iteration 11337: loss: 0.23223701119422913\n",
      "iteration 11338: loss: 0.23223614692687988\n",
      "iteration 11339: loss: 0.23223522305488586\n",
      "iteration 11340: loss: 0.23223426938056946\n",
      "iteration 11341: loss: 0.23223309218883514\n",
      "iteration 11342: loss: 0.23223212361335754\n",
      "iteration 11343: loss: 0.23223111033439636\n",
      "iteration 11344: loss: 0.23223018646240234\n",
      "iteration 11345: loss: 0.23222918808460236\n",
      "iteration 11346: loss: 0.23222818970680237\n",
      "iteration 11347: loss: 0.23222723603248596\n",
      "iteration 11348: loss: 0.2322262078523636\n",
      "iteration 11349: loss: 0.23222534358501434\n",
      "iteration 11350: loss: 0.23222431540489197\n",
      "iteration 11351: loss: 0.23222339153289795\n",
      "iteration 11352: loss: 0.23222239315509796\n",
      "iteration 11353: loss: 0.23222124576568604\n",
      "iteration 11354: loss: 0.23222024738788605\n",
      "iteration 11355: loss: 0.23221926391124725\n",
      "iteration 11356: loss: 0.23221829533576965\n",
      "iteration 11357: loss: 0.23221728205680847\n",
      "iteration 11358: loss: 0.23221632838249207\n",
      "iteration 11359: loss: 0.23221533000469208\n",
      "iteration 11360: loss: 0.23221436142921448\n",
      "iteration 11361: loss: 0.23221349716186523\n",
      "iteration 11362: loss: 0.23221249878406525\n",
      "iteration 11363: loss: 0.23221156001091003\n",
      "iteration 11364: loss: 0.23221059143543243\n",
      "iteration 11365: loss: 0.23220941424369812\n",
      "iteration 11366: loss: 0.23220844566822052\n",
      "iteration 11367: loss: 0.23220746219158173\n",
      "iteration 11368: loss: 0.23220649361610413\n",
      "iteration 11369: loss: 0.23220551013946533\n",
      "iteration 11370: loss: 0.23220451176166534\n",
      "iteration 11371: loss: 0.2322036474943161\n",
      "iteration 11372: loss: 0.2322026789188385\n",
      "iteration 11373: loss: 0.23220165073871613\n",
      "iteration 11374: loss: 0.2322005331516266\n",
      "iteration 11375: loss: 0.2321995198726654\n",
      "iteration 11376: loss: 0.2321985512971878\n",
      "iteration 11377: loss: 0.2321975976228714\n",
      "iteration 11378: loss: 0.232196643948555\n",
      "iteration 11379: loss: 0.23219561576843262\n",
      "iteration 11380: loss: 0.23219463229179382\n",
      "iteration 11381: loss: 0.2321937531232834\n",
      "iteration 11382: loss: 0.2321927845478058\n",
      "iteration 11383: loss: 0.232191801071167\n",
      "iteration 11384: loss: 0.23219060897827148\n",
      "iteration 11385: loss: 0.23218965530395508\n",
      "iteration 11386: loss: 0.23218867182731628\n",
      "iteration 11387: loss: 0.23218771815299988\n",
      "iteration 11388: loss: 0.2321867197751999\n",
      "iteration 11389: loss: 0.2321857511997223\n",
      "iteration 11390: loss: 0.2321847677230835\n",
      "iteration 11391: loss: 0.2321837842464447\n",
      "iteration 11392: loss: 0.23218278586864471\n",
      "iteration 11393: loss: 0.23218174278736115\n",
      "iteration 11394: loss: 0.23218078911304474\n",
      "iteration 11395: loss: 0.23217979073524475\n",
      "iteration 11396: loss: 0.23217883706092834\n",
      "iteration 11397: loss: 0.23217785358428955\n",
      "iteration 11398: loss: 0.23217687010765076\n",
      "iteration 11399: loss: 0.23217587172985077\n",
      "iteration 11400: loss: 0.23217490315437317\n",
      "iteration 11401: loss: 0.23217394948005676\n",
      "iteration 11402: loss: 0.23217293620109558\n",
      "iteration 11403: loss: 0.23217180371284485\n",
      "iteration 11404: loss: 0.23217090964317322\n",
      "iteration 11405: loss: 0.23216994106769562\n",
      "iteration 11406: loss: 0.2321690022945404\n",
      "iteration 11407: loss: 0.2321680337190628\n",
      "iteration 11408: loss: 0.232167050242424\n",
      "iteration 11409: loss: 0.2321660965681076\n",
      "iteration 11410: loss: 0.23216506838798523\n",
      "iteration 11411: loss: 0.2321639508008957\n",
      "iteration 11412: loss: 0.2321629524230957\n",
      "iteration 11413: loss: 0.2321619987487793\n",
      "iteration 11414: loss: 0.2321610152721405\n",
      "iteration 11415: loss: 0.23216000199317932\n",
      "iteration 11416: loss: 0.23215916752815247\n",
      "iteration 11417: loss: 0.23215815424919128\n",
      "iteration 11418: loss: 0.23215718567371368\n",
      "iteration 11419: loss: 0.23215623199939728\n",
      "iteration 11420: loss: 0.23215505480766296\n",
      "iteration 11421: loss: 0.23215408623218536\n",
      "iteration 11422: loss: 0.23215313255786896\n",
      "iteration 11423: loss: 0.23215214908123016\n",
      "iteration 11424: loss: 0.23215115070343018\n",
      "iteration 11425: loss: 0.23215016722679138\n",
      "iteration 11426: loss: 0.23214921355247498\n",
      "iteration 11427: loss: 0.232148215174675\n",
      "iteration 11428: loss: 0.23214717209339142\n",
      "iteration 11429: loss: 0.23214617371559143\n",
      "iteration 11430: loss: 0.23214522004127502\n",
      "iteration 11431: loss: 0.23214420676231384\n",
      "iteration 11432: loss: 0.23214325308799744\n",
      "iteration 11433: loss: 0.23214228451251984\n",
      "iteration 11434: loss: 0.23214130103588104\n",
      "iteration 11435: loss: 0.23214030265808105\n",
      "iteration 11436: loss: 0.23213918507099152\n",
      "iteration 11437: loss: 0.23213820159435272\n",
      "iteration 11438: loss: 0.23213720321655273\n",
      "iteration 11439: loss: 0.23213627934455872\n",
      "iteration 11440: loss: 0.2321353256702423\n",
      "iteration 11441: loss: 0.2321343719959259\n",
      "iteration 11442: loss: 0.2321334183216095\n",
      "iteration 11443: loss: 0.23213239014148712\n",
      "iteration 11444: loss: 0.2321312427520752\n",
      "iteration 11445: loss: 0.23213031888008118\n",
      "iteration 11446: loss: 0.23212933540344238\n",
      "iteration 11447: loss: 0.2321283370256424\n",
      "iteration 11448: loss: 0.23212742805480957\n",
      "iteration 11449: loss: 0.23212642967700958\n",
      "iteration 11450: loss: 0.23212547600269318\n",
      "iteration 11451: loss: 0.2321244180202484\n",
      "iteration 11452: loss: 0.23212340474128723\n",
      "iteration 11453: loss: 0.23212245106697083\n",
      "iteration 11454: loss: 0.23212146759033203\n",
      "iteration 11455: loss: 0.23212048411369324\n",
      "iteration 11456: loss: 0.23211951553821564\n",
      "iteration 11457: loss: 0.23211856186389923\n",
      "iteration 11458: loss: 0.23211756348609924\n",
      "iteration 11459: loss: 0.23211641609668732\n",
      "iteration 11460: loss: 0.23211543262004852\n",
      "iteration 11461: loss: 0.23211446404457092\n",
      "iteration 11462: loss: 0.2321135699748993\n",
      "iteration 11463: loss: 0.2321126013994217\n",
      "iteration 11464: loss: 0.2321116179227829\n",
      "iteration 11465: loss: 0.2321106642484665\n",
      "iteration 11466: loss: 0.23210950195789337\n",
      "iteration 11467: loss: 0.23210851848125458\n",
      "iteration 11468: loss: 0.23210754990577698\n",
      "iteration 11469: loss: 0.23210656642913818\n",
      "iteration 11470: loss: 0.2321055829524994\n",
      "iteration 11471: loss: 0.2321045845746994\n",
      "iteration 11472: loss: 0.23210366070270538\n",
      "iteration 11473: loss: 0.23210260272026062\n",
      "iteration 11474: loss: 0.23210163414478302\n",
      "iteration 11475: loss: 0.23210065066814423\n",
      "iteration 11476: loss: 0.23209962248802185\n",
      "iteration 11477: loss: 0.23209866881370544\n",
      "iteration 11478: loss: 0.23209771513938904\n",
      "iteration 11479: loss: 0.23209664225578308\n",
      "iteration 11480: loss: 0.23209567368030548\n",
      "iteration 11481: loss: 0.23209472000598907\n",
      "iteration 11482: loss: 0.23209373652935028\n",
      "iteration 11483: loss: 0.23209276795387268\n",
      "iteration 11484: loss: 0.23209181427955627\n",
      "iteration 11485: loss: 0.23209090530872345\n",
      "iteration 11486: loss: 0.23208972811698914\n",
      "iteration 11487: loss: 0.23208875954151154\n",
      "iteration 11488: loss: 0.23208777606487274\n",
      "iteration 11489: loss: 0.23208680748939514\n",
      "iteration 11490: loss: 0.23208589851856232\n",
      "iteration 11491: loss: 0.23208490014076233\n",
      "iteration 11492: loss: 0.23208394646644592\n",
      "iteration 11493: loss: 0.2320827692747116\n",
      "iteration 11494: loss: 0.232081800699234\n",
      "iteration 11495: loss: 0.2320808470249176\n",
      "iteration 11496: loss: 0.23207998275756836\n",
      "iteration 11497: loss: 0.23207898437976837\n",
      "iteration 11498: loss: 0.23207800090312958\n",
      "iteration 11499: loss: 0.23207704722881317\n",
      "iteration 11500: loss: 0.23207588493824005\n",
      "iteration 11501: loss: 0.23207490146160126\n",
      "iteration 11502: loss: 0.23207393288612366\n",
      "iteration 11503: loss: 0.23207297921180725\n",
      "iteration 11504: loss: 0.23207196593284607\n",
      "iteration 11505: loss: 0.23207101225852966\n",
      "iteration 11506: loss: 0.23206987977027893\n",
      "iteration 11507: loss: 0.2320689857006073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11508: loss: 0.2320680320262909\n",
      "iteration 11509: loss: 0.2320670336484909\n",
      "iteration 11510: loss: 0.2320660799741745\n",
      "iteration 11511: loss: 0.23206505179405212\n",
      "iteration 11512: loss: 0.23206393420696259\n",
      "iteration 11513: loss: 0.23206296563148499\n",
      "iteration 11514: loss: 0.2320619821548462\n",
      "iteration 11515: loss: 0.23206102848052979\n",
      "iteration 11516: loss: 0.2320600301027298\n",
      "iteration 11517: loss: 0.23205912113189697\n",
      "iteration 11518: loss: 0.23205800354480743\n",
      "iteration 11519: loss: 0.23205704987049103\n",
      "iteration 11520: loss: 0.23205605149269104\n",
      "iteration 11521: loss: 0.23205506801605225\n",
      "iteration 11522: loss: 0.23205408453941345\n",
      "iteration 11523: loss: 0.23205311596393585\n",
      "iteration 11524: loss: 0.2320519983768463\n",
      "iteration 11525: loss: 0.23205101490020752\n",
      "iteration 11526: loss: 0.23205003142356873\n",
      "iteration 11527: loss: 0.23204907774925232\n",
      "iteration 11528: loss: 0.2320481240749359\n",
      "iteration 11529: loss: 0.23204724490642548\n",
      "iteration 11530: loss: 0.23204608261585236\n",
      "iteration 11531: loss: 0.23204512894153595\n",
      "iteration 11532: loss: 0.23204414546489716\n",
      "iteration 11533: loss: 0.23204317688941956\n",
      "iteration 11534: loss: 0.23204222321510315\n",
      "iteration 11535: loss: 0.23204123973846436\n",
      "iteration 11536: loss: 0.23204012215137482\n",
      "iteration 11537: loss: 0.23203912377357483\n",
      "iteration 11538: loss: 0.23203825950622559\n",
      "iteration 11539: loss: 0.2320372760295868\n",
      "iteration 11540: loss: 0.2320363074541092\n",
      "iteration 11541: loss: 0.23203516006469727\n",
      "iteration 11542: loss: 0.23203420639038086\n",
      "iteration 11543: loss: 0.23203322291374207\n",
      "iteration 11544: loss: 0.23203225433826447\n",
      "iteration 11545: loss: 0.23203127086162567\n",
      "iteration 11546: loss: 0.23203031718730927\n",
      "iteration 11547: loss: 0.23202915489673615\n",
      "iteration 11548: loss: 0.23202817142009735\n",
      "iteration 11549: loss: 0.23202724754810333\n",
      "iteration 11550: loss: 0.2320263385772705\n",
      "iteration 11551: loss: 0.2320253551006317\n",
      "iteration 11552: loss: 0.2320243865251541\n",
      "iteration 11553: loss: 0.2320232391357422\n",
      "iteration 11554: loss: 0.2320222556591034\n",
      "iteration 11555: loss: 0.2320212870836258\n",
      "iteration 11556: loss: 0.2320203334093094\n",
      "iteration 11557: loss: 0.2320193350315094\n",
      "iteration 11558: loss: 0.2320183515548706\n",
      "iteration 11559: loss: 0.23201720416545868\n",
      "iteration 11560: loss: 0.23201636970043182\n",
      "iteration 11561: loss: 0.23201537132263184\n",
      "iteration 11562: loss: 0.23201441764831543\n",
      "iteration 11563: loss: 0.23201343417167664\n",
      "iteration 11564: loss: 0.2320123016834259\n",
      "iteration 11565: loss: 0.2320113182067871\n",
      "iteration 11566: loss: 0.23201033473014832\n",
      "iteration 11567: loss: 0.23200936615467072\n",
      "iteration 11568: loss: 0.2320084124803543\n",
      "iteration 11569: loss: 0.2320072203874588\n",
      "iteration 11570: loss: 0.2320062816143036\n",
      "iteration 11571: loss: 0.232005313038826\n",
      "iteration 11572: loss: 0.23200435936450958\n",
      "iteration 11573: loss: 0.23200348019599915\n",
      "iteration 11574: loss: 0.23200233280658722\n",
      "iteration 11575: loss: 0.232001394033432\n",
      "iteration 11576: loss: 0.23200039565563202\n",
      "iteration 11577: loss: 0.23199942708015442\n",
      "iteration 11578: loss: 0.231998473405838\n",
      "iteration 11579: loss: 0.2319975346326828\n",
      "iteration 11580: loss: 0.2319963425397873\n",
      "iteration 11581: loss: 0.2319953888654709\n",
      "iteration 11582: loss: 0.2319944202899933\n",
      "iteration 11583: loss: 0.23199352622032166\n",
      "iteration 11584: loss: 0.23199257254600525\n",
      "iteration 11585: loss: 0.23199138045310974\n",
      "iteration 11586: loss: 0.23199045658111572\n",
      "iteration 11587: loss: 0.23198947310447693\n",
      "iteration 11588: loss: 0.23198850452899933\n",
      "iteration 11589: loss: 0.23198752105236053\n",
      "iteration 11590: loss: 0.231986403465271\n",
      "iteration 11591: loss: 0.2319854199886322\n",
      "iteration 11592: loss: 0.2319844514131546\n",
      "iteration 11593: loss: 0.2319834977388382\n",
      "iteration 11594: loss: 0.23198235034942627\n",
      "iteration 11595: loss: 0.23198147118091583\n",
      "iteration 11596: loss: 0.23198051750659943\n",
      "iteration 11597: loss: 0.23197953402996063\n",
      "iteration 11598: loss: 0.23197858035564423\n",
      "iteration 11599: loss: 0.23197755217552185\n",
      "iteration 11600: loss: 0.2319764345884323\n",
      "iteration 11601: loss: 0.2319754809141159\n",
      "iteration 11602: loss: 0.23197448253631592\n",
      "iteration 11603: loss: 0.23197349905967712\n",
      "iteration 11604: loss: 0.23197248578071594\n",
      "iteration 11605: loss: 0.23197150230407715\n",
      "iteration 11606: loss: 0.23197051882743835\n",
      "iteration 11607: loss: 0.23196955025196075\n",
      "iteration 11608: loss: 0.23196859657764435\n",
      "iteration 11609: loss: 0.23196744918823242\n",
      "iteration 11610: loss: 0.23196649551391602\n",
      "iteration 11611: loss: 0.23196551203727722\n",
      "iteration 11612: loss: 0.2319645881652832\n",
      "iteration 11613: loss: 0.23196366429328918\n",
      "iteration 11614: loss: 0.23196256160736084\n",
      "iteration 11615: loss: 0.23196153342723846\n",
      "iteration 11616: loss: 0.2319606989622116\n",
      "iteration 11617: loss: 0.2319597452878952\n",
      "iteration 11618: loss: 0.2319587767124176\n",
      "iteration 11619: loss: 0.23195764422416687\n",
      "iteration 11620: loss: 0.23195667564868927\n",
      "iteration 11621: loss: 0.23195567727088928\n",
      "iteration 11622: loss: 0.23195472359657288\n",
      "iteration 11623: loss: 0.23195374011993408\n",
      "iteration 11624: loss: 0.23195259273052216\n",
      "iteration 11625: loss: 0.23195166885852814\n",
      "iteration 11626: loss: 0.23195071518421173\n",
      "iteration 11627: loss: 0.2319498360157013\n",
      "iteration 11628: loss: 0.23194868862628937\n",
      "iteration 11629: loss: 0.23194770514965057\n",
      "iteration 11630: loss: 0.23194670677185059\n",
      "iteration 11631: loss: 0.23194575309753418\n",
      "iteration 11632: loss: 0.23194482922554016\n",
      "iteration 11633: loss: 0.23194365203380585\n",
      "iteration 11634: loss: 0.23194269835948944\n",
      "iteration 11635: loss: 0.23194174468517303\n",
      "iteration 11636: loss: 0.23194074630737305\n",
      "iteration 11637: loss: 0.23193959891796112\n",
      "iteration 11638: loss: 0.23193871974945068\n",
      "iteration 11639: loss: 0.23193776607513428\n",
      "iteration 11640: loss: 0.23193678259849548\n",
      "iteration 11641: loss: 0.23193582892417908\n",
      "iteration 11642: loss: 0.23193466663360596\n",
      "iteration 11643: loss: 0.23193368315696716\n",
      "iteration 11644: loss: 0.23193275928497314\n",
      "iteration 11645: loss: 0.23193177580833435\n",
      "iteration 11646: loss: 0.23193082213401794\n",
      "iteration 11647: loss: 0.2319296896457672\n",
      "iteration 11648: loss: 0.2319287359714508\n",
      "iteration 11649: loss: 0.23192772269248962\n",
      "iteration 11650: loss: 0.23192676901817322\n",
      "iteration 11651: loss: 0.23192572593688965\n",
      "iteration 11652: loss: 0.23192474246025085\n",
      "iteration 11653: loss: 0.23192381858825684\n",
      "iteration 11654: loss: 0.23192283511161804\n",
      "iteration 11655: loss: 0.23192167282104492\n",
      "iteration 11656: loss: 0.23192071914672852\n",
      "iteration 11657: loss: 0.2319197654724121\n",
      "iteration 11658: loss: 0.23191885650157928\n",
      "iteration 11659: loss: 0.23191788792610168\n",
      "iteration 11660: loss: 0.23191669583320618\n",
      "iteration 11661: loss: 0.23191586136817932\n",
      "iteration 11662: loss: 0.23191484808921814\n",
      "iteration 11663: loss: 0.23191389441490173\n",
      "iteration 11664: loss: 0.231912761926651\n",
      "iteration 11665: loss: 0.2319118082523346\n",
      "iteration 11666: loss: 0.2319108545780182\n",
      "iteration 11667: loss: 0.2319098711013794\n",
      "iteration 11668: loss: 0.23190870881080627\n",
      "iteration 11669: loss: 0.23190775513648987\n",
      "iteration 11670: loss: 0.23190680146217346\n",
      "iteration 11671: loss: 0.23190584778785706\n",
      "iteration 11672: loss: 0.23190486431121826\n",
      "iteration 11673: loss: 0.23190371692180634\n",
      "iteration 11674: loss: 0.2319028377532959\n",
      "iteration 11675: loss: 0.23190191388130188\n",
      "iteration 11676: loss: 0.2319009006023407\n",
      "iteration 11677: loss: 0.23189976811408997\n",
      "iteration 11678: loss: 0.23189881443977356\n",
      "iteration 11679: loss: 0.23189783096313477\n",
      "iteration 11680: loss: 0.23189687728881836\n",
      "iteration 11681: loss: 0.23189572989940643\n",
      "iteration 11682: loss: 0.23189473152160645\n",
      "iteration 11683: loss: 0.23189380764961243\n",
      "iteration 11684: loss: 0.23189285397529602\n",
      "iteration 11685: loss: 0.23189178109169006\n",
      "iteration 11686: loss: 0.23189082741737366\n",
      "iteration 11687: loss: 0.23188987374305725\n",
      "iteration 11688: loss: 0.23188892006874084\n",
      "iteration 11689: loss: 0.23188774287700653\n",
      "iteration 11690: loss: 0.23188678920269012\n",
      "iteration 11691: loss: 0.23188582062721252\n",
      "iteration 11692: loss: 0.23188486695289612\n",
      "iteration 11693: loss: 0.23188385367393494\n",
      "iteration 11694: loss: 0.23188276588916779\n",
      "iteration 11695: loss: 0.2318817675113678\n",
      "iteration 11696: loss: 0.23188090324401855\n",
      "iteration 11697: loss: 0.23187991976737976\n",
      "iteration 11698: loss: 0.23187880218029022\n",
      "iteration 11699: loss: 0.23187783360481262\n",
      "iteration 11700: loss: 0.231876939535141\n",
      "iteration 11701: loss: 0.2318759709596634\n",
      "iteration 11702: loss: 0.23187482357025146\n",
      "iteration 11703: loss: 0.23187384009361267\n",
      "iteration 11704: loss: 0.23187288641929626\n",
      "iteration 11705: loss: 0.23187188804149628\n",
      "iteration 11706: loss: 0.23187077045440674\n",
      "iteration 11707: loss: 0.23186981678009033\n",
      "iteration 11708: loss: 0.23186883330345154\n",
      "iteration 11709: loss: 0.2318679839372635\n",
      "iteration 11710: loss: 0.23186683654785156\n",
      "iteration 11711: loss: 0.23186588287353516\n",
      "iteration 11712: loss: 0.23186492919921875\n",
      "iteration 11713: loss: 0.23186393082141876\n",
      "iteration 11714: loss: 0.23186282813549042\n",
      "iteration 11715: loss: 0.23186182975769043\n",
      "iteration 11716: loss: 0.23186087608337402\n",
      "iteration 11717: loss: 0.23185992240905762\n",
      "iteration 11718: loss: 0.2318587750196457\n",
      "iteration 11719: loss: 0.2318577766418457\n",
      "iteration 11720: loss: 0.23185685276985168\n",
      "iteration 11721: loss: 0.23185595870018005\n",
      "iteration 11722: loss: 0.23185482621192932\n",
      "iteration 11723: loss: 0.23185387253761292\n",
      "iteration 11724: loss: 0.23185285925865173\n",
      "iteration 11725: loss: 0.23185193538665771\n",
      "iteration 11726: loss: 0.23185081779956818\n",
      "iteration 11727: loss: 0.2318497896194458\n",
      "iteration 11728: loss: 0.2318488359451294\n",
      "iteration 11729: loss: 0.23184773325920105\n",
      "iteration 11730: loss: 0.23184676468372345\n",
      "iteration 11731: loss: 0.23184581100940704\n",
      "iteration 11732: loss: 0.2318449318408966\n",
      "iteration 11733: loss: 0.23184379935264587\n",
      "iteration 11734: loss: 0.23184283077716827\n",
      "iteration 11735: loss: 0.23184187710285187\n",
      "iteration 11736: loss: 0.23184089362621307\n",
      "iteration 11737: loss: 0.2318398505449295\n",
      "iteration 11738: loss: 0.2318388670682907\n",
      "iteration 11739: loss: 0.2318379431962967\n",
      "iteration 11740: loss: 0.2318369597196579\n",
      "iteration 11741: loss: 0.23183587193489075\n",
      "iteration 11742: loss: 0.23183488845825195\n",
      "iteration 11743: loss: 0.23183393478393555\n",
      "iteration 11744: loss: 0.2318331003189087\n",
      "iteration 11745: loss: 0.23183190822601318\n",
      "iteration 11746: loss: 0.23183095455169678\n",
      "iteration 11747: loss: 0.23183000087738037\n",
      "iteration 11748: loss: 0.23182901740074158\n",
      "iteration 11749: loss: 0.23182789981365204\n",
      "iteration 11750: loss: 0.23182693123817444\n",
      "iteration 11751: loss: 0.23182594776153564\n",
      "iteration 11752: loss: 0.23182502388954163\n",
      "iteration 11753: loss: 0.2318238765001297\n",
      "iteration 11754: loss: 0.2318228930234909\n",
      "iteration 11755: loss: 0.2318219393491745\n",
      "iteration 11756: loss: 0.23182086646556854\n",
      "iteration 11757: loss: 0.23181994259357452\n",
      "iteration 11758: loss: 0.23181898891925812\n",
      "iteration 11759: loss: 0.23181800544261932\n",
      "iteration 11760: loss: 0.23181688785552979\n",
      "iteration 11761: loss: 0.231815904378891\n",
      "iteration 11762: loss: 0.23181495070457458\n",
      "iteration 11763: loss: 0.23181386291980743\n",
      "iteration 11764: loss: 0.23181286454200745\n",
      "iteration 11765: loss: 0.23181191086769104\n",
      "iteration 11766: loss: 0.23181095719337463\n",
      "iteration 11767: loss: 0.2318098098039627\n",
      "iteration 11768: loss: 0.23180893063545227\n",
      "iteration 11769: loss: 0.23180802166461945\n",
      "iteration 11770: loss: 0.23180682957172394\n",
      "iteration 11771: loss: 0.23180589079856873\n",
      "iteration 11772: loss: 0.23180492222309113\n",
      "iteration 11773: loss: 0.23180396854877472\n",
      "iteration 11774: loss: 0.2318028211593628\n",
      "iteration 11775: loss: 0.231801837682724\n",
      "iteration 11776: loss: 0.2318008840084076\n",
      "iteration 11777: loss: 0.2317999303340912\n",
      "iteration 11778: loss: 0.23179881274700165\n",
      "iteration 11779: loss: 0.23179784417152405\n",
      "iteration 11780: loss: 0.23179693520069122\n",
      "iteration 11781: loss: 0.23179583251476288\n",
      "iteration 11782: loss: 0.23179492354393005\n",
      "iteration 11783: loss: 0.23179395496845245\n",
      "iteration 11784: loss: 0.23179280757904053\n",
      "iteration 11785: loss: 0.23179185390472412\n",
      "iteration 11786: loss: 0.2317909300327301\n",
      "iteration 11787: loss: 0.2317899465560913\n",
      "iteration 11788: loss: 0.23178882896900177\n",
      "iteration 11789: loss: 0.23178784549236298\n",
      "iteration 11790: loss: 0.23178689181804657\n",
      "iteration 11791: loss: 0.23178572952747345\n",
      "iteration 11792: loss: 0.23178479075431824\n",
      "iteration 11793: loss: 0.2317839413881302\n",
      "iteration 11794: loss: 0.2317829430103302\n",
      "iteration 11795: loss: 0.23178181052207947\n",
      "iteration 11796: loss: 0.23178088665008545\n",
      "iteration 11797: loss: 0.23177990317344666\n",
      "iteration 11798: loss: 0.23177877068519592\n",
      "iteration 11799: loss: 0.2317778319120407\n",
      "iteration 11800: loss: 0.2317768633365631\n",
      "iteration 11801: loss: 0.23177587985992432\n",
      "iteration 11802: loss: 0.2317747324705124\n",
      "iteration 11803: loss: 0.23177380859851837\n",
      "iteration 11804: loss: 0.23177285492420197\n",
      "iteration 11805: loss: 0.23177170753479004\n",
      "iteration 11806: loss: 0.2317708432674408\n",
      "iteration 11807: loss: 0.2317698895931244\n",
      "iteration 11808: loss: 0.23176875710487366\n",
      "iteration 11809: loss: 0.23176780343055725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11810: loss: 0.23176681995391846\n",
      "iteration 11811: loss: 0.23176589608192444\n",
      "iteration 11812: loss: 0.2317647486925125\n",
      "iteration 11813: loss: 0.23176375031471252\n",
      "iteration 11814: loss: 0.2317628115415573\n",
      "iteration 11815: loss: 0.23176166415214539\n",
      "iteration 11816: loss: 0.23176074028015137\n",
      "iteration 11817: loss: 0.23175975680351257\n",
      "iteration 11818: loss: 0.23175887763500214\n",
      "iteration 11819: loss: 0.2317577600479126\n",
      "iteration 11820: loss: 0.2317568063735962\n",
      "iteration 11821: loss: 0.23175585269927979\n",
      "iteration 11822: loss: 0.23175473511219025\n",
      "iteration 11823: loss: 0.23175378143787384\n",
      "iteration 11824: loss: 0.23175284266471863\n",
      "iteration 11825: loss: 0.2317517250776291\n",
      "iteration 11826: loss: 0.23175077140331268\n",
      "iteration 11827: loss: 0.23174981772899628\n",
      "iteration 11828: loss: 0.23174867033958435\n",
      "iteration 11829: loss: 0.23174774646759033\n",
      "iteration 11830: loss: 0.23174676299095154\n",
      "iteration 11831: loss: 0.2317456305027008\n",
      "iteration 11832: loss: 0.23174473643302917\n",
      "iteration 11833: loss: 0.23174378275871277\n",
      "iteration 11834: loss: 0.23174281418323517\n",
      "iteration 11835: loss: 0.23174169659614563\n",
      "iteration 11836: loss: 0.23174074292182922\n",
      "iteration 11837: loss: 0.23173975944519043\n",
      "iteration 11838: loss: 0.23173865675926208\n",
      "iteration 11839: loss: 0.23173770308494568\n",
      "iteration 11840: loss: 0.23173674941062927\n",
      "iteration 11841: loss: 0.23173561692237854\n",
      "iteration 11842: loss: 0.23173466324806213\n",
      "iteration 11843: loss: 0.23173370957374573\n",
      "iteration 11844: loss: 0.231732577085495\n",
      "iteration 11845: loss: 0.2317316234111786\n",
      "iteration 11846: loss: 0.23173078894615173\n",
      "iteration 11847: loss: 0.231729656457901\n",
      "iteration 11848: loss: 0.23172864317893982\n",
      "iteration 11849: loss: 0.2317276895046234\n",
      "iteration 11850: loss: 0.23172657191753387\n",
      "iteration 11851: loss: 0.23172561824321747\n",
      "iteration 11852: loss: 0.23172464966773987\n",
      "iteration 11853: loss: 0.23172369599342346\n",
      "iteration 11854: loss: 0.23172256350517273\n",
      "iteration 11855: loss: 0.23172159492969513\n",
      "iteration 11856: loss: 0.23172076046466827\n",
      "iteration 11857: loss: 0.23171964287757874\n",
      "iteration 11858: loss: 0.23171865940093994\n",
      "iteration 11859: loss: 0.23171770572662354\n",
      "iteration 11860: loss: 0.231716588139534\n",
      "iteration 11861: loss: 0.23171572387218475\n",
      "iteration 11862: loss: 0.23171477019786835\n",
      "iteration 11863: loss: 0.23171362280845642\n",
      "iteration 11864: loss: 0.2317127287387848\n",
      "iteration 11865: loss: 0.23171178996562958\n",
      "iteration 11866: loss: 0.23171064257621765\n",
      "iteration 11867: loss: 0.23170968890190125\n",
      "iteration 11868: loss: 0.23170873522758484\n",
      "iteration 11869: loss: 0.23170757293701172\n",
      "iteration 11870: loss: 0.2317066490650177\n",
      "iteration 11871: loss: 0.2317056655883789\n",
      "iteration 11872: loss: 0.23170454800128937\n",
      "iteration 11873: loss: 0.23170359432697296\n",
      "iteration 11874: loss: 0.23170271515846252\n",
      "iteration 11875: loss: 0.23170176148414612\n",
      "iteration 11876: loss: 0.23170065879821777\n",
      "iteration 11877: loss: 0.23169970512390137\n",
      "iteration 11878: loss: 0.23169870674610138\n",
      "iteration 11879: loss: 0.23169760406017303\n",
      "iteration 11880: loss: 0.23169665038585663\n",
      "iteration 11881: loss: 0.2316955327987671\n",
      "iteration 11882: loss: 0.23169460892677307\n",
      "iteration 11883: loss: 0.23169365525245667\n",
      "iteration 11884: loss: 0.23169250786304474\n",
      "iteration 11885: loss: 0.23169155418872833\n",
      "iteration 11886: loss: 0.23169060051441193\n",
      "iteration 11887: loss: 0.2316894829273224\n",
      "iteration 11888: loss: 0.2316884994506836\n",
      "iteration 11889: loss: 0.23168757557868958\n",
      "iteration 11890: loss: 0.23168668150901794\n",
      "iteration 11891: loss: 0.2316855490207672\n",
      "iteration 11892: loss: 0.231684610247612\n",
      "iteration 11893: loss: 0.23168346285820007\n",
      "iteration 11894: loss: 0.23168249428272247\n",
      "iteration 11895: loss: 0.23168154060840607\n",
      "iteration 11896: loss: 0.23168058693408966\n",
      "iteration 11897: loss: 0.23167946934700012\n",
      "iteration 11898: loss: 0.2316785603761673\n",
      "iteration 11899: loss: 0.23167738318443298\n",
      "iteration 11900: loss: 0.23167645931243896\n",
      "iteration 11901: loss: 0.23167553544044495\n",
      "iteration 11902: loss: 0.23167435824871063\n",
      "iteration 11903: loss: 0.2316734492778778\n",
      "iteration 11904: loss: 0.2316725254058838\n",
      "iteration 11905: loss: 0.23167145252227783\n",
      "iteration 11906: loss: 0.2316705286502838\n",
      "iteration 11907: loss: 0.2316695749759674\n",
      "iteration 11908: loss: 0.23166844248771667\n",
      "iteration 11909: loss: 0.23166748881340027\n",
      "iteration 11910: loss: 0.23166652023792267\n",
      "iteration 11911: loss: 0.23166541755199432\n",
      "iteration 11912: loss: 0.23166444897651672\n",
      "iteration 11913: loss: 0.23166346549987793\n",
      "iteration 11914: loss: 0.2316623479127884\n",
      "iteration 11915: loss: 0.23166140913963318\n",
      "iteration 11916: loss: 0.23166044056415558\n",
      "iteration 11917: loss: 0.23165936768054962\n",
      "iteration 11918: loss: 0.23165836930274963\n",
      "iteration 11919: loss: 0.23165743052959442\n",
      "iteration 11920: loss: 0.23165638744831085\n",
      "iteration 11921: loss: 0.23165544867515564\n",
      "iteration 11922: loss: 0.23165448009967804\n",
      "iteration 11923: loss: 0.23165340721607208\n",
      "iteration 11924: loss: 0.2316524088382721\n",
      "iteration 11925: loss: 0.23165127635002136\n",
      "iteration 11926: loss: 0.23165032267570496\n",
      "iteration 11927: loss: 0.23164936900138855\n",
      "iteration 11928: loss: 0.231648251414299\n",
      "iteration 11929: loss: 0.2316472977399826\n",
      "iteration 11930: loss: 0.2316463440656662\n",
      "iteration 11931: loss: 0.23164519667625427\n",
      "iteration 11932: loss: 0.23164430260658264\n",
      "iteration 11933: loss: 0.23164328932762146\n",
      "iteration 11934: loss: 0.23164217174053192\n",
      "iteration 11935: loss: 0.23164129257202148\n",
      "iteration 11936: loss: 0.23164036870002747\n",
      "iteration 11937: loss: 0.23163922131061554\n",
      "iteration 11938: loss: 0.23163826763629913\n",
      "iteration 11939: loss: 0.23163732886314392\n",
      "iteration 11940: loss: 0.23163621127605438\n",
      "iteration 11941: loss: 0.23163525760173798\n",
      "iteration 11942: loss: 0.23163411021232605\n",
      "iteration 11943: loss: 0.23163321614265442\n",
      "iteration 11944: loss: 0.23163223266601562\n",
      "iteration 11945: loss: 0.23163112998008728\n",
      "iteration 11946: loss: 0.23163017630577087\n",
      "iteration 11947: loss: 0.23162928223609924\n",
      "iteration 11948: loss: 0.23162813484668732\n",
      "iteration 11949: loss: 0.2316271811723709\n",
      "iteration 11950: loss: 0.2316262274980545\n",
      "iteration 11951: loss: 0.23162508010864258\n",
      "iteration 11952: loss: 0.23162421584129333\n",
      "iteration 11953: loss: 0.2316233217716217\n",
      "iteration 11954: loss: 0.2316221445798874\n",
      "iteration 11955: loss: 0.23162122070789337\n",
      "iteration 11956: loss: 0.23162007331848145\n",
      "iteration 11957: loss: 0.23161914944648743\n",
      "iteration 11958: loss: 0.23161816596984863\n",
      "iteration 11959: loss: 0.2316170632839203\n",
      "iteration 11960: loss: 0.23161610960960388\n",
      "iteration 11961: loss: 0.23161515593528748\n",
      "iteration 11962: loss: 0.23161402344703674\n",
      "iteration 11963: loss: 0.23161308467388153\n",
      "iteration 11964: loss: 0.23161213099956512\n",
      "iteration 11965: loss: 0.23161101341247559\n",
      "iteration 11966: loss: 0.23161005973815918\n",
      "iteration 11967: loss: 0.23160891234874725\n",
      "iteration 11968: loss: 0.23160795867443085\n",
      "iteration 11969: loss: 0.23160719871520996\n",
      "iteration 11970: loss: 0.23160608112812042\n",
      "iteration 11971: loss: 0.2316051423549652\n",
      "iteration 11972: loss: 0.2316041737794876\n",
      "iteration 11973: loss: 0.23160305619239807\n",
      "iteration 11974: loss: 0.23160210251808167\n",
      "iteration 11975: loss: 0.23160114884376526\n",
      "iteration 11976: loss: 0.23160000145435333\n",
      "iteration 11977: loss: 0.2315990924835205\n",
      "iteration 11978: loss: 0.23159794509410858\n",
      "iteration 11979: loss: 0.23159699141979218\n",
      "iteration 11980: loss: 0.23159603774547577\n",
      "iteration 11981: loss: 0.23159492015838623\n",
      "iteration 11982: loss: 0.2315939962863922\n",
      "iteration 11983: loss: 0.2315930426120758\n",
      "iteration 11984: loss: 0.23159198462963104\n",
      "iteration 11985: loss: 0.2315911054611206\n",
      "iteration 11986: loss: 0.23158995807170868\n",
      "iteration 11987: loss: 0.23158903419971466\n",
      "iteration 11988: loss: 0.23158805072307587\n",
      "iteration 11989: loss: 0.2315869778394699\n",
      "iteration 11990: loss: 0.2315860092639923\n",
      "iteration 11991: loss: 0.23158502578735352\n",
      "iteration 11992: loss: 0.23158392310142517\n",
      "iteration 11993: loss: 0.23158296942710876\n",
      "iteration 11994: loss: 0.23158185184001923\n",
      "iteration 11995: loss: 0.23158088326454163\n",
      "iteration 11996: loss: 0.2315799444913864\n",
      "iteration 11997: loss: 0.2315787822008133\n",
      "iteration 11998: loss: 0.23157784342765808\n",
      "iteration 11999: loss: 0.23157672584056854\n",
      "iteration 12000: loss: 0.2315758913755417\n",
      "iteration 12001: loss: 0.2315749228000641\n",
      "iteration 12002: loss: 0.23157379031181335\n",
      "iteration 12003: loss: 0.23157286643981934\n",
      "iteration 12004: loss: 0.23157194256782532\n",
      "iteration 12005: loss: 0.23157081007957458\n",
      "iteration 12006: loss: 0.23156985640525818\n",
      "iteration 12007: loss: 0.23156873881816864\n",
      "iteration 12008: loss: 0.23156778514385223\n",
      "iteration 12009: loss: 0.23156683146953583\n",
      "iteration 12010: loss: 0.2315656840801239\n",
      "iteration 12011: loss: 0.2315647304058075\n",
      "iteration 12012: loss: 0.23156383633613586\n",
      "iteration 12013: loss: 0.23156268894672394\n",
      "iteration 12014: loss: 0.23156175017356873\n",
      "iteration 12015: loss: 0.231560617685318\n",
      "iteration 12016: loss: 0.2315596640110016\n",
      "iteration 12017: loss: 0.23155871033668518\n",
      "iteration 12018: loss: 0.231557697057724\n",
      "iteration 12019: loss: 0.2315567433834076\n",
      "iteration 12020: loss: 0.23155562579631805\n",
      "iteration 12021: loss: 0.23155467212200165\n",
      "iteration 12022: loss: 0.23155371844768524\n",
      "iteration 12023: loss: 0.2315526306629181\n",
      "iteration 12024: loss: 0.2315516471862793\n",
      "iteration 12025: loss: 0.23155057430267334\n",
      "iteration 12026: loss: 0.23154966533184052\n",
      "iteration 12027: loss: 0.23154869675636292\n",
      "iteration 12028: loss: 0.231547549366951\n",
      "iteration 12029: loss: 0.23154664039611816\n",
      "iteration 12030: loss: 0.23154565691947937\n",
      "iteration 12031: loss: 0.23154453933238983\n",
      "iteration 12032: loss: 0.23154358565807343\n",
      "iteration 12033: loss: 0.2315424680709839\n",
      "iteration 12034: loss: 0.23154154419898987\n",
      "iteration 12035: loss: 0.23154056072235107\n",
      "iteration 12036: loss: 0.2315395623445511\n",
      "iteration 12037: loss: 0.23153862357139587\n",
      "iteration 12038: loss: 0.23153746128082275\n",
      "iteration 12039: loss: 0.23153658211231232\n",
      "iteration 12040: loss: 0.23153558373451233\n",
      "iteration 12041: loss: 0.2315344512462616\n",
      "iteration 12042: loss: 0.2315334975719452\n",
      "iteration 12043: loss: 0.23153240978717804\n",
      "iteration 12044: loss: 0.23153145611286163\n",
      "iteration 12045: loss: 0.23153050243854523\n",
      "iteration 12046: loss: 0.2315293848514557\n",
      "iteration 12047: loss: 0.23152844607830048\n",
      "iteration 12048: loss: 0.23152735829353333\n",
      "iteration 12049: loss: 0.23152637481689453\n",
      "iteration 12050: loss: 0.231525257229805\n",
      "iteration 12051: loss: 0.2315243035554886\n",
      "iteration 12052: loss: 0.23152336478233337\n",
      "iteration 12053: loss: 0.23152224719524384\n",
      "iteration 12054: loss: 0.2315213680267334\n",
      "iteration 12055: loss: 0.23152026534080505\n",
      "iteration 12056: loss: 0.23151931166648865\n",
      "iteration 12057: loss: 0.23151835799217224\n",
      "iteration 12058: loss: 0.2315172404050827\n",
      "iteration 12059: loss: 0.23151633143424988\n",
      "iteration 12060: loss: 0.23151519894599915\n",
      "iteration 12061: loss: 0.23151424527168274\n",
      "iteration 12062: loss: 0.23151330649852753\n",
      "iteration 12063: loss: 0.231512188911438\n",
      "iteration 12064: loss: 0.23151123523712158\n",
      "iteration 12065: loss: 0.23151016235351562\n",
      "iteration 12066: loss: 0.2315092384815216\n",
      "iteration 12067: loss: 0.2315082848072052\n",
      "iteration 12068: loss: 0.23150713741779327\n",
      "iteration 12069: loss: 0.23150619864463806\n",
      "iteration 12070: loss: 0.2315051108598709\n",
      "iteration 12071: loss: 0.23150412738323212\n",
      "iteration 12072: loss: 0.2315031737089157\n",
      "iteration 12073: loss: 0.23150210082530975\n",
      "iteration 12074: loss: 0.23150114715099335\n",
      "iteration 12075: loss: 0.23150010406970978\n",
      "iteration 12076: loss: 0.23149926960468292\n",
      "iteration 12077: loss: 0.231498122215271\n",
      "iteration 12078: loss: 0.23149721324443817\n",
      "iteration 12079: loss: 0.23149624466896057\n",
      "iteration 12080: loss: 0.23149514198303223\n",
      "iteration 12081: loss: 0.23149418830871582\n",
      "iteration 12082: loss: 0.23149308562278748\n",
      "iteration 12083: loss: 0.23149213194847107\n",
      "iteration 12084: loss: 0.23149104416370392\n",
      "iteration 12085: loss: 0.23149006068706512\n",
      "iteration 12086: loss: 0.2314891368150711\n",
      "iteration 12087: loss: 0.23148798942565918\n",
      "iteration 12088: loss: 0.23148703575134277\n",
      "iteration 12089: loss: 0.23148593306541443\n",
      "iteration 12090: loss: 0.23148496448993683\n",
      "iteration 12091: loss: 0.23148402571678162\n",
      "iteration 12092: loss: 0.23148301243782043\n",
      "iteration 12093: loss: 0.23148207366466522\n",
      "iteration 12094: loss: 0.23148095607757568\n",
      "iteration 12095: loss: 0.23148000240325928\n",
      "iteration 12096: loss: 0.23147892951965332\n",
      "iteration 12097: loss: 0.23147797584533691\n",
      "iteration 12098: loss: 0.23147699236869812\n",
      "iteration 12099: loss: 0.23147591948509216\n",
      "iteration 12100: loss: 0.23147496581077576\n",
      "iteration 12101: loss: 0.23147384822368622\n",
      "iteration 12102: loss: 0.23147287964820862\n",
      "iteration 12103: loss: 0.2314719706773758\n",
      "iteration 12104: loss: 0.23147089779376984\n",
      "iteration 12105: loss: 0.23146991431713104\n",
      "iteration 12106: loss: 0.2314688265323639\n",
      "iteration 12107: loss: 0.23146787285804749\n",
      "iteration 12108: loss: 0.23146677017211914\n",
      "iteration 12109: loss: 0.23146581649780273\n",
      "iteration 12110: loss: 0.2314646989107132\n",
      "iteration 12111: loss: 0.23146376013755798\n",
      "iteration 12112: loss: 0.23146280646324158\n",
      "iteration 12113: loss: 0.23146171867847443\n",
      "iteration 12114: loss: 0.23146076500415802\n",
      "iteration 12115: loss: 0.2314596176147461\n",
      "iteration 12116: loss: 0.23145878314971924\n",
      "iteration 12117: loss: 0.23145756125450134\n",
      "iteration 12118: loss: 0.23145675659179688\n",
      "iteration 12119: loss: 0.23145580291748047\n",
      "iteration 12120: loss: 0.23145465552806854\n",
      "iteration 12121: loss: 0.23145374655723572\n",
      "iteration 12122: loss: 0.23145265877246857\n",
      "iteration 12123: loss: 0.23145170509815216\n",
      "iteration 12124: loss: 0.23145055770874023\n",
      "iteration 12125: loss: 0.23144963383674622\n",
      "iteration 12126: loss: 0.231448695063591\n",
      "iteration 12127: loss: 0.23144754767417908\n",
      "iteration 12128: loss: 0.23144665360450745\n",
      "iteration 12129: loss: 0.23144550621509552\n",
      "iteration 12130: loss: 0.2314445972442627\n",
      "iteration 12131: loss: 0.23144343495368958\n",
      "iteration 12132: loss: 0.23144252598285675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12133: loss: 0.2314414083957672\n",
      "iteration 12134: loss: 0.231440469622612\n",
      "iteration 12135: loss: 0.2314395010471344\n",
      "iteration 12136: loss: 0.23143842816352844\n",
      "iteration 12137: loss: 0.23143747448921204\n",
      "iteration 12138: loss: 0.2314363420009613\n",
      "iteration 12139: loss: 0.2314353883266449\n",
      "iteration 12140: loss: 0.23143427073955536\n",
      "iteration 12141: loss: 0.23143334686756134\n",
      "iteration 12142: loss: 0.2314322292804718\n",
      "iteration 12143: loss: 0.23143139481544495\n",
      "iteration 12144: loss: 0.23143048584461212\n",
      "iteration 12145: loss: 0.2314293384552002\n",
      "iteration 12146: loss: 0.23142845928668976\n",
      "iteration 12147: loss: 0.23142731189727783\n",
      "iteration 12148: loss: 0.2314263880252838\n",
      "iteration 12149: loss: 0.23142528533935547\n",
      "iteration 12150: loss: 0.23142430186271667\n",
      "iteration 12151: loss: 0.23142321407794952\n",
      "iteration 12152: loss: 0.23142226040363312\n",
      "iteration 12153: loss: 0.23142138123512268\n",
      "iteration 12154: loss: 0.23142023384571075\n",
      "iteration 12155: loss: 0.23141932487487793\n",
      "iteration 12156: loss: 0.231418177485466\n",
      "iteration 12157: loss: 0.2314172238111496\n",
      "iteration 12158: loss: 0.23141610622406006\n",
      "iteration 12159: loss: 0.23141518235206604\n",
      "iteration 12160: loss: 0.2314140796661377\n",
      "iteration 12161: loss: 0.2314131259918213\n",
      "iteration 12162: loss: 0.23141202330589294\n",
      "iteration 12163: loss: 0.23141109943389893\n",
      "iteration 12164: loss: 0.23141011595726013\n",
      "iteration 12165: loss: 0.23140902817249298\n",
      "iteration 12166: loss: 0.23140811920166016\n",
      "iteration 12167: loss: 0.23140697181224823\n",
      "iteration 12168: loss: 0.23140601813793182\n",
      "iteration 12169: loss: 0.2314048707485199\n",
      "iteration 12170: loss: 0.23140397667884827\n",
      "iteration 12171: loss: 0.23140296339988708\n",
      "iteration 12172: loss: 0.23140200972557068\n",
      "iteration 12173: loss: 0.23140092194080353\n",
      "iteration 12174: loss: 0.23139998316764832\n",
      "iteration 12175: loss: 0.2313990592956543\n",
      "iteration 12176: loss: 0.23139795660972595\n",
      "iteration 12177: loss: 0.23139700293540955\n",
      "iteration 12178: loss: 0.23139588534832\n",
      "iteration 12179: loss: 0.23139503598213196\n",
      "iteration 12180: loss: 0.2313939332962036\n",
      "iteration 12181: loss: 0.23139294981956482\n",
      "iteration 12182: loss: 0.23139186203479767\n",
      "iteration 12183: loss: 0.23139095306396484\n",
      "iteration 12184: loss: 0.2313898503780365\n",
      "iteration 12185: loss: 0.23138892650604248\n",
      "iteration 12186: loss: 0.23138776421546936\n",
      "iteration 12187: loss: 0.23138685524463654\n",
      "iteration 12188: loss: 0.23138591647148132\n",
      "iteration 12189: loss: 0.23138479888439178\n",
      "iteration 12190: loss: 0.23138387501239777\n",
      "iteration 12191: loss: 0.23138277232646942\n",
      "iteration 12192: loss: 0.23138180375099182\n",
      "iteration 12193: loss: 0.23138073086738586\n",
      "iteration 12194: loss: 0.23137979209423065\n",
      "iteration 12195: loss: 0.2313786745071411\n",
      "iteration 12196: loss: 0.2313777506351471\n",
      "iteration 12197: loss: 0.23137661814689636\n",
      "iteration 12198: loss: 0.23137572407722473\n",
      "iteration 12199: loss: 0.231374591588974\n",
      "iteration 12200: loss: 0.23137366771697998\n",
      "iteration 12201: loss: 0.23137256503105164\n",
      "iteration 12202: loss: 0.23137161135673523\n",
      "iteration 12203: loss: 0.23137064278125763\n",
      "iteration 12204: loss: 0.23136954009532928\n",
      "iteration 12205: loss: 0.23136861622333527\n",
      "iteration 12206: loss: 0.2313675880432129\n",
      "iteration 12207: loss: 0.23136667907238007\n",
      "iteration 12208: loss: 0.23136553168296814\n",
      "iteration 12209: loss: 0.2313646376132965\n",
      "iteration 12210: loss: 0.23136349022388458\n",
      "iteration 12211: loss: 0.23136258125305176\n",
      "iteration 12212: loss: 0.23136146366596222\n",
      "iteration 12213: loss: 0.231360524892807\n",
      "iteration 12214: loss: 0.23135940730571747\n",
      "iteration 12215: loss: 0.23135845363140106\n",
      "iteration 12216: loss: 0.2313573807477951\n",
      "iteration 12217: loss: 0.2313564270734787\n",
      "iteration 12218: loss: 0.23135533928871155\n",
      "iteration 12219: loss: 0.23135438561439514\n",
      "iteration 12220: loss: 0.23135331273078918\n",
      "iteration 12221: loss: 0.23135237395763397\n",
      "iteration 12222: loss: 0.23135128617286682\n",
      "iteration 12223: loss: 0.23135033249855042\n",
      "iteration 12224: loss: 0.2313493937253952\n",
      "iteration 12225: loss: 0.23134827613830566\n",
      "iteration 12226: loss: 0.23134732246398926\n",
      "iteration 12227: loss: 0.23134620487689972\n",
      "iteration 12228: loss: 0.2313452959060669\n",
      "iteration 12229: loss: 0.23134419322013855\n",
      "iteration 12230: loss: 0.23134326934814453\n",
      "iteration 12231: loss: 0.231342151761055\n",
      "iteration 12232: loss: 0.23134121298789978\n",
      "iteration 12233: loss: 0.23134012520313263\n",
      "iteration 12234: loss: 0.23133918642997742\n",
      "iteration 12235: loss: 0.23133806884288788\n",
      "iteration 12236: loss: 0.23133714497089386\n",
      "iteration 12237: loss: 0.23133604228496552\n",
      "iteration 12238: loss: 0.2313351184129715\n",
      "iteration 12239: loss: 0.23133400082588196\n",
      "iteration 12240: loss: 0.23133304715156555\n",
      "iteration 12241: loss: 0.2313319444656372\n",
      "iteration 12242: loss: 0.2313310205936432\n",
      "iteration 12243: loss: 0.23132987320423126\n",
      "iteration 12244: loss: 0.23132896423339844\n",
      "iteration 12245: loss: 0.23132793605327606\n",
      "iteration 12246: loss: 0.23132701218128204\n",
      "iteration 12247: loss: 0.2313259094953537\n",
      "iteration 12248: loss: 0.2313249558210373\n",
      "iteration 12249: loss: 0.23132386803627014\n",
      "iteration 12250: loss: 0.23132291436195374\n",
      "iteration 12251: loss: 0.23132184147834778\n",
      "iteration 12252: loss: 0.23132088780403137\n",
      "iteration 12253: loss: 0.23131978511810303\n",
      "iteration 12254: loss: 0.231318861246109\n",
      "iteration 12255: loss: 0.23131772875785828\n",
      "iteration 12256: loss: 0.23131683468818665\n",
      "iteration 12257: loss: 0.23131568729877472\n",
      "iteration 12258: loss: 0.2313147485256195\n",
      "iteration 12259: loss: 0.23131367564201355\n",
      "iteration 12260: loss: 0.2313127964735031\n",
      "iteration 12261: loss: 0.2313118427991867\n",
      "iteration 12262: loss: 0.23131072521209717\n",
      "iteration 12263: loss: 0.23130977153778076\n",
      "iteration 12264: loss: 0.23130866885185242\n",
      "iteration 12265: loss: 0.2313077747821808\n",
      "iteration 12266: loss: 0.23130664229393005\n",
      "iteration 12267: loss: 0.23130571842193604\n",
      "iteration 12268: loss: 0.2313046008348465\n",
      "iteration 12269: loss: 0.2313036471605301\n",
      "iteration 12270: loss: 0.23130258917808533\n",
      "iteration 12271: loss: 0.2313014715909958\n",
      "iteration 12272: loss: 0.23130054771900177\n",
      "iteration 12273: loss: 0.23129959404468536\n",
      "iteration 12274: loss: 0.23129849135875702\n",
      "iteration 12275: loss: 0.231297567486763\n",
      "iteration 12276: loss: 0.23129646480083466\n",
      "iteration 12277: loss: 0.23129551112651825\n",
      "iteration 12278: loss: 0.23129448294639587\n",
      "iteration 12279: loss: 0.23129358887672424\n",
      "iteration 12280: loss: 0.2312924563884735\n",
      "iteration 12281: loss: 0.2312915027141571\n",
      "iteration 12282: loss: 0.23129041492938995\n",
      "iteration 12283: loss: 0.23128947615623474\n",
      "iteration 12284: loss: 0.2312883883714676\n",
      "iteration 12285: loss: 0.23128743469715118\n",
      "iteration 12286: loss: 0.23128633201122284\n",
      "iteration 12287: loss: 0.23128542304039001\n",
      "iteration 12288: loss: 0.23128430545330048\n",
      "iteration 12289: loss: 0.23128335177898407\n",
      "iteration 12290: loss: 0.23128226399421692\n",
      "iteration 12291: loss: 0.23128119111061096\n",
      "iteration 12292: loss: 0.23128023743629456\n",
      "iteration 12293: loss: 0.23127928376197815\n",
      "iteration 12294: loss: 0.2312782108783722\n",
      "iteration 12295: loss: 0.23127710819244385\n",
      "iteration 12296: loss: 0.23127615451812744\n",
      "iteration 12297: loss: 0.23127508163452148\n",
      "iteration 12298: loss: 0.23127417266368866\n",
      "iteration 12299: loss: 0.2312730848789215\n",
      "iteration 12300: loss: 0.2312721461057663\n",
      "iteration 12301: loss: 0.23127102851867676\n",
      "iteration 12302: loss: 0.23127011954784393\n",
      "iteration 12303: loss: 0.23126907646656036\n",
      "iteration 12304: loss: 0.23126816749572754\n",
      "iteration 12305: loss: 0.23126694560050964\n",
      "iteration 12306: loss: 0.23126602172851562\n",
      "iteration 12307: loss: 0.23126502335071564\n",
      "iteration 12308: loss: 0.23126406967639923\n",
      "iteration 12309: loss: 0.2312629669904709\n",
      "iteration 12310: loss: 0.23126208782196045\n",
      "iteration 12311: loss: 0.2312609702348709\n",
      "iteration 12312: loss: 0.2312600314617157\n",
      "iteration 12313: loss: 0.23125891387462616\n",
      "iteration 12314: loss: 0.23125799000263214\n",
      "iteration 12315: loss: 0.2312568873167038\n",
      "iteration 12316: loss: 0.2312559336423874\n",
      "iteration 12317: loss: 0.23125484585762024\n",
      "iteration 12318: loss: 0.23125390708446503\n",
      "iteration 12319: loss: 0.23125281929969788\n",
      "iteration 12320: loss: 0.23125188052654266\n",
      "iteration 12321: loss: 0.23125076293945312\n",
      "iteration 12322: loss: 0.2312498390674591\n",
      "iteration 12323: loss: 0.23124876618385315\n",
      "iteration 12324: loss: 0.23124763369560242\n",
      "iteration 12325: loss: 0.2312467098236084\n",
      "iteration 12326: loss: 0.23124560713768005\n",
      "iteration 12327: loss: 0.23124471306800842\n",
      "iteration 12328: loss: 0.2312435805797577\n",
      "iteration 12329: loss: 0.23124265670776367\n",
      "iteration 12330: loss: 0.23124155402183533\n",
      "iteration 12331: loss: 0.2312406301498413\n",
      "iteration 12332: loss: 0.23123952746391296\n",
      "iteration 12333: loss: 0.23123860359191895\n",
      "iteration 12334: loss: 0.2312375009059906\n",
      "iteration 12335: loss: 0.2312365472316742\n",
      "iteration 12336: loss: 0.23123545944690704\n",
      "iteration 12337: loss: 0.23123455047607422\n",
      "iteration 12338: loss: 0.23123344779014587\n",
      "iteration 12339: loss: 0.23123252391815186\n",
      "iteration 12340: loss: 0.2312314212322235\n",
      "iteration 12341: loss: 0.23123052716255188\n",
      "iteration 12342: loss: 0.23122939467430115\n",
      "iteration 12343: loss: 0.23122844099998474\n",
      "iteration 12344: loss: 0.23122736811637878\n",
      "iteration 12345: loss: 0.23122629523277283\n",
      "iteration 12346: loss: 0.23122534155845642\n",
      "iteration 12347: loss: 0.23122426867485046\n",
      "iteration 12348: loss: 0.23122331500053406\n",
      "iteration 12349: loss: 0.23122219741344452\n",
      "iteration 12350: loss: 0.2312212884426117\n",
      "iteration 12351: loss: 0.23122017085552216\n",
      "iteration 12352: loss: 0.23121926188468933\n",
      "iteration 12353: loss: 0.2312181442975998\n",
      "iteration 12354: loss: 0.2312171906232834\n",
      "iteration 12355: loss: 0.23121611773967743\n",
      "iteration 12356: loss: 0.2312152087688446\n",
      "iteration 12357: loss: 0.23121412098407745\n",
      "iteration 12358: loss: 0.23121300339698792\n",
      "iteration 12359: loss: 0.2312120944261551\n",
      "iteration 12360: loss: 0.23121097683906555\n",
      "iteration 12361: loss: 0.23121003806591034\n",
      "iteration 12362: loss: 0.2312089502811432\n",
      "iteration 12363: loss: 0.23120804131031036\n",
      "iteration 12364: loss: 0.23120689392089844\n",
      "iteration 12365: loss: 0.2312059849500656\n",
      "iteration 12366: loss: 0.23120489716529846\n",
      "iteration 12367: loss: 0.23120398819446564\n",
      "iteration 12368: loss: 0.2312028855085373\n",
      "iteration 12369: loss: 0.2312019318342209\n",
      "iteration 12370: loss: 0.23120084404945374\n",
      "iteration 12371: loss: 0.2311997413635254\n",
      "iteration 12372: loss: 0.23119878768920898\n",
      "iteration 12373: loss: 0.2311978042125702\n",
      "iteration 12374: loss: 0.23119688034057617\n",
      "iteration 12375: loss: 0.2311958372592926\n",
      "iteration 12376: loss: 0.2311948984861374\n",
      "iteration 12377: loss: 0.23119378089904785\n",
      "iteration 12378: loss: 0.23119287192821503\n",
      "iteration 12379: loss: 0.23119178414344788\n",
      "iteration 12380: loss: 0.23119068145751953\n",
      "iteration 12381: loss: 0.23118972778320312\n",
      "iteration 12382: loss: 0.23118862509727478\n",
      "iteration 12383: loss: 0.23118773102760315\n",
      "iteration 12384: loss: 0.2311866283416748\n",
      "iteration 12385: loss: 0.23118571937084198\n",
      "iteration 12386: loss: 0.23118463158607483\n",
      "iteration 12387: loss: 0.23118369281291962\n",
      "iteration 12388: loss: 0.23118257522583008\n",
      "iteration 12389: loss: 0.23118166625499725\n",
      "iteration 12390: loss: 0.2311805784702301\n",
      "iteration 12391: loss: 0.23117947578430176\n",
      "iteration 12392: loss: 0.23117849230766296\n",
      "iteration 12393: loss: 0.231177419424057\n",
      "iteration 12394: loss: 0.23117652535438538\n",
      "iteration 12395: loss: 0.23117542266845703\n",
      "iteration 12396: loss: 0.23117446899414062\n",
      "iteration 12397: loss: 0.23117342591285706\n",
      "iteration 12398: loss: 0.23117244243621826\n",
      "iteration 12399: loss: 0.2311713695526123\n",
      "iteration 12400: loss: 0.23117046058177948\n",
      "iteration 12401: loss: 0.23116934299468994\n",
      "iteration 12402: loss: 0.23116827011108398\n",
      "iteration 12403: loss: 0.23116731643676758\n",
      "iteration 12404: loss: 0.23116624355316162\n",
      "iteration 12405: loss: 0.2311653196811676\n",
      "iteration 12406: loss: 0.23116421699523926\n",
      "iteration 12407: loss: 0.23116330802440643\n",
      "iteration 12408: loss: 0.23116222023963928\n",
      "iteration 12409: loss: 0.23116107285022736\n",
      "iteration 12410: loss: 0.23116007447242737\n",
      "iteration 12411: loss: 0.23115897178649902\n",
      "iteration 12412: loss: 0.23115813732147217\n",
      "iteration 12413: loss: 0.23115701973438263\n",
      "iteration 12414: loss: 0.2311561107635498\n",
      "iteration 12415: loss: 0.23115511238574982\n",
      "iteration 12416: loss: 0.23115405440330505\n",
      "iteration 12417: loss: 0.2311529666185379\n",
      "iteration 12418: loss: 0.23115189373493195\n",
      "iteration 12419: loss: 0.23115094006061554\n",
      "iteration 12420: loss: 0.23114988207817078\n",
      "iteration 12421: loss: 0.23114891350269318\n",
      "iteration 12422: loss: 0.23114784061908722\n",
      "iteration 12423: loss: 0.231146901845932\n",
      "iteration 12424: loss: 0.23114582896232605\n",
      "iteration 12425: loss: 0.2311447113752365\n",
      "iteration 12426: loss: 0.2311438024044037\n",
      "iteration 12427: loss: 0.23114272952079773\n",
      "iteration 12428: loss: 0.2311418056488037\n",
      "iteration 12429: loss: 0.23114068806171417\n",
      "iteration 12430: loss: 0.23113974928855896\n",
      "iteration 12431: loss: 0.231138676404953\n",
      "iteration 12432: loss: 0.23113755881786346\n",
      "iteration 12433: loss: 0.23113664984703064\n",
      "iteration 12434: loss: 0.23113557696342468\n",
      "iteration 12435: loss: 0.23113462328910828\n",
      "iteration 12436: loss: 0.23113353550434113\n",
      "iteration 12437: loss: 0.2311326265335083\n",
      "iteration 12438: loss: 0.23113150894641876\n",
      "iteration 12439: loss: 0.2311304360628128\n",
      "iteration 12440: loss: 0.2311294972896576\n",
      "iteration 12441: loss: 0.23112842440605164\n",
      "iteration 12442: loss: 0.23112753033638\n",
      "iteration 12443: loss: 0.23112639784812927\n",
      "iteration 12444: loss: 0.23112550377845764\n",
      "iteration 12445: loss: 0.2311244010925293\n",
      "iteration 12446: loss: 0.23112332820892334\n",
      "iteration 12447: loss: 0.23112237453460693\n",
      "iteration 12448: loss: 0.23112130165100098\n",
      "iteration 12449: loss: 0.23112039268016815\n",
      "iteration 12450: loss: 0.2311193197965622\n",
      "iteration 12451: loss: 0.2311183661222458\n",
      "iteration 12452: loss: 0.23111729323863983\n",
      "iteration 12453: loss: 0.23111622035503387\n",
      "iteration 12454: loss: 0.23111526668071747\n",
      "iteration 12455: loss: 0.2311141937971115\n",
      "iteration 12456: loss: 0.2311132699251175\n",
      "iteration 12457: loss: 0.23111207783222198\n",
      "iteration 12458: loss: 0.23111113905906677\n",
      "iteration 12459: loss: 0.23111006617546082\n",
      "iteration 12460: loss: 0.23110899329185486\n",
      "iteration 12461: loss: 0.23110803961753845\n",
      "iteration 12462: loss: 0.2311069518327713\n",
      "iteration 12463: loss: 0.23110604286193848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12464: loss: 0.23110494017601013\n",
      "iteration 12465: loss: 0.2311040163040161\n",
      "iteration 12466: loss: 0.2311030626296997\n",
      "iteration 12467: loss: 0.23110191524028778\n",
      "iteration 12468: loss: 0.23110100626945496\n",
      "iteration 12469: loss: 0.2310999631881714\n",
      "iteration 12470: loss: 0.23109900951385498\n",
      "iteration 12471: loss: 0.23109790682792664\n",
      "iteration 12472: loss: 0.23109683394432068\n",
      "iteration 12473: loss: 0.23109588027000427\n",
      "iteration 12474: loss: 0.2310948371887207\n",
      "iteration 12475: loss: 0.2310938835144043\n",
      "iteration 12476: loss: 0.23109281063079834\n",
      "iteration 12477: loss: 0.23109185695648193\n",
      "iteration 12478: loss: 0.2310907542705536\n",
      "iteration 12479: loss: 0.23108968138694763\n",
      "iteration 12480: loss: 0.2310887575149536\n",
      "iteration 12481: loss: 0.23108765482902527\n",
      "iteration 12482: loss: 0.23108676075935364\n",
      "iteration 12483: loss: 0.2310856580734253\n",
      "iteration 12484: loss: 0.23108455538749695\n",
      "iteration 12485: loss: 0.2310836762189865\n",
      "iteration 12486: loss: 0.23108258843421936\n",
      "iteration 12487: loss: 0.23108163475990295\n",
      "iteration 12488: loss: 0.23108050227165222\n",
      "iteration 12489: loss: 0.23107942938804626\n",
      "iteration 12490: loss: 0.23107853531837463\n",
      "iteration 12491: loss: 0.2310774326324463\n",
      "iteration 12492: loss: 0.23107647895812988\n",
      "iteration 12493: loss: 0.23107537627220154\n",
      "iteration 12494: loss: 0.23107433319091797\n",
      "iteration 12495: loss: 0.23107342422008514\n",
      "iteration 12496: loss: 0.23107227683067322\n",
      "iteration 12497: loss: 0.23107139766216278\n",
      "iteration 12498: loss: 0.23107028007507324\n",
      "iteration 12499: loss: 0.23106920719146729\n",
      "iteration 12500: loss: 0.23106828331947327\n",
      "iteration 12501: loss: 0.23106718063354492\n",
      "iteration 12502: loss: 0.2310662716627121\n",
      "iteration 12503: loss: 0.23106518387794495\n",
      "iteration 12504: loss: 0.23106412589550018\n",
      "iteration 12505: loss: 0.23106317222118378\n",
      "iteration 12506: loss: 0.23106209933757782\n",
      "iteration 12507: loss: 0.23106122016906738\n",
      "iteration 12508: loss: 0.23106007277965546\n",
      "iteration 12509: loss: 0.2310590296983719\n",
      "iteration 12510: loss: 0.2310580462217331\n",
      "iteration 12511: loss: 0.23105700314044952\n",
      "iteration 12512: loss: 0.2310560941696167\n",
      "iteration 12513: loss: 0.23105497658252716\n",
      "iteration 12514: loss: 0.2310539036989212\n",
      "iteration 12515: loss: 0.23105299472808838\n",
      "iteration 12516: loss: 0.23105189204216003\n",
      "iteration 12517: loss: 0.2310509979724884\n",
      "iteration 12518: loss: 0.23104989528656006\n",
      "iteration 12519: loss: 0.2310487926006317\n",
      "iteration 12520: loss: 0.23104779422283173\n",
      "iteration 12521: loss: 0.23104672133922577\n",
      "iteration 12522: loss: 0.23104581236839294\n",
      "iteration 12523: loss: 0.23104476928710938\n",
      "iteration 12524: loss: 0.23104366660118103\n",
      "iteration 12525: loss: 0.23104271292686462\n",
      "iteration 12526: loss: 0.23104166984558105\n",
      "iteration 12527: loss: 0.23104074597358704\n",
      "iteration 12528: loss: 0.2310396432876587\n",
      "iteration 12529: loss: 0.23103854060173035\n",
      "iteration 12530: loss: 0.23103761672973633\n",
      "iteration 12531: loss: 0.23103654384613037\n",
      "iteration 12532: loss: 0.23103563487529755\n",
      "iteration 12533: loss: 0.2310345619916916\n",
      "iteration 12534: loss: 0.23103344440460205\n",
      "iteration 12535: loss: 0.23103253543376923\n",
      "iteration 12536: loss: 0.23103144764900208\n",
      "iteration 12537: loss: 0.23103053867816925\n",
      "iteration 12538: loss: 0.2310294657945633\n",
      "iteration 12539: loss: 0.23102836310863495\n",
      "iteration 12540: loss: 0.23102745413780212\n",
      "iteration 12541: loss: 0.23102626204490662\n",
      "iteration 12542: loss: 0.23102521896362305\n",
      "iteration 12543: loss: 0.23102429509162903\n",
      "iteration 12544: loss: 0.2310231626033783\n",
      "iteration 12545: loss: 0.23102228343486786\n",
      "iteration 12546: loss: 0.2310211956501007\n",
      "iteration 12547: loss: 0.23102006316184998\n",
      "iteration 12548: loss: 0.23101916909217834\n",
      "iteration 12549: loss: 0.2310180962085724\n",
      "iteration 12550: loss: 0.23101715743541718\n",
      "iteration 12551: loss: 0.23101608455181122\n",
      "iteration 12552: loss: 0.23101496696472168\n",
      "iteration 12553: loss: 0.23101405799388885\n",
      "iteration 12554: loss: 0.23101305961608887\n",
      "iteration 12555: loss: 0.23101195693016052\n",
      "iteration 12556: loss: 0.23101110756397247\n",
      "iteration 12557: loss: 0.23101000487804413\n",
      "iteration 12558: loss: 0.2310090810060501\n",
      "iteration 12559: loss: 0.23100800812244415\n",
      "iteration 12560: loss: 0.23100697994232178\n",
      "iteration 12561: loss: 0.23100602626800537\n",
      "iteration 12562: loss: 0.23100492358207703\n",
      "iteration 12563: loss: 0.23100385069847107\n",
      "iteration 12564: loss: 0.23100295662879944\n",
      "iteration 12565: loss: 0.23100177943706512\n",
      "iteration 12566: loss: 0.2310008555650711\n",
      "iteration 12567: loss: 0.23099979758262634\n",
      "iteration 12568: loss: 0.2309986799955368\n",
      "iteration 12569: loss: 0.23099777102470398\n",
      "iteration 12570: loss: 0.23099669814109802\n",
      "iteration 12571: loss: 0.23099562525749207\n",
      "iteration 12572: loss: 0.23099470138549805\n",
      "iteration 12573: loss: 0.2309935986995697\n",
      "iteration 12574: loss: 0.23099267482757568\n",
      "iteration 12575: loss: 0.23099160194396973\n",
      "iteration 12576: loss: 0.23099049925804138\n",
      "iteration 12577: loss: 0.23098960518836975\n",
      "iteration 12578: loss: 0.2309885323047638\n",
      "iteration 12579: loss: 0.23098742961883545\n",
      "iteration 12580: loss: 0.23098650574684143\n",
      "iteration 12581: loss: 0.23098544776439667\n",
      "iteration 12582: loss: 0.23098435997962952\n",
      "iteration 12583: loss: 0.23098346590995789\n",
      "iteration 12584: loss: 0.23098234832286835\n",
      "iteration 12585: loss: 0.23098143935203552\n",
      "iteration 12586: loss: 0.2309802770614624\n",
      "iteration 12587: loss: 0.23097920417785645\n",
      "iteration 12588: loss: 0.23097828030586243\n",
      "iteration 12589: loss: 0.2309771478176117\n",
      "iteration 12590: loss: 0.23097610473632812\n",
      "iteration 12591: loss: 0.2309751957654953\n",
      "iteration 12592: loss: 0.23097407817840576\n",
      "iteration 12593: loss: 0.23097319900989532\n",
      "iteration 12594: loss: 0.23097212612628937\n",
      "iteration 12595: loss: 0.2309710532426834\n",
      "iteration 12596: loss: 0.23097014427185059\n",
      "iteration 12597: loss: 0.23096910119056702\n",
      "iteration 12598: loss: 0.23096802830696106\n",
      "iteration 12599: loss: 0.23096704483032227\n",
      "iteration 12600: loss: 0.2309660166501999\n",
      "iteration 12601: loss: 0.23096489906311035\n",
      "iteration 12602: loss: 0.23096399009227753\n",
      "iteration 12603: loss: 0.23096294701099396\n",
      "iteration 12604: loss: 0.230961874127388\n",
      "iteration 12605: loss: 0.23096084594726562\n",
      "iteration 12606: loss: 0.23095977306365967\n",
      "iteration 12607: loss: 0.23095867037773132\n",
      "iteration 12608: loss: 0.2309577465057373\n",
      "iteration 12609: loss: 0.23095670342445374\n",
      "iteration 12610: loss: 0.2309557944536209\n",
      "iteration 12611: loss: 0.23095469176769257\n",
      "iteration 12612: loss: 0.230953648686409\n",
      "iteration 12613: loss: 0.2309526950120926\n",
      "iteration 12614: loss: 0.23095162212848663\n",
      "iteration 12615: loss: 0.2309505194425583\n",
      "iteration 12616: loss: 0.23094961047172546\n",
      "iteration 12617: loss: 0.2309485375881195\n",
      "iteration 12618: loss: 0.2309476137161255\n",
      "iteration 12619: loss: 0.23094654083251953\n",
      "iteration 12620: loss: 0.23094546794891357\n",
      "iteration 12621: loss: 0.2309444695711136\n",
      "iteration 12622: loss: 0.23094336688518524\n",
      "iteration 12623: loss: 0.23094229400157928\n",
      "iteration 12624: loss: 0.23094137012958527\n",
      "iteration 12625: loss: 0.2309402972459793\n",
      "iteration 12626: loss: 0.23093923926353455\n",
      "iteration 12627: loss: 0.23093831539154053\n",
      "iteration 12628: loss: 0.23093721270561218\n",
      "iteration 12629: loss: 0.2309361696243286\n",
      "iteration 12630: loss: 0.2309352606534958\n",
      "iteration 12631: loss: 0.23093414306640625\n",
      "iteration 12632: loss: 0.23093314468860626\n",
      "iteration 12633: loss: 0.23093223571777344\n",
      "iteration 12634: loss: 0.2309311330318451\n",
      "iteration 12635: loss: 0.23093004524707794\n",
      "iteration 12636: loss: 0.23092904686927795\n",
      "iteration 12637: loss: 0.2309279888868332\n",
      "iteration 12638: loss: 0.23092691600322723\n",
      "iteration 12639: loss: 0.23092599213123322\n",
      "iteration 12640: loss: 0.23092499375343323\n",
      "iteration 12641: loss: 0.23092393577098846\n",
      "iteration 12642: loss: 0.23092298209667206\n",
      "iteration 12643: loss: 0.2309219390153885\n",
      "iteration 12644: loss: 0.23092086613178253\n",
      "iteration 12645: loss: 0.23091992735862732\n",
      "iteration 12646: loss: 0.23091888427734375\n",
      "iteration 12647: loss: 0.2309178113937378\n",
      "iteration 12648: loss: 0.23091688752174377\n",
      "iteration 12649: loss: 0.23091581463813782\n",
      "iteration 12650: loss: 0.2309148758649826\n",
      "iteration 12651: loss: 0.23091383278369904\n",
      "iteration 12652: loss: 0.23091264069080353\n",
      "iteration 12653: loss: 0.23091168701648712\n",
      "iteration 12654: loss: 0.23091062903404236\n",
      "iteration 12655: loss: 0.2309095561504364\n",
      "iteration 12656: loss: 0.23090866208076477\n",
      "iteration 12657: loss: 0.2309075891971588\n",
      "iteration 12658: loss: 0.23090648651123047\n",
      "iteration 12659: loss: 0.23090557754039764\n",
      "iteration 12660: loss: 0.2309045046567917\n",
      "iteration 12661: loss: 0.23090343177318573\n",
      "iteration 12662: loss: 0.2309025079011917\n",
      "iteration 12663: loss: 0.23090144991874695\n",
      "iteration 12664: loss: 0.23090028762817383\n",
      "iteration 12665: loss: 0.230899378657341\n",
      "iteration 12666: loss: 0.23089829087257385\n",
      "iteration 12667: loss: 0.2308972328901291\n",
      "iteration 12668: loss: 0.23089638352394104\n",
      "iteration 12669: loss: 0.2308952808380127\n",
      "iteration 12670: loss: 0.23089423775672913\n",
      "iteration 12671: loss: 0.2308932989835739\n",
      "iteration 12672: loss: 0.23089222609996796\n",
      "iteration 12673: loss: 0.230891153216362\n",
      "iteration 12674: loss: 0.23089022934436798\n",
      "iteration 12675: loss: 0.23088915646076202\n",
      "iteration 12676: loss: 0.23088808357715607\n",
      "iteration 12677: loss: 0.23088717460632324\n",
      "iteration 12678: loss: 0.23088601231575012\n",
      "iteration 12679: loss: 0.23088493943214417\n",
      "iteration 12680: loss: 0.23088403046131134\n",
      "iteration 12681: loss: 0.230882927775383\n",
      "iteration 12682: loss: 0.23088185489177704\n",
      "iteration 12683: loss: 0.230881005525589\n",
      "iteration 12684: loss: 0.23087987303733826\n",
      "iteration 12685: loss: 0.2308788001537323\n",
      "iteration 12686: loss: 0.23087787628173828\n",
      "iteration 12687: loss: 0.2308768332004547\n",
      "iteration 12688: loss: 0.23087576031684875\n",
      "iteration 12689: loss: 0.23087473213672638\n",
      "iteration 12690: loss: 0.23087365925312042\n",
      "iteration 12691: loss: 0.23087258636951447\n",
      "iteration 12692: loss: 0.2308715283870697\n",
      "iteration 12693: loss: 0.23087063431739807\n",
      "iteration 12694: loss: 0.23086956143379211\n",
      "iteration 12695: loss: 0.23086850345134735\n",
      "iteration 12696: loss: 0.23086754977703094\n",
      "iteration 12697: loss: 0.23086650669574738\n",
      "iteration 12698: loss: 0.23086540400981903\n",
      "iteration 12699: loss: 0.2308644950389862\n",
      "iteration 12700: loss: 0.23086342215538025\n",
      "iteration 12701: loss: 0.2308623492717743\n",
      "iteration 12702: loss: 0.23086142539978027\n",
      "iteration 12703: loss: 0.23086032271385193\n",
      "iteration 12704: loss: 0.23085923492908478\n",
      "iteration 12705: loss: 0.23085829615592957\n",
      "iteration 12706: loss: 0.2308572232723236\n",
      "iteration 12707: loss: 0.23085618019104004\n",
      "iteration 12708: loss: 0.23085527122020721\n",
      "iteration 12709: loss: 0.23085418343544006\n",
      "iteration 12710: loss: 0.2308531254529953\n",
      "iteration 12711: loss: 0.23085221648216248\n",
      "iteration 12712: loss: 0.23085112869739532\n",
      "iteration 12713: loss: 0.23085005581378937\n",
      "iteration 12714: loss: 0.23084907233715057\n",
      "iteration 12715: loss: 0.23084798455238342\n",
      "iteration 12716: loss: 0.23084692656993866\n",
      "iteration 12717: loss: 0.23084601759910583\n",
      "iteration 12718: loss: 0.23084494471549988\n",
      "iteration 12719: loss: 0.23084387183189392\n",
      "iteration 12720: loss: 0.23084278404712677\n",
      "iteration 12721: loss: 0.23084190487861633\n",
      "iteration 12722: loss: 0.230840802192688\n",
      "iteration 12723: loss: 0.23083987832069397\n",
      "iteration 12724: loss: 0.2308388650417328\n",
      "iteration 12725: loss: 0.23083782196044922\n",
      "iteration 12726: loss: 0.2308366745710373\n",
      "iteration 12727: loss: 0.23083575069904327\n",
      "iteration 12728: loss: 0.2308346927165985\n",
      "iteration 12729: loss: 0.23083361983299255\n",
      "iteration 12730: loss: 0.23083272576332092\n",
      "iteration 12731: loss: 0.23083162307739258\n",
      "iteration 12732: loss: 0.230830579996109\n",
      "iteration 12733: loss: 0.23082947731018066\n",
      "iteration 12734: loss: 0.23082859814167023\n",
      "iteration 12735: loss: 0.23082752525806427\n",
      "iteration 12736: loss: 0.23082642257213593\n",
      "iteration 12737: loss: 0.23082546889781952\n",
      "iteration 12738: loss: 0.23082439601421356\n",
      "iteration 12739: loss: 0.2308233678340912\n",
      "iteration 12740: loss: 0.23082247376441956\n",
      "iteration 12741: loss: 0.2308214008808136\n",
      "iteration 12742: loss: 0.23082029819488525\n",
      "iteration 12743: loss: 0.23081938922405243\n",
      "iteration 12744: loss: 0.23081831634044647\n",
      "iteration 12745: loss: 0.2308172732591629\n",
      "iteration 12746: loss: 0.23081617057323456\n",
      "iteration 12747: loss: 0.23081526160240173\n",
      "iteration 12748: loss: 0.230814129114151\n",
      "iteration 12749: loss: 0.23081305623054504\n",
      "iteration 12750: loss: 0.23081211745738983\n",
      "iteration 12751: loss: 0.23081111907958984\n",
      "iteration 12752: loss: 0.2308100163936615\n",
      "iteration 12753: loss: 0.23080897331237793\n",
      "iteration 12754: loss: 0.2308080494403839\n",
      "iteration 12755: loss: 0.23080697655677795\n",
      "iteration 12756: loss: 0.230805903673172\n",
      "iteration 12757: loss: 0.2308049499988556\n",
      "iteration 12758: loss: 0.23080381751060486\n",
      "iteration 12759: loss: 0.23080280423164368\n",
      "iteration 12760: loss: 0.23080189526081085\n",
      "iteration 12761: loss: 0.2308007925748825\n",
      "iteration 12762: loss: 0.23079971969127655\n",
      "iteration 12763: loss: 0.23079867660999298\n",
      "iteration 12764: loss: 0.23079773783683777\n",
      "iteration 12765: loss: 0.2307966947555542\n",
      "iteration 12766: loss: 0.23079562187194824\n",
      "iteration 12767: loss: 0.23079471290111542\n",
      "iteration 12768: loss: 0.2307935506105423\n",
      "iteration 12769: loss: 0.23079252243041992\n",
      "iteration 12770: loss: 0.23079140484333038\n",
      "iteration 12771: loss: 0.23079052567481995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12772: loss: 0.23078946769237518\n",
      "iteration 12773: loss: 0.23078837990760803\n",
      "iteration 12774: loss: 0.23078754544258118\n",
      "iteration 12775: loss: 0.23078647255897522\n",
      "iteration 12776: loss: 0.23078536987304688\n",
      "iteration 12777: loss: 0.23078422248363495\n",
      "iteration 12778: loss: 0.23078331351280212\n",
      "iteration 12779: loss: 0.23078227043151855\n",
      "iteration 12780: loss: 0.2307811677455902\n",
      "iteration 12781: loss: 0.23078028857707977\n",
      "iteration 12782: loss: 0.23077921569347382\n",
      "iteration 12783: loss: 0.23077814280986786\n",
      "iteration 12784: loss: 0.2307770997285843\n",
      "iteration 12785: loss: 0.2307761162519455\n",
      "iteration 12786: loss: 0.23077502846717834\n",
      "iteration 12787: loss: 0.23077397048473358\n",
      "iteration 12788: loss: 0.23077306151390076\n",
      "iteration 12789: loss: 0.2307720184326172\n",
      "iteration 12790: loss: 0.23077091574668884\n",
      "iteration 12791: loss: 0.23076987266540527\n",
      "iteration 12792: loss: 0.23076896369457245\n",
      "iteration 12793: loss: 0.2307678908109665\n",
      "iteration 12794: loss: 0.23076677322387695\n",
      "iteration 12795: loss: 0.23076584935188293\n",
      "iteration 12796: loss: 0.23076477646827698\n",
      "iteration 12797: loss: 0.23076370358467102\n",
      "iteration 12798: loss: 0.23076267540454865\n",
      "iteration 12799: loss: 0.23076172173023224\n",
      "iteration 12800: loss: 0.23076066374778748\n",
      "iteration 12801: loss: 0.23075957596302032\n",
      "iteration 12802: loss: 0.23075862228870392\n",
      "iteration 12803: loss: 0.23075751960277557\n",
      "iteration 12804: loss: 0.230756476521492\n",
      "iteration 12805: loss: 0.2307555228471756\n",
      "iteration 12806: loss: 0.23075461387634277\n",
      "iteration 12807: loss: 0.23075354099273682\n",
      "iteration 12808: loss: 0.23075249791145325\n",
      "iteration 12809: loss: 0.2307514250278473\n",
      "iteration 12810: loss: 0.23075047135353088\n",
      "iteration 12811: loss: 0.23074936866760254\n",
      "iteration 12812: loss: 0.23074834048748016\n",
      "iteration 12813: loss: 0.2307472676038742\n",
      "iteration 12814: loss: 0.23074638843536377\n",
      "iteration 12815: loss: 0.23074527084827423\n",
      "iteration 12816: loss: 0.23074421286582947\n",
      "iteration 12817: loss: 0.23074331879615784\n",
      "iteration 12818: loss: 0.23074224591255188\n",
      "iteration 12819: loss: 0.23074111342430115\n",
      "iteration 12820: loss: 0.2307399958372116\n",
      "iteration 12821: loss: 0.23073914647102356\n",
      "iteration 12822: loss: 0.2307380735874176\n",
      "iteration 12823: loss: 0.23073700070381165\n",
      "iteration 12824: loss: 0.23073610663414001\n",
      "iteration 12825: loss: 0.23073506355285645\n",
      "iteration 12826: loss: 0.2307339459657669\n",
      "iteration 12827: loss: 0.23073294758796692\n",
      "iteration 12828: loss: 0.23073191940784454\n",
      "iteration 12829: loss: 0.23073086142539978\n",
      "iteration 12830: loss: 0.23072978854179382\n",
      "iteration 12831: loss: 0.23072871565818787\n",
      "iteration 12832: loss: 0.23072779178619385\n",
      "iteration 12833: loss: 0.23072674870491028\n",
      "iteration 12834: loss: 0.23072567582130432\n",
      "iteration 12835: loss: 0.2307247668504715\n",
      "iteration 12836: loss: 0.23072364926338196\n",
      "iteration 12837: loss: 0.23072251677513123\n",
      "iteration 12838: loss: 0.23072150349617004\n",
      "iteration 12839: loss: 0.23072059452533722\n",
      "iteration 12840: loss: 0.23071953654289246\n",
      "iteration 12841: loss: 0.2307184934616089\n",
      "iteration 12842: loss: 0.23071739077568054\n",
      "iteration 12843: loss: 0.2307165563106537\n",
      "iteration 12844: loss: 0.23071542382240295\n",
      "iteration 12845: loss: 0.2307143658399582\n",
      "iteration 12846: loss: 0.23071327805519104\n",
      "iteration 12847: loss: 0.23071236908435822\n",
      "iteration 12848: loss: 0.23071134090423584\n",
      "iteration 12849: loss: 0.23071026802062988\n",
      "iteration 12850: loss: 0.23070919513702393\n",
      "iteration 12851: loss: 0.2307083159685135\n",
      "iteration 12852: loss: 0.23070719838142395\n",
      "iteration 12853: loss: 0.23070606589317322\n",
      "iteration 12854: loss: 0.23070502281188965\n",
      "iteration 12855: loss: 0.2307041585445404\n",
      "iteration 12856: loss: 0.23070304095745087\n",
      "iteration 12857: loss: 0.2307019680738449\n",
      "iteration 12858: loss: 0.23070092499256134\n",
      "iteration 12859: loss: 0.23070001602172852\n",
      "iteration 12860: loss: 0.23069897294044495\n",
      "iteration 12861: loss: 0.23069779574871063\n",
      "iteration 12862: loss: 0.23069675266742706\n",
      "iteration 12863: loss: 0.23069584369659424\n",
      "iteration 12864: loss: 0.23069480061531067\n",
      "iteration 12865: loss: 0.2306937277317047\n",
      "iteration 12866: loss: 0.23069269955158234\n",
      "iteration 12867: loss: 0.23069176077842712\n",
      "iteration 12868: loss: 0.23069067299365997\n",
      "iteration 12869: loss: 0.23068955540657043\n",
      "iteration 12870: loss: 0.23068849742412567\n",
      "iteration 12871: loss: 0.23068758845329285\n",
      "iteration 12872: loss: 0.23068654537200928\n",
      "iteration 12873: loss: 0.2306855022907257\n",
      "iteration 12874: loss: 0.2306845635175705\n",
      "iteration 12875: loss: 0.23068344593048096\n",
      "iteration 12876: loss: 0.230682373046875\n",
      "iteration 12877: loss: 0.23068132996559143\n",
      "iteration 12878: loss: 0.23068030178546906\n",
      "iteration 12879: loss: 0.23067939281463623\n",
      "iteration 12880: loss: 0.23067831993103027\n",
      "iteration 12881: loss: 0.2306772768497467\n",
      "iteration 12882: loss: 0.23067614436149597\n",
      "iteration 12883: loss: 0.23067522048950195\n",
      "iteration 12884: loss: 0.23067423701286316\n",
      "iteration 12885: loss: 0.2306731939315796\n",
      "iteration 12886: loss: 0.23067215085029602\n",
      "iteration 12887: loss: 0.2306712120771408\n",
      "iteration 12888: loss: 0.23067018389701843\n",
      "iteration 12889: loss: 0.2306690216064453\n",
      "iteration 12890: loss: 0.23066797852516174\n",
      "iteration 12891: loss: 0.23066706955432892\n",
      "iteration 12892: loss: 0.23066601157188416\n",
      "iteration 12893: loss: 0.2306649386882782\n",
      "iteration 12894: loss: 0.23066386580467224\n",
      "iteration 12895: loss: 0.2306629866361618\n",
      "iteration 12896: loss: 0.23066186904907227\n",
      "iteration 12897: loss: 0.2306608259677887\n",
      "iteration 12898: loss: 0.23065975308418274\n",
      "iteration 12899: loss: 0.23065868020057678\n",
      "iteration 12900: loss: 0.23065777122974396\n",
      "iteration 12901: loss: 0.2306567132472992\n",
      "iteration 12902: loss: 0.23065564036369324\n",
      "iteration 12903: loss: 0.23065462708473206\n",
      "iteration 12904: loss: 0.23065361380577087\n",
      "iteration 12905: loss: 0.23065254092216492\n",
      "iteration 12906: loss: 0.23065149784088135\n",
      "iteration 12907: loss: 0.23065045475959778\n",
      "iteration 12908: loss: 0.23064950108528137\n",
      "iteration 12909: loss: 0.2306484878063202\n",
      "iteration 12910: loss: 0.23064740002155304\n",
      "iteration 12911: loss: 0.23064632713794708\n",
      "iteration 12912: loss: 0.23064541816711426\n",
      "iteration 12913: loss: 0.23064438998699188\n",
      "iteration 12914: loss: 0.23064331710338593\n",
      "iteration 12915: loss: 0.23064222931861877\n",
      "iteration 12916: loss: 0.230641171336174\n",
      "iteration 12917: loss: 0.2306402176618576\n",
      "iteration 12918: loss: 0.23063917458057404\n",
      "iteration 12919: loss: 0.2306380718946457\n",
      "iteration 12920: loss: 0.23063704371452332\n",
      "iteration 12921: loss: 0.23063614964485168\n",
      "iteration 12922: loss: 0.23063507676124573\n",
      "iteration 12923: loss: 0.23063404858112335\n",
      "iteration 12924: loss: 0.23063287138938904\n",
      "iteration 12925: loss: 0.2306319773197174\n",
      "iteration 12926: loss: 0.23063087463378906\n",
      "iteration 12927: loss: 0.23062989115715027\n",
      "iteration 12928: loss: 0.23062880337238312\n",
      "iteration 12929: loss: 0.2306278944015503\n",
      "iteration 12930: loss: 0.23062674701213837\n",
      "iteration 12931: loss: 0.2306257039308548\n",
      "iteration 12932: loss: 0.23062464594841003\n",
      "iteration 12933: loss: 0.23062355816364288\n",
      "iteration 12934: loss: 0.23062272369861603\n",
      "iteration 12935: loss: 0.23062162101268768\n",
      "iteration 12936: loss: 0.2306205779314041\n",
      "iteration 12937: loss: 0.2306194305419922\n",
      "iteration 12938: loss: 0.23061852157115936\n",
      "iteration 12939: loss: 0.230617493391037\n",
      "iteration 12940: loss: 0.23061645030975342\n",
      "iteration 12941: loss: 0.23061534762382507\n",
      "iteration 12942: loss: 0.2306143343448639\n",
      "iteration 12943: loss: 0.2306133508682251\n",
      "iteration 12944: loss: 0.23061232268810272\n",
      "iteration 12945: loss: 0.23061127960681915\n",
      "iteration 12946: loss: 0.2306102216243744\n",
      "iteration 12947: loss: 0.23060932755470276\n",
      "iteration 12948: loss: 0.230608269572258\n",
      "iteration 12949: loss: 0.23060719668865204\n",
      "iteration 12950: loss: 0.2306060791015625\n",
      "iteration 12951: loss: 0.23060503602027893\n",
      "iteration 12952: loss: 0.2306041270494461\n",
      "iteration 12953: loss: 0.23060306906700134\n",
      "iteration 12954: loss: 0.23060202598571777\n",
      "iteration 12955: loss: 0.2306009829044342\n",
      "iteration 12956: loss: 0.2305998057126999\n",
      "iteration 12957: loss: 0.23059892654418945\n",
      "iteration 12958: loss: 0.23059789836406708\n",
      "iteration 12959: loss: 0.23059682548046112\n",
      "iteration 12960: loss: 0.23059587180614471\n",
      "iteration 12961: loss: 0.23059482872486115\n",
      "iteration 12962: loss: 0.23059383034706116\n",
      "iteration 12963: loss: 0.2305927723646164\n",
      "iteration 12964: loss: 0.23059169948101044\n",
      "iteration 12965: loss: 0.23059067130088806\n",
      "iteration 12966: loss: 0.23058977723121643\n",
      "iteration 12967: loss: 0.23058870434761047\n",
      "iteration 12968: loss: 0.23058755695819855\n",
      "iteration 12969: loss: 0.23058649897575378\n",
      "iteration 12970: loss: 0.23058542609214783\n",
      "iteration 12971: loss: 0.2305845320224762\n",
      "iteration 12972: loss: 0.23058347404003143\n",
      "iteration 12973: loss: 0.23058247566223145\n",
      "iteration 12974: loss: 0.23058131337165833\n",
      "iteration 12975: loss: 0.23058024048805237\n",
      "iteration 12976: loss: 0.23057937622070312\n",
      "iteration 12977: loss: 0.23057833313941956\n",
      "iteration 12978: loss: 0.2305772751569748\n",
      "iteration 12979: loss: 0.23057615756988525\n",
      "iteration 12980: loss: 0.23057512938976288\n",
      "iteration 12981: loss: 0.23057420551776886\n",
      "iteration 12982: loss: 0.23057317733764648\n",
      "iteration 12983: loss: 0.23057213425636292\n",
      "iteration 12984: loss: 0.230570986866951\n",
      "iteration 12985: loss: 0.23057007789611816\n",
      "iteration 12986: loss: 0.2305690497159958\n",
      "iteration 12987: loss: 0.23056797683238983\n",
      "iteration 12988: loss: 0.23056693375110626\n",
      "iteration 12989: loss: 0.2305658757686615\n",
      "iteration 12990: loss: 0.2305649220943451\n",
      "iteration 12991: loss: 0.23056383430957794\n",
      "iteration 12992: loss: 0.23056280612945557\n",
      "iteration 12993: loss: 0.230561763048172\n",
      "iteration 12994: loss: 0.23056070506572723\n",
      "iteration 12995: loss: 0.23055973649024963\n",
      "iteration 12996: loss: 0.23055867850780487\n",
      "iteration 12997: loss: 0.2305576354265213\n",
      "iteration 12998: loss: 0.23055657744407654\n",
      "iteration 12999: loss: 0.23055553436279297\n",
      "iteration 13000: loss: 0.2305545061826706\n",
      "iteration 13001: loss: 0.23055358231067657\n",
      "iteration 13002: loss: 0.23055246472358704\n",
      "iteration 13003: loss: 0.23055139183998108\n",
      "iteration 13004: loss: 0.23055033385753632\n",
      "iteration 13005: loss: 0.23054926097393036\n",
      "iteration 13006: loss: 0.23054838180541992\n",
      "iteration 13007: loss: 0.23054727911949158\n",
      "iteration 13008: loss: 0.23054620623588562\n",
      "iteration 13009: loss: 0.23054516315460205\n",
      "iteration 13010: loss: 0.23054412007331848\n",
      "iteration 13011: loss: 0.23054321110248566\n",
      "iteration 13012: loss: 0.23054225742816925\n",
      "iteration 13013: loss: 0.23054108023643494\n",
      "iteration 13014: loss: 0.23054006695747375\n",
      "iteration 13015: loss: 0.230539008975029\n",
      "iteration 13016: loss: 0.23053796589374542\n",
      "iteration 13017: loss: 0.2305370569229126\n",
      "iteration 13018: loss: 0.23053590953350067\n",
      "iteration 13019: loss: 0.2305348664522171\n",
      "iteration 13020: loss: 0.23053379356861115\n",
      "iteration 13021: loss: 0.2305329144001007\n",
      "iteration 13022: loss: 0.23053185641765594\n",
      "iteration 13023: loss: 0.23053070902824402\n",
      "iteration 13024: loss: 0.23052969574928284\n",
      "iteration 13025: loss: 0.23052863776683807\n",
      "iteration 13026: loss: 0.23052763938903809\n",
      "iteration 13027: loss: 0.23052671551704407\n",
      "iteration 13028: loss: 0.23052558302879333\n",
      "iteration 13029: loss: 0.23052453994750977\n",
      "iteration 13030: loss: 0.2305234968662262\n",
      "iteration 13031: loss: 0.23052242398262024\n",
      "iteration 13032: loss: 0.23052139580249786\n",
      "iteration 13033: loss: 0.23052041232585907\n",
      "iteration 13034: loss: 0.2305193692445755\n",
      "iteration 13035: loss: 0.2305183857679367\n",
      "iteration 13036: loss: 0.23051734268665314\n",
      "iteration 13037: loss: 0.23051628470420837\n",
      "iteration 13038: loss: 0.23051539063453674\n",
      "iteration 13039: loss: 0.230514258146286\n",
      "iteration 13040: loss: 0.23051324486732483\n",
      "iteration 13041: loss: 0.23051218688488007\n",
      "iteration 13042: loss: 0.23051109910011292\n",
      "iteration 13043: loss: 0.23051008582115173\n",
      "iteration 13044: loss: 0.23050911724567413\n",
      "iteration 13045: loss: 0.23050811886787415\n",
      "iteration 13046: loss: 0.23050706088542938\n",
      "iteration 13047: loss: 0.2305060178041458\n",
      "iteration 13048: loss: 0.23050490021705627\n",
      "iteration 13049: loss: 0.23050400614738464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 13050: loss: 0.23050296306610107\n",
      "iteration 13051: loss: 0.2305019199848175\n",
      "iteration 13052: loss: 0.23050084710121155\n",
      "iteration 13053: loss: 0.2304997444152832\n",
      "iteration 13054: loss: 0.23049867153167725\n",
      "iteration 13055: loss: 0.2304977923631668\n",
      "iteration 13056: loss: 0.23049673438072205\n",
      "iteration 13057: loss: 0.23049569129943848\n",
      "iteration 13058: loss: 0.2304946482181549\n",
      "iteration 13059: loss: 0.23049350082874298\n",
      "iteration 13060: loss: 0.23049244284629822\n",
      "iteration 13061: loss: 0.23049156367778778\n",
      "iteration 13062: loss: 0.23049049079418182\n",
      "iteration 13063: loss: 0.23048946261405945\n",
      "iteration 13064: loss: 0.23048833012580872\n",
      "iteration 13065: loss: 0.23048730194568634\n",
      "iteration 13066: loss: 0.2304864227771759\n",
      "iteration 13067: loss: 0.23048539459705353\n",
      "iteration 13068: loss: 0.2304842472076416\n",
      "iteration 13069: loss: 0.23048317432403564\n",
      "iteration 13070: loss: 0.23048214614391327\n",
      "iteration 13071: loss: 0.2304811030626297\n",
      "iteration 13072: loss: 0.23048022389411926\n",
      "iteration 13073: loss: 0.23047909140586853\n",
      "iteration 13074: loss: 0.23047804832458496\n",
      "iteration 13075: loss: 0.230476975440979\n",
      "iteration 13076: loss: 0.23047594726085663\n",
      "iteration 13077: loss: 0.23047490417957306\n",
      "iteration 13078: loss: 0.23047396540641785\n",
      "iteration 13079: loss: 0.2304728925228119\n",
      "iteration 13080: loss: 0.2304718792438507\n",
      "iteration 13081: loss: 0.23047080636024475\n",
      "iteration 13082: loss: 0.2304697483778\n",
      "iteration 13083: loss: 0.23046870529651642\n",
      "iteration 13084: loss: 0.23046772181987762\n",
      "iteration 13085: loss: 0.23046669363975525\n",
      "iteration 13086: loss: 0.23046565055847168\n",
      "iteration 13087: loss: 0.2304646074771881\n",
      "iteration 13088: loss: 0.23046350479125977\n",
      "iteration 13089: loss: 0.23046240210533142\n",
      "iteration 13090: loss: 0.23046156764030457\n",
      "iteration 13091: loss: 0.2304604947566986\n",
      "iteration 13092: loss: 0.23045948147773743\n",
      "iteration 13093: loss: 0.23045845329761505\n",
      "iteration 13094: loss: 0.23045730590820312\n",
      "iteration 13095: loss: 0.23045627772808075\n",
      "iteration 13096: loss: 0.2304552048444748\n",
      "iteration 13097: loss: 0.23045432567596436\n",
      "iteration 13098: loss: 0.23045317828655243\n",
      "iteration 13099: loss: 0.23045213520526886\n",
      "iteration 13100: loss: 0.23045110702514648\n",
      "iteration 13101: loss: 0.23045006394386292\n",
      "iteration 13102: loss: 0.23044903576374054\n",
      "iteration 13103: loss: 0.23044803738594055\n",
      "iteration 13104: loss: 0.23044700920581818\n",
      "iteration 13105: loss: 0.2304459512233734\n",
      "iteration 13106: loss: 0.23044490814208984\n",
      "iteration 13107: loss: 0.2304437905550003\n",
      "iteration 13108: loss: 0.2304428070783615\n",
      "iteration 13109: loss: 0.23044192790985107\n",
      "iteration 13110: loss: 0.2304408997297287\n",
      "iteration 13111: loss: 0.2304399013519287\n",
      "iteration 13112: loss: 0.23043875396251678\n",
      "iteration 13113: loss: 0.23043768107891083\n",
      "iteration 13114: loss: 0.23043665289878845\n",
      "iteration 13115: loss: 0.23043565452098846\n",
      "iteration 13116: loss: 0.23043473064899445\n",
      "iteration 13117: loss: 0.2304336279630661\n",
      "iteration 13118: loss: 0.23043258488178253\n",
      "iteration 13119: loss: 0.23043155670166016\n",
      "iteration 13120: loss: 0.2304304838180542\n",
      "iteration 13121: loss: 0.23042936623096466\n",
      "iteration 13122: loss: 0.23042836785316467\n",
      "iteration 13123: loss: 0.23042742908000946\n",
      "iteration 13124: loss: 0.2304263859987259\n",
      "iteration 13125: loss: 0.23042535781860352\n",
      "iteration 13126: loss: 0.23042424023151398\n",
      "iteration 13127: loss: 0.2304231822490692\n",
      "iteration 13128: loss: 0.23042216897010803\n",
      "iteration 13129: loss: 0.23042114078998566\n",
      "iteration 13130: loss: 0.23042020201683044\n",
      "iteration 13131: loss: 0.2304191142320633\n",
      "iteration 13132: loss: 0.23041808605194092\n",
      "iteration 13133: loss: 0.23041701316833496\n",
      "iteration 13134: loss: 0.23041601479053497\n",
      "iteration 13135: loss: 0.23041489720344543\n",
      "iteration 13136: loss: 0.23041383922100067\n",
      "iteration 13137: loss: 0.23041291534900665\n",
      "iteration 13138: loss: 0.2304118573665619\n",
      "iteration 13139: loss: 0.23041076958179474\n",
      "iteration 13140: loss: 0.23040974140167236\n",
      "iteration 13141: loss: 0.23040871322155\n",
      "iteration 13142: loss: 0.2304076850414276\n",
      "iteration 13143: loss: 0.23040661215782166\n",
      "iteration 13144: loss: 0.23040565848350525\n",
      "iteration 13145: loss: 0.23040464520454407\n",
      "iteration 13146: loss: 0.2304035872220993\n",
      "iteration 13147: loss: 0.23040255904197693\n",
      "iteration 13148: loss: 0.2304014414548874\n",
      "iteration 13149: loss: 0.23040039837360382\n",
      "iteration 13150: loss: 0.23039937019348145\n",
      "iteration 13151: loss: 0.23039846122264862\n",
      "iteration 13152: loss: 0.23039737343788147\n",
      "iteration 13153: loss: 0.2303963601589203\n",
      "iteration 13154: loss: 0.23039528727531433\n",
      "iteration 13155: loss: 0.23039427399635315\n",
      "iteration 13156: loss: 0.23039314150810242\n",
      "iteration 13157: loss: 0.23039209842681885\n",
      "iteration 13158: loss: 0.23039107024669647\n",
      "iteration 13159: loss: 0.23039016127586365\n",
      "iteration 13160: loss: 0.23038899898529053\n",
      "iteration 13161: loss: 0.23038801550865173\n",
      "iteration 13162: loss: 0.23038697242736816\n",
      "iteration 13163: loss: 0.2303859293460846\n",
      "iteration 13164: loss: 0.23038479685783386\n",
      "iteration 13165: loss: 0.2303837537765503\n",
      "iteration 13166: loss: 0.23038288950920105\n",
      "iteration 13167: loss: 0.23038184642791748\n",
      "iteration 13168: loss: 0.23038069903850555\n",
      "iteration 13169: loss: 0.23037967085838318\n",
      "iteration 13170: loss: 0.2303786277770996\n",
      "iteration 13171: loss: 0.23037762939929962\n",
      "iteration 13172: loss: 0.23037655651569366\n",
      "iteration 13173: loss: 0.23037543892860413\n",
      "iteration 13174: loss: 0.23037457466125488\n",
      "iteration 13175: loss: 0.2303735464811325\n",
      "iteration 13176: loss: 0.23037251830101013\n",
      "iteration 13177: loss: 0.2303714007139206\n",
      "iteration 13178: loss: 0.23037037253379822\n",
      "iteration 13179: loss: 0.2303694188594818\n",
      "iteration 13180: loss: 0.23036828637123108\n",
      "iteration 13181: loss: 0.2303672581911087\n",
      "iteration 13182: loss: 0.23036639392375946\n",
      "iteration 13183: loss: 0.2303653210401535\n",
      "iteration 13184: loss: 0.23036427795886993\n",
      "iteration 13185: loss: 0.2303631752729416\n",
      "iteration 13186: loss: 0.23036213219165802\n",
      "iteration 13187: loss: 0.23036110401153564\n",
      "iteration 13188: loss: 0.23036006093025208\n",
      "iteration 13189: loss: 0.23035892844200134\n",
      "iteration 13190: loss: 0.2303580790758133\n",
      "iteration 13191: loss: 0.23035702109336853\n",
      "iteration 13192: loss: 0.23035597801208496\n",
      "iteration 13193: loss: 0.23035487532615662\n",
      "iteration 13194: loss: 0.23035383224487305\n",
      "iteration 13195: loss: 0.23035280406475067\n",
      "iteration 13196: loss: 0.23035165667533875\n",
      "iteration 13197: loss: 0.23035064339637756\n",
      "iteration 13198: loss: 0.2303495854139328\n",
      "iteration 13199: loss: 0.23034875094890594\n",
      "iteration 13200: loss: 0.2303476333618164\n",
      "iteration 13201: loss: 0.23034659028053284\n",
      "iteration 13202: loss: 0.23034556210041046\n",
      "iteration 13203: loss: 0.23034453392028809\n",
      "iteration 13204: loss: 0.23034338653087616\n",
      "iteration 13205: loss: 0.2303423434495926\n",
      "iteration 13206: loss: 0.2303413450717926\n",
      "iteration 13207: loss: 0.23034033179283142\n",
      "iteration 13208: loss: 0.2303394377231598\n",
      "iteration 13209: loss: 0.23033833503723145\n",
      "iteration 13210: loss: 0.23033729195594788\n",
      "iteration 13211: loss: 0.2303362637758255\n",
      "iteration 13212: loss: 0.23033514618873596\n",
      "iteration 13213: loss: 0.2303340882062912\n",
      "iteration 13214: loss: 0.2303330898284912\n",
      "iteration 13215: loss: 0.23033204674720764\n",
      "iteration 13216: loss: 0.2303309142589569\n",
      "iteration 13217: loss: 0.23033006489276886\n",
      "iteration 13218: loss: 0.2303289920091629\n",
      "iteration 13219: loss: 0.23032799363136292\n",
      "iteration 13220: loss: 0.23032686114311218\n",
      "iteration 13221: loss: 0.230325847864151\n",
      "iteration 13222: loss: 0.23032478988170624\n",
      "iteration 13223: loss: 0.23032376170158386\n",
      "iteration 13224: loss: 0.2303226739168167\n",
      "iteration 13225: loss: 0.23032161593437195\n",
      "iteration 13226: loss: 0.23032069206237793\n",
      "iteration 13227: loss: 0.23031969368457794\n",
      "iteration 13228: loss: 0.23031854629516602\n",
      "iteration 13229: loss: 0.2303175926208496\n",
      "iteration 13230: loss: 0.23031648993492126\n",
      "iteration 13231: loss: 0.2303154021501541\n",
      "iteration 13232: loss: 0.23031434416770935\n",
      "iteration 13233: loss: 0.23031330108642578\n",
      "iteration 13234: loss: 0.2303123027086258\n",
      "iteration 13235: loss: 0.2303113043308258\n",
      "iteration 13236: loss: 0.23031029105186462\n",
      "iteration 13237: loss: 0.23030927777290344\n",
      "iteration 13238: loss: 0.23030824959278107\n",
      "iteration 13239: loss: 0.23030714690685272\n",
      "iteration 13240: loss: 0.23030610382556915\n",
      "iteration 13241: loss: 0.23030507564544678\n",
      "iteration 13242: loss: 0.23030397295951843\n",
      "iteration 13243: loss: 0.23030292987823486\n",
      "iteration 13244: loss: 0.2303019016981125\n",
      "iteration 13245: loss: 0.23030102252960205\n",
      "iteration 13246: loss: 0.23029997944831848\n",
      "iteration 13247: loss: 0.23029887676239014\n",
      "iteration 13248: loss: 0.23029784858226776\n",
      "iteration 13249: loss: 0.23029692471027374\n",
      "iteration 13250: loss: 0.23029573261737823\n",
      "iteration 13251: loss: 0.23029473423957825\n",
      "iteration 13252: loss: 0.23029370605945587\n",
      "iteration 13253: loss: 0.2302926778793335\n",
      "iteration 13254: loss: 0.23029156029224396\n",
      "iteration 13255: loss: 0.23029053211212158\n",
      "iteration 13256: loss: 0.23028965294361115\n",
      "iteration 13257: loss: 0.23028865456581116\n",
      "iteration 13258: loss: 0.23028750717639923\n",
      "iteration 13259: loss: 0.23028647899627686\n",
      "iteration 13260: loss: 0.2302854359149933\n",
      "iteration 13261: loss: 0.23028430342674255\n",
      "iteration 13262: loss: 0.23028329014778137\n",
      "iteration 13263: loss: 0.2302822768688202\n",
      "iteration 13264: loss: 0.23028123378753662\n",
      "iteration 13265: loss: 0.23028016090393066\n",
      "iteration 13266: loss: 0.2302791178226471\n",
      "iteration 13267: loss: 0.23027820885181427\n",
      "iteration 13268: loss: 0.2302771508693695\n",
      "iteration 13269: loss: 0.23027607798576355\n",
      "iteration 13270: loss: 0.23027506470680237\n",
      "iteration 13271: loss: 0.23027403652668\n",
      "iteration 13272: loss: 0.23027296364307404\n",
      "iteration 13273: loss: 0.23027193546295166\n",
      "iteration 13274: loss: 0.2302708923816681\n",
      "iteration 13275: loss: 0.23026975989341736\n",
      "iteration 13276: loss: 0.2302687168121338\n",
      "iteration 13277: loss: 0.23026788234710693\n",
      "iteration 13278: loss: 0.23026685416698456\n",
      "iteration 13279: loss: 0.23026570677757263\n",
      "iteration 13280: loss: 0.23026473820209503\n",
      "iteration 13281: loss: 0.23026368021965027\n",
      "iteration 13282: loss: 0.23026254773139954\n",
      "iteration 13283: loss: 0.23026153445243835\n",
      "iteration 13284: loss: 0.23026052117347717\n",
      "iteration 13285: loss: 0.2302594631910324\n",
      "iteration 13286: loss: 0.23025837540626526\n",
      "iteration 13287: loss: 0.2302573174238205\n",
      "iteration 13288: loss: 0.23025627434253693\n",
      "iteration 13289: loss: 0.23025517165660858\n",
      "iteration 13290: loss: 0.23025432229042053\n",
      "iteration 13291: loss: 0.23025326430797577\n",
      "iteration 13292: loss: 0.23025226593017578\n",
      "iteration 13293: loss: 0.23025114834308624\n",
      "iteration 13294: loss: 0.23025009036064148\n",
      "iteration 13295: loss: 0.2302490770816803\n",
      "iteration 13296: loss: 0.23024794459342957\n",
      "iteration 13297: loss: 0.23024694621562958\n",
      "iteration 13298: loss: 0.2302459180355072\n",
      "iteration 13299: loss: 0.23024487495422363\n",
      "iteration 13300: loss: 0.23024383187294006\n",
      "iteration 13301: loss: 0.2302427738904953\n",
      "iteration 13302: loss: 0.23024174571037292\n",
      "iteration 13303: loss: 0.23024067282676697\n",
      "iteration 13304: loss: 0.23023977875709534\n",
      "iteration 13305: loss: 0.23023872077465057\n",
      "iteration 13306: loss: 0.2302376925945282\n",
      "iteration 13307: loss: 0.23023661971092224\n",
      "iteration 13308: loss: 0.23023557662963867\n",
      "iteration 13309: loss: 0.23023457825183868\n",
      "iteration 13310: loss: 0.23023347556591034\n",
      "iteration 13311: loss: 0.23023240268230438\n",
      "iteration 13312: loss: 0.230231374502182\n",
      "iteration 13313: loss: 0.23023024201393127\n",
      "iteration 13314: loss: 0.23022925853729248\n",
      "iteration 13315: loss: 0.2302282601594925\n",
      "iteration 13316: loss: 0.23022735118865967\n",
      "iteration 13317: loss: 0.2302263081073761\n",
      "iteration 13318: loss: 0.23022527992725372\n",
      "iteration 13319: loss: 0.23022422194480896\n",
      "iteration 13320: loss: 0.230223149061203\n",
      "iteration 13321: loss: 0.23022213578224182\n",
      "iteration 13322: loss: 0.23022107779979706\n",
      "iteration 13323: loss: 0.2302200049161911\n",
      "iteration 13324: loss: 0.23021897673606873\n",
      "iteration 13325: loss: 0.23021796345710754\n",
      "iteration 13326: loss: 0.2302168607711792\n",
      "iteration 13327: loss: 0.23021581768989563\n",
      "iteration 13328: loss: 0.23021478950977325\n",
      "iteration 13329: loss: 0.2302137166261673\n",
      "iteration 13330: loss: 0.2302127182483673\n",
      "iteration 13331: loss: 0.23021169006824493\n",
      "iteration 13332: loss: 0.2302108108997345\n",
      "iteration 13333: loss: 0.23020967841148376\n",
      "iteration 13334: loss: 0.23020870983600616\n",
      "iteration 13335: loss: 0.2302076369524002\n",
      "iteration 13336: loss: 0.23020653426647186\n",
      "iteration 13337: loss: 0.23020553588867188\n",
      "iteration 13338: loss: 0.2302044928073883\n",
      "iteration 13339: loss: 0.23020339012145996\n",
      "iteration 13340: loss: 0.23020236194133759\n",
      "iteration 13341: loss: 0.2302013337612152\n",
      "iteration 13342: loss: 0.23020033538341522\n",
      "iteration 13343: loss: 0.23019924759864807\n",
      "iteration 13344: loss: 0.2301982343196869\n",
      "iteration 13345: loss: 0.23019719123840332\n",
      "iteration 13346: loss: 0.2301960438489914\n",
      "iteration 13347: loss: 0.23019501566886902\n",
      "iteration 13348: loss: 0.23019400238990784\n",
      "iteration 13349: loss: 0.23019304871559143\n",
      "iteration 13350: loss: 0.23019203543663025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 13351: loss: 0.23019099235534668\n",
      "iteration 13352: loss: 0.23018988966941833\n",
      "iteration 13353: loss: 0.23018887639045715\n",
      "iteration 13354: loss: 0.23018784821033478\n",
      "iteration 13355: loss: 0.23018674552440643\n",
      "iteration 13356: loss: 0.23018570244312286\n",
      "iteration 13357: loss: 0.23018470406532288\n",
      "iteration 13358: loss: 0.23018360137939453\n",
      "iteration 13359: loss: 0.23018257319927216\n",
      "iteration 13360: loss: 0.2301815003156662\n",
      "iteration 13361: loss: 0.2301805019378662\n",
      "iteration 13362: loss: 0.23017947375774384\n",
      "iteration 13363: loss: 0.23017844557762146\n",
      "iteration 13364: loss: 0.2301773577928543\n",
      "iteration 13365: loss: 0.23017629981040955\n",
      "iteration 13366: loss: 0.23017530143260956\n",
      "iteration 13367: loss: 0.23017434775829315\n",
      "iteration 13368: loss: 0.23017330467700958\n",
      "iteration 13369: loss: 0.2301722764968872\n",
      "iteration 13370: loss: 0.23017115890979767\n",
      "iteration 13371: loss: 0.2301701307296753\n",
      "iteration 13372: loss: 0.2301691472530365\n",
      "iteration 13373: loss: 0.23016802966594696\n",
      "iteration 13374: loss: 0.23016700148582458\n",
      "iteration 13375: loss: 0.2301660031080246\n",
      "iteration 13376: loss: 0.23016491532325745\n",
      "iteration 13377: loss: 0.23016390204429626\n",
      "iteration 13378: loss: 0.2301628589630127\n",
      "iteration 13379: loss: 0.23016174137592316\n",
      "iteration 13380: loss: 0.23016071319580078\n",
      "iteration 13381: loss: 0.2301597148180008\n",
      "iteration 13382: loss: 0.23015861213207245\n",
      "iteration 13383: loss: 0.23015764355659485\n",
      "iteration 13384: loss: 0.23015661537647247\n",
      "iteration 13385: loss: 0.2301555573940277\n",
      "iteration 13386: loss: 0.23015451431274414\n",
      "iteration 13387: loss: 0.23015347123146057\n",
      "iteration 13388: loss: 0.2301524430513382\n",
      "iteration 13389: loss: 0.23015150427818298\n",
      "iteration 13390: loss: 0.2301504909992218\n",
      "iteration 13391: loss: 0.230149507522583\n",
      "iteration 13392: loss: 0.23014840483665466\n",
      "iteration 13393: loss: 0.23014739155769348\n",
      "iteration 13394: loss: 0.23014631867408752\n",
      "iteration 13395: loss: 0.23014526069164276\n",
      "iteration 13396: loss: 0.2301442176103592\n",
      "iteration 13397: loss: 0.2301432192325592\n",
      "iteration 13398: loss: 0.23014214634895325\n",
      "iteration 13399: loss: 0.23014108836650848\n",
      "iteration 13400: loss: 0.2301400899887085\n",
      "iteration 13401: loss: 0.23013897240161896\n",
      "iteration 13402: loss: 0.23013798892498016\n",
      "iteration 13403: loss: 0.2301369607448578\n",
      "iteration 13404: loss: 0.23013587296009064\n",
      "iteration 13405: loss: 0.23013484477996826\n",
      "iteration 13406: loss: 0.2301338165998459\n",
      "iteration 13407: loss: 0.23013272881507874\n",
      "iteration 13408: loss: 0.23013171553611755\n",
      "iteration 13409: loss: 0.23013067245483398\n",
      "iteration 13410: loss: 0.23012952506542206\n",
      "iteration 13411: loss: 0.23012849688529968\n",
      "iteration 13412: loss: 0.23012754321098328\n",
      "iteration 13413: loss: 0.23012641072273254\n",
      "iteration 13414: loss: 0.23012542724609375\n",
      "iteration 13415: loss: 0.2301243096590042\n",
      "iteration 13416: loss: 0.23012328147888184\n",
      "iteration 13417: loss: 0.23012228310108185\n",
      "iteration 13418: loss: 0.2301211804151535\n",
      "iteration 13419: loss: 0.23012015223503113\n",
      "iteration 13420: loss: 0.23011915385723114\n",
      "iteration 13421: loss: 0.23011808097362518\n",
      "iteration 13422: loss: 0.2301170378923416\n",
      "iteration 13423: loss: 0.23011617362499237\n",
      "iteration 13424: loss: 0.23011508584022522\n",
      "iteration 13425: loss: 0.23011405766010284\n",
      "iteration 13426: loss: 0.23011302947998047\n",
      "iteration 13427: loss: 0.2301119565963745\n",
      "iteration 13428: loss: 0.23011092841625214\n",
      "iteration 13429: loss: 0.23010990023612976\n",
      "iteration 13430: loss: 0.2301088124513626\n",
      "iteration 13431: loss: 0.23010778427124023\n",
      "iteration 13432: loss: 0.23010675609111786\n",
      "iteration 13433: loss: 0.23010563850402832\n",
      "iteration 13434: loss: 0.23010465502738953\n",
      "iteration 13435: loss: 0.23010361194610596\n",
      "iteration 13436: loss: 0.2301025390625\n",
      "iteration 13437: loss: 0.2301015406847\n",
      "iteration 13438: loss: 0.23010043799877167\n",
      "iteration 13439: loss: 0.2300993949174881\n",
      "iteration 13440: loss: 0.23009836673736572\n",
      "iteration 13441: loss: 0.23009726405143738\n",
      "iteration 13442: loss: 0.23009629547595978\n",
      "iteration 13443: loss: 0.2300952672958374\n",
      "iteration 13444: loss: 0.23009414970874786\n",
      "iteration 13445: loss: 0.2300930917263031\n",
      "iteration 13446: loss: 0.2300920933485031\n",
      "iteration 13447: loss: 0.23009102046489716\n",
      "iteration 13448: loss: 0.23009005188941956\n",
      "iteration 13449: loss: 0.23008902370929718\n",
      "iteration 13450: loss: 0.2300879955291748\n",
      "iteration 13451: loss: 0.23008695244789124\n",
      "iteration 13452: loss: 0.23008587956428528\n",
      "iteration 13453: loss: 0.2300848513841629\n",
      "iteration 13454: loss: 0.23008385300636292\n",
      "iteration 13455: loss: 0.23008278012275696\n",
      "iteration 13456: loss: 0.230081707239151\n",
      "iteration 13457: loss: 0.2300807237625122\n",
      "iteration 13458: loss: 0.23007962107658386\n",
      "iteration 13459: loss: 0.23007865250110626\n",
      "iteration 13460: loss: 0.2300775945186615\n",
      "iteration 13461: loss: 0.23007650673389435\n",
      "iteration 13462: loss: 0.23007547855377197\n",
      "iteration 13463: loss: 0.2300744503736496\n",
      "iteration 13464: loss: 0.23007337749004364\n",
      "iteration 13465: loss: 0.23007233440876007\n",
      "iteration 13466: loss: 0.23007139563560486\n",
      "iteration 13467: loss: 0.23007023334503174\n",
      "iteration 13468: loss: 0.23006924986839294\n",
      "iteration 13469: loss: 0.23006828129291534\n",
      "iteration 13470: loss: 0.23006710410118103\n",
      "iteration 13471: loss: 0.23006610572338104\n",
      "iteration 13472: loss: 0.23006503283977509\n",
      "iteration 13473: loss: 0.2300640344619751\n",
      "iteration 13474: loss: 0.2300630509853363\n",
      "iteration 13475: loss: 0.23006203770637512\n",
      "iteration 13476: loss: 0.23006100952625275\n",
      "iteration 13477: loss: 0.2300599366426468\n",
      "iteration 13478: loss: 0.23005890846252441\n",
      "iteration 13479: loss: 0.23005792498588562\n",
      "iteration 13480: loss: 0.23005683720111847\n",
      "iteration 13481: loss: 0.2300558090209961\n",
      "iteration 13482: loss: 0.23005473613739014\n",
      "iteration 13483: loss: 0.23005370795726776\n",
      "iteration 13484: loss: 0.23005267977714539\n",
      "iteration 13485: loss: 0.23005160689353943\n",
      "iteration 13486: loss: 0.23005060851573944\n",
      "iteration 13487: loss: 0.23004961013793945\n",
      "iteration 13488: loss: 0.2300485372543335\n",
      "iteration 13489: loss: 0.23004750907421112\n",
      "iteration 13490: loss: 0.23004627227783203\n",
      "iteration 13491: loss: 0.2300454080104828\n",
      "iteration 13492: loss: 0.23004421591758728\n",
      "iteration 13493: loss: 0.23004314303398132\n",
      "iteration 13494: loss: 0.23004229366779327\n",
      "iteration 13495: loss: 0.2300412356853485\n",
      "iteration 13496: loss: 0.2300400286912918\n",
      "iteration 13497: loss: 0.23003911972045898\n",
      "iteration 13498: loss: 0.23003807663917542\n",
      "iteration 13499: loss: 0.23003706336021423\n",
      "iteration 13500: loss: 0.23003602027893066\n",
      "iteration 13501: loss: 0.23003491759300232\n",
      "iteration 13502: loss: 0.23003390431404114\n",
      "iteration 13503: loss: 0.23003284633159637\n",
      "iteration 13504: loss: 0.230031818151474\n",
      "iteration 13505: loss: 0.230030819773674\n",
      "iteration 13506: loss: 0.23002973198890686\n",
      "iteration 13507: loss: 0.23002871870994568\n",
      "iteration 13508: loss: 0.23002763092517853\n",
      "iteration 13509: loss: 0.23002663254737854\n",
      "iteration 13510: loss: 0.2300254851579666\n",
      "iteration 13511: loss: 0.23002441227436066\n",
      "iteration 13512: loss: 0.23002362251281738\n",
      "iteration 13513: loss: 0.23002254962921143\n",
      "iteration 13514: loss: 0.23002150654792786\n",
      "iteration 13515: loss: 0.23002049326896667\n",
      "iteration 13516: loss: 0.23001937568187714\n",
      "iteration 13517: loss: 0.23001834750175476\n",
      "iteration 13518: loss: 0.2300173044204712\n",
      "iteration 13519: loss: 0.23001627624034882\n",
      "iteration 13520: loss: 0.23001527786254883\n",
      "iteration 13521: loss: 0.23001417517662048\n",
      "iteration 13522: loss: 0.2300131618976593\n",
      "iteration 13523: loss: 0.23001191020011902\n",
      "iteration 13524: loss: 0.23001094162464142\n",
      "iteration 13525: loss: 0.23000994324684143\n",
      "iteration 13526: loss: 0.23000884056091309\n",
      "iteration 13527: loss: 0.2300078123807907\n",
      "iteration 13528: loss: 0.23000681400299072\n",
      "iteration 13529: loss: 0.23000574111938477\n",
      "iteration 13530: loss: 0.2300046980381012\n",
      "iteration 13531: loss: 0.23000362515449524\n",
      "iteration 13532: loss: 0.23000264167785645\n",
      "iteration 13533: loss: 0.23000161349773407\n",
      "iteration 13534: loss: 0.23000052571296692\n",
      "iteration 13535: loss: 0.22999949753284454\n",
      "iteration 13536: loss: 0.22999849915504456\n",
      "iteration 13537: loss: 0.2299973964691162\n",
      "iteration 13538: loss: 0.22999641299247742\n",
      "iteration 13539: loss: 0.22999532520771027\n",
      "iteration 13540: loss: 0.2299942970275879\n",
      "iteration 13541: loss: 0.22999334335327148\n",
      "iteration 13542: loss: 0.22999219596385956\n",
      "iteration 13543: loss: 0.22999119758605957\n",
      "iteration 13544: loss: 0.229990154504776\n",
      "iteration 13545: loss: 0.22998912632465363\n",
      "iteration 13546: loss: 0.22998802363872528\n",
      "iteration 13547: loss: 0.2299870252609253\n",
      "iteration 13548: loss: 0.2299860417842865\n",
      "iteration 13549: loss: 0.22998492419719696\n",
      "iteration 13550: loss: 0.22998392581939697\n",
      "iteration 13551: loss: 0.22998282313346863\n",
      "iteration 13552: loss: 0.22998180985450745\n",
      "iteration 13553: loss: 0.22998085618019104\n",
      "iteration 13554: loss: 0.2299797087907791\n",
      "iteration 13555: loss: 0.22997871041297913\n",
      "iteration 13556: loss: 0.22997768223285675\n",
      "iteration 13557: loss: 0.2299765646457672\n",
      "iteration 13558: loss: 0.2299756109714508\n",
      "iteration 13559: loss: 0.22997450828552246\n",
      "iteration 13560: loss: 0.2299734652042389\n",
      "iteration 13561: loss: 0.2299724817276001\n",
      "iteration 13562: loss: 0.22997140884399414\n",
      "iteration 13563: loss: 0.22997041046619415\n",
      "iteration 13564: loss: 0.2299693077802658\n",
      "iteration 13565: loss: 0.22996827960014343\n",
      "iteration 13566: loss: 0.22996708750724792\n",
      "iteration 13567: loss: 0.22996607422828674\n",
      "iteration 13568: loss: 0.22996512055397034\n",
      "iteration 13569: loss: 0.229964017868042\n",
      "iteration 13570: loss: 0.229963019490242\n",
      "iteration 13571: loss: 0.22996191680431366\n",
      "iteration 13572: loss: 0.22996091842651367\n",
      "iteration 13573: loss: 0.22995993494987488\n",
      "iteration 13574: loss: 0.2299589216709137\n",
      "iteration 13575: loss: 0.22995790839195251\n",
      "iteration 13576: loss: 0.22995679080486298\n",
      "iteration 13577: loss: 0.2299557626247406\n",
      "iteration 13578: loss: 0.22995468974113464\n",
      "iteration 13579: loss: 0.22995369136333466\n",
      "iteration 13580: loss: 0.22995266318321228\n",
      "iteration 13581: loss: 0.22995157539844513\n",
      "iteration 13582: loss: 0.22995057702064514\n",
      "iteration 13583: loss: 0.22994951903820038\n",
      "iteration 13584: loss: 0.229948490858078\n",
      "iteration 13585: loss: 0.22994740307331085\n",
      "iteration 13586: loss: 0.22994640469551086\n",
      "iteration 13587: loss: 0.22994542121887207\n",
      "iteration 13588: loss: 0.22994427382946014\n",
      "iteration 13589: loss: 0.22994327545166016\n",
      "iteration 13590: loss: 0.2299422323703766\n",
      "iteration 13591: loss: 0.2299412190914154\n",
      "iteration 13592: loss: 0.22994022071361542\n",
      "iteration 13593: loss: 0.22993913292884827\n",
      "iteration 13594: loss: 0.22993811964988708\n",
      "iteration 13595: loss: 0.22993695735931396\n",
      "iteration 13596: loss: 0.2299359291791916\n",
      "iteration 13597: loss: 0.22993484139442444\n",
      "iteration 13598: loss: 0.22993385791778564\n",
      "iteration 13599: loss: 0.22993282973766327\n",
      "iteration 13600: loss: 0.2299317568540573\n",
      "iteration 13601: loss: 0.22993075847625732\n",
      "iteration 13602: loss: 0.22992964088916779\n",
      "iteration 13603: loss: 0.229928657412529\n",
      "iteration 13604: loss: 0.22992758452892303\n",
      "iteration 13605: loss: 0.22992655634880066\n",
      "iteration 13606: loss: 0.22992555797100067\n",
      "iteration 13607: loss: 0.22992448508739471\n",
      "iteration 13608: loss: 0.22992348670959473\n",
      "iteration 13609: loss: 0.22992238402366638\n",
      "iteration 13610: loss: 0.22992141544818878\n",
      "iteration 13611: loss: 0.22992031276226044\n",
      "iteration 13612: loss: 0.22991931438446045\n",
      "iteration 13613: loss: 0.2299182415008545\n",
      "iteration 13614: loss: 0.22991721332073212\n",
      "iteration 13615: loss: 0.22991609573364258\n",
      "iteration 13616: loss: 0.22991511225700378\n",
      "iteration 13617: loss: 0.22991399466991425\n",
      "iteration 13618: loss: 0.2299129217863083\n",
      "iteration 13619: loss: 0.2299118936061859\n",
      "iteration 13620: loss: 0.22991082072257996\n",
      "iteration 13621: loss: 0.22990980744361877\n",
      "iteration 13622: loss: 0.2299087792634964\n",
      "iteration 13623: loss: 0.22990772128105164\n",
      "iteration 13624: loss: 0.22990679740905762\n",
      "iteration 13625: loss: 0.22990567982196808\n",
      "iteration 13626: loss: 0.2299046814441681\n",
      "iteration 13627: loss: 0.22990360856056213\n",
      "iteration 13628: loss: 0.22990258038043976\n",
      "iteration 13629: loss: 0.2299015074968338\n",
      "iteration 13630: loss: 0.2299005091190338\n",
      "iteration 13631: loss: 0.22989945113658905\n",
      "iteration 13632: loss: 0.22989848256111145\n",
      "iteration 13633: loss: 0.22989735007286072\n",
      "iteration 13634: loss: 0.22989635169506073\n",
      "iteration 13635: loss: 0.22989535331726074\n",
      "iteration 13636: loss: 0.2298942357301712\n",
      "iteration 13637: loss: 0.22989320755004883\n",
      "iteration 13638: loss: 0.22989210486412048\n",
      "iteration 13639: loss: 0.22989113628864288\n",
      "iteration 13640: loss: 0.22989006340503693\n",
      "iteration 13641: loss: 0.22988903522491455\n",
      "iteration 13642: loss: 0.2298879623413086\n",
      "iteration 13643: loss: 0.2298869639635086\n",
      "iteration 13644: loss: 0.2298859804868698\n",
      "iteration 13645: loss: 0.22988490760326385\n",
      "iteration 13646: loss: 0.22988387942314148\n",
      "iteration 13647: loss: 0.2298828363418579\n",
      "iteration 13648: loss: 0.22988183796405792\n",
      "iteration 13649: loss: 0.22988073527812958\n",
      "iteration 13650: loss: 0.2298797070980072\n",
      "iteration 13651: loss: 0.22987863421440125\n",
      "iteration 13652: loss: 0.22987768054008484\n",
      "iteration 13653: loss: 0.22987644374370575\n",
      "iteration 13654: loss: 0.22987544536590576\n",
      "iteration 13655: loss: 0.2298744171857834\n",
      "iteration 13656: loss: 0.22987337410449982\n",
      "iteration 13657: loss: 0.22987237572669983\n",
      "iteration 13658: loss: 0.2298712432384491\n",
      "iteration 13659: loss: 0.2298702746629715\n",
      "iteration 13660: loss: 0.22986920177936554\n",
      "iteration 13661: loss: 0.22986817359924316\n",
      "iteration 13662: loss: 0.2298671305179596\n",
      "iteration 13663: loss: 0.2298661470413208\n",
      "iteration 13664: loss: 0.22986504435539246\n",
      "iteration 13665: loss: 0.22986403107643127\n",
      "iteration 13666: loss: 0.22986304759979248\n",
      "iteration 13667: loss: 0.2298618108034134\n",
      "iteration 13668: loss: 0.2298608273267746\n",
      "iteration 13669: loss: 0.22985978424549103\n",
      "iteration 13670: loss: 0.22985875606536865\n",
      "iteration 13671: loss: 0.2298576533794403\n",
      "iteration 13672: loss: 0.2298566848039627\n",
      "iteration 13673: loss: 0.22985558211803436\n",
      "iteration 13674: loss: 0.22985458374023438\n",
      "iteration 13675: loss: 0.2298535406589508\n",
      "iteration 13676: loss: 0.22985252737998962\n",
      "iteration 13677: loss: 0.22985151410102844\n",
      "iteration 13678: loss: 0.22985045611858368\n",
      "iteration 13679: loss: 0.2298494279384613\n",
      "iteration 13680: loss: 0.22984838485717773\n",
      "iteration 13681: loss: 0.2298472821712494\n",
      "iteration 13682: loss: 0.22984616458415985\n",
      "iteration 13683: loss: 0.22984519600868225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 13684: loss: 0.2298441231250763\n",
      "iteration 13685: loss: 0.2298431098461151\n",
      "iteration 13686: loss: 0.22984203696250916\n",
      "iteration 13687: loss: 0.22984103858470917\n",
      "iteration 13688: loss: 0.2298399657011032\n",
      "iteration 13689: loss: 0.22983893752098083\n",
      "iteration 13690: loss: 0.22983786463737488\n",
      "iteration 13691: loss: 0.2298368662595749\n",
      "iteration 13692: loss: 0.22983583807945251\n",
      "iteration 13693: loss: 0.22983479499816895\n",
      "iteration 13694: loss: 0.229833722114563\n",
      "iteration 13695: loss: 0.229832723736763\n",
      "iteration 13696: loss: 0.2298317849636078\n",
      "iteration 13697: loss: 0.2298305779695511\n",
      "iteration 13698: loss: 0.2298295795917511\n",
      "iteration 13699: loss: 0.22982847690582275\n",
      "iteration 13700: loss: 0.22982752323150635\n",
      "iteration 13701: loss: 0.229826420545578\n",
      "iteration 13702: loss: 0.22982542216777802\n",
      "iteration 13703: loss: 0.22982434928417206\n",
      "iteration 13704: loss: 0.22982335090637207\n",
      "iteration 13705: loss: 0.2298223078250885\n",
      "iteration 13706: loss: 0.22982129454612732\n",
      "iteration 13707: loss: 0.22982025146484375\n",
      "iteration 13708: loss: 0.22981925308704376\n",
      "iteration 13709: loss: 0.22981803119182587\n",
      "iteration 13710: loss: 0.2298170030117035\n",
      "iteration 13711: loss: 0.2298160344362259\n",
      "iteration 13712: loss: 0.22981493175029755\n",
      "iteration 13713: loss: 0.22981396317481995\n",
      "iteration 13714: loss: 0.2298128604888916\n",
      "iteration 13715: loss: 0.2298118770122528\n",
      "iteration 13716: loss: 0.22981080412864685\n",
      "iteration 13717: loss: 0.22980983555316925\n",
      "iteration 13718: loss: 0.2298087328672409\n",
      "iteration 13719: loss: 0.22980761528015137\n",
      "iteration 13720: loss: 0.22980651259422302\n",
      "iteration 13721: loss: 0.22980554401874542\n",
      "iteration 13722: loss: 0.22980447113513947\n",
      "iteration 13723: loss: 0.22980347275733948\n",
      "iteration 13724: loss: 0.22980241477489471\n",
      "iteration 13725: loss: 0.22980138659477234\n",
      "iteration 13726: loss: 0.22980031371116638\n",
      "iteration 13727: loss: 0.22979934513568878\n",
      "iteration 13728: loss: 0.22979824244976044\n",
      "iteration 13729: loss: 0.22979727387428284\n",
      "iteration 13730: loss: 0.22979608178138733\n",
      "iteration 13731: loss: 0.22979506850242615\n",
      "iteration 13732: loss: 0.22979402542114258\n",
      "iteration 13733: loss: 0.2297929972410202\n",
      "iteration 13734: loss: 0.22979192435741425\n",
      "iteration 13735: loss: 0.22979100048542023\n",
      "iteration 13736: loss: 0.22978992760181427\n",
      "iteration 13737: loss: 0.22978892922401428\n",
      "iteration 13738: loss: 0.22978782653808594\n",
      "iteration 13739: loss: 0.22978684306144714\n",
      "iteration 13740: loss: 0.22978563606739044\n",
      "iteration 13741: loss: 0.22978463768959045\n",
      "iteration 13742: loss: 0.22978360950946808\n",
      "iteration 13743: loss: 0.2297825813293457\n",
      "iteration 13744: loss: 0.22978147864341736\n",
      "iteration 13745: loss: 0.22978051006793976\n",
      "iteration 13746: loss: 0.229779452085495\n",
      "iteration 13747: loss: 0.2297784388065338\n",
      "iteration 13748: loss: 0.22977738082408905\n",
      "iteration 13749: loss: 0.22977638244628906\n",
      "iteration 13750: loss: 0.2297753095626831\n",
      "iteration 13751: loss: 0.22977419197559357\n",
      "iteration 13752: loss: 0.22977308928966522\n",
      "iteration 13753: loss: 0.22977212071418762\n",
      "iteration 13754: loss: 0.22977101802825928\n",
      "iteration 13755: loss: 0.22977010905742645\n",
      "iteration 13756: loss: 0.2297690212726593\n",
      "iteration 13757: loss: 0.22976796329021454\n",
      "iteration 13758: loss: 0.22976699471473694\n",
      "iteration 13759: loss: 0.22976592183113098\n",
      "iteration 13760: loss: 0.2297649383544922\n",
      "iteration 13761: loss: 0.2297637015581131\n",
      "iteration 13762: loss: 0.2297627478837967\n",
      "iteration 13763: loss: 0.22976168990135193\n",
      "iteration 13764: loss: 0.22976072132587433\n",
      "iteration 13765: loss: 0.22975961863994598\n",
      "iteration 13766: loss: 0.22975865006446838\n",
      "iteration 13767: loss: 0.22975759208202362\n",
      "iteration 13768: loss: 0.22975656390190125\n",
      "iteration 13769: loss: 0.2297554910182953\n",
      "iteration 13770: loss: 0.2297545224428177\n",
      "iteration 13771: loss: 0.22975333034992218\n",
      "iteration 13772: loss: 0.2297523021697998\n",
      "iteration 13773: loss: 0.22975125908851624\n",
      "iteration 13774: loss: 0.22975027561187744\n",
      "iteration 13775: loss: 0.22974920272827148\n",
      "iteration 13776: loss: 0.2297482192516327\n",
      "iteration 13777: loss: 0.22974714636802673\n",
      "iteration 13778: loss: 0.229746013879776\n",
      "iteration 13779: loss: 0.22974491119384766\n",
      "iteration 13780: loss: 0.22974392771720886\n",
      "iteration 13781: loss: 0.2297428846359253\n",
      "iteration 13782: loss: 0.22974181175231934\n",
      "iteration 13783: loss: 0.22974082827568054\n",
      "iteration 13784: loss: 0.2297397404909134\n",
      "iteration 13785: loss: 0.2297387570142746\n",
      "iteration 13786: loss: 0.22973766922950745\n",
      "iteration 13787: loss: 0.22973665595054626\n",
      "iteration 13788: loss: 0.22973549365997314\n",
      "iteration 13789: loss: 0.22973451018333435\n",
      "iteration 13790: loss: 0.2297334372997284\n",
      "iteration 13791: loss: 0.2297324687242508\n",
      "iteration 13792: loss: 0.22973141074180603\n",
      "iteration 13793: loss: 0.22973041236400604\n",
      "iteration 13794: loss: 0.22972936928272247\n",
      "iteration 13795: loss: 0.22972825169563293\n",
      "iteration 13796: loss: 0.22972717881202698\n",
      "iteration 13797: loss: 0.22972619533538818\n",
      "iteration 13798: loss: 0.22972509264945984\n",
      "iteration 13799: loss: 0.22972412407398224\n",
      "iteration 13800: loss: 0.2297230213880539\n",
      "iteration 13801: loss: 0.2297220677137375\n",
      "iteration 13802: loss: 0.22972102463245392\n",
      "iteration 13803: loss: 0.2297198474407196\n",
      "iteration 13804: loss: 0.22971877455711365\n",
      "iteration 13805: loss: 0.22971780598163605\n",
      "iteration 13806: loss: 0.2297167330980301\n",
      "iteration 13807: loss: 0.22971566021442413\n",
      "iteration 13808: loss: 0.22971467673778534\n",
      "iteration 13809: loss: 0.22971360385417938\n",
      "iteration 13810: loss: 0.22971263527870178\n",
      "iteration 13811: loss: 0.22971153259277344\n",
      "iteration 13812: loss: 0.2297104299068451\n",
      "iteration 13813: loss: 0.2297094166278839\n",
      "iteration 13814: loss: 0.2297084629535675\n",
      "iteration 13815: loss: 0.22970739006996155\n",
      "iteration 13816: loss: 0.22970633208751678\n",
      "iteration 13817: loss: 0.22970537841320038\n",
      "iteration 13818: loss: 0.2297043353319168\n",
      "iteration 13819: loss: 0.22970330715179443\n",
      "iteration 13820: loss: 0.22970211505889893\n",
      "iteration 13821: loss: 0.22970111668109894\n",
      "iteration 13822: loss: 0.22970004379749298\n",
      "iteration 13823: loss: 0.22969909012317657\n",
      "iteration 13824: loss: 0.22969798743724823\n",
      "iteration 13825: loss: 0.22969701886177063\n",
      "iteration 13826: loss: 0.22969594597816467\n",
      "iteration 13827: loss: 0.22969481348991394\n",
      "iteration 13828: loss: 0.22969377040863037\n",
      "iteration 13829: loss: 0.229692742228508\n",
      "iteration 13830: loss: 0.22969165444374084\n",
      "iteration 13831: loss: 0.22969067096710205\n",
      "iteration 13832: loss: 0.2296895980834961\n",
      "iteration 13833: loss: 0.22968855500221252\n",
      "iteration 13834: loss: 0.22968760132789612\n",
      "iteration 13835: loss: 0.22968646883964539\n",
      "iteration 13836: loss: 0.22968550026416779\n",
      "iteration 13837: loss: 0.22968430817127228\n",
      "iteration 13838: loss: 0.2296833097934723\n",
      "iteration 13839: loss: 0.22968225181102753\n",
      "iteration 13840: loss: 0.22968128323554993\n",
      "iteration 13841: loss: 0.22968025505542755\n",
      "iteration 13842: loss: 0.2296791970729828\n",
      "iteration 13843: loss: 0.2296782284975052\n",
      "iteration 13844: loss: 0.2296770066022873\n",
      "iteration 13845: loss: 0.2296760082244873\n",
      "iteration 13846: loss: 0.22967496514320374\n",
      "iteration 13847: loss: 0.22967401146888733\n",
      "iteration 13848: loss: 0.2296728789806366\n",
      "iteration 13849: loss: 0.229671910405159\n",
      "iteration 13850: loss: 0.2296707183122635\n",
      "iteration 13851: loss: 0.22966964542865753\n",
      "iteration 13852: loss: 0.22966869175434113\n",
      "iteration 13853: loss: 0.22966758906841278\n",
      "iteration 13854: loss: 0.22966663539409637\n",
      "iteration 13855: loss: 0.22966554760932922\n",
      "iteration 13856: loss: 0.22966456413269043\n",
      "iteration 13857: loss: 0.22966349124908447\n",
      "iteration 13858: loss: 0.22966238856315613\n",
      "iteration 13859: loss: 0.22966131567955017\n",
      "iteration 13860: loss: 0.229660302400589\n",
      "iteration 13861: loss: 0.2296592742204666\n",
      "iteration 13862: loss: 0.22965824604034424\n",
      "iteration 13863: loss: 0.22965721786022186\n",
      "iteration 13864: loss: 0.22965602576732635\n",
      "iteration 13865: loss: 0.2296549528837204\n",
      "iteration 13866: loss: 0.229653999209404\n",
      "iteration 13867: loss: 0.22965292632579803\n",
      "iteration 13868: loss: 0.22965197265148163\n",
      "iteration 13869: loss: 0.22965092957019806\n",
      "iteration 13870: loss: 0.22965002059936523\n",
      "iteration 13871: loss: 0.22964879870414734\n",
      "iteration 13872: loss: 0.22964783012866974\n",
      "iteration 13873: loss: 0.22964677214622498\n",
      "iteration 13874: loss: 0.22964569926261902\n",
      "iteration 13875: loss: 0.22964470088481903\n",
      "iteration 13876: loss: 0.22964365780353546\n",
      "iteration 13877: loss: 0.22964270412921906\n",
      "iteration 13878: loss: 0.22964148223400116\n",
      "iteration 13879: loss: 0.2296404391527176\n",
      "iteration 13880: loss: 0.2296394556760788\n",
      "iteration 13881: loss: 0.22963841259479523\n",
      "iteration 13882: loss: 0.22963741421699524\n",
      "iteration 13883: loss: 0.22963638603687286\n",
      "iteration 13884: loss: 0.22963523864746094\n",
      "iteration 13885: loss: 0.22963419556617737\n",
      "iteration 13886: loss: 0.22963309288024902\n",
      "iteration 13887: loss: 0.22963213920593262\n",
      "iteration 13888: loss: 0.22963106632232666\n",
      "iteration 13889: loss: 0.22963008284568787\n",
      "iteration 13890: loss: 0.22962899506092072\n",
      "iteration 13891: loss: 0.22962787747383118\n",
      "iteration 13892: loss: 0.2296268194913864\n",
      "iteration 13893: loss: 0.22962574660778046\n",
      "iteration 13894: loss: 0.22962479293346405\n",
      "iteration 13895: loss: 0.22962376475334167\n",
      "iteration 13896: loss: 0.22962269186973572\n",
      "iteration 13897: loss: 0.22962172329425812\n",
      "iteration 13898: loss: 0.2296205461025238\n",
      "iteration 13899: loss: 0.22961954772472382\n",
      "iteration 13900: loss: 0.22961850464344025\n",
      "iteration 13901: loss: 0.22961750626564026\n",
      "iteration 13902: loss: 0.2296164482831955\n",
      "iteration 13903: loss: 0.22961536049842834\n",
      "iteration 13904: loss: 0.2296142876148224\n",
      "iteration 13905: loss: 0.2296132743358612\n",
      "iteration 13906: loss: 0.22961223125457764\n",
      "iteration 13907: loss: 0.22961118817329407\n",
      "iteration 13908: loss: 0.22961020469665527\n",
      "iteration 13909: loss: 0.2296091616153717\n",
      "iteration 13910: loss: 0.2296079695224762\n",
      "iteration 13911: loss: 0.22960695624351501\n",
      "iteration 13912: loss: 0.22960594296455383\n",
      "iteration 13913: loss: 0.22960492968559265\n",
      "iteration 13914: loss: 0.22960388660430908\n",
      "iteration 13915: loss: 0.22960281372070312\n",
      "iteration 13916: loss: 0.2296016663312912\n",
      "iteration 13917: loss: 0.22960063815116882\n",
      "iteration 13918: loss: 0.22959963977336884\n",
      "iteration 13919: loss: 0.22959856688976288\n",
      "iteration 13920: loss: 0.22959759831428528\n",
      "iteration 13921: loss: 0.2295965701341629\n",
      "iteration 13922: loss: 0.2295953929424286\n",
      "iteration 13923: loss: 0.229594424366951\n",
      "iteration 13924: loss: 0.22959336638450623\n",
      "iteration 13925: loss: 0.22959236800670624\n",
      "iteration 13926: loss: 0.22959139943122864\n",
      "iteration 13927: loss: 0.22959041595458984\n",
      "iteration 13928: loss: 0.22958922386169434\n",
      "iteration 13929: loss: 0.229588121175766\n",
      "iteration 13930: loss: 0.22958716750144958\n",
      "iteration 13931: loss: 0.2295861542224884\n",
      "iteration 13932: loss: 0.22958508133888245\n",
      "iteration 13933: loss: 0.22958406805992126\n",
      "iteration 13934: loss: 0.22958287596702576\n",
      "iteration 13935: loss: 0.22958190739154816\n",
      "iteration 13936: loss: 0.2295808494091034\n",
      "iteration 13937: loss: 0.2295798510313034\n",
      "iteration 13938: loss: 0.22957880795001984\n",
      "iteration 13939: loss: 0.22957774996757507\n",
      "iteration 13940: loss: 0.22957678139209747\n",
      "iteration 13941: loss: 0.22957558929920197\n",
      "iteration 13942: loss: 0.2295745313167572\n",
      "iteration 13943: loss: 0.22957353293895721\n",
      "iteration 13944: loss: 0.22957248985767365\n",
      "iteration 13945: loss: 0.22957153618335724\n",
      "iteration 13946: loss: 0.22957031428813934\n",
      "iteration 13947: loss: 0.22956939041614532\n",
      "iteration 13948: loss: 0.22956831753253937\n",
      "iteration 13949: loss: 0.229567289352417\n",
      "iteration 13950: loss: 0.22956626117229462\n",
      "iteration 13951: loss: 0.2295650988817215\n",
      "iteration 13952: loss: 0.2295641452074051\n",
      "iteration 13953: loss: 0.22956307232379913\n",
      "iteration 13954: loss: 0.22956201434135437\n",
      "iteration 13955: loss: 0.22956104576587677\n",
      "iteration 13956: loss: 0.2295599728822708\n",
      "iteration 13957: loss: 0.22955887019634247\n",
      "iteration 13958: loss: 0.2295577973127365\n",
      "iteration 13959: loss: 0.22955675423145294\n",
      "iteration 13960: loss: 0.22955577075481415\n",
      "iteration 13961: loss: 0.22955474257469177\n",
      "iteration 13962: loss: 0.22955353558063507\n",
      "iteration 13963: loss: 0.22955253720283508\n",
      "iteration 13964: loss: 0.2295515090227127\n",
      "iteration 13965: loss: 0.22955045104026794\n",
      "iteration 13966: loss: 0.22954948246479034\n",
      "iteration 13967: loss: 0.22954845428466797\n",
      "iteration 13968: loss: 0.22954723238945007\n",
      "iteration 13969: loss: 0.22954626381397247\n",
      "iteration 13970: loss: 0.2295452356338501\n",
      "iteration 13971: loss: 0.22954420745372772\n",
      "iteration 13972: loss: 0.22954320907592773\n",
      "iteration 13973: loss: 0.22954201698303223\n",
      "iteration 13974: loss: 0.22954106330871582\n",
      "iteration 13975: loss: 0.22954003512859344\n",
      "iteration 13976: loss: 0.22953906655311584\n",
      "iteration 13977: loss: 0.22953800857067108\n",
      "iteration 13978: loss: 0.22953680157661438\n",
      "iteration 13979: loss: 0.22953584790229797\n",
      "iteration 13980: loss: 0.22953477501869202\n",
      "iteration 13981: loss: 0.22953379154205322\n",
      "iteration 13982: loss: 0.22953283786773682\n",
      "iteration 13983: loss: 0.2295316755771637\n",
      "iteration 13984: loss: 0.22953061759471893\n",
      "iteration 13985: loss: 0.22952961921691895\n",
      "iteration 13986: loss: 0.229528546333313\n",
      "iteration 13987: loss: 0.2295275628566742\n",
      "iteration 13988: loss: 0.22952651977539062\n",
      "iteration 13989: loss: 0.2295253574848175\n",
      "iteration 13990: loss: 0.2295243740081787\n",
      "iteration 13991: loss: 0.22952333092689514\n",
      "iteration 13992: loss: 0.22952227294445038\n",
      "iteration 13993: loss: 0.2295212745666504\n",
      "iteration 13994: loss: 0.22952011227607727\n",
      "iteration 13995: loss: 0.2295190542936325\n",
      "iteration 13996: loss: 0.2295181304216385\n",
      "iteration 13997: loss: 0.22951702773571014\n",
      "iteration 13998: loss: 0.22951602935791016\n",
      "iteration 13999: loss: 0.2295149266719818\n",
      "iteration 14000: loss: 0.22951388359069824\n",
      "iteration 14001: loss: 0.22951284050941467\n",
      "iteration 14002: loss: 0.22951185703277588\n",
      "iteration 14003: loss: 0.22951078414916992\n",
      "iteration 14004: loss: 0.22950968146324158\n",
      "iteration 14005: loss: 0.229508638381958\n",
      "iteration 14006: loss: 0.22950759530067444\n",
      "iteration 14007: loss: 0.22950661182403564\n",
      "iteration 14008: loss: 0.22950558364391327\n",
      "iteration 14009: loss: 0.22950439155101776\n",
      "iteration 14010: loss: 0.22950342297554016\n",
      "iteration 14011: loss: 0.2295023649930954\n",
      "iteration 14012: loss: 0.22950132191181183\n",
      "iteration 14013: loss: 0.22950033843517303\n",
      "iteration 14014: loss: 0.22949914634227753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14015: loss: 0.22949810326099396\n",
      "iteration 14016: loss: 0.22949714958667755\n",
      "iteration 14017: loss: 0.2294960469007492\n",
      "iteration 14018: loss: 0.22949513792991638\n",
      "iteration 14019: loss: 0.2294938862323761\n",
      "iteration 14020: loss: 0.22949285805225372\n",
      "iteration 14021: loss: 0.22949187457561493\n",
      "iteration 14022: loss: 0.22949084639549255\n",
      "iteration 14023: loss: 0.22948980331420898\n",
      "iteration 14024: loss: 0.22948870062828064\n",
      "iteration 14025: loss: 0.22948770225048065\n",
      "iteration 14026: loss: 0.2294866144657135\n",
      "iteration 14027: loss: 0.2294856607913971\n",
      "iteration 14028: loss: 0.2294844388961792\n",
      "iteration 14029: loss: 0.2294834405183792\n",
      "iteration 14030: loss: 0.22948245704174042\n",
      "iteration 14031: loss: 0.22948141396045685\n",
      "iteration 14032: loss: 0.22948043048381805\n",
      "iteration 14033: loss: 0.22947922348976135\n",
      "iteration 14034: loss: 0.22947819530963898\n",
      "iteration 14035: loss: 0.22947725653648376\n",
      "iteration 14036: loss: 0.22947625815868378\n",
      "iteration 14037: loss: 0.22947518527507782\n",
      "iteration 14038: loss: 0.22947409749031067\n",
      "iteration 14039: loss: 0.2294730693101883\n",
      "iteration 14040: loss: 0.22947201132774353\n",
      "iteration 14041: loss: 0.22947101294994354\n",
      "iteration 14042: loss: 0.22946986556053162\n",
      "iteration 14043: loss: 0.22946879267692566\n",
      "iteration 14044: loss: 0.22946786880493164\n",
      "iteration 14045: loss: 0.2294667661190033\n",
      "iteration 14046: loss: 0.22946575284004211\n",
      "iteration 14047: loss: 0.22946462035179138\n",
      "iteration 14048: loss: 0.2294636219739914\n",
      "iteration 14049: loss: 0.22946257889270782\n",
      "iteration 14050: loss: 0.22946159541606903\n",
      "iteration 14051: loss: 0.22946056723594666\n",
      "iteration 14052: loss: 0.22945937514305115\n",
      "iteration 14053: loss: 0.22945837676525116\n",
      "iteration 14054: loss: 0.22945734858512878\n",
      "iteration 14055: loss: 0.2294563353061676\n",
      "iteration 14056: loss: 0.2294553518295288\n",
      "iteration 14057: loss: 0.2294541895389557\n",
      "iteration 14058: loss: 0.22945313155651093\n",
      "iteration 14059: loss: 0.22945213317871094\n",
      "iteration 14060: loss: 0.22945110499858856\n",
      "iteration 14061: loss: 0.22944991290569305\n",
      "iteration 14062: loss: 0.22944888472557068\n",
      "iteration 14063: loss: 0.22944791615009308\n",
      "iteration 14064: loss: 0.22944684326648712\n",
      "iteration 14065: loss: 0.22944578528404236\n",
      "iteration 14066: loss: 0.2294447124004364\n",
      "iteration 14067: loss: 0.22944366931915283\n",
      "iteration 14068: loss: 0.22944259643554688\n",
      "iteration 14069: loss: 0.22944167256355286\n",
      "iteration 14070: loss: 0.2294405996799469\n",
      "iteration 14071: loss: 0.22943945229053497\n",
      "iteration 14072: loss: 0.22943846881389618\n",
      "iteration 14073: loss: 0.2294374406337738\n",
      "iteration 14074: loss: 0.22943639755249023\n",
      "iteration 14075: loss: 0.2294352948665619\n",
      "iteration 14076: loss: 0.22943425178527832\n",
      "iteration 14077: loss: 0.22943320870399475\n",
      "iteration 14078: loss: 0.22943226993083954\n",
      "iteration 14079: loss: 0.22943106293678284\n",
      "iteration 14080: loss: 0.22943004965782166\n",
      "iteration 14081: loss: 0.22942908108234406\n",
      "iteration 14082: loss: 0.22942805290222168\n",
      "iteration 14083: loss: 0.2294270098209381\n",
      "iteration 14084: loss: 0.22942590713500977\n",
      "iteration 14085: loss: 0.2294248342514038\n",
      "iteration 14086: loss: 0.22942379117012024\n",
      "iteration 14087: loss: 0.22942273318767548\n",
      "iteration 14088: loss: 0.22942161560058594\n",
      "iteration 14089: loss: 0.22942066192626953\n",
      "iteration 14090: loss: 0.22941961884498596\n",
      "iteration 14091: loss: 0.22941863536834717\n",
      "iteration 14092: loss: 0.22941747307777405\n",
      "iteration 14093: loss: 0.22941644489765167\n",
      "iteration 14094: loss: 0.22941549122333527\n",
      "iteration 14095: loss: 0.2294144332408905\n",
      "iteration 14096: loss: 0.22941327095031738\n",
      "iteration 14097: loss: 0.22941231727600098\n",
      "iteration 14098: loss: 0.2294112741947174\n",
      "iteration 14099: loss: 0.22941026091575623\n",
      "iteration 14100: loss: 0.22940906882286072\n",
      "iteration 14101: loss: 0.2294081151485443\n",
      "iteration 14102: loss: 0.22940711677074432\n",
      "iteration 14103: loss: 0.22940604388713837\n",
      "iteration 14104: loss: 0.22940492630004883\n",
      "iteration 14105: loss: 0.22940389811992645\n",
      "iteration 14106: loss: 0.22940285503864288\n",
      "iteration 14107: loss: 0.22940190136432648\n",
      "iteration 14108: loss: 0.2294008433818817\n",
      "iteration 14109: loss: 0.2293996512889862\n",
      "iteration 14110: loss: 0.22939863801002502\n",
      "iteration 14111: loss: 0.22939765453338623\n",
      "iteration 14112: loss: 0.22939662635326385\n",
      "iteration 14113: loss: 0.22939546406269073\n",
      "iteration 14114: loss: 0.22939443588256836\n",
      "iteration 14115: loss: 0.22939343750476837\n",
      "iteration 14116: loss: 0.2293923795223236\n",
      "iteration 14117: loss: 0.2293912172317505\n",
      "iteration 14118: loss: 0.22939026355743408\n",
      "iteration 14119: loss: 0.2293892353773117\n",
      "iteration 14120: loss: 0.22938819229602814\n",
      "iteration 14121: loss: 0.22938701510429382\n",
      "iteration 14122: loss: 0.2293861210346222\n",
      "iteration 14123: loss: 0.22938504815101624\n",
      "iteration 14124: loss: 0.22938399016857147\n",
      "iteration 14125: loss: 0.22938291728496552\n",
      "iteration 14126: loss: 0.22938188910484314\n",
      "iteration 14127: loss: 0.22938084602355957\n",
      "iteration 14128: loss: 0.22937984764575958\n",
      "iteration 14129: loss: 0.2293788194656372\n",
      "iteration 14130: loss: 0.2293776571750641\n",
      "iteration 14131: loss: 0.22937658429145813\n",
      "iteration 14132: loss: 0.22937564551830292\n",
      "iteration 14133: loss: 0.22937457263469696\n",
      "iteration 14134: loss: 0.2293734848499298\n",
      "iteration 14135: loss: 0.22937250137329102\n",
      "iteration 14136: loss: 0.22937145829200745\n",
      "iteration 14137: loss: 0.2293703854084015\n",
      "iteration 14138: loss: 0.22936920821666718\n",
      "iteration 14139: loss: 0.22936825454235077\n",
      "iteration 14140: loss: 0.2293672114610672\n",
      "iteration 14141: loss: 0.2293662279844284\n",
      "iteration 14142: loss: 0.22936515510082245\n",
      "iteration 14143: loss: 0.22936411201953888\n",
      "iteration 14144: loss: 0.22936305403709412\n",
      "iteration 14145: loss: 0.22936196625232697\n",
      "iteration 14146: loss: 0.2293609380722046\n",
      "iteration 14147: loss: 0.22935990989208221\n",
      "iteration 14148: loss: 0.22935888171195984\n",
      "iteration 14149: loss: 0.2293577939271927\n",
      "iteration 14150: loss: 0.2293567657470703\n",
      "iteration 14151: loss: 0.22935569286346436\n",
      "iteration 14152: loss: 0.22935478389263153\n",
      "iteration 14153: loss: 0.22935359179973602\n",
      "iteration 14154: loss: 0.22935254871845245\n",
      "iteration 14155: loss: 0.22935152053833008\n",
      "iteration 14156: loss: 0.22935056686401367\n",
      "iteration 14157: loss: 0.22934937477111816\n",
      "iteration 14158: loss: 0.22934837639331818\n",
      "iteration 14159: loss: 0.2293473184108734\n",
      "iteration 14160: loss: 0.2293463498353958\n",
      "iteration 14161: loss: 0.2293451726436615\n",
      "iteration 14162: loss: 0.22934412956237793\n",
      "iteration 14163: loss: 0.2293432056903839\n",
      "iteration 14164: loss: 0.22934214770793915\n",
      "iteration 14165: loss: 0.22934098541736603\n",
      "iteration 14166: loss: 0.22933992743492126\n",
      "iteration 14167: loss: 0.22933897376060486\n",
      "iteration 14168: loss: 0.22933796048164368\n",
      "iteration 14169: loss: 0.22933673858642578\n",
      "iteration 14170: loss: 0.2293357402086258\n",
      "iteration 14171: loss: 0.22933480143547058\n",
      "iteration 14172: loss: 0.229333758354187\n",
      "iteration 14173: loss: 0.2293325662612915\n",
      "iteration 14174: loss: 0.22933164238929749\n",
      "iteration 14175: loss: 0.22933058440685272\n",
      "iteration 14176: loss: 0.22932955622673035\n",
      "iteration 14177: loss: 0.22932839393615723\n",
      "iteration 14178: loss: 0.22932736575603485\n",
      "iteration 14179: loss: 0.22932641208171844\n",
      "iteration 14180: loss: 0.22932520508766174\n",
      "iteration 14181: loss: 0.22932419180870056\n",
      "iteration 14182: loss: 0.22932323813438416\n",
      "iteration 14183: loss: 0.22932219505310059\n",
      "iteration 14184: loss: 0.22932103276252747\n",
      "iteration 14185: loss: 0.2293200045824051\n",
      "iteration 14186: loss: 0.22931905090808868\n",
      "iteration 14187: loss: 0.2293180525302887\n",
      "iteration 14188: loss: 0.2293168306350708\n",
      "iteration 14189: loss: 0.2293158322572708\n",
      "iteration 14190: loss: 0.22931483387947083\n",
      "iteration 14191: loss: 0.22931380569934845\n",
      "iteration 14192: loss: 0.22931265830993652\n",
      "iteration 14193: loss: 0.22931165993213654\n",
      "iteration 14194: loss: 0.22931063175201416\n",
      "iteration 14195: loss: 0.22930960357189178\n",
      "iteration 14196: loss: 0.22930856049060822\n",
      "iteration 14197: loss: 0.22930750250816345\n",
      "iteration 14198: loss: 0.22930648922920227\n",
      "iteration 14199: loss: 0.22930535674095154\n",
      "iteration 14200: loss: 0.22930434346199036\n",
      "iteration 14201: loss: 0.22930333018302917\n",
      "iteration 14202: loss: 0.229302316904068\n",
      "iteration 14203: loss: 0.22930121421813965\n",
      "iteration 14204: loss: 0.22930017113685608\n",
      "iteration 14205: loss: 0.2292991578578949\n",
      "iteration 14206: loss: 0.2292979657649994\n",
      "iteration 14207: loss: 0.22929701209068298\n",
      "iteration 14208: loss: 0.2292959988117218\n",
      "iteration 14209: loss: 0.22929494082927704\n",
      "iteration 14210: loss: 0.2292937934398651\n",
      "iteration 14211: loss: 0.22929275035858154\n",
      "iteration 14212: loss: 0.22929179668426514\n",
      "iteration 14213: loss: 0.2292906790971756\n",
      "iteration 14214: loss: 0.22928957641124725\n",
      "iteration 14215: loss: 0.22928869724273682\n",
      "iteration 14216: loss: 0.22928762435913086\n",
      "iteration 14217: loss: 0.22928647696971893\n",
      "iteration 14218: loss: 0.22928540408611298\n",
      "iteration 14219: loss: 0.22928443551063538\n",
      "iteration 14220: loss: 0.22928336262702942\n",
      "iteration 14221: loss: 0.22928233444690704\n",
      "iteration 14222: loss: 0.22928127646446228\n",
      "iteration 14223: loss: 0.2292802780866623\n",
      "iteration 14224: loss: 0.22927920520305634\n",
      "iteration 14225: loss: 0.22927816212177277\n",
      "iteration 14226: loss: 0.2292771339416504\n",
      "iteration 14227: loss: 0.22927594184875488\n",
      "iteration 14228: loss: 0.2292749136686325\n",
      "iteration 14229: loss: 0.2292739897966385\n",
      "iteration 14230: loss: 0.22927293181419373\n",
      "iteration 14231: loss: 0.2292717695236206\n",
      "iteration 14232: loss: 0.22927074134349823\n",
      "iteration 14233: loss: 0.22926971316337585\n",
      "iteration 14234: loss: 0.22926859557628632\n",
      "iteration 14235: loss: 0.22926759719848633\n",
      "iteration 14236: loss: 0.22926652431488037\n",
      "iteration 14237: loss: 0.229265496134758\n",
      "iteration 14238: loss: 0.22926442325115204\n",
      "iteration 14239: loss: 0.22926339507102966\n",
      "iteration 14240: loss: 0.22926239669322968\n",
      "iteration 14241: loss: 0.22926120460033417\n",
      "iteration 14242: loss: 0.22926023602485657\n",
      "iteration 14243: loss: 0.22925928235054016\n",
      "iteration 14244: loss: 0.22925829887390137\n",
      "iteration 14245: loss: 0.22925710678100586\n",
      "iteration 14246: loss: 0.22925610840320587\n",
      "iteration 14247: loss: 0.2292550504207611\n",
      "iteration 14248: loss: 0.2292541265487671\n",
      "iteration 14249: loss: 0.22925296425819397\n",
      "iteration 14250: loss: 0.2292519360780716\n",
      "iteration 14251: loss: 0.22925087809562683\n",
      "iteration 14252: loss: 0.22924980521202087\n",
      "iteration 14253: loss: 0.2292487919330597\n",
      "iteration 14254: loss: 0.22924773395061493\n",
      "iteration 14255: loss: 0.2292466163635254\n",
      "iteration 14256: loss: 0.22924557328224182\n",
      "iteration 14257: loss: 0.2292446345090866\n",
      "iteration 14258: loss: 0.2292434424161911\n",
      "iteration 14259: loss: 0.22924241423606873\n",
      "iteration 14260: loss: 0.22924140095710754\n",
      "iteration 14261: loss: 0.22924044728279114\n",
      "iteration 14262: loss: 0.2292392998933792\n",
      "iteration 14263: loss: 0.22923827171325684\n",
      "iteration 14264: loss: 0.22923727333545685\n",
      "iteration 14265: loss: 0.22923612594604492\n",
      "iteration 14266: loss: 0.22923517227172852\n",
      "iteration 14267: loss: 0.22923412919044495\n",
      "iteration 14268: loss: 0.22923311591148376\n",
      "iteration 14269: loss: 0.22923190891742706\n",
      "iteration 14270: loss: 0.22923097014427185\n",
      "iteration 14271: loss: 0.22922997176647186\n",
      "iteration 14272: loss: 0.22922877967357635\n",
      "iteration 14273: loss: 0.22922775149345398\n",
      "iteration 14274: loss: 0.2292267382144928\n",
      "iteration 14275: loss: 0.22922563552856445\n",
      "iteration 14276: loss: 0.22922463715076447\n",
      "iteration 14277: loss: 0.2292235791683197\n",
      "iteration 14278: loss: 0.22922258079051971\n",
      "iteration 14279: loss: 0.2292214184999466\n",
      "iteration 14280: loss: 0.22922050952911377\n",
      "iteration 14281: loss: 0.2292194813489914\n",
      "iteration 14282: loss: 0.2292182445526123\n",
      "iteration 14283: loss: 0.22921724617481232\n",
      "iteration 14284: loss: 0.22921624779701233\n",
      "iteration 14285: loss: 0.22921514511108398\n",
      "iteration 14286: loss: 0.229214146733284\n",
      "iteration 14287: loss: 0.22921311855316162\n",
      "iteration 14288: loss: 0.22921204566955566\n",
      "iteration 14289: loss: 0.22921094298362732\n",
      "iteration 14290: loss: 0.2292099893093109\n",
      "iteration 14291: loss: 0.22920897603034973\n",
      "iteration 14292: loss: 0.22920779883861542\n",
      "iteration 14293: loss: 0.22920675575733185\n",
      "iteration 14294: loss: 0.22920581698417664\n",
      "iteration 14295: loss: 0.22920465469360352\n",
      "iteration 14296: loss: 0.2292037308216095\n",
      "iteration 14297: loss: 0.22920270264148712\n",
      "iteration 14298: loss: 0.2292015552520752\n",
      "iteration 14299: loss: 0.22920051217079163\n",
      "iteration 14300: loss: 0.22919952869415283\n",
      "iteration 14301: loss: 0.2291983664035797\n",
      "iteration 14302: loss: 0.2291974127292633\n",
      "iteration 14303: loss: 0.22919639945030212\n",
      "iteration 14304: loss: 0.22919535636901855\n",
      "iteration 14305: loss: 0.22919419407844543\n",
      "iteration 14306: loss: 0.22919316589832306\n",
      "iteration 14307: loss: 0.22919213771820068\n",
      "iteration 14308: loss: 0.22919106483459473\n",
      "iteration 14309: loss: 0.22919006645679474\n",
      "iteration 14310: loss: 0.22918903827667236\n",
      "iteration 14311: loss: 0.22918787598609924\n",
      "iteration 14312: loss: 0.22918689250946045\n",
      "iteration 14313: loss: 0.22918584942817688\n",
      "iteration 14314: loss: 0.22918474674224854\n",
      "iteration 14315: loss: 0.22918374836444855\n",
      "iteration 14316: loss: 0.22918272018432617\n",
      "iteration 14317: loss: 0.22918160259723663\n",
      "iteration 14318: loss: 0.22918057441711426\n",
      "iteration 14319: loss: 0.22917957603931427\n",
      "iteration 14320: loss: 0.2291785031557083\n",
      "iteration 14321: loss: 0.22917747497558594\n",
      "iteration 14322: loss: 0.22917647659778595\n",
      "iteration 14323: loss: 0.22917529940605164\n",
      "iteration 14324: loss: 0.22917425632476807\n",
      "iteration 14325: loss: 0.22917327284812927\n",
      "iteration 14326: loss: 0.22917219996452332\n",
      "iteration 14327: loss: 0.22917112708091736\n",
      "iteration 14328: loss: 0.2291700839996338\n",
      "iteration 14329: loss: 0.2291690856218338\n",
      "iteration 14330: loss: 0.22916793823242188\n",
      "iteration 14331: loss: 0.2291669100522995\n",
      "iteration 14332: loss: 0.22916586697101593\n",
      "iteration 14333: loss: 0.22916480898857117\n",
      "iteration 14334: loss: 0.22916381061077118\n",
      "iteration 14335: loss: 0.22916284203529358\n",
      "iteration 14336: loss: 0.22916166484355927\n",
      "iteration 14337: loss: 0.2291606366634369\n",
      "iteration 14338: loss: 0.22915959358215332\n",
      "iteration 14339: loss: 0.22915855050086975\n",
      "iteration 14340: loss: 0.22915753722190857\n",
      "iteration 14341: loss: 0.229156494140625\n",
      "iteration 14342: loss: 0.22915534675121307\n",
      "iteration 14343: loss: 0.2291543036699295\n",
      "iteration 14344: loss: 0.22915339469909668\n",
      "iteration 14345: loss: 0.22915224730968475\n",
      "iteration 14346: loss: 0.22915127873420715\n",
      "iteration 14347: loss: 0.2291502207517624\n",
      "iteration 14348: loss: 0.22914907336235046\n",
      "iteration 14349: loss: 0.2291480004787445\n",
      "iteration 14350: loss: 0.2291470319032669\n",
      "iteration 14351: loss: 0.22914597392082214\n",
      "iteration 14352: loss: 0.22914496064186096\n",
      "iteration 14353: loss: 0.2291439324617386\n",
      "iteration 14354: loss: 0.22914278507232666\n",
      "iteration 14355: loss: 0.22914178669452667\n",
      "iteration 14356: loss: 0.22914060950279236\n",
      "iteration 14357: loss: 0.22913965582847595\n",
      "iteration 14358: loss: 0.22913868725299835\n",
      "iteration 14359: loss: 0.22913765907287598\n",
      "iteration 14360: loss: 0.22913651168346405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14361: loss: 0.22913551330566406\n",
      "iteration 14362: loss: 0.2291344702243805\n",
      "iteration 14363: loss: 0.22913332283496857\n",
      "iteration 14364: loss: 0.22913233935832977\n",
      "iteration 14365: loss: 0.2291313111782074\n",
      "iteration 14366: loss: 0.22913017868995667\n",
      "iteration 14367: loss: 0.22912922501564026\n",
      "iteration 14368: loss: 0.2291281670331955\n",
      "iteration 14369: loss: 0.22912701964378357\n",
      "iteration 14370: loss: 0.2291260063648224\n",
      "iteration 14371: loss: 0.2291249781847\n",
      "iteration 14372: loss: 0.22912390530109406\n",
      "iteration 14373: loss: 0.22912287712097168\n",
      "iteration 14374: loss: 0.2291218340396881\n",
      "iteration 14375: loss: 0.22912073135375977\n",
      "iteration 14376: loss: 0.2291196882724762\n",
      "iteration 14377: loss: 0.22911867499351501\n",
      "iteration 14378: loss: 0.22911755740642548\n",
      "iteration 14379: loss: 0.22911658883094788\n",
      "iteration 14380: loss: 0.2291155606508255\n",
      "iteration 14381: loss: 0.22911448776721954\n",
      "iteration 14382: loss: 0.22911342978477478\n",
      "iteration 14383: loss: 0.22911229729652405\n",
      "iteration 14384: loss: 0.22911128401756287\n",
      "iteration 14385: loss: 0.22911036014556885\n",
      "iteration 14386: loss: 0.22910913825035095\n",
      "iteration 14387: loss: 0.22910816967487335\n",
      "iteration 14388: loss: 0.22910714149475098\n",
      "iteration 14389: loss: 0.22910599410533905\n",
      "iteration 14390: loss: 0.22910496592521667\n",
      "iteration 14391: loss: 0.22910399734973907\n",
      "iteration 14392: loss: 0.22910289466381073\n",
      "iteration 14393: loss: 0.22910186648368835\n",
      "iteration 14394: loss: 0.22910091280937195\n",
      "iteration 14395: loss: 0.2290997952222824\n",
      "iteration 14396: loss: 0.22909875214099884\n",
      "iteration 14397: loss: 0.22909775376319885\n",
      "iteration 14398: loss: 0.22909660637378693\n",
      "iteration 14399: loss: 0.22909554839134216\n",
      "iteration 14400: loss: 0.22909455001354218\n",
      "iteration 14401: loss: 0.22909343242645264\n",
      "iteration 14402: loss: 0.22909240424633026\n",
      "iteration 14403: loss: 0.2290915548801422\n",
      "iteration 14404: loss: 0.22909040749073029\n",
      "iteration 14405: loss: 0.22908934950828552\n",
      "iteration 14406: loss: 0.22908835113048553\n",
      "iteration 14407: loss: 0.2290872037410736\n",
      "iteration 14408: loss: 0.22908620536327362\n",
      "iteration 14409: loss: 0.2290850430727005\n",
      "iteration 14410: loss: 0.22908398509025574\n",
      "iteration 14411: loss: 0.22908301651477814\n",
      "iteration 14412: loss: 0.22908183932304382\n",
      "iteration 14413: loss: 0.22908087074756622\n",
      "iteration 14414: loss: 0.229079931974411\n",
      "iteration 14415: loss: 0.2290787249803543\n",
      "iteration 14416: loss: 0.22907774150371552\n",
      "iteration 14417: loss: 0.22907671332359314\n",
      "iteration 14418: loss: 0.2290755808353424\n",
      "iteration 14419: loss: 0.22907456755638123\n",
      "iteration 14420: loss: 0.22907352447509766\n",
      "iteration 14421: loss: 0.22907237708568573\n",
      "iteration 14422: loss: 0.22907140851020813\n",
      "iteration 14423: loss: 0.22907033562660217\n",
      "iteration 14424: loss: 0.22906935214996338\n",
      "iteration 14425: loss: 0.22906836867332458\n",
      "iteration 14426: loss: 0.22906723618507385\n",
      "iteration 14427: loss: 0.2290661782026291\n",
      "iteration 14428: loss: 0.2290652096271515\n",
      "iteration 14429: loss: 0.22906403243541718\n",
      "iteration 14430: loss: 0.2290630340576172\n",
      "iteration 14431: loss: 0.2290620356798172\n",
      "iteration 14432: loss: 0.22906085848808289\n",
      "iteration 14433: loss: 0.22905990481376648\n",
      "iteration 14434: loss: 0.2290589064359665\n",
      "iteration 14435: loss: 0.22905775904655457\n",
      "iteration 14436: loss: 0.22905674576759338\n",
      "iteration 14437: loss: 0.22905559837818146\n",
      "iteration 14438: loss: 0.22905460000038147\n",
      "iteration 14439: loss: 0.2290535718202591\n",
      "iteration 14440: loss: 0.22905249893665314\n",
      "iteration 14441: loss: 0.22905150055885315\n",
      "iteration 14442: loss: 0.22905048727989197\n",
      "iteration 14443: loss: 0.229049414396286\n",
      "iteration 14444: loss: 0.22904841601848602\n",
      "iteration 14445: loss: 0.22904729843139648\n",
      "iteration 14446: loss: 0.2290462702512741\n",
      "iteration 14447: loss: 0.22904527187347412\n",
      "iteration 14448: loss: 0.2290441244840622\n",
      "iteration 14449: loss: 0.2290431559085846\n",
      "iteration 14450: loss: 0.22904209792613983\n",
      "iteration 14451: loss: 0.2290409803390503\n",
      "iteration 14452: loss: 0.22903993725776672\n",
      "iteration 14453: loss: 0.22903895378112793\n",
      "iteration 14454: loss: 0.22903785109519958\n",
      "iteration 14455: loss: 0.2290368527173996\n",
      "iteration 14456: loss: 0.22903573513031006\n",
      "iteration 14457: loss: 0.22903473675251007\n",
      "iteration 14458: loss: 0.22903373837471008\n",
      "iteration 14459: loss: 0.22903254628181458\n",
      "iteration 14460: loss: 0.22903156280517578\n",
      "iteration 14461: loss: 0.2290305197238922\n",
      "iteration 14462: loss: 0.22902938723564148\n",
      "iteration 14463: loss: 0.2290283739566803\n",
      "iteration 14464: loss: 0.22902724146842957\n",
      "iteration 14465: loss: 0.22902624309062958\n",
      "iteration 14466: loss: 0.2290252149105072\n",
      "iteration 14467: loss: 0.22902417182922363\n",
      "iteration 14468: loss: 0.22902312874794006\n",
      "iteration 14469: loss: 0.22902211546897888\n",
      "iteration 14470: loss: 0.22902102768421173\n",
      "iteration 14471: loss: 0.22902002930641174\n",
      "iteration 14472: loss: 0.22901901602745056\n",
      "iteration 14473: loss: 0.22901788353919983\n",
      "iteration 14474: loss: 0.22901687026023865\n",
      "iteration 14475: loss: 0.22901570796966553\n",
      "iteration 14476: loss: 0.22901470959186554\n",
      "iteration 14477: loss: 0.22901372611522675\n",
      "iteration 14478: loss: 0.22901257872581482\n",
      "iteration 14479: loss: 0.2290116548538208\n",
      "iteration 14480: loss: 0.22901053726673126\n",
      "iteration 14481: loss: 0.2290095090866089\n",
      "iteration 14482: loss: 0.2290084809064865\n",
      "iteration 14483: loss: 0.22900733351707458\n",
      "iteration 14484: loss: 0.2290063351392746\n",
      "iteration 14485: loss: 0.22900518774986267\n",
      "iteration 14486: loss: 0.2290041744709015\n",
      "iteration 14487: loss: 0.2290031909942627\n",
      "iteration 14488: loss: 0.22900208830833435\n",
      "iteration 14489: loss: 0.22900108993053436\n",
      "iteration 14490: loss: 0.22900009155273438\n",
      "iteration 14491: loss: 0.22899897396564484\n",
      "iteration 14492: loss: 0.22899797558784485\n",
      "iteration 14493: loss: 0.22899684309959412\n",
      "iteration 14494: loss: 0.22899594902992249\n",
      "iteration 14495: loss: 0.22899489104747772\n",
      "iteration 14496: loss: 0.22899377346038818\n",
      "iteration 14497: loss: 0.2289927750825882\n",
      "iteration 14498: loss: 0.22899162769317627\n",
      "iteration 14499: loss: 0.22899062931537628\n",
      "iteration 14500: loss: 0.2289896309375763\n",
      "iteration 14501: loss: 0.22898848354816437\n",
      "iteration 14502: loss: 0.22898748517036438\n",
      "iteration 14503: loss: 0.22898635268211365\n",
      "iteration 14504: loss: 0.22898533940315247\n",
      "iteration 14505: loss: 0.2289843112230301\n",
      "iteration 14506: loss: 0.22898320853710175\n",
      "iteration 14507: loss: 0.22898221015930176\n",
      "iteration 14508: loss: 0.22898106276988983\n",
      "iteration 14509: loss: 0.22898006439208984\n",
      "iteration 14510: loss: 0.22897906601428986\n",
      "iteration 14511: loss: 0.22897788882255554\n",
      "iteration 14512: loss: 0.22897692024707794\n",
      "iteration 14513: loss: 0.2289758026599884\n",
      "iteration 14514: loss: 0.2289748638868332\n",
      "iteration 14515: loss: 0.2289738655090332\n",
      "iteration 14516: loss: 0.22897274792194366\n",
      "iteration 14517: loss: 0.2289716750383377\n",
      "iteration 14518: loss: 0.22897067666053772\n",
      "iteration 14519: loss: 0.22896960377693176\n",
      "iteration 14520: loss: 0.2289685755968094\n",
      "iteration 14521: loss: 0.22896742820739746\n",
      "iteration 14522: loss: 0.22896650433540344\n",
      "iteration 14523: loss: 0.22896544635295868\n",
      "iteration 14524: loss: 0.22896432876586914\n",
      "iteration 14525: loss: 0.22896330058574677\n",
      "iteration 14526: loss: 0.22896218299865723\n",
      "iteration 14527: loss: 0.22896118462085724\n",
      "iteration 14528: loss: 0.22896020114421844\n",
      "iteration 14529: loss: 0.22895903885364532\n",
      "iteration 14530: loss: 0.22895805537700653\n",
      "iteration 14531: loss: 0.2289569079875946\n",
      "iteration 14532: loss: 0.22895586490631104\n",
      "iteration 14533: loss: 0.22895488142967224\n",
      "iteration 14534: loss: 0.2289537638425827\n",
      "iteration 14535: loss: 0.22895283997058868\n",
      "iteration 14536: loss: 0.2289518415927887\n",
      "iteration 14537: loss: 0.22895070910453796\n",
      "iteration 14538: loss: 0.22894974052906036\n",
      "iteration 14539: loss: 0.22894859313964844\n",
      "iteration 14540: loss: 0.22894759476184845\n",
      "iteration 14541: loss: 0.22894644737243652\n",
      "iteration 14542: loss: 0.22894544899463654\n",
      "iteration 14543: loss: 0.22894442081451416\n",
      "iteration 14544: loss: 0.22894330322742462\n",
      "iteration 14545: loss: 0.22894231975078583\n",
      "iteration 14546: loss: 0.2289411574602127\n",
      "iteration 14547: loss: 0.22894012928009033\n",
      "iteration 14548: loss: 0.22893917560577393\n",
      "iteration 14549: loss: 0.2289380580186844\n",
      "iteration 14550: loss: 0.228937029838562\n",
      "iteration 14551: loss: 0.22893595695495605\n",
      "iteration 14552: loss: 0.22893495857715607\n",
      "iteration 14553: loss: 0.22893396019935608\n",
      "iteration 14554: loss: 0.22893281280994415\n",
      "iteration 14555: loss: 0.22893182933330536\n",
      "iteration 14556: loss: 0.22893071174621582\n",
      "iteration 14557: loss: 0.22892971336841583\n",
      "iteration 14558: loss: 0.22892871499061584\n",
      "iteration 14559: loss: 0.2289276123046875\n",
      "iteration 14560: loss: 0.2289266139268875\n",
      "iteration 14561: loss: 0.22892549633979797\n",
      "iteration 14562: loss: 0.2289244830608368\n",
      "iteration 14563: loss: 0.22892335057258606\n",
      "iteration 14564: loss: 0.22892233729362488\n",
      "iteration 14565: loss: 0.22892136871814728\n",
      "iteration 14566: loss: 0.22892022132873535\n",
      "iteration 14567: loss: 0.22891922295093536\n",
      "iteration 14568: loss: 0.22891807556152344\n",
      "iteration 14569: loss: 0.22891707718372345\n",
      "iteration 14570: loss: 0.22891607880592346\n",
      "iteration 14571: loss: 0.22891497611999512\n",
      "iteration 14572: loss: 0.22891393303871155\n",
      "iteration 14573: loss: 0.2289128303527832\n",
      "iteration 14574: loss: 0.22891183197498322\n",
      "iteration 14575: loss: 0.22891080379486084\n",
      "iteration 14576: loss: 0.2289096862077713\n",
      "iteration 14577: loss: 0.2289086878299713\n",
      "iteration 14578: loss: 0.22890755534172058\n",
      "iteration 14579: loss: 0.22890658676624298\n",
      "iteration 14580: loss: 0.2289055585861206\n",
      "iteration 14581: loss: 0.22890448570251465\n",
      "iteration 14582: loss: 0.22890350222587585\n",
      "iteration 14583: loss: 0.22890238463878632\n",
      "iteration 14584: loss: 0.22890138626098633\n",
      "iteration 14585: loss: 0.22890040278434753\n",
      "iteration 14586: loss: 0.228899285197258\n",
      "iteration 14587: loss: 0.22889824211597443\n",
      "iteration 14588: loss: 0.22889713943004608\n",
      "iteration 14589: loss: 0.22889618575572968\n",
      "iteration 14590: loss: 0.22889502346515656\n",
      "iteration 14591: loss: 0.22889403998851776\n",
      "iteration 14592: loss: 0.22889304161071777\n",
      "iteration 14593: loss: 0.22889189422130585\n",
      "iteration 14594: loss: 0.22889092564582825\n",
      "iteration 14595: loss: 0.22888974845409393\n",
      "iteration 14596: loss: 0.22888877987861633\n",
      "iteration 14597: loss: 0.2288876473903656\n",
      "iteration 14598: loss: 0.22888663411140442\n",
      "iteration 14599: loss: 0.228885680437088\n",
      "iteration 14600: loss: 0.22888454794883728\n",
      "iteration 14601: loss: 0.2288835346698761\n",
      "iteration 14602: loss: 0.22888243198394775\n",
      "iteration 14603: loss: 0.22888143360614777\n",
      "iteration 14604: loss: 0.22888031601905823\n",
      "iteration 14605: loss: 0.22887930274009705\n",
      "iteration 14606: loss: 0.2288782149553299\n",
      "iteration 14607: loss: 0.22887711226940155\n",
      "iteration 14608: loss: 0.22887611389160156\n",
      "iteration 14609: loss: 0.228875070810318\n",
      "iteration 14610: loss: 0.2288741171360016\n",
      "iteration 14611: loss: 0.2288728952407837\n",
      "iteration 14612: loss: 0.22887186706066132\n",
      "iteration 14613: loss: 0.22887077927589417\n",
      "iteration 14614: loss: 0.22886982560157776\n",
      "iteration 14615: loss: 0.22886881232261658\n",
      "iteration 14616: loss: 0.22886767983436584\n",
      "iteration 14617: loss: 0.22886665165424347\n",
      "iteration 14618: loss: 0.22886550426483154\n",
      "iteration 14619: loss: 0.22886455059051514\n",
      "iteration 14620: loss: 0.22886356711387634\n",
      "iteration 14621: loss: 0.2288624346256256\n",
      "iteration 14622: loss: 0.2288614809513092\n",
      "iteration 14623: loss: 0.22886033356189728\n",
      "iteration 14624: loss: 0.2288593351840973\n",
      "iteration 14625: loss: 0.22885823249816895\n",
      "iteration 14626: loss: 0.22885723412036896\n",
      "iteration 14627: loss: 0.22885620594024658\n",
      "iteration 14628: loss: 0.228855162858963\n",
      "iteration 14629: loss: 0.22885408997535706\n",
      "iteration 14630: loss: 0.22885307669639587\n",
      "iteration 14631: loss: 0.2288520783185959\n",
      "iteration 14632: loss: 0.22885099053382874\n",
      "iteration 14633: loss: 0.22884993255138397\n",
      "iteration 14634: loss: 0.22884884476661682\n",
      "iteration 14635: loss: 0.22884783148765564\n",
      "iteration 14636: loss: 0.2288466989994049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14637: loss: 0.22884567081928253\n",
      "iteration 14638: loss: 0.22884459793567657\n",
      "iteration 14639: loss: 0.2288435995578766\n",
      "iteration 14640: loss: 0.22884254157543182\n",
      "iteration 14641: loss: 0.22884145379066467\n",
      "iteration 14642: loss: 0.2288404256105423\n",
      "iteration 14643: loss: 0.22883930802345276\n",
      "iteration 14644: loss: 0.22883832454681396\n",
      "iteration 14645: loss: 0.22883732616901398\n",
      "iteration 14646: loss: 0.22883617877960205\n",
      "iteration 14647: loss: 0.22883522510528564\n",
      "iteration 14648: loss: 0.22883407771587372\n",
      "iteration 14649: loss: 0.22883307933807373\n",
      "iteration 14650: loss: 0.22883197665214539\n",
      "iteration 14651: loss: 0.2288309782743454\n",
      "iteration 14652: loss: 0.22882983088493347\n",
      "iteration 14653: loss: 0.22882883250713348\n",
      "iteration 14654: loss: 0.22882790863513947\n",
      "iteration 14655: loss: 0.22882676124572754\n",
      "iteration 14656: loss: 0.22882571816444397\n",
      "iteration 14657: loss: 0.22882458567619324\n",
      "iteration 14658: loss: 0.22882358729839325\n",
      "iteration 14659: loss: 0.2288224995136261\n",
      "iteration 14660: loss: 0.22882147133350372\n",
      "iteration 14661: loss: 0.22882041335105896\n",
      "iteration 14662: loss: 0.22881941497325897\n",
      "iteration 14663: loss: 0.22881826758384705\n",
      "iteration 14664: loss: 0.22881726920604706\n",
      "iteration 14665: loss: 0.22881612181663513\n",
      "iteration 14666: loss: 0.22881515324115753\n",
      "iteration 14667: loss: 0.22881416976451874\n",
      "iteration 14668: loss: 0.2288130521774292\n",
      "iteration 14669: loss: 0.2288120985031128\n",
      "iteration 14670: loss: 0.22881098091602325\n",
      "iteration 14671: loss: 0.22880995273590088\n",
      "iteration 14672: loss: 0.2288089096546173\n",
      "iteration 14673: loss: 0.22880789637565613\n",
      "iteration 14674: loss: 0.2288067787885666\n",
      "iteration 14675: loss: 0.228805810213089\n",
      "iteration 14676: loss: 0.2288048267364502\n",
      "iteration 14677: loss: 0.2288036346435547\n",
      "iteration 14678: loss: 0.2288026362657547\n",
      "iteration 14679: loss: 0.22880148887634277\n",
      "iteration 14680: loss: 0.22880050539970398\n",
      "iteration 14681: loss: 0.22879941761493683\n",
      "iteration 14682: loss: 0.22879843413829803\n",
      "iteration 14683: loss: 0.22879746556282043\n",
      "iteration 14684: loss: 0.22879628837108612\n",
      "iteration 14685: loss: 0.22879533469676971\n",
      "iteration 14686: loss: 0.2287941873073578\n",
      "iteration 14687: loss: 0.2287932187318802\n",
      "iteration 14688: loss: 0.22879210114479065\n",
      "iteration 14689: loss: 0.22879111766815186\n",
      "iteration 14690: loss: 0.22879000008106232\n",
      "iteration 14691: loss: 0.22878894209861755\n",
      "iteration 14692: loss: 0.2287878543138504\n",
      "iteration 14693: loss: 0.22878685593605042\n",
      "iteration 14694: loss: 0.2287857085466385\n",
      "iteration 14695: loss: 0.22878475487232208\n",
      "iteration 14696: loss: 0.22878363728523254\n",
      "iteration 14697: loss: 0.22878265380859375\n",
      "iteration 14698: loss: 0.22878150641918182\n",
      "iteration 14699: loss: 0.22878050804138184\n",
      "iteration 14700: loss: 0.2287794053554535\n",
      "iteration 14701: loss: 0.2287784069776535\n",
      "iteration 14702: loss: 0.22877748310565948\n",
      "iteration 14703: loss: 0.22877629101276398\n",
      "iteration 14704: loss: 0.22877530753612518\n",
      "iteration 14705: loss: 0.22877421975135803\n",
      "iteration 14706: loss: 0.22877320647239685\n",
      "iteration 14707: loss: 0.22877207398414612\n",
      "iteration 14708: loss: 0.22877109050750732\n",
      "iteration 14709: loss: 0.22876997292041779\n",
      "iteration 14710: loss: 0.2287689745426178\n",
      "iteration 14711: loss: 0.228767991065979\n",
      "iteration 14712: loss: 0.2287667691707611\n",
      "iteration 14713: loss: 0.2287658154964447\n",
      "iteration 14714: loss: 0.22876469790935516\n",
      "iteration 14715: loss: 0.22876372933387756\n",
      "iteration 14716: loss: 0.2287627011537552\n",
      "iteration 14717: loss: 0.2287617027759552\n",
      "iteration 14718: loss: 0.22876062989234924\n",
      "iteration 14719: loss: 0.22875961661338806\n",
      "iteration 14720: loss: 0.22875845432281494\n",
      "iteration 14721: loss: 0.22875753045082092\n",
      "iteration 14722: loss: 0.228756383061409\n",
      "iteration 14723: loss: 0.22875535488128662\n",
      "iteration 14724: loss: 0.22875425219535828\n",
      "iteration 14725: loss: 0.2287532538175583\n",
      "iteration 14726: loss: 0.22875213623046875\n",
      "iteration 14727: loss: 0.22875115275382996\n",
      "iteration 14728: loss: 0.22875002026557922\n",
      "iteration 14729: loss: 0.22874906659126282\n",
      "iteration 14730: loss: 0.22874793410301208\n",
      "iteration 14731: loss: 0.2287469357252121\n",
      "iteration 14732: loss: 0.22874581813812256\n",
      "iteration 14733: loss: 0.22874481976032257\n",
      "iteration 14734: loss: 0.22874382138252258\n",
      "iteration 14735: loss: 0.22874267399311066\n",
      "iteration 14736: loss: 0.22874169051647186\n",
      "iteration 14737: loss: 0.2287406027317047\n",
      "iteration 14738: loss: 0.2287396490573883\n",
      "iteration 14739: loss: 0.22873854637145996\n",
      "iteration 14740: loss: 0.22873751819133759\n",
      "iteration 14741: loss: 0.22873632609844208\n",
      "iteration 14742: loss: 0.22873537242412567\n",
      "iteration 14743: loss: 0.22873428463935852\n",
      "iteration 14744: loss: 0.22873327136039734\n",
      "iteration 14745: loss: 0.2287321835756302\n",
      "iteration 14746: loss: 0.2287312000989914\n",
      "iteration 14747: loss: 0.2287302315235138\n",
      "iteration 14748: loss: 0.22872905433177948\n",
      "iteration 14749: loss: 0.22872808575630188\n",
      "iteration 14750: loss: 0.22872690856456757\n",
      "iteration 14751: loss: 0.22872593998908997\n",
      "iteration 14752: loss: 0.22872480750083923\n",
      "iteration 14753: loss: 0.22872383892536163\n",
      "iteration 14754: loss: 0.2287227213382721\n",
      "iteration 14755: loss: 0.2287217378616333\n",
      "iteration 14756: loss: 0.22872063517570496\n",
      "iteration 14757: loss: 0.22871951758861542\n",
      "iteration 14758: loss: 0.22871851921081543\n",
      "iteration 14759: loss: 0.2287174016237259\n",
      "iteration 14760: loss: 0.2287164032459259\n",
      "iteration 14761: loss: 0.2287154197692871\n",
      "iteration 14762: loss: 0.22871437668800354\n",
      "iteration 14763: loss: 0.22871342301368713\n",
      "iteration 14764: loss: 0.2287122756242752\n",
      "iteration 14765: loss: 0.2287113219499588\n",
      "iteration 14766: loss: 0.22871017456054688\n",
      "iteration 14767: loss: 0.22870922088623047\n",
      "iteration 14768: loss: 0.22870802879333496\n",
      "iteration 14769: loss: 0.22870703041553497\n",
      "iteration 14770: loss: 0.22870595753192902\n",
      "iteration 14771: loss: 0.22870495915412903\n",
      "iteration 14772: loss: 0.2287038266658783\n",
      "iteration 14773: loss: 0.2287028580904007\n",
      "iteration 14774: loss: 0.2287016659975052\n",
      "iteration 14775: loss: 0.2287006825208664\n",
      "iteration 14776: loss: 0.2286996841430664\n",
      "iteration 14777: loss: 0.22869858145713806\n",
      "iteration 14778: loss: 0.2286975085735321\n",
      "iteration 14779: loss: 0.2286965399980545\n",
      "iteration 14780: loss: 0.22869542241096497\n",
      "iteration 14781: loss: 0.228694349527359\n",
      "iteration 14782: loss: 0.22869329154491425\n",
      "iteration 14783: loss: 0.22869233787059784\n",
      "iteration 14784: loss: 0.2286912500858307\n",
      "iteration 14785: loss: 0.2286902368068695\n",
      "iteration 14786: loss: 0.22868914902210236\n",
      "iteration 14787: loss: 0.22868815064430237\n",
      "iteration 14788: loss: 0.22868704795837402\n",
      "iteration 14789: loss: 0.22868594527244568\n",
      "iteration 14790: loss: 0.22868485748767853\n",
      "iteration 14791: loss: 0.22868390381336212\n",
      "iteration 14792: loss: 0.22868280112743378\n",
      "iteration 14793: loss: 0.2286817729473114\n",
      "iteration 14794: loss: 0.22868070006370544\n",
      "iteration 14795: loss: 0.2286796271800995\n",
      "iteration 14796: loss: 0.22867855429649353\n",
      "iteration 14797: loss: 0.22867758572101593\n",
      "iteration 14798: loss: 0.22867648303508759\n",
      "iteration 14799: loss: 0.22867551445960999\n",
      "iteration 14800: loss: 0.22867436707019806\n",
      "iteration 14801: loss: 0.22867336869239807\n",
      "iteration 14802: loss: 0.22867229580879211\n",
      "iteration 14803: loss: 0.2286713421344757\n",
      "iteration 14804: loss: 0.22867020964622498\n",
      "iteration 14805: loss: 0.22866925597190857\n",
      "iteration 14806: loss: 0.22866806387901306\n",
      "iteration 14807: loss: 0.22866709530353546\n",
      "iteration 14808: loss: 0.22866597771644592\n",
      "iteration 14809: loss: 0.22866496443748474\n",
      "iteration 14810: loss: 0.22866392135620117\n",
      "iteration 14811: loss: 0.22866292297840118\n",
      "iteration 14812: loss: 0.22866177558898926\n",
      "iteration 14813: loss: 0.22866074740886688\n",
      "iteration 14814: loss: 0.22865965962409973\n",
      "iteration 14815: loss: 0.22865872085094452\n",
      "iteration 14816: loss: 0.22865763306617737\n",
      "iteration 14817: loss: 0.2286566197872162\n",
      "iteration 14818: loss: 0.22865553200244904\n",
      "iteration 14819: loss: 0.22865447402000427\n",
      "iteration 14820: loss: 0.22865335643291473\n",
      "iteration 14821: loss: 0.22865240275859833\n",
      "iteration 14822: loss: 0.22865132987499237\n",
      "iteration 14823: loss: 0.22865033149719238\n",
      "iteration 14824: loss: 0.22864921391010284\n",
      "iteration 14825: loss: 0.22864818572998047\n",
      "iteration 14826: loss: 0.22864708304405212\n",
      "iteration 14827: loss: 0.22864611446857452\n",
      "iteration 14828: loss: 0.22864499688148499\n",
      "iteration 14829: loss: 0.22864404320716858\n",
      "iteration 14830: loss: 0.22864291071891785\n",
      "iteration 14831: loss: 0.22864186763763428\n",
      "iteration 14832: loss: 0.22864075005054474\n",
      "iteration 14833: loss: 0.22863976657390594\n",
      "iteration 14834: loss: 0.2286386787891388\n",
      "iteration 14835: loss: 0.2286376953125\n",
      "iteration 14836: loss: 0.22863657772541046\n",
      "iteration 14837: loss: 0.22863554954528809\n",
      "iteration 14838: loss: 0.22863447666168213\n",
      "iteration 14839: loss: 0.22863347828388214\n",
      "iteration 14840: loss: 0.2286323606967926\n",
      "iteration 14841: loss: 0.2286314070224762\n",
      "iteration 14842: loss: 0.22863025963306427\n",
      "iteration 14843: loss: 0.22862915694713593\n",
      "iteration 14844: loss: 0.22862820327281952\n",
      "iteration 14845: loss: 0.22862713038921356\n",
      "iteration 14846: loss: 0.22862620651721954\n",
      "iteration 14847: loss: 0.22862498462200165\n",
      "iteration 14848: loss: 0.22862401604652405\n",
      "iteration 14849: loss: 0.2286229431629181\n",
      "iteration 14850: loss: 0.2286219596862793\n",
      "iteration 14851: loss: 0.22862084209918976\n",
      "iteration 14852: loss: 0.22861985862255096\n",
      "iteration 14853: loss: 0.22861866652965546\n",
      "iteration 14854: loss: 0.22861771285533905\n",
      "iteration 14855: loss: 0.2286166399717331\n",
      "iteration 14856: loss: 0.22861561179161072\n",
      "iteration 14857: loss: 0.22861449420452118\n",
      "iteration 14858: loss: 0.22861352562904358\n",
      "iteration 14859: loss: 0.22861239314079285\n",
      "iteration 14860: loss: 0.22861143946647644\n",
      "iteration 14861: loss: 0.22861036658287048\n",
      "iteration 14862: loss: 0.2286093533039093\n",
      "iteration 14863: loss: 0.228608176112175\n",
      "iteration 14864: loss: 0.22860725224018097\n",
      "iteration 14865: loss: 0.22860614955425262\n",
      "iteration 14866: loss: 0.22860515117645264\n",
      "iteration 14867: loss: 0.22860398888587952\n",
      "iteration 14868: loss: 0.2286030352115631\n",
      "iteration 14869: loss: 0.22860193252563477\n",
      "iteration 14870: loss: 0.22860081493854523\n",
      "iteration 14871: loss: 0.22859983146190643\n",
      "iteration 14872: loss: 0.2285986840724945\n",
      "iteration 14873: loss: 0.2285977154970169\n",
      "iteration 14874: loss: 0.22859661281108856\n",
      "iteration 14875: loss: 0.2285955846309662\n",
      "iteration 14876: loss: 0.22859451174736023\n",
      "iteration 14877: loss: 0.22859351336956024\n",
      "iteration 14878: loss: 0.22859236598014832\n",
      "iteration 14879: loss: 0.2285914123058319\n",
      "iteration 14880: loss: 0.22859036922454834\n",
      "iteration 14881: loss: 0.22858937084674835\n",
      "iteration 14882: loss: 0.22858817875385284\n",
      "iteration 14883: loss: 0.22858726978302002\n",
      "iteration 14884: loss: 0.22858616709709167\n",
      "iteration 14885: loss: 0.22858507931232452\n",
      "iteration 14886: loss: 0.22858409583568573\n",
      "iteration 14887: loss: 0.2285829335451126\n",
      "iteration 14888: loss: 0.22858195006847382\n",
      "iteration 14889: loss: 0.22858095169067383\n",
      "iteration 14890: loss: 0.22857995331287384\n",
      "iteration 14891: loss: 0.22857877612113953\n",
      "iteration 14892: loss: 0.22857780754566193\n",
      "iteration 14893: loss: 0.22857670485973358\n",
      "iteration 14894: loss: 0.22857575118541718\n",
      "iteration 14895: loss: 0.22857458889484406\n",
      "iteration 14896: loss: 0.22857359051704407\n",
      "iteration 14897: loss: 0.2285725176334381\n",
      "iteration 14898: loss: 0.22857153415679932\n",
      "iteration 14899: loss: 0.22857049107551575\n",
      "iteration 14900: loss: 0.22856931388378143\n",
      "iteration 14901: loss: 0.22856834530830383\n",
      "iteration 14902: loss: 0.2285672128200531\n",
      "iteration 14903: loss: 0.2285662442445755\n",
      "iteration 14904: loss: 0.22856512665748596\n",
      "iteration 14905: loss: 0.22856414318084717\n",
      "iteration 14906: loss: 0.22856302559375763\n",
      "iteration 14907: loss: 0.22856207191944122\n",
      "iteration 14908: loss: 0.2285608947277069\n",
      "iteration 14909: loss: 0.2285599410533905\n",
      "iteration 14910: loss: 0.22855885326862335\n",
      "iteration 14911: loss: 0.22855789959430695\n",
      "iteration 14912: loss: 0.2285567820072174\n",
      "iteration 14913: loss: 0.22855563461780548\n",
      "iteration 14914: loss: 0.2285546362400055\n",
      "iteration 14915: loss: 0.22855356335639954\n",
      "iteration 14916: loss: 0.22855255007743835\n",
      "iteration 14917: loss: 0.22855141758918762\n",
      "iteration 14918: loss: 0.2285504788160324\n",
      "iteration 14919: loss: 0.22854939103126526\n",
      "iteration 14920: loss: 0.22854837775230408\n",
      "iteration 14921: loss: 0.22854724526405334\n",
      "iteration 14922: loss: 0.22854629158973694\n",
      "iteration 14923: loss: 0.2285451889038086\n",
      "iteration 14924: loss: 0.2285442054271698\n",
      "iteration 14925: loss: 0.22854308784008026\n",
      "iteration 14926: loss: 0.22854192554950714\n",
      "iteration 14927: loss: 0.22854098677635193\n",
      "iteration 14928: loss: 0.22853989899158478\n",
      "iteration 14929: loss: 0.22853891551494598\n",
      "iteration 14930: loss: 0.22853782773017883\n",
      "iteration 14931: loss: 0.22853684425354004\n",
      "iteration 14932: loss: 0.22853577136993408\n",
      "iteration 14933: loss: 0.22853469848632812\n",
      "iteration 14934: loss: 0.22853359580039978\n",
      "iteration 14935: loss: 0.22853264212608337\n",
      "iteration 14936: loss: 0.2285315990447998\n",
      "iteration 14937: loss: 0.22853045165538788\n",
      "iteration 14938: loss: 0.22852949798107147\n",
      "iteration 14939: loss: 0.22852838039398193\n",
      "iteration 14940: loss: 0.22852735221385956\n",
      "iteration 14941: loss: 0.2285262644290924\n",
      "iteration 14942: loss: 0.2285252809524536\n",
      "iteration 14943: loss: 0.22852420806884766\n",
      "iteration 14944: loss: 0.22852322459220886\n",
      "iteration 14945: loss: 0.22852206230163574\n",
      "iteration 14946: loss: 0.22852098941802979\n",
      "iteration 14947: loss: 0.228520005941391\n",
      "iteration 14948: loss: 0.22851891815662384\n",
      "iteration 14949: loss: 0.22851788997650146\n",
      "iteration 14950: loss: 0.22851677238941193\n",
      "iteration 14951: loss: 0.22851581871509552\n",
      "iteration 14952: loss: 0.22851471602916718\n",
      "iteration 14953: loss: 0.22851359844207764\n",
      "iteration 14954: loss: 0.22851257026195526\n",
      "iteration 14955: loss: 0.2285114973783493\n",
      "iteration 14956: loss: 0.22851058840751648\n",
      "iteration 14957: loss: 0.22850945591926575\n",
      "iteration 14958: loss: 0.22850850224494934\n",
      "iteration 14959: loss: 0.2285073697566986\n",
      "iteration 14960: loss: 0.22850635647773743\n",
      "iteration 14961: loss: 0.22850525379180908\n",
      "iteration 14962: loss: 0.22850415110588074\n",
      "iteration 14963: loss: 0.22850318253040314\n",
      "iteration 14964: loss: 0.22850200533866882\n",
      "iteration 14965: loss: 0.22850105166435242\n",
      "iteration 14966: loss: 0.22850000858306885\n",
      "iteration 14967: loss: 0.2284989356994629\n",
      "iteration 14968: loss: 0.22849786281585693\n",
      "iteration 14969: loss: 0.22849678993225098\n",
      "iteration 14970: loss: 0.22849583625793457\n",
      "iteration 14971: loss: 0.22849464416503906\n",
      "iteration 14972: loss: 0.22849376499652863\n",
      "iteration 14973: loss: 0.22849266231060028\n",
      "iteration 14974: loss: 0.2284916341304779\n",
      "iteration 14975: loss: 0.22849059104919434\n",
      "iteration 14976: loss: 0.22848960757255554\n",
      "iteration 14977: loss: 0.228488489985466\n",
      "iteration 14978: loss: 0.22848737239837646\n",
      "iteration 14979: loss: 0.22848641872406006\n",
      "iteration 14980: loss: 0.2284853458404541\n",
      "iteration 14981: loss: 0.22848431766033173\n",
      "iteration 14982: loss: 0.22848322987556458\n",
      "iteration 14983: loss: 0.22848224639892578\n",
      "iteration 14984: loss: 0.22848108410835266\n",
      "iteration 14985: loss: 0.22848013043403625\n",
      "iteration 14986: loss: 0.2284790575504303\n",
      "iteration 14987: loss: 0.2284778654575348\n",
      "iteration 14988: loss: 0.22847691178321838\n",
      "iteration 14989: loss: 0.22847583889961243\n",
      "iteration 14990: loss: 0.22847488522529602\n",
      "iteration 14991: loss: 0.2284737080335617\n",
      "iteration 14992: loss: 0.2284727543592453\n",
      "iteration 14993: loss: 0.22847172617912292\n",
      "iteration 14994: loss: 0.2284705638885498\n",
      "iteration 14995: loss: 0.2284696102142334\n",
      "iteration 14996: loss: 0.22846850752830505\n",
      "iteration 14997: loss: 0.22846753895282745\n",
      "iteration 14998: loss: 0.22846636176109314\n",
      "iteration 14999: loss: 0.22846531867980957\n",
      "iteration 15000: loss: 0.22846436500549316\n",
      "iteration 15001: loss: 0.22846320271492004\n",
      "iteration 15002: loss: 0.22846221923828125\n",
      "iteration 15003: loss: 0.2284611165523529\n",
      "iteration 15004: loss: 0.22846007347106934\n",
      "iteration 15005: loss: 0.22845900058746338\n",
      "iteration 15006: loss: 0.22845792770385742\n",
      "iteration 15007: loss: 0.22845694422721863\n",
      "iteration 15008: loss: 0.2284557819366455\n",
      "iteration 15009: loss: 0.2284548282623291\n",
      "iteration 15010: loss: 0.22845375537872314\n",
      "iteration 15011: loss: 0.22845260798931122\n",
      "iteration 15012: loss: 0.2284516841173172\n",
      "iteration 15013: loss: 0.2284506857395172\n",
      "iteration 15014: loss: 0.22844965755939484\n",
      "iteration 15015: loss: 0.2284485548734665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15016: loss: 0.2284475862979889\n",
      "iteration 15017: loss: 0.22844651341438293\n",
      "iteration 15018: loss: 0.228445366024971\n",
      "iteration 15019: loss: 0.2284444123506546\n",
      "iteration 15020: loss: 0.22844333946704865\n",
      "iteration 15021: loss: 0.22844228148460388\n",
      "iteration 15022: loss: 0.22844119369983673\n",
      "iteration 15023: loss: 0.22844013571739197\n",
      "iteration 15024: loss: 0.2284391224384308\n",
      "iteration 15025: loss: 0.22843801975250244\n",
      "iteration 15026: loss: 0.22843703627586365\n",
      "iteration 15027: loss: 0.22843590378761292\n",
      "iteration 15028: loss: 0.2284349501132965\n",
      "iteration 15029: loss: 0.22843387722969055\n",
      "iteration 15030: loss: 0.22843274474143982\n",
      "iteration 15031: loss: 0.2284317910671234\n",
      "iteration 15032: loss: 0.22843067348003387\n",
      "iteration 15033: loss: 0.2284296452999115\n",
      "iteration 15034: loss: 0.22842863202095032\n",
      "iteration 15035: loss: 0.22842755913734436\n",
      "iteration 15036: loss: 0.22842645645141602\n",
      "iteration 15037: loss: 0.22842542827129364\n",
      "iteration 15038: loss: 0.22842442989349365\n",
      "iteration 15039: loss: 0.22842328250408173\n",
      "iteration 15040: loss: 0.2284223586320877\n",
      "iteration 15041: loss: 0.22842128574848175\n",
      "iteration 15042: loss: 0.22842013835906982\n",
      "iteration 15043: loss: 0.22841918468475342\n",
      "iteration 15044: loss: 0.2284179925918579\n",
      "iteration 15045: loss: 0.22841700911521912\n",
      "iteration 15046: loss: 0.22841599583625793\n",
      "iteration 15047: loss: 0.22841480374336243\n",
      "iteration 15048: loss: 0.2284139096736908\n",
      "iteration 15049: loss: 0.22841279208660126\n",
      "iteration 15050: loss: 0.22841182351112366\n",
      "iteration 15051: loss: 0.22841069102287292\n",
      "iteration 15052: loss: 0.22840972244739532\n",
      "iteration 15053: loss: 0.22840861976146698\n",
      "iteration 15054: loss: 0.2284075766801834\n",
      "iteration 15055: loss: 0.22840659320354462\n",
      "iteration 15056: loss: 0.22840550541877747\n",
      "iteration 15057: loss: 0.22840449213981628\n",
      "iteration 15058: loss: 0.22840344905853271\n",
      "iteration 15059: loss: 0.2284022867679596\n",
      "iteration 15060: loss: 0.2284013330936432\n",
      "iteration 15061: loss: 0.22840023040771484\n",
      "iteration 15062: loss: 0.2283991277217865\n",
      "iteration 15063: loss: 0.2283981591463089\n",
      "iteration 15064: loss: 0.22839705646038055\n",
      "iteration 15065: loss: 0.22839605808258057\n",
      "iteration 15066: loss: 0.2283949851989746\n",
      "iteration 15067: loss: 0.22839394211769104\n",
      "iteration 15068: loss: 0.22839288413524628\n",
      "iteration 15069: loss: 0.22839181125164032\n",
      "iteration 15070: loss: 0.22839076817035675\n",
      "iteration 15071: loss: 0.228389710187912\n",
      "iteration 15072: loss: 0.22838878631591797\n",
      "iteration 15073: loss: 0.22838762402534485\n",
      "iteration 15074: loss: 0.2283865213394165\n",
      "iteration 15075: loss: 0.22838559746742249\n",
      "iteration 15076: loss: 0.22838445007801056\n",
      "iteration 15077: loss: 0.22838346660137177\n",
      "iteration 15078: loss: 0.22838231921195984\n",
      "iteration 15079: loss: 0.22838127613067627\n",
      "iteration 15080: loss: 0.22838035225868225\n",
      "iteration 15081: loss: 0.22837920486927032\n",
      "iteration 15082: loss: 0.22837810218334198\n",
      "iteration 15083: loss: 0.22837713360786438\n",
      "iteration 15084: loss: 0.22837600111961365\n",
      "iteration 15085: loss: 0.22837503254413605\n",
      "iteration 15086: loss: 0.22837400436401367\n",
      "iteration 15087: loss: 0.22837288677692413\n",
      "iteration 15088: loss: 0.22837190330028534\n",
      "iteration 15089: loss: 0.2283708155155182\n",
      "iteration 15090: loss: 0.228369802236557\n",
      "iteration 15091: loss: 0.22836871445178986\n",
      "iteration 15092: loss: 0.22836756706237793\n",
      "iteration 15093: loss: 0.2283666878938675\n",
      "iteration 15094: loss: 0.22836561501026154\n",
      "iteration 15095: loss: 0.22836455702781677\n",
      "iteration 15096: loss: 0.2283635139465332\n",
      "iteration 15097: loss: 0.22836244106292725\n",
      "iteration 15098: loss: 0.22836139798164368\n",
      "iteration 15099: loss: 0.2283603698015213\n",
      "iteration 15100: loss: 0.22835934162139893\n",
      "iteration 15101: loss: 0.228358194231987\n",
      "iteration 15102: loss: 0.22835715115070343\n",
      "iteration 15103: loss: 0.22835616767406464\n",
      "iteration 15104: loss: 0.22835507988929749\n",
      "iteration 15105: loss: 0.22835394740104675\n",
      "iteration 15106: loss: 0.22835299372673035\n",
      "iteration 15107: loss: 0.22835195064544678\n",
      "iteration 15108: loss: 0.2283509224653244\n",
      "iteration 15109: loss: 0.22834983468055725\n",
      "iteration 15110: loss: 0.2283487319946289\n",
      "iteration 15111: loss: 0.22834773361682892\n",
      "iteration 15112: loss: 0.22834666073322296\n",
      "iteration 15113: loss: 0.22834563255310059\n",
      "iteration 15114: loss: 0.22834455966949463\n",
      "iteration 15115: loss: 0.22834353148937225\n",
      "iteration 15116: loss: 0.22834248840808868\n",
      "iteration 15117: loss: 0.22834138572216034\n",
      "iteration 15118: loss: 0.2283402681350708\n",
      "iteration 15119: loss: 0.2283393144607544\n",
      "iteration 15120: loss: 0.22833824157714844\n",
      "iteration 15121: loss: 0.22833724319934845\n",
      "iteration 15122: loss: 0.2283361405134201\n",
      "iteration 15123: loss: 0.22833511233329773\n",
      "iteration 15124: loss: 0.22833409905433655\n",
      "iteration 15125: loss: 0.2283330261707306\n",
      "iteration 15126: loss: 0.22833184897899628\n",
      "iteration 15127: loss: 0.22833092510700226\n",
      "iteration 15128: loss: 0.2283298522233963\n",
      "iteration 15129: loss: 0.22832882404327393\n",
      "iteration 15130: loss: 0.22832775115966797\n",
      "iteration 15131: loss: 0.22832664847373962\n",
      "iteration 15132: loss: 0.22832565009593964\n",
      "iteration 15133: loss: 0.22832457721233368\n",
      "iteration 15134: loss: 0.22832348942756653\n",
      "iteration 15135: loss: 0.22832255065441132\n",
      "iteration 15136: loss: 0.2283214032649994\n",
      "iteration 15137: loss: 0.22832047939300537\n",
      "iteration 15138: loss: 0.22831940650939941\n",
      "iteration 15139: loss: 0.22831830382347107\n",
      "iteration 15140: loss: 0.22831737995147705\n",
      "iteration 15141: loss: 0.22831618785858154\n",
      "iteration 15142: loss: 0.22831523418426514\n",
      "iteration 15143: loss: 0.2283141314983368\n",
      "iteration 15144: loss: 0.22831305861473083\n",
      "iteration 15145: loss: 0.22831204533576965\n",
      "iteration 15146: loss: 0.2283109724521637\n",
      "iteration 15147: loss: 0.22830986976623535\n",
      "iteration 15148: loss: 0.22830887138843536\n",
      "iteration 15149: loss: 0.2283077985048294\n",
      "iteration 15150: loss: 0.22830680012702942\n",
      "iteration 15151: loss: 0.22830572724342346\n",
      "iteration 15152: loss: 0.22830462455749512\n",
      "iteration 15153: loss: 0.22830362617969513\n",
      "iteration 15154: loss: 0.22830256819725037\n",
      "iteration 15155: loss: 0.22830143570899963\n",
      "iteration 15156: loss: 0.22830049693584442\n",
      "iteration 15157: loss: 0.22829937934875488\n",
      "iteration 15158: loss: 0.2282983362674713\n",
      "iteration 15159: loss: 0.2282973825931549\n",
      "iteration 15160: loss: 0.22829623520374298\n",
      "iteration 15161: loss: 0.22829516232013702\n",
      "iteration 15162: loss: 0.22829417884349823\n",
      "iteration 15163: loss: 0.22829309105873108\n",
      "iteration 15164: loss: 0.22829203307628632\n",
      "iteration 15165: loss: 0.22829100489616394\n",
      "iteration 15166: loss: 0.22828993201255798\n",
      "iteration 15167: loss: 0.228288933634758\n",
      "iteration 15168: loss: 0.22828784584999084\n",
      "iteration 15169: loss: 0.22828666865825653\n",
      "iteration 15170: loss: 0.2282857447862625\n",
      "iteration 15171: loss: 0.22828459739685059\n",
      "iteration 15172: loss: 0.22828368842601776\n",
      "iteration 15173: loss: 0.22828269004821777\n",
      "iteration 15174: loss: 0.22828154265880585\n",
      "iteration 15175: loss: 0.22828058898448944\n",
      "iteration 15176: loss: 0.2282794713973999\n",
      "iteration 15177: loss: 0.22827839851379395\n",
      "iteration 15178: loss: 0.22827741503715515\n",
      "iteration 15179: loss: 0.228276327252388\n",
      "iteration 15180: loss: 0.22827529907226562\n",
      "iteration 15181: loss: 0.22827425599098206\n",
      "iteration 15182: loss: 0.2282731831073761\n",
      "iteration 15183: loss: 0.22827208042144775\n",
      "iteration 15184: loss: 0.22827109694480896\n",
      "iteration 15185: loss: 0.22826996445655823\n",
      "iteration 15186: loss: 0.22826889157295227\n",
      "iteration 15187: loss: 0.2282678782939911\n",
      "iteration 15188: loss: 0.22826680541038513\n",
      "iteration 15189: loss: 0.2282658815383911\n",
      "iteration 15190: loss: 0.2282647341489792\n",
      "iteration 15191: loss: 0.22826369106769562\n",
      "iteration 15192: loss: 0.22826270759105682\n",
      "iteration 15193: loss: 0.22826163470745087\n",
      "iteration 15194: loss: 0.22826051712036133\n",
      "iteration 15195: loss: 0.22825956344604492\n",
      "iteration 15196: loss: 0.228258416056633\n",
      "iteration 15197: loss: 0.22825737297534943\n",
      "iteration 15198: loss: 0.22825634479522705\n",
      "iteration 15199: loss: 0.2282552719116211\n",
      "iteration 15200: loss: 0.22825424373149872\n",
      "iteration 15201: loss: 0.22825320065021515\n",
      "iteration 15202: loss: 0.2282521277666092\n",
      "iteration 15203: loss: 0.22825109958648682\n",
      "iteration 15204: loss: 0.22825005650520325\n",
      "iteration 15205: loss: 0.22824892401695251\n",
      "iteration 15206: loss: 0.2282479703426361\n",
      "iteration 15207: loss: 0.22824685275554657\n",
      "iteration 15208: loss: 0.2282458245754242\n",
      "iteration 15209: loss: 0.2282448261976242\n",
      "iteration 15210: loss: 0.22824375331401825\n",
      "iteration 15211: loss: 0.22824272513389587\n",
      "iteration 15212: loss: 0.2282417267560959\n",
      "iteration 15213: loss: 0.22824068367481232\n",
      "iteration 15214: loss: 0.2282395362854004\n",
      "iteration 15215: loss: 0.22823858261108398\n",
      "iteration 15216: loss: 0.22823743522167206\n",
      "iteration 15217: loss: 0.22823648154735565\n",
      "iteration 15218: loss: 0.2282353937625885\n",
      "iteration 15219: loss: 0.22823433578014374\n",
      "iteration 15220: loss: 0.22823333740234375\n",
      "iteration 15221: loss: 0.2282322645187378\n",
      "iteration 15222: loss: 0.22823111712932587\n",
      "iteration 15223: loss: 0.22823019325733185\n",
      "iteration 15224: loss: 0.22822904586791992\n",
      "iteration 15225: loss: 0.22822797298431396\n",
      "iteration 15226: loss: 0.22822698950767517\n",
      "iteration 15227: loss: 0.2282259166240692\n",
      "iteration 15228: loss: 0.22822479903697968\n",
      "iteration 15229: loss: 0.22822387516498566\n",
      "iteration 15230: loss: 0.22822275757789612\n",
      "iteration 15231: loss: 0.22822169959545135\n",
      "iteration 15232: loss: 0.22822070121765137\n",
      "iteration 15233: loss: 0.2282196283340454\n",
      "iteration 15234: loss: 0.22821848094463348\n",
      "iteration 15235: loss: 0.22821755707263947\n",
      "iteration 15236: loss: 0.22821637988090515\n",
      "iteration 15237: loss: 0.22821533679962158\n",
      "iteration 15238: loss: 0.22821442782878876\n",
      "iteration 15239: loss: 0.22821328043937683\n",
      "iteration 15240: loss: 0.2282121628522873\n",
      "iteration 15241: loss: 0.22821123898029327\n",
      "iteration 15242: loss: 0.22821009159088135\n",
      "iteration 15243: loss: 0.22820909321308136\n",
      "iteration 15244: loss: 0.22820813953876495\n",
      "iteration 15245: loss: 0.22820702195167542\n",
      "iteration 15246: loss: 0.22820594906806946\n",
      "iteration 15247: loss: 0.22820496559143066\n",
      "iteration 15248: loss: 0.22820380330085754\n",
      "iteration 15249: loss: 0.22820284962654114\n",
      "iteration 15250: loss: 0.22820182144641876\n",
      "iteration 15251: loss: 0.228200763463974\n",
      "iteration 15252: loss: 0.22819964587688446\n",
      "iteration 15253: loss: 0.22819869220256805\n",
      "iteration 15254: loss: 0.22819750010967255\n",
      "iteration 15255: loss: 0.22819647192955017\n",
      "iteration 15256: loss: 0.22819547355175018\n",
      "iteration 15257: loss: 0.22819440066814423\n",
      "iteration 15258: loss: 0.22819337248802185\n",
      "iteration 15259: loss: 0.22819232940673828\n",
      "iteration 15260: loss: 0.22819125652313232\n",
      "iteration 15261: loss: 0.22819027304649353\n",
      "iteration 15262: loss: 0.22818918526172638\n",
      "iteration 15263: loss: 0.22818812727928162\n",
      "iteration 15264: loss: 0.22818705439567566\n",
      "iteration 15265: loss: 0.22818605601787567\n",
      "iteration 15266: loss: 0.2281850129365921\n",
      "iteration 15267: loss: 0.22818389534950256\n",
      "iteration 15268: loss: 0.22818298637866974\n",
      "iteration 15269: loss: 0.2281818687915802\n",
      "iteration 15270: loss: 0.22818072140216827\n",
      "iteration 15271: loss: 0.22817976772785187\n",
      "iteration 15272: loss: 0.22817865014076233\n",
      "iteration 15273: loss: 0.22817754745483398\n",
      "iteration 15274: loss: 0.22817659378051758\n",
      "iteration 15275: loss: 0.22817552089691162\n",
      "iteration 15276: loss: 0.2281743735074997\n",
      "iteration 15277: loss: 0.22817349433898926\n",
      "iteration 15278: loss: 0.22817234694957733\n",
      "iteration 15279: loss: 0.22817130386829376\n",
      "iteration 15280: loss: 0.22817030549049377\n",
      "iteration 15281: loss: 0.228169247508049\n",
      "iteration 15282: loss: 0.22816812992095947\n",
      "iteration 15283: loss: 0.22816714644432068\n",
      "iteration 15284: loss: 0.22816602885723114\n",
      "iteration 15285: loss: 0.22816503047943115\n",
      "iteration 15286: loss: 0.22816403210163116\n",
      "iteration 15287: loss: 0.22816291451454163\n",
      "iteration 15288: loss: 0.22816188633441925\n",
      "iteration 15289: loss: 0.2281607687473297\n",
      "iteration 15290: loss: 0.2281598299741745\n",
      "iteration 15291: loss: 0.22815868258476257\n",
      "iteration 15292: loss: 0.22815768420696259\n",
      "iteration 15293: loss: 0.22815671563148499\n",
      "iteration 15294: loss: 0.22815565764904022\n",
      "iteration 15295: loss: 0.2281545102596283\n",
      "iteration 15296: loss: 0.22815358638763428\n",
      "iteration 15297: loss: 0.22815246880054474\n",
      "iteration 15298: loss: 0.22815144062042236\n",
      "iteration 15299: loss: 0.2281503975391388\n",
      "iteration 15300: loss: 0.22814933955669403\n",
      "iteration 15301: loss: 0.2281481772661209\n",
      "iteration 15302: loss: 0.22814729809761047\n",
      "iteration 15303: loss: 0.22814615070819855\n",
      "iteration 15304: loss: 0.2281450480222702\n",
      "iteration 15305: loss: 0.2281440943479538\n",
      "iteration 15306: loss: 0.22814297676086426\n",
      "iteration 15307: loss: 0.2281419336795807\n",
      "iteration 15308: loss: 0.2281409204006195\n",
      "iteration 15309: loss: 0.22813987731933594\n",
      "iteration 15310: loss: 0.2281387597322464\n",
      "iteration 15311: loss: 0.2281378209590912\n",
      "iteration 15312: loss: 0.22813673317432404\n",
      "iteration 15313: loss: 0.2281356155872345\n",
      "iteration 15314: loss: 0.22813454270362854\n",
      "iteration 15315: loss: 0.22813352942466736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15316: loss: 0.2281324863433838\n",
      "iteration 15317: loss: 0.22813138365745544\n",
      "iteration 15318: loss: 0.22813042998313904\n",
      "iteration 15319: loss: 0.2281293421983719\n",
      "iteration 15320: loss: 0.22812826931476593\n",
      "iteration 15321: loss: 0.22812728583812714\n",
      "iteration 15322: loss: 0.2281261384487152\n",
      "iteration 15323: loss: 0.22812509536743164\n",
      "iteration 15324: loss: 0.2281239926815033\n",
      "iteration 15325: loss: 0.22812309861183167\n",
      "iteration 15326: loss: 0.22812199592590332\n",
      "iteration 15327: loss: 0.22812096774578094\n",
      "iteration 15328: loss: 0.22811999917030334\n",
      "iteration 15329: loss: 0.22811885178089142\n",
      "iteration 15330: loss: 0.22811779379844666\n",
      "iteration 15331: loss: 0.22811679542064667\n",
      "iteration 15332: loss: 0.2281157523393631\n",
      "iteration 15333: loss: 0.22811464965343475\n",
      "iteration 15334: loss: 0.22811360657215118\n",
      "iteration 15335: loss: 0.2281126081943512\n",
      "iteration 15336: loss: 0.22811155021190643\n",
      "iteration 15337: loss: 0.22811046242713928\n",
      "iteration 15338: loss: 0.2281094342470169\n",
      "iteration 15339: loss: 0.22810837626457214\n",
      "iteration 15340: loss: 0.2281072437763214\n",
      "iteration 15341: loss: 0.22810634970664978\n",
      "iteration 15342: loss: 0.22810521721839905\n",
      "iteration 15343: loss: 0.22810418903827667\n",
      "iteration 15344: loss: 0.22810307145118713\n",
      "iteration 15345: loss: 0.22810205817222595\n",
      "iteration 15346: loss: 0.22810098528862\n",
      "iteration 15347: loss: 0.22809991240501404\n",
      "iteration 15348: loss: 0.22809898853302002\n",
      "iteration 15349: loss: 0.2280978411436081\n",
      "iteration 15350: loss: 0.22809681296348572\n",
      "iteration 15351: loss: 0.22809576988220215\n",
      "iteration 15352: loss: 0.22809474170207977\n",
      "iteration 15353: loss: 0.22809362411499023\n",
      "iteration 15354: loss: 0.2280924767255783\n",
      "iteration 15355: loss: 0.22809159755706787\n",
      "iteration 15356: loss: 0.22809047996997833\n",
      "iteration 15357: loss: 0.22808940708637238\n",
      "iteration 15358: loss: 0.22808842360973358\n",
      "iteration 15359: loss: 0.22808738052845\n",
      "iteration 15360: loss: 0.22808632254600525\n",
      "iteration 15361: loss: 0.22808519005775452\n",
      "iteration 15362: loss: 0.22808432579040527\n",
      "iteration 15363: loss: 0.22808320820331573\n",
      "iteration 15364: loss: 0.22808218002319336\n",
      "iteration 15365: loss: 0.22808115184307098\n",
      "iteration 15366: loss: 0.22808006405830383\n",
      "iteration 15367: loss: 0.22807900607585907\n",
      "iteration 15368: loss: 0.22807788848876953\n",
      "iteration 15369: loss: 0.2280769646167755\n",
      "iteration 15370: loss: 0.22807586193084717\n",
      "iteration 15371: loss: 0.22807486355304718\n",
      "iteration 15372: loss: 0.22807380557060242\n",
      "iteration 15373: loss: 0.22807268798351288\n",
      "iteration 15374: loss: 0.22807161509990692\n",
      "iteration 15375: loss: 0.22807066142559052\n",
      "iteration 15376: loss: 0.22806958854198456\n",
      "iteration 15377: loss: 0.22806847095489502\n",
      "iteration 15378: loss: 0.2280675172805786\n",
      "iteration 15379: loss: 0.22806639969348907\n",
      "iteration 15380: loss: 0.2280653417110443\n",
      "iteration 15381: loss: 0.22806422412395477\n",
      "iteration 15382: loss: 0.22806325554847717\n",
      "iteration 15383: loss: 0.2280621975660324\n",
      "iteration 15384: loss: 0.22806107997894287\n",
      "iteration 15385: loss: 0.22806017100811005\n",
      "iteration 15386: loss: 0.2280590832233429\n",
      "iteration 15387: loss: 0.22805801033973694\n",
      "iteration 15388: loss: 0.2280569076538086\n",
      "iteration 15389: loss: 0.22805587947368622\n",
      "iteration 15390: loss: 0.22805485129356384\n",
      "iteration 15391: loss: 0.2280537635087967\n",
      "iteration 15392: loss: 0.22805270552635193\n",
      "iteration 15393: loss: 0.22805175185203552\n",
      "iteration 15394: loss: 0.22805066406726837\n",
      "iteration 15395: loss: 0.22804959118366241\n",
      "iteration 15396: loss: 0.22804860770702362\n",
      "iteration 15397: loss: 0.22804749011993408\n",
      "iteration 15398: loss: 0.2280464917421341\n",
      "iteration 15399: loss: 0.22804546356201172\n",
      "iteration 15400: loss: 0.22804446518421173\n",
      "iteration 15401: loss: 0.2280433177947998\n",
      "iteration 15402: loss: 0.22804224491119385\n",
      "iteration 15403: loss: 0.22804132103919983\n",
      "iteration 15404: loss: 0.22804021835327148\n",
      "iteration 15405: loss: 0.22803917527198792\n",
      "iteration 15406: loss: 0.22803807258605957\n",
      "iteration 15407: loss: 0.2280370444059372\n",
      "iteration 15408: loss: 0.22803601622581482\n",
      "iteration 15409: loss: 0.22803494334220886\n",
      "iteration 15410: loss: 0.22803394496440887\n",
      "iteration 15411: loss: 0.2280329018831253\n",
      "iteration 15412: loss: 0.22803179919719696\n",
      "iteration 15413: loss: 0.22803077101707458\n",
      "iteration 15414: loss: 0.2280297726392746\n",
      "iteration 15415: loss: 0.22802868485450745\n",
      "iteration 15416: loss: 0.22802762687206268\n",
      "iteration 15417: loss: 0.22802653908729553\n",
      "iteration 15418: loss: 0.22802551090717316\n",
      "iteration 15419: loss: 0.22802448272705078\n",
      "iteration 15420: loss: 0.22802338004112244\n",
      "iteration 15421: loss: 0.2280222475528717\n",
      "iteration 15422: loss: 0.22802133858203888\n",
      "iteration 15423: loss: 0.22802023589611053\n",
      "iteration 15424: loss: 0.22801914811134338\n",
      "iteration 15425: loss: 0.22801823914051056\n",
      "iteration 15426: loss: 0.22801712155342102\n",
      "iteration 15427: loss: 0.22801606357097626\n",
      "iteration 15428: loss: 0.2280149757862091\n",
      "iteration 15429: loss: 0.2280139923095703\n",
      "iteration 15430: loss: 0.22801294922828674\n",
      "iteration 15431: loss: 0.2280118465423584\n",
      "iteration 15432: loss: 0.22801074385643005\n",
      "iteration 15433: loss: 0.22800979018211365\n",
      "iteration 15434: loss: 0.2280087172985077\n",
      "iteration 15435: loss: 0.22800759971141815\n",
      "iteration 15436: loss: 0.22800663113594055\n",
      "iteration 15437: loss: 0.22800561785697937\n",
      "iteration 15438: loss: 0.2280046045780182\n",
      "iteration 15439: loss: 0.22800345718860626\n",
      "iteration 15440: loss: 0.2280023843050003\n",
      "iteration 15441: loss: 0.2280014008283615\n",
      "iteration 15442: loss: 0.22800035774707794\n",
      "iteration 15443: loss: 0.22799928486347198\n",
      "iteration 15444: loss: 0.2279983013868332\n",
      "iteration 15445: loss: 0.22799725830554962\n",
      "iteration 15446: loss: 0.2279961109161377\n",
      "iteration 15447: loss: 0.22799500823020935\n",
      "iteration 15448: loss: 0.22799408435821533\n",
      "iteration 15449: loss: 0.22799301147460938\n",
      "iteration 15450: loss: 0.22799189388751984\n",
      "iteration 15451: loss: 0.22799095511436462\n",
      "iteration 15452: loss: 0.22798988223075867\n",
      "iteration 15453: loss: 0.22798874974250793\n",
      "iteration 15454: loss: 0.22798773646354675\n",
      "iteration 15455: loss: 0.22798673808574677\n",
      "iteration 15456: loss: 0.22798562049865723\n",
      "iteration 15457: loss: 0.22798463702201843\n",
      "iteration 15458: loss: 0.2279835194349289\n",
      "iteration 15459: loss: 0.2279825508594513\n",
      "iteration 15460: loss: 0.22798152267932892\n",
      "iteration 15461: loss: 0.227980375289917\n",
      "iteration 15462: loss: 0.22797933220863342\n",
      "iteration 15463: loss: 0.22797825932502747\n",
      "iteration 15464: loss: 0.22797730565071106\n",
      "iteration 15465: loss: 0.22797620296478271\n",
      "iteration 15466: loss: 0.22797513008117676\n",
      "iteration 15467: loss: 0.2279740273952484\n",
      "iteration 15468: loss: 0.2279730588197708\n",
      "iteration 15469: loss: 0.22797203063964844\n",
      "iteration 15470: loss: 0.2279709130525589\n",
      "iteration 15471: loss: 0.2279699295759201\n",
      "iteration 15472: loss: 0.22796893119812012\n",
      "iteration 15473: loss: 0.22796785831451416\n",
      "iteration 15474: loss: 0.22796685993671417\n",
      "iteration 15475: loss: 0.2279658317565918\n",
      "iteration 15476: loss: 0.22796472907066345\n",
      "iteration 15477: loss: 0.22796359658241272\n",
      "iteration 15478: loss: 0.2279626876115799\n",
      "iteration 15479: loss: 0.22796161472797394\n",
      "iteration 15480: loss: 0.2279604971408844\n",
      "iteration 15481: loss: 0.2279595136642456\n",
      "iteration 15482: loss: 0.22795835137367249\n",
      "iteration 15483: loss: 0.22795741260051727\n",
      "iteration 15484: loss: 0.22795633971691132\n",
      "iteration 15485: loss: 0.22795525193214417\n",
      "iteration 15486: loss: 0.2279541790485382\n",
      "iteration 15487: loss: 0.227953240275383\n",
      "iteration 15488: loss: 0.22795216739177704\n",
      "iteration 15489: loss: 0.22795109450817108\n",
      "iteration 15490: loss: 0.22795002162456512\n",
      "iteration 15491: loss: 0.22794905304908752\n",
      "iteration 15492: loss: 0.22794798016548157\n",
      "iteration 15493: loss: 0.22794684767723083\n",
      "iteration 15494: loss: 0.22794583439826965\n",
      "iteration 15495: loss: 0.22794480621814728\n",
      "iteration 15496: loss: 0.22794373333454132\n",
      "iteration 15497: loss: 0.22794263064861298\n",
      "iteration 15498: loss: 0.22794155776500702\n",
      "iteration 15499: loss: 0.2279406040906906\n",
      "iteration 15500: loss: 0.22793953120708466\n",
      "iteration 15501: loss: 0.2279384583234787\n",
      "iteration 15502: loss: 0.22793738543987274\n",
      "iteration 15503: loss: 0.22793638706207275\n",
      "iteration 15504: loss: 0.2279352843761444\n",
      "iteration 15505: loss: 0.2279343158006668\n",
      "iteration 15506: loss: 0.22793324291706085\n",
      "iteration 15507: loss: 0.2279321253299713\n",
      "iteration 15508: loss: 0.2279311865568161\n",
      "iteration 15509: loss: 0.22793011367321014\n",
      "iteration 15510: loss: 0.2279290407896042\n",
      "iteration 15511: loss: 0.22792795300483704\n",
      "iteration 15512: loss: 0.2279270440340042\n",
      "iteration 15513: loss: 0.22792594134807587\n",
      "iteration 15514: loss: 0.22792479395866394\n",
      "iteration 15515: loss: 0.22792379558086395\n",
      "iteration 15516: loss: 0.22792282700538635\n",
      "iteration 15517: loss: 0.227921724319458\n",
      "iteration 15518: loss: 0.22792062163352966\n",
      "iteration 15519: loss: 0.2279195785522461\n",
      "iteration 15520: loss: 0.22791865468025208\n",
      "iteration 15521: loss: 0.22791752219200134\n",
      "iteration 15522: loss: 0.22791652381420135\n",
      "iteration 15523: loss: 0.2279154360294342\n",
      "iteration 15524: loss: 0.22791440784931183\n",
      "iteration 15525: loss: 0.22791337966918945\n",
      "iteration 15526: loss: 0.2279123067855835\n",
      "iteration 15527: loss: 0.22791123390197754\n",
      "iteration 15528: loss: 0.2279100865125656\n",
      "iteration 15529: loss: 0.22790920734405518\n",
      "iteration 15530: loss: 0.22790810465812683\n",
      "iteration 15531: loss: 0.22790703177452087\n",
      "iteration 15532: loss: 0.22790589928627014\n",
      "iteration 15533: loss: 0.22790499031543732\n",
      "iteration 15534: loss: 0.22790391743183136\n",
      "iteration 15535: loss: 0.22790279984474182\n",
      "iteration 15536: loss: 0.22790181636810303\n",
      "iteration 15537: loss: 0.2279006987810135\n",
      "iteration 15538: loss: 0.2278997153043747\n",
      "iteration 15539: loss: 0.22789862751960754\n",
      "iteration 15540: loss: 0.22789761424064636\n",
      "iteration 15541: loss: 0.22789661586284637\n",
      "iteration 15542: loss: 0.22789554297924042\n",
      "iteration 15543: loss: 0.22789442539215088\n",
      "iteration 15544: loss: 0.2278933972120285\n",
      "iteration 15545: loss: 0.22789247334003448\n",
      "iteration 15546: loss: 0.22789140045642853\n",
      "iteration 15547: loss: 0.227890282869339\n",
      "iteration 15548: loss: 0.22788920998573303\n",
      "iteration 15549: loss: 0.22788825631141663\n",
      "iteration 15550: loss: 0.22788719832897186\n",
      "iteration 15551: loss: 0.2278861254453659\n",
      "iteration 15552: loss: 0.22788500785827637\n",
      "iteration 15553: loss: 0.22788400948047638\n",
      "iteration 15554: loss: 0.2278830111026764\n",
      "iteration 15555: loss: 0.22788193821907043\n",
      "iteration 15556: loss: 0.2278808355331421\n",
      "iteration 15557: loss: 0.22787980735301971\n",
      "iteration 15558: loss: 0.22787873446941376\n",
      "iteration 15559: loss: 0.22787776589393616\n",
      "iteration 15560: loss: 0.22787673771381378\n",
      "iteration 15561: loss: 0.22787566483020782\n",
      "iteration 15562: loss: 0.22787456214427948\n",
      "iteration 15563: loss: 0.22787344455718994\n",
      "iteration 15564: loss: 0.2278725653886795\n",
      "iteration 15565: loss: 0.22787146270275116\n",
      "iteration 15566: loss: 0.22787034511566162\n",
      "iteration 15567: loss: 0.22786927223205566\n",
      "iteration 15568: loss: 0.22786839306354523\n",
      "iteration 15569: loss: 0.22786729037761688\n",
      "iteration 15570: loss: 0.22786617279052734\n",
      "iteration 15571: loss: 0.227865070104599\n",
      "iteration 15572: loss: 0.227864071726799\n",
      "iteration 15573: loss: 0.2278631031513214\n",
      "iteration 15574: loss: 0.22786200046539307\n",
      "iteration 15575: loss: 0.22786089777946472\n",
      "iteration 15576: loss: 0.22785980999469757\n",
      "iteration 15577: loss: 0.2278587818145752\n",
      "iteration 15578: loss: 0.2278578281402588\n",
      "iteration 15579: loss: 0.2278567999601364\n",
      "iteration 15580: loss: 0.22785571217536926\n",
      "iteration 15581: loss: 0.22785460948944092\n",
      "iteration 15582: loss: 0.22785361111164093\n",
      "iteration 15583: loss: 0.22785262763500214\n",
      "iteration 15584: loss: 0.22785155475139618\n",
      "iteration 15585: loss: 0.22785048186779022\n",
      "iteration 15586: loss: 0.22784948348999023\n",
      "iteration 15587: loss: 0.2278483659029007\n",
      "iteration 15588: loss: 0.2278473824262619\n",
      "iteration 15589: loss: 0.22784629464149475\n",
      "iteration 15590: loss: 0.22784526646137238\n",
      "iteration 15591: loss: 0.22784414887428284\n",
      "iteration 15592: loss: 0.22784321010112762\n",
      "iteration 15593: loss: 0.22784213721752167\n",
      "iteration 15594: loss: 0.22784098982810974\n",
      "iteration 15595: loss: 0.22783991694450378\n",
      "iteration 15596: loss: 0.22783902287483215\n",
      "iteration 15597: loss: 0.2278379648923874\n",
      "iteration 15598: loss: 0.22783689200878143\n",
      "iteration 15599: loss: 0.2278357744216919\n",
      "iteration 15600: loss: 0.22783470153808594\n",
      "iteration 15601: loss: 0.2278337925672531\n",
      "iteration 15602: loss: 0.22783267498016357\n",
      "iteration 15603: loss: 0.22783160209655762\n",
      "iteration 15604: loss: 0.22783055901527405\n",
      "iteration 15605: loss: 0.22782950103282928\n",
      "iteration 15606: loss: 0.22782853245735168\n",
      "iteration 15607: loss: 0.2278275042772293\n",
      "iteration 15608: loss: 0.22782635688781738\n",
      "iteration 15609: loss: 0.2278253585100174\n",
      "iteration 15610: loss: 0.22782424092292786\n",
      "iteration 15611: loss: 0.2278231382369995\n",
      "iteration 15612: loss: 0.22782215476036072\n",
      "iteration 15613: loss: 0.22782115638256073\n",
      "iteration 15614: loss: 0.22782018780708313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15615: loss: 0.22781908512115479\n",
      "iteration 15616: loss: 0.227818101644516\n",
      "iteration 15617: loss: 0.22781701385974884\n",
      "iteration 15618: loss: 0.22781594097614288\n",
      "iteration 15619: loss: 0.22781482338905334\n",
      "iteration 15620: loss: 0.22781388461589813\n",
      "iteration 15621: loss: 0.22781284153461456\n",
      "iteration 15622: loss: 0.22781173884868622\n",
      "iteration 15623: loss: 0.22781066596508026\n",
      "iteration 15624: loss: 0.2278095781803131\n",
      "iteration 15625: loss: 0.22780855000019073\n",
      "iteration 15626: loss: 0.22780756652355194\n",
      "iteration 15627: loss: 0.22780649363994598\n",
      "iteration 15628: loss: 0.22780545055866241\n",
      "iteration 15629: loss: 0.22780437767505646\n",
      "iteration 15630: loss: 0.22780334949493408\n",
      "iteration 15631: loss: 0.2278023064136505\n",
      "iteration 15632: loss: 0.2278013527393341\n",
      "iteration 15633: loss: 0.22780022025108337\n",
      "iteration 15634: loss: 0.22779913246631622\n",
      "iteration 15635: loss: 0.22779813408851624\n",
      "iteration 15636: loss: 0.22779706120491028\n",
      "iteration 15637: loss: 0.22779607772827148\n",
      "iteration 15638: loss: 0.22779500484466553\n",
      "iteration 15639: loss: 0.22779390215873718\n",
      "iteration 15640: loss: 0.22779281437397003\n",
      "iteration 15641: loss: 0.2277917116880417\n",
      "iteration 15642: loss: 0.22779080271720886\n",
      "iteration 15643: loss: 0.2277897298336029\n",
      "iteration 15644: loss: 0.22778868675231934\n",
      "iteration 15645: loss: 0.227787584066391\n",
      "iteration 15646: loss: 0.22778654098510742\n",
      "iteration 15647: loss: 0.22778558731079102\n",
      "iteration 15648: loss: 0.22778454422950745\n",
      "iteration 15649: loss: 0.22778348624706268\n",
      "iteration 15650: loss: 0.22778241336345673\n",
      "iteration 15651: loss: 0.22778144478797913\n",
      "iteration 15652: loss: 0.22778034210205078\n",
      "iteration 15653: loss: 0.22777926921844482\n",
      "iteration 15654: loss: 0.22777824103832245\n",
      "iteration 15655: loss: 0.2277771681547165\n",
      "iteration 15656: loss: 0.22777609527111053\n",
      "iteration 15657: loss: 0.22777502238750458\n",
      "iteration 15658: loss: 0.22777406871318817\n",
      "iteration 15659: loss: 0.22777307033538818\n",
      "iteration 15660: loss: 0.22777196764945984\n",
      "iteration 15661: loss: 0.22777089476585388\n",
      "iteration 15662: loss: 0.22776980698108673\n",
      "iteration 15663: loss: 0.22776886820793152\n",
      "iteration 15664: loss: 0.22776779532432556\n",
      "iteration 15665: loss: 0.2277667075395584\n",
      "iteration 15666: loss: 0.22776564955711365\n",
      "iteration 15667: loss: 0.2277645617723465\n",
      "iteration 15668: loss: 0.22776350378990173\n",
      "iteration 15669: loss: 0.22776250541210175\n",
      "iteration 15670: loss: 0.22776146233081818\n",
      "iteration 15671: loss: 0.22776035964488983\n",
      "iteration 15672: loss: 0.22775931656360626\n",
      "iteration 15673: loss: 0.22775831818580627\n",
      "iteration 15674: loss: 0.22775733470916748\n",
      "iteration 15675: loss: 0.2277562916278839\n",
      "iteration 15676: loss: 0.22775521874427795\n",
      "iteration 15677: loss: 0.227754145860672\n",
      "iteration 15678: loss: 0.22775301337242126\n",
      "iteration 15679: loss: 0.22775201499462128\n",
      "iteration 15680: loss: 0.22775104641914368\n",
      "iteration 15681: loss: 0.22774997353553772\n",
      "iteration 15682: loss: 0.22774887084960938\n",
      "iteration 15683: loss: 0.227747842669487\n",
      "iteration 15684: loss: 0.22774676978588104\n",
      "iteration 15685: loss: 0.2277458906173706\n",
      "iteration 15686: loss: 0.22774478793144226\n",
      "iteration 15687: loss: 0.2277437150478363\n",
      "iteration 15688: loss: 0.22774267196655273\n",
      "iteration 15689: loss: 0.2277415692806244\n",
      "iteration 15690: loss: 0.22774048149585724\n",
      "iteration 15691: loss: 0.22773952782154083\n",
      "iteration 15692: loss: 0.22773854434490204\n",
      "iteration 15693: loss: 0.22773747146129608\n",
      "iteration 15694: loss: 0.22773638367652893\n",
      "iteration 15695: loss: 0.22773528099060059\n",
      "iteration 15696: loss: 0.22773420810699463\n",
      "iteration 15697: loss: 0.22773322463035583\n",
      "iteration 15698: loss: 0.22773218154907227\n",
      "iteration 15699: loss: 0.22773118317127228\n",
      "iteration 15700: loss: 0.22773008048534393\n",
      "iteration 15701: loss: 0.2277289628982544\n",
      "iteration 15702: loss: 0.22772793471813202\n",
      "iteration 15703: loss: 0.2277269810438156\n",
      "iteration 15704: loss: 0.22772590816020966\n",
      "iteration 15705: loss: 0.2277248352766037\n",
      "iteration 15706: loss: 0.22772374749183655\n",
      "iteration 15707: loss: 0.22772271931171417\n",
      "iteration 15708: loss: 0.22772173583507538\n",
      "iteration 15709: loss: 0.22772066295146942\n",
      "iteration 15710: loss: 0.22771963477134705\n",
      "iteration 15711: loss: 0.22771862149238586\n",
      "iteration 15712: loss: 0.22771751880645752\n",
      "iteration 15713: loss: 0.22771644592285156\n",
      "iteration 15714: loss: 0.22771546244621277\n",
      "iteration 15715: loss: 0.2277143895626068\n",
      "iteration 15716: loss: 0.22771331667900085\n",
      "iteration 15717: loss: 0.22771231830120087\n",
      "iteration 15718: loss: 0.2277112454175949\n",
      "iteration 15719: loss: 0.22771024703979492\n",
      "iteration 15720: loss: 0.22770920395851135\n",
      "iteration 15721: loss: 0.2277081459760666\n",
      "iteration 15722: loss: 0.2277071177959442\n",
      "iteration 15723: loss: 0.22770603001117706\n",
      "iteration 15724: loss: 0.2277049720287323\n",
      "iteration 15725: loss: 0.22770389914512634\n",
      "iteration 15726: loss: 0.22770294547080994\n",
      "iteration 15727: loss: 0.22770187258720398\n",
      "iteration 15728: loss: 0.22770079970359802\n",
      "iteration 15729: loss: 0.22769972681999207\n",
      "iteration 15730: loss: 0.22769871354103088\n",
      "iteration 15731: loss: 0.2276976853609085\n",
      "iteration 15732: loss: 0.22769658267498016\n",
      "iteration 15733: loss: 0.22769562900066376\n",
      "iteration 15734: loss: 0.2276945561170578\n",
      "iteration 15735: loss: 0.22769346833229065\n",
      "iteration 15736: loss: 0.2276924103498459\n",
      "iteration 15737: loss: 0.22769133746623993\n",
      "iteration 15738: loss: 0.22769033908843994\n",
      "iteration 15739: loss: 0.22768929600715637\n",
      "iteration 15740: loss: 0.2276882827281952\n",
      "iteration 15741: loss: 0.22768720984458923\n",
      "iteration 15742: loss: 0.22768613696098328\n",
      "iteration 15743: loss: 0.2276850938796997\n",
      "iteration 15744: loss: 0.22768399119377136\n",
      "iteration 15745: loss: 0.2276829481124878\n",
      "iteration 15746: loss: 0.22768202424049377\n",
      "iteration 15747: loss: 0.22768095135688782\n",
      "iteration 15748: loss: 0.22767987847328186\n",
      "iteration 15749: loss: 0.2276788055896759\n",
      "iteration 15750: loss: 0.22767777740955353\n",
      "iteration 15751: loss: 0.22767674922943115\n",
      "iteration 15752: loss: 0.2276756763458252\n",
      "iteration 15753: loss: 0.22767476737499237\n",
      "iteration 15754: loss: 0.2276736944913864\n",
      "iteration 15755: loss: 0.22767257690429688\n",
      "iteration 15756: loss: 0.2276715338230133\n",
      "iteration 15757: loss: 0.22767046093940735\n",
      "iteration 15758: loss: 0.2276693880558014\n",
      "iteration 15759: loss: 0.22766831517219543\n",
      "iteration 15760: loss: 0.22766737639904022\n",
      "iteration 15761: loss: 0.22766634821891785\n",
      "iteration 15762: loss: 0.2276652604341507\n",
      "iteration 15763: loss: 0.22766418755054474\n",
      "iteration 15764: loss: 0.22766311466693878\n",
      "iteration 15765: loss: 0.2276621162891388\n",
      "iteration 15766: loss: 0.2276611030101776\n",
      "iteration 15767: loss: 0.22766008973121643\n",
      "iteration 15768: loss: 0.2276589572429657\n",
      "iteration 15769: loss: 0.22765794396400452\n",
      "iteration 15770: loss: 0.22765687108039856\n",
      "iteration 15771: loss: 0.22765591740608215\n",
      "iteration 15772: loss: 0.2276548147201538\n",
      "iteration 15773: loss: 0.22765374183654785\n",
      "iteration 15774: loss: 0.2276526391506195\n",
      "iteration 15775: loss: 0.22765164077281952\n",
      "iteration 15776: loss: 0.22765056788921356\n",
      "iteration 15777: loss: 0.22764952480793\n",
      "iteration 15778: loss: 0.22764852643013\n",
      "iteration 15779: loss: 0.22764746844768524\n",
      "iteration 15780: loss: 0.22764639556407928\n",
      "iteration 15781: loss: 0.22764532268047333\n",
      "iteration 15782: loss: 0.22764432430267334\n",
      "iteration 15783: loss: 0.22764325141906738\n",
      "iteration 15784: loss: 0.2276422530412674\n",
      "iteration 15785: loss: 0.22764118015766144\n",
      "iteration 15786: loss: 0.22764010727405548\n",
      "iteration 15787: loss: 0.22763916850090027\n",
      "iteration 15788: loss: 0.2276380956172943\n",
      "iteration 15789: loss: 0.22763705253601074\n",
      "iteration 15790: loss: 0.22763597965240479\n",
      "iteration 15791: loss: 0.22763487696647644\n",
      "iteration 15792: loss: 0.22763383388519287\n",
      "iteration 15793: loss: 0.22763285040855408\n",
      "iteration 15794: loss: 0.2276318371295929\n",
      "iteration 15795: loss: 0.22763076424598694\n",
      "iteration 15796: loss: 0.22762970626354218\n",
      "iteration 15797: loss: 0.22762861847877502\n",
      "iteration 15798: loss: 0.22762756049633026\n",
      "iteration 15799: loss: 0.2276264876127243\n",
      "iteration 15800: loss: 0.22762541472911835\n",
      "iteration 15801: loss: 0.22762437164783478\n",
      "iteration 15802: loss: 0.22762343287467957\n",
      "iteration 15803: loss: 0.2276223599910736\n",
      "iteration 15804: loss: 0.22762134671211243\n",
      "iteration 15805: loss: 0.22762024402618408\n",
      "iteration 15806: loss: 0.2276192009449005\n",
      "iteration 15807: loss: 0.22761818766593933\n",
      "iteration 15808: loss: 0.22761714458465576\n",
      "iteration 15809: loss: 0.2276160717010498\n",
      "iteration 15810: loss: 0.227615088224411\n",
      "iteration 15811: loss: 0.22761401534080505\n",
      "iteration 15812: loss: 0.2276129424571991\n",
      "iteration 15813: loss: 0.22761186957359314\n",
      "iteration 15814: loss: 0.22761082649230957\n",
      "iteration 15815: loss: 0.227609783411026\n",
      "iteration 15816: loss: 0.22760868072509766\n",
      "iteration 15817: loss: 0.22760780155658722\n",
      "iteration 15818: loss: 0.22760677337646484\n",
      "iteration 15819: loss: 0.22760573029518127\n",
      "iteration 15820: loss: 0.22760465741157532\n",
      "iteration 15821: loss: 0.22760355472564697\n",
      "iteration 15822: loss: 0.22760263085365295\n",
      "iteration 15823: loss: 0.227601557970047\n",
      "iteration 15824: loss: 0.22760048508644104\n",
      "iteration 15825: loss: 0.22759942710399628\n",
      "iteration 15826: loss: 0.22759833931922913\n",
      "iteration 15827: loss: 0.22759726643562317\n",
      "iteration 15828: loss: 0.2275962084531784\n",
      "iteration 15829: loss: 0.22759516537189484\n",
      "iteration 15830: loss: 0.22759422659873962\n",
      "iteration 15831: loss: 0.22759313881397247\n",
      "iteration 15832: loss: 0.22759214043617249\n",
      "iteration 15833: loss: 0.22759108245372772\n",
      "iteration 15834: loss: 0.22758996486663818\n",
      "iteration 15835: loss: 0.2275889366865158\n",
      "iteration 15836: loss: 0.22758789360523224\n",
      "iteration 15837: loss: 0.22758683562278748\n",
      "iteration 15838: loss: 0.22758574783802032\n",
      "iteration 15839: loss: 0.22758471965789795\n",
      "iteration 15840: loss: 0.22758376598358154\n",
      "iteration 15841: loss: 0.22758272290229797\n",
      "iteration 15842: loss: 0.22758162021636963\n",
      "iteration 15843: loss: 0.22758059203624725\n",
      "iteration 15844: loss: 0.2275795191526413\n",
      "iteration 15845: loss: 0.22757844626903534\n",
      "iteration 15846: loss: 0.22757737338542938\n",
      "iteration 15847: loss: 0.22757641971111298\n",
      "iteration 15848: loss: 0.22757534682750702\n",
      "iteration 15849: loss: 0.22757431864738464\n",
      "iteration 15850: loss: 0.22757332026958466\n",
      "iteration 15851: loss: 0.2275722473859787\n",
      "iteration 15852: loss: 0.22757121920585632\n",
      "iteration 15853: loss: 0.22757013142108917\n",
      "iteration 15854: loss: 0.2275690734386444\n",
      "iteration 15855: loss: 0.22756800055503845\n",
      "iteration 15856: loss: 0.22756695747375488\n",
      "iteration 15857: loss: 0.22756603360176086\n",
      "iteration 15858: loss: 0.2275649607181549\n",
      "iteration 15859: loss: 0.22756388783454895\n",
      "iteration 15860: loss: 0.22756290435791016\n",
      "iteration 15861: loss: 0.2275618314743042\n",
      "iteration 15862: loss: 0.22756075859069824\n",
      "iteration 15863: loss: 0.2275596559047699\n",
      "iteration 15864: loss: 0.22755873203277588\n",
      "iteration 15865: loss: 0.22755756974220276\n",
      "iteration 15866: loss: 0.22755658626556396\n",
      "iteration 15867: loss: 0.22755558788776398\n",
      "iteration 15868: loss: 0.22755448520183563\n",
      "iteration 15869: loss: 0.22755339741706848\n",
      "iteration 15870: loss: 0.2275523692369461\n",
      "iteration 15871: loss: 0.22755125164985657\n",
      "iteration 15872: loss: 0.2275502234697342\n",
      "iteration 15873: loss: 0.22754916548728943\n",
      "iteration 15874: loss: 0.22754815220832825\n",
      "iteration 15875: loss: 0.22754716873168945\n",
      "iteration 15876: loss: 0.22754612565040588\n",
      "iteration 15877: loss: 0.22754506766796112\n",
      "iteration 15878: loss: 0.22754399478435516\n",
      "iteration 15879: loss: 0.2275429517030716\n",
      "iteration 15880: loss: 0.22754189372062683\n",
      "iteration 15881: loss: 0.22754085063934326\n",
      "iteration 15882: loss: 0.2275398224592209\n",
      "iteration 15883: loss: 0.2275387942790985\n",
      "iteration 15884: loss: 0.22753770649433136\n",
      "iteration 15885: loss: 0.22753679752349854\n",
      "iteration 15886: loss: 0.22753572463989258\n",
      "iteration 15887: loss: 0.227534681558609\n",
      "iteration 15888: loss: 0.22753365337848663\n",
      "iteration 15889: loss: 0.22753258049488068\n",
      "iteration 15890: loss: 0.2275315225124359\n",
      "iteration 15891: loss: 0.22753040492534637\n",
      "iteration 15892: loss: 0.2275294065475464\n",
      "iteration 15893: loss: 0.22752830386161804\n",
      "iteration 15894: loss: 0.22752729058265686\n",
      "iteration 15895: loss: 0.22752630710601807\n",
      "iteration 15896: loss: 0.2275252789258957\n",
      "iteration 15897: loss: 0.22752419114112854\n",
      "iteration 15898: loss: 0.22752316296100616\n",
      "iteration 15899: loss: 0.2275220900774002\n",
      "iteration 15900: loss: 0.22752101719379425\n",
      "iteration 15901: loss: 0.22751998901367188\n",
      "iteration 15902: loss: 0.2275189608335495\n",
      "iteration 15903: loss: 0.2275179922580719\n",
      "iteration 15904: loss: 0.22751693427562714\n",
      "iteration 15905: loss: 0.22751589119434357\n",
      "iteration 15906: loss: 0.2275148332118988\n",
      "iteration 15907: loss: 0.22751376032829285\n",
      "iteration 15908: loss: 0.22751271724700928\n",
      "iteration 15909: loss: 0.2275116741657257\n",
      "iteration 15910: loss: 0.22751061618328094\n",
      "iteration 15911: loss: 0.22750946879386902\n",
      "iteration 15912: loss: 0.22750845551490784\n",
      "iteration 15913: loss: 0.2275073528289795\n",
      "iteration 15914: loss: 0.2275063544511795\n",
      "iteration 15915: loss: 0.22750547528266907\n",
      "iteration 15916: loss: 0.2275044173002243\n",
      "iteration 15917: loss: 0.22750334441661835\n",
      "iteration 15918: loss: 0.2275022715330124\n",
      "iteration 15919: loss: 0.22750124335289001\n",
      "iteration 15920: loss: 0.22750020027160645\n",
      "iteration 15921: loss: 0.2274991273880005\n",
      "iteration 15922: loss: 0.22749808430671692\n",
      "iteration 15923: loss: 0.22749705612659454\n",
      "iteration 15924: loss: 0.2274959832429886\n",
      "iteration 15925: loss: 0.22749502956867218\n",
      "iteration 15926: loss: 0.2274940311908722\n",
      "iteration 15927: loss: 0.22749288380146027\n",
      "iteration 15928: loss: 0.2274918258190155\n",
      "iteration 15929: loss: 0.22749078273773193\n",
      "iteration 15930: loss: 0.22748973965644836\n",
      "iteration 15931: loss: 0.2274886667728424\n",
      "iteration 15932: loss: 0.22748765349388123\n",
      "iteration 15933: loss: 0.22748658061027527\n",
      "iteration 15934: loss: 0.22748549282550812\n",
      "iteration 15935: loss: 0.22748443484306335\n",
      "iteration 15936: loss: 0.22748354077339172\n",
      "iteration 15937: loss: 0.22748246788978577\n",
      "iteration 15938: loss: 0.227481409907341\n",
      "iteration 15939: loss: 0.22748041152954102\n",
      "iteration 15940: loss: 0.22747930884361267\n",
      "iteration 15941: loss: 0.2274782657623291\n",
      "iteration 15942: loss: 0.22747719287872314\n",
      "iteration 15943: loss: 0.2274761199951172\n",
      "iteration 15944: loss: 0.2274751216173172\n",
      "iteration 15945: loss: 0.22747401893138885\n",
      "iteration 15946: loss: 0.22747306525707245\n",
      "iteration 15947: loss: 0.2274719923734665\n",
      "iteration 15948: loss: 0.22747106850147247\n",
      "iteration 15949: loss: 0.2274700105190277\n",
      "iteration 15950: loss: 0.22746893763542175\n",
      "iteration 15951: loss: 0.2274678498506546\n",
      "iteration 15952: loss: 0.22746679186820984\n",
      "iteration 15953: loss: 0.22746574878692627\n",
      "iteration 15954: loss: 0.2274646759033203\n",
      "iteration 15955: loss: 0.22746364772319794\n",
      "iteration 15956: loss: 0.22746261954307556\n",
      "iteration 15957: loss: 0.2274615466594696\n",
      "iteration 15958: loss: 0.22746047377586365\n",
      "iteration 15959: loss: 0.2274594008922577\n",
      "iteration 15960: loss: 0.22745847702026367\n",
      "iteration 15961: loss: 0.2274574339389801\n",
      "iteration 15962: loss: 0.22745637595653534\n",
      "iteration 15963: loss: 0.22745534777641296\n",
      "iteration 15964: loss: 0.227454274892807\n",
      "iteration 15965: loss: 0.22745326161384583\n",
      "iteration 15966: loss: 0.2274521142244339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15967: loss: 0.22745105624198914\n",
      "iteration 15968: loss: 0.22745001316070557\n",
      "iteration 15969: loss: 0.22744901478290558\n",
      "iteration 15970: loss: 0.22744794189929962\n",
      "iteration 15971: loss: 0.22744688391685486\n",
      "iteration 15972: loss: 0.22744588553905487\n",
      "iteration 15973: loss: 0.2274448424577713\n",
      "iteration 15974: loss: 0.22744378447532654\n",
      "iteration 15975: loss: 0.22744286060333252\n",
      "iteration 15976: loss: 0.22744174301624298\n",
      "iteration 15977: loss: 0.22744068503379822\n",
      "iteration 15978: loss: 0.22743968665599823\n",
      "iteration 15979: loss: 0.22743865847587585\n",
      "iteration 15980: loss: 0.2274375706911087\n",
      "iteration 15981: loss: 0.22743654251098633\n",
      "iteration 15982: loss: 0.22743549942970276\n",
      "iteration 15983: loss: 0.22743448615074158\n",
      "iteration 15984: loss: 0.22743336856365204\n",
      "iteration 15985: loss: 0.22743234038352966\n",
      "iteration 15986: loss: 0.2274312525987625\n",
      "iteration 15987: loss: 0.2274303138256073\n",
      "iteration 15988: loss: 0.22742930054664612\n",
      "iteration 15989: loss: 0.22742824256420135\n",
      "iteration 15990: loss: 0.22742719948291779\n",
      "iteration 15991: loss: 0.22742612659931183\n",
      "iteration 15992: loss: 0.22742502391338348\n",
      "iteration 15993: loss: 0.2274239957332611\n",
      "iteration 15994: loss: 0.22742298245429993\n",
      "iteration 15995: loss: 0.22742192447185516\n",
      "iteration 15996: loss: 0.2274208962917328\n",
      "iteration 15997: loss: 0.2274198830127716\n",
      "iteration 15998: loss: 0.22741875052452087\n",
      "iteration 15999: loss: 0.2274177074432373\n",
      "iteration 16000: loss: 0.22741666436195374\n",
      "iteration 16001: loss: 0.22741560637950897\n",
      "iteration 16002: loss: 0.2274145632982254\n",
      "iteration 16003: loss: 0.22741365432739258\n",
      "iteration 16004: loss: 0.22741258144378662\n",
      "iteration 16005: loss: 0.2274114340543747\n",
      "iteration 16006: loss: 0.2274104356765747\n",
      "iteration 16007: loss: 0.22740939259529114\n",
      "iteration 16008: loss: 0.22740836441516876\n",
      "iteration 16009: loss: 0.227407306432724\n",
      "iteration 16010: loss: 0.2274063527584076\n",
      "iteration 16011: loss: 0.22740527987480164\n",
      "iteration 16012: loss: 0.2274041622877121\n",
      "iteration 16013: loss: 0.22740313410758972\n",
      "iteration 16014: loss: 0.22740206122398376\n",
      "iteration 16015: loss: 0.22740104794502258\n",
      "iteration 16016: loss: 0.2274000197649002\n",
      "iteration 16017: loss: 0.22739896178245544\n",
      "iteration 16018: loss: 0.22739800810813904\n",
      "iteration 16019: loss: 0.22739693522453308\n",
      "iteration 16020: loss: 0.22739586234092712\n",
      "iteration 16021: loss: 0.22739481925964355\n",
      "iteration 16022: loss: 0.22739379107952118\n",
      "iteration 16023: loss: 0.2273927927017212\n",
      "iteration 16024: loss: 0.22739176452159882\n",
      "iteration 16025: loss: 0.22739067673683167\n",
      "iteration 16026: loss: 0.2273896187543869\n",
      "iteration 16027: loss: 0.22738857567310333\n",
      "iteration 16028: loss: 0.22738751769065857\n",
      "iteration 16029: loss: 0.2273864448070526\n",
      "iteration 16030: loss: 0.22738540172576904\n",
      "iteration 16031: loss: 0.22738432884216309\n",
      "iteration 16032: loss: 0.22738328576087952\n",
      "iteration 16033: loss: 0.22738225758075714\n",
      "iteration 16034: loss: 0.22738131880760193\n",
      "iteration 16035: loss: 0.22738030552864075\n",
      "iteration 16036: loss: 0.2273792326450348\n",
      "iteration 16037: loss: 0.22737815976142883\n",
      "iteration 16038: loss: 0.22737714648246765\n",
      "iteration 16039: loss: 0.22737610340118408\n",
      "iteration 16040: loss: 0.2273750752210617\n",
      "iteration 16041: loss: 0.22737403213977814\n",
      "iteration 16042: loss: 0.22737297415733337\n",
      "iteration 16043: loss: 0.22737190127372742\n",
      "iteration 16044: loss: 0.22737088799476624\n",
      "iteration 16045: loss: 0.22736985981464386\n",
      "iteration 16046: loss: 0.22736875712871552\n",
      "iteration 16047: loss: 0.22736771404743195\n",
      "iteration 16048: loss: 0.22736665606498718\n",
      "iteration 16049: loss: 0.22736568748950958\n",
      "iteration 16050: loss: 0.22736462950706482\n",
      "iteration 16051: loss: 0.22736354172229767\n",
      "iteration 16052: loss: 0.2273625135421753\n",
      "iteration 16053: loss: 0.22736141085624695\n",
      "iteration 16054: loss: 0.2273605316877365\n",
      "iteration 16055: loss: 0.22735938429832458\n",
      "iteration 16056: loss: 0.2273583710193634\n",
      "iteration 16057: loss: 0.22735735774040222\n",
      "iteration 16058: loss: 0.22735628485679626\n",
      "iteration 16059: loss: 0.2273552417755127\n",
      "iteration 16060: loss: 0.22735421359539032\n",
      "iteration 16061: loss: 0.22735309600830078\n",
      "iteration 16062: loss: 0.227352112531662\n",
      "iteration 16063: loss: 0.2273510992527008\n",
      "iteration 16064: loss: 0.22735004127025604\n",
      "iteration 16065: loss: 0.22734896838665009\n",
      "iteration 16066: loss: 0.22734792530536652\n",
      "iteration 16067: loss: 0.22734689712524414\n",
      "iteration 16068: loss: 0.22734582424163818\n",
      "iteration 16069: loss: 0.22734478116035461\n",
      "iteration 16070: loss: 0.22734379768371582\n",
      "iteration 16071: loss: 0.22734269499778748\n",
      "iteration 16072: loss: 0.2273416519165039\n",
      "iteration 16073: loss: 0.22734065353870392\n",
      "iteration 16074: loss: 0.22733959555625916\n",
      "iteration 16075: loss: 0.22733862698078156\n",
      "iteration 16076: loss: 0.2273375242948532\n",
      "iteration 16077: loss: 0.22733643651008606\n",
      "iteration 16078: loss: 0.22733549773693085\n",
      "iteration 16079: loss: 0.22733449935913086\n",
      "iteration 16080: loss: 0.2273334562778473\n",
      "iteration 16081: loss: 0.22733235359191895\n",
      "iteration 16082: loss: 0.22733132541179657\n",
      "iteration 16083: loss: 0.2273302525281906\n",
      "iteration 16084: loss: 0.22732925415039062\n",
      "iteration 16085: loss: 0.2273281365633011\n",
      "iteration 16086: loss: 0.2273271083831787\n",
      "iteration 16087: loss: 0.22732611000537872\n",
      "iteration 16088: loss: 0.22732508182525635\n",
      "iteration 16089: loss: 0.2273239642381668\n",
      "iteration 16090: loss: 0.22732293605804443\n",
      "iteration 16091: loss: 0.22732190787792206\n",
      "iteration 16092: loss: 0.2273208647966385\n",
      "iteration 16093: loss: 0.22731980681419373\n",
      "iteration 16094: loss: 0.22731876373291016\n",
      "iteration 16095: loss: 0.2273176908493042\n",
      "iteration 16096: loss: 0.22731666266918182\n",
      "iteration 16097: loss: 0.22731563448905945\n",
      "iteration 16098: loss: 0.2273145616054535\n",
      "iteration 16099: loss: 0.22731351852416992\n",
      "iteration 16100: loss: 0.22731252014636993\n",
      "iteration 16101: loss: 0.22731149196624756\n",
      "iteration 16102: loss: 0.227310448884964\n",
      "iteration 16103: loss: 0.22730939090251923\n",
      "iteration 16104: loss: 0.22730839252471924\n",
      "iteration 16105: loss: 0.2273072898387909\n",
      "iteration 16106: loss: 0.2273062914609909\n",
      "iteration 16107: loss: 0.22730520367622375\n",
      "iteration 16108: loss: 0.22730417549610138\n",
      "iteration 16109: loss: 0.22730319201946259\n",
      "iteration 16110: loss: 0.2273021936416626\n",
      "iteration 16111: loss: 0.22730109095573425\n",
      "iteration 16112: loss: 0.22730004787445068\n",
      "iteration 16113: loss: 0.2272990494966507\n",
      "iteration 16114: loss: 0.22729797661304474\n",
      "iteration 16115: loss: 0.22729691863059998\n",
      "iteration 16116: loss: 0.2272959053516388\n",
      "iteration 16117: loss: 0.22729484736919403\n",
      "iteration 16118: loss: 0.22729381918907166\n",
      "iteration 16119: loss: 0.2272927314043045\n",
      "iteration 16120: loss: 0.22729170322418213\n",
      "iteration 16121: loss: 0.22729067504405975\n",
      "iteration 16122: loss: 0.2272895872592926\n",
      "iteration 16123: loss: 0.22728852927684784\n",
      "iteration 16124: loss: 0.22728753089904785\n",
      "iteration 16125: loss: 0.22728650271892548\n",
      "iteration 16126: loss: 0.22728542983531952\n",
      "iteration 16127: loss: 0.22728440165519714\n",
      "iteration 16128: loss: 0.22728338837623596\n",
      "iteration 16129: loss: 0.2272823601961136\n",
      "iteration 16130: loss: 0.22728125751018524\n",
      "iteration 16131: loss: 0.22728025913238525\n",
      "iteration 16132: loss: 0.22727921605110168\n",
      "iteration 16133: loss: 0.2272781878709793\n",
      "iteration 16134: loss: 0.22727712988853455\n",
      "iteration 16135: loss: 0.22727608680725098\n",
      "iteration 16136: loss: 0.22727510333061218\n",
      "iteration 16137: loss: 0.22727401554584503\n",
      "iteration 16138: loss: 0.22727298736572266\n",
      "iteration 16139: loss: 0.22727195918560028\n",
      "iteration 16140: loss: 0.2272709310054779\n",
      "iteration 16141: loss: 0.22726984322071075\n",
      "iteration 16142: loss: 0.22726884484291077\n",
      "iteration 16143: loss: 0.2272678166627884\n",
      "iteration 16144: loss: 0.22726669907569885\n",
      "iteration 16145: loss: 0.2272656410932541\n",
      "iteration 16146: loss: 0.2272646725177765\n",
      "iteration 16147: loss: 0.22726354002952576\n",
      "iteration 16148: loss: 0.22726254165172577\n",
      "iteration 16149: loss: 0.2272614687681198\n",
      "iteration 16150: loss: 0.22726044058799744\n",
      "iteration 16151: loss: 0.22725939750671387\n",
      "iteration 16152: loss: 0.2272583693265915\n",
      "iteration 16153: loss: 0.2272573709487915\n",
      "iteration 16154: loss: 0.22725632786750793\n",
      "iteration 16155: loss: 0.22725525498390198\n",
      "iteration 16156: loss: 0.2272542268037796\n",
      "iteration 16157: loss: 0.22725319862365723\n",
      "iteration 16158: loss: 0.2272520810365677\n",
      "iteration 16159: loss: 0.2272510528564453\n",
      "iteration 16160: loss: 0.22725005447864532\n",
      "iteration 16161: loss: 0.22724899649620056\n",
      "iteration 16162: loss: 0.2272479087114334\n",
      "iteration 16163: loss: 0.22724691033363342\n",
      "iteration 16164: loss: 0.22724595665931702\n",
      "iteration 16165: loss: 0.22724485397338867\n",
      "iteration 16166: loss: 0.2272438257932663\n",
      "iteration 16167: loss: 0.22724279761314392\n",
      "iteration 16168: loss: 0.22724170982837677\n",
      "iteration 16169: loss: 0.22724071145057678\n",
      "iteration 16170: loss: 0.22723965346813202\n",
      "iteration 16171: loss: 0.22723858058452606\n",
      "iteration 16172: loss: 0.2272375524044037\n",
      "iteration 16173: loss: 0.2272365540266037\n",
      "iteration 16174: loss: 0.2272356003522873\n",
      "iteration 16175: loss: 0.22723452746868134\n",
      "iteration 16176: loss: 0.22723349928855896\n",
      "iteration 16177: loss: 0.22723250091075897\n",
      "iteration 16178: loss: 0.22723141312599182\n",
      "iteration 16179: loss: 0.22723038494586945\n",
      "iteration 16180: loss: 0.22722932696342468\n",
      "iteration 16181: loss: 0.22722825407981873\n",
      "iteration 16182: loss: 0.22722721099853516\n",
      "iteration 16183: loss: 0.22722625732421875\n",
      "iteration 16184: loss: 0.22722509503364563\n",
      "iteration 16185: loss: 0.22722411155700684\n",
      "iteration 16186: loss: 0.22722308337688446\n",
      "iteration 16187: loss: 0.22722205519676208\n",
      "iteration 16188: loss: 0.22722098231315613\n",
      "iteration 16189: loss: 0.22721993923187256\n",
      "iteration 16190: loss: 0.22721895575523376\n",
      "iteration 16191: loss: 0.22721782326698303\n",
      "iteration 16192: loss: 0.22721676528453827\n",
      "iteration 16193: loss: 0.2272157222032547\n",
      "iteration 16194: loss: 0.2272147238254547\n",
      "iteration 16195: loss: 0.22721365094184875\n",
      "iteration 16196: loss: 0.22721263766288757\n",
      "iteration 16197: loss: 0.22721156477928162\n",
      "iteration 16198: loss: 0.22721052169799805\n",
      "iteration 16199: loss: 0.22720949351787567\n",
      "iteration 16200: loss: 0.22720840573310852\n",
      "iteration 16201: loss: 0.2272075116634369\n",
      "iteration 16202: loss: 0.22720642387866974\n",
      "iteration 16203: loss: 0.22720535099506378\n",
      "iteration 16204: loss: 0.2272043228149414\n",
      "iteration 16205: loss: 0.22720322012901306\n",
      "iteration 16206: loss: 0.22720222175121307\n",
      "iteration 16207: loss: 0.2272011935710907\n",
      "iteration 16208: loss: 0.22720010578632355\n",
      "iteration 16209: loss: 0.22719912230968475\n",
      "iteration 16210: loss: 0.22719807922840118\n",
      "iteration 16211: loss: 0.22719702124595642\n",
      "iteration 16212: loss: 0.22719597816467285\n",
      "iteration 16213: loss: 0.22719494998455048\n",
      "iteration 16214: loss: 0.22719386219978333\n",
      "iteration 16215: loss: 0.22719287872314453\n",
      "iteration 16216: loss: 0.22719188034534454\n",
      "iteration 16217: loss: 0.2271907776594162\n",
      "iteration 16218: loss: 0.22718974947929382\n",
      "iteration 16219: loss: 0.22718873620033264\n",
      "iteration 16220: loss: 0.2271876335144043\n",
      "iteration 16221: loss: 0.2271866351366043\n",
      "iteration 16222: loss: 0.22718556225299835\n",
      "iteration 16223: loss: 0.2271845042705536\n",
      "iteration 16224: loss: 0.227183535695076\n",
      "iteration 16225: loss: 0.22718246281147003\n",
      "iteration 16226: loss: 0.22718146443367004\n",
      "iteration 16227: loss: 0.2271803915500641\n",
      "iteration 16228: loss: 0.2271793782711029\n",
      "iteration 16229: loss: 0.22717836499214172\n",
      "iteration 16230: loss: 0.22717729210853577\n",
      "iteration 16231: loss: 0.2271762639284134\n",
      "iteration 16232: loss: 0.2271752655506134\n",
      "iteration 16233: loss: 0.22717419266700745\n",
      "iteration 16234: loss: 0.22717316448688507\n",
      "iteration 16235: loss: 0.2271721363067627\n",
      "iteration 16236: loss: 0.22717106342315674\n",
      "iteration 16237: loss: 0.22717003524303436\n",
      "iteration 16238: loss: 0.2271689921617508\n",
      "iteration 16239: loss: 0.22716796398162842\n",
      "iteration 16240: loss: 0.22716693580150604\n",
      "iteration 16241: loss: 0.22716593742370605\n",
      "iteration 16242: loss: 0.2271648347377777\n",
      "iteration 16243: loss: 0.22716383635997772\n",
      "iteration 16244: loss: 0.22716283798217773\n",
      "iteration 16245: loss: 0.2271617203950882\n",
      "iteration 16246: loss: 0.22716069221496582\n",
      "iteration 16247: loss: 0.22715969383716583\n",
      "iteration 16248: loss: 0.2271585762500763\n",
      "iteration 16249: loss: 0.2271576225757599\n",
      "iteration 16250: loss: 0.22715651988983154\n",
      "iteration 16251: loss: 0.22715553641319275\n",
      "iteration 16252: loss: 0.2271544635295868\n",
      "iteration 16253: loss: 0.2271534502506256\n",
      "iteration 16254: loss: 0.22715243697166443\n",
      "iteration 16255: loss: 0.22715139389038086\n",
      "iteration 16256: loss: 0.2271503508090973\n",
      "iteration 16257: loss: 0.2271493375301361\n",
      "iteration 16258: loss: 0.22714829444885254\n",
      "iteration 16259: loss: 0.22714725136756897\n",
      "iteration 16260: loss: 0.227146178483963\n",
      "iteration 16261: loss: 0.22714516520500183\n",
      "iteration 16262: loss: 0.22714407742023468\n",
      "iteration 16263: loss: 0.2271430492401123\n",
      "iteration 16264: loss: 0.22714200615882874\n",
      "iteration 16265: loss: 0.2271409034729004\n",
      "iteration 16266: loss: 0.22713994979858398\n",
      "iteration 16267: loss: 0.22713887691497803\n",
      "iteration 16268: loss: 0.22713780403137207\n",
      "iteration 16269: loss: 0.22713680565357208\n",
      "iteration 16270: loss: 0.22713570296764374\n",
      "iteration 16271: loss: 0.22713470458984375\n",
      "iteration 16272: loss: 0.22713370621204376\n",
      "iteration 16273: loss: 0.22713260352611542\n",
      "iteration 16274: loss: 0.22713163495063782\n",
      "iteration 16275: loss: 0.22713056206703186\n",
      "iteration 16276: loss: 0.22712953388690948\n",
      "iteration 16277: loss: 0.2271285355091095\n",
      "iteration 16278: loss: 0.22712746262550354\n",
      "iteration 16279: loss: 0.22712644934654236\n",
      "iteration 16280: loss: 0.22712545096874237\n",
      "iteration 16281: loss: 0.22712433338165283\n",
      "iteration 16282: loss: 0.22712334990501404\n",
      "iteration 16283: loss: 0.2271222621202469\n",
      "iteration 16284: loss: 0.22712130844593048\n",
      "iteration 16285: loss: 0.22712023556232452\n",
      "iteration 16286: loss: 0.22711920738220215\n",
      "iteration 16287: loss: 0.22711817920207977\n",
      "iteration 16288: loss: 0.2271171361207962\n",
      "iteration 16289: loss: 0.2271161526441574\n",
      "iteration 16290: loss: 0.22711507976055145\n",
      "iteration 16291: loss: 0.22711405158042908\n",
      "iteration 16292: loss: 0.22711297869682312\n",
      "iteration 16293: loss: 0.22711196541786194\n",
      "iteration 16294: loss: 0.22711089253425598\n",
      "iteration 16295: loss: 0.2271098643541336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16296: loss: 0.22710879147052765\n",
      "iteration 16297: loss: 0.22710780799388885\n",
      "iteration 16298: loss: 0.2271067202091217\n",
      "iteration 16299: loss: 0.22710569202899933\n",
      "iteration 16300: loss: 0.22710469365119934\n",
      "iteration 16301: loss: 0.22710354626178741\n",
      "iteration 16302: loss: 0.22710254788398743\n",
      "iteration 16303: loss: 0.22710147500038147\n",
      "iteration 16304: loss: 0.2271004617214203\n",
      "iteration 16305: loss: 0.22709938883781433\n",
      "iteration 16306: loss: 0.22709842026233673\n",
      "iteration 16307: loss: 0.2270973026752472\n",
      "iteration 16308: loss: 0.227096289396286\n",
      "iteration 16309: loss: 0.2270953208208084\n",
      "iteration 16310: loss: 0.22709424793720245\n",
      "iteration 16311: loss: 0.2270931899547577\n",
      "iteration 16312: loss: 0.22709223628044128\n",
      "iteration 16313: loss: 0.2270912230014801\n",
      "iteration 16314: loss: 0.22709020972251892\n",
      "iteration 16315: loss: 0.22708912193775177\n",
      "iteration 16316: loss: 0.22708801925182343\n",
      "iteration 16317: loss: 0.22708694636821747\n",
      "iteration 16318: loss: 0.2270859181880951\n",
      "iteration 16319: loss: 0.2270849198102951\n",
      "iteration 16320: loss: 0.22708384692668915\n",
      "iteration 16321: loss: 0.22708287835121155\n",
      "iteration 16322: loss: 0.2270818054676056\n",
      "iteration 16323: loss: 0.2270807921886444\n",
      "iteration 16324: loss: 0.22707971930503845\n",
      "iteration 16325: loss: 0.22707872092723846\n",
      "iteration 16326: loss: 0.2270776331424713\n",
      "iteration 16327: loss: 0.22707664966583252\n",
      "iteration 16328: loss: 0.22707554697990417\n",
      "iteration 16329: loss: 0.22707457840442657\n",
      "iteration 16330: loss: 0.2270735502243042\n",
      "iteration 16331: loss: 0.22707250714302063\n",
      "iteration 16332: loss: 0.22707143425941467\n",
      "iteration 16333: loss: 0.2270704060792923\n",
      "iteration 16334: loss: 0.2270694077014923\n",
      "iteration 16335: loss: 0.22706830501556396\n",
      "iteration 16336: loss: 0.22706738114356995\n",
      "iteration 16337: loss: 0.2270662784576416\n",
      "iteration 16338: loss: 0.2270652800798416\n",
      "iteration 16339: loss: 0.2270640879869461\n",
      "iteration 16340: loss: 0.22706322371959686\n",
      "iteration 16341: loss: 0.22706203162670135\n",
      "iteration 16342: loss: 0.22706107795238495\n",
      "iteration 16343: loss: 0.227060005068779\n",
      "iteration 16344: loss: 0.22705897688865662\n",
      "iteration 16345: loss: 0.22705793380737305\n",
      "iteration 16346: loss: 0.22705690562725067\n",
      "iteration 16347: loss: 0.2270558625459671\n",
      "iteration 16348: loss: 0.2270548790693283\n",
      "iteration 16349: loss: 0.22705388069152832\n",
      "iteration 16350: loss: 0.22705280780792236\n",
      "iteration 16351: loss: 0.2270517796278\n",
      "iteration 16352: loss: 0.22705073654651642\n",
      "iteration 16353: loss: 0.22704970836639404\n",
      "iteration 16354: loss: 0.22704866528511047\n",
      "iteration 16355: loss: 0.22704759240150452\n",
      "iteration 16356: loss: 0.22704657912254333\n",
      "iteration 16357: loss: 0.22704550623893738\n",
      "iteration 16358: loss: 0.22704441845417023\n",
      "iteration 16359: loss: 0.22704342007637024\n",
      "iteration 16360: loss: 0.22704239189624786\n",
      "iteration 16361: loss: 0.22704128921031952\n",
      "iteration 16362: loss: 0.2270403355360031\n",
      "iteration 16363: loss: 0.22703930735588074\n",
      "iteration 16364: loss: 0.2270382195711136\n",
      "iteration 16365: loss: 0.2270372211933136\n",
      "iteration 16366: loss: 0.22703616321086884\n",
      "iteration 16367: loss: 0.22703516483306885\n",
      "iteration 16368: loss: 0.22703413665294647\n",
      "iteration 16369: loss: 0.2270330935716629\n",
      "iteration 16370: loss: 0.22703203558921814\n",
      "iteration 16371: loss: 0.22703106701374054\n",
      "iteration 16372: loss: 0.22703006863594055\n",
      "iteration 16373: loss: 0.2270289957523346\n",
      "iteration 16374: loss: 0.22702786326408386\n",
      "iteration 16375: loss: 0.22702686488628387\n",
      "iteration 16376: loss: 0.22702579200267792\n",
      "iteration 16377: loss: 0.22702479362487793\n",
      "iteration 16378: loss: 0.22702375054359436\n",
      "iteration 16379: loss: 0.22702273726463318\n",
      "iteration 16380: loss: 0.227021723985672\n",
      "iteration 16381: loss: 0.22702065110206604\n",
      "iteration 16382: loss: 0.22701963782310486\n",
      "iteration 16383: loss: 0.2270185500383377\n",
      "iteration 16384: loss: 0.2270175665616989\n",
      "iteration 16385: loss: 0.22701644897460938\n",
      "iteration 16386: loss: 0.227015420794487\n",
      "iteration 16387: loss: 0.22701437771320343\n",
      "iteration 16388: loss: 0.22701334953308105\n",
      "iteration 16389: loss: 0.22701230645179749\n",
      "iteration 16390: loss: 0.2270112782716751\n",
      "iteration 16391: loss: 0.22701022028923035\n",
      "iteration 16392: loss: 0.22700922191143036\n",
      "iteration 16393: loss: 0.2270081490278244\n",
      "iteration 16394: loss: 0.22700715065002441\n",
      "iteration 16395: loss: 0.22700615227222443\n",
      "iteration 16396: loss: 0.22700512409210205\n",
      "iteration 16397: loss: 0.22700408101081848\n",
      "iteration 16398: loss: 0.2270030677318573\n",
      "iteration 16399: loss: 0.22700195014476776\n",
      "iteration 16400: loss: 0.22700098156929016\n",
      "iteration 16401: loss: 0.2269999086856842\n",
      "iteration 16402: loss: 0.22699889540672302\n",
      "iteration 16403: loss: 0.22699782252311707\n",
      "iteration 16404: loss: 0.22699682414531708\n",
      "iteration 16405: loss: 0.22699575126171112\n",
      "iteration 16406: loss: 0.22699478268623352\n",
      "iteration 16407: loss: 0.22699372470378876\n",
      "iteration 16408: loss: 0.22699269652366638\n",
      "iteration 16409: loss: 0.2269916832447052\n",
      "iteration 16410: loss: 0.22699061036109924\n",
      "iteration 16411: loss: 0.22698959708213806\n",
      "iteration 16412: loss: 0.2269885540008545\n",
      "iteration 16413: loss: 0.2269875556230545\n",
      "iteration 16414: loss: 0.22698648273944855\n",
      "iteration 16415: loss: 0.2269853800535202\n",
      "iteration 16416: loss: 0.22698430716991425\n",
      "iteration 16417: loss: 0.22698333859443665\n",
      "iteration 16418: loss: 0.2269822657108307\n",
      "iteration 16419: loss: 0.2269812524318695\n",
      "iteration 16420: loss: 0.22698020935058594\n",
      "iteration 16421: loss: 0.22697921097278595\n",
      "iteration 16422: loss: 0.2269781529903412\n",
      "iteration 16423: loss: 0.22697713971138\n",
      "iteration 16424: loss: 0.22697608172893524\n",
      "iteration 16425: loss: 0.22697503864765167\n",
      "iteration 16426: loss: 0.22697404026985168\n",
      "iteration 16427: loss: 0.22697289288043976\n",
      "iteration 16428: loss: 0.22697195410728455\n",
      "iteration 16429: loss: 0.22697091102600098\n",
      "iteration 16430: loss: 0.22696992754936218\n",
      "iteration 16431: loss: 0.22696885466575623\n",
      "iteration 16432: loss: 0.22696785628795624\n",
      "iteration 16433: loss: 0.2269667685031891\n",
      "iteration 16434: loss: 0.2269657403230667\n",
      "iteration 16435: loss: 0.22696474194526672\n",
      "iteration 16436: loss: 0.22696366906166077\n",
      "iteration 16437: loss: 0.22696271538734436\n",
      "iteration 16438: loss: 0.22696161270141602\n",
      "iteration 16439: loss: 0.22696061432361603\n",
      "iteration 16440: loss: 0.22695958614349365\n",
      "iteration 16441: loss: 0.2269584685564041\n",
      "iteration 16442: loss: 0.22695747017860413\n",
      "iteration 16443: loss: 0.22695639729499817\n",
      "iteration 16444: loss: 0.2269553691148758\n",
      "iteration 16445: loss: 0.22695434093475342\n",
      "iteration 16446: loss: 0.22695335745811462\n",
      "iteration 16447: loss: 0.22695228457450867\n",
      "iteration 16448: loss: 0.2269512414932251\n",
      "iteration 16449: loss: 0.2269502431154251\n",
      "iteration 16450: loss: 0.22694917023181915\n",
      "iteration 16451: loss: 0.2269480973482132\n",
      "iteration 16452: loss: 0.22694702446460724\n",
      "iteration 16453: loss: 0.22694607079029083\n",
      "iteration 16454: loss: 0.22694501280784607\n",
      "iteration 16455: loss: 0.2269439697265625\n",
      "iteration 16456: loss: 0.2269430160522461\n",
      "iteration 16457: loss: 0.22694194316864014\n",
      "iteration 16458: loss: 0.22694098949432373\n",
      "iteration 16459: loss: 0.22693994641304016\n",
      "iteration 16460: loss: 0.22693893313407898\n",
      "iteration 16461: loss: 0.22693777084350586\n",
      "iteration 16462: loss: 0.2269367277622223\n",
      "iteration 16463: loss: 0.22693569958209991\n",
      "iteration 16464: loss: 0.22693464159965515\n",
      "iteration 16465: loss: 0.22693371772766113\n",
      "iteration 16466: loss: 0.22693267464637756\n",
      "iteration 16467: loss: 0.2269316166639328\n",
      "iteration 16468: loss: 0.2269306182861328\n",
      "iteration 16469: loss: 0.22692951560020447\n",
      "iteration 16470: loss: 0.22692854702472687\n",
      "iteration 16471: loss: 0.22692742943763733\n",
      "iteration 16472: loss: 0.22692637145519257\n",
      "iteration 16473: loss: 0.22692537307739258\n",
      "iteration 16474: loss: 0.226924329996109\n",
      "iteration 16475: loss: 0.22692322731018066\n",
      "iteration 16476: loss: 0.22692227363586426\n",
      "iteration 16477: loss: 0.2269212305545807\n",
      "iteration 16478: loss: 0.22692017257213593\n",
      "iteration 16479: loss: 0.22691920399665833\n",
      "iteration 16480: loss: 0.22691814601421356\n",
      "iteration 16481: loss: 0.2269170731306076\n",
      "iteration 16482: loss: 0.22691610455513\n",
      "iteration 16483: loss: 0.22691504657268524\n",
      "iteration 16484: loss: 0.22691401839256287\n",
      "iteration 16485: loss: 0.2269129455089569\n",
      "iteration 16486: loss: 0.22691193222999573\n",
      "iteration 16487: loss: 0.22691094875335693\n",
      "iteration 16488: loss: 0.22690995037555695\n",
      "iteration 16489: loss: 0.22690889239311218\n",
      "iteration 16490: loss: 0.22690781950950623\n",
      "iteration 16491: loss: 0.22690686583518982\n",
      "iteration 16492: loss: 0.2269057333469391\n",
      "iteration 16493: loss: 0.22690463066101074\n",
      "iteration 16494: loss: 0.22690363228321075\n",
      "iteration 16495: loss: 0.22690260410308838\n",
      "iteration 16496: loss: 0.22690162062644958\n",
      "iteration 16497: loss: 0.22690057754516602\n",
      "iteration 16498: loss: 0.22689954936504364\n",
      "iteration 16499: loss: 0.22689855098724365\n",
      "iteration 16500: loss: 0.22689750790596008\n",
      "iteration 16501: loss: 0.22689643502235413\n",
      "iteration 16502: loss: 0.22689536213874817\n",
      "iteration 16503: loss: 0.2268942892551422\n",
      "iteration 16504: loss: 0.226893350481987\n",
      "iteration 16505: loss: 0.22689227759838104\n",
      "iteration 16506: loss: 0.22689123451709747\n",
      "iteration 16507: loss: 0.22689025104045868\n",
      "iteration 16508: loss: 0.2268892079591751\n",
      "iteration 16509: loss: 0.22688820958137512\n",
      "iteration 16510: loss: 0.2268870770931244\n",
      "iteration 16511: loss: 0.2268860638141632\n",
      "iteration 16512: loss: 0.22688508033752441\n",
      "iteration 16513: loss: 0.22688400745391846\n",
      "iteration 16514: loss: 0.22688297927379608\n",
      "iteration 16515: loss: 0.2268819808959961\n",
      "iteration 16516: loss: 0.22688095271587372\n",
      "iteration 16517: loss: 0.22687992453575134\n",
      "iteration 16518: loss: 0.22687892615795135\n",
      "iteration 16519: loss: 0.2268778532743454\n",
      "iteration 16520: loss: 0.2268768548965454\n",
      "iteration 16521: loss: 0.22687575221061707\n",
      "iteration 16522: loss: 0.2268746793270111\n",
      "iteration 16523: loss: 0.2268737256526947\n",
      "iteration 16524: loss: 0.22687268257141113\n",
      "iteration 16525: loss: 0.2268715798854828\n",
      "iteration 16526: loss: 0.2268706113100052\n",
      "iteration 16527: loss: 0.2268695831298828\n",
      "iteration 16528: loss: 0.22686854004859924\n",
      "iteration 16529: loss: 0.2268674373626709\n",
      "iteration 16530: loss: 0.22686640918254852\n",
      "iteration 16531: loss: 0.22686533629894257\n",
      "iteration 16532: loss: 0.22686436772346497\n",
      "iteration 16533: loss: 0.22686338424682617\n",
      "iteration 16534: loss: 0.22686238586902618\n",
      "iteration 16535: loss: 0.22686132788658142\n",
      "iteration 16536: loss: 0.2268601953983307\n",
      "iteration 16537: loss: 0.2268591672182083\n",
      "iteration 16538: loss: 0.22685818374156952\n",
      "iteration 16539: loss: 0.22685709595680237\n",
      "iteration 16540: loss: 0.22685611248016357\n",
      "iteration 16541: loss: 0.22685512900352478\n",
      "iteration 16542: loss: 0.22685405611991882\n",
      "iteration 16543: loss: 0.22685304284095764\n",
      "iteration 16544: loss: 0.22685208916664124\n",
      "iteration 16545: loss: 0.22685103118419647\n",
      "iteration 16546: loss: 0.22684991359710693\n",
      "iteration 16547: loss: 0.22684888541698456\n",
      "iteration 16548: loss: 0.22684788703918457\n",
      "iteration 16549: loss: 0.226846843957901\n",
      "iteration 16550: loss: 0.22684577107429504\n",
      "iteration 16551: loss: 0.22684478759765625\n",
      "iteration 16552: loss: 0.22684374451637268\n",
      "iteration 16553: loss: 0.22684259712696075\n",
      "iteration 16554: loss: 0.22684161365032196\n",
      "iteration 16555: loss: 0.22684064507484436\n",
      "iteration 16556: loss: 0.2268395870923996\n",
      "iteration 16557: loss: 0.226838618516922\n",
      "iteration 16558: loss: 0.22683756053447723\n",
      "iteration 16559: loss: 0.22683653235435486\n",
      "iteration 16560: loss: 0.22683541476726532\n",
      "iteration 16561: loss: 0.22683441638946533\n",
      "iteration 16562: loss: 0.22683334350585938\n",
      "iteration 16563: loss: 0.22683236002922058\n",
      "iteration 16564: loss: 0.226831316947937\n",
      "iteration 16565: loss: 0.22683027386665344\n",
      "iteration 16566: loss: 0.22682932019233704\n",
      "iteration 16567: loss: 0.22682824730873108\n",
      "iteration 16568: loss: 0.2268272340297699\n",
      "iteration 16569: loss: 0.22682619094848633\n",
      "iteration 16570: loss: 0.22682516276836395\n",
      "iteration 16571: loss: 0.22682411968708038\n",
      "iteration 16572: loss: 0.2268231362104416\n",
      "iteration 16573: loss: 0.22682206332683563\n",
      "iteration 16574: loss: 0.22682102024555206\n",
      "iteration 16575: loss: 0.22682006657123566\n",
      "iteration 16576: loss: 0.2268189936876297\n",
      "iteration 16577: loss: 0.22681789100170135\n",
      "iteration 16578: loss: 0.22681684792041779\n",
      "iteration 16579: loss: 0.2268158495426178\n",
      "iteration 16580: loss: 0.22681482136249542\n",
      "iteration 16581: loss: 0.22681376338005066\n",
      "iteration 16582: loss: 0.22681275010108948\n",
      "iteration 16583: loss: 0.22681179642677307\n",
      "iteration 16584: loss: 0.22681061923503876\n",
      "iteration 16585: loss: 0.22680959105491638\n",
      "iteration 16586: loss: 0.22680862247943878\n",
      "iteration 16587: loss: 0.2268075942993164\n",
      "iteration 16588: loss: 0.22680656611919403\n",
      "iteration 16589: loss: 0.22680553793907166\n",
      "iteration 16590: loss: 0.22680452466011047\n",
      "iteration 16591: loss: 0.2268034964799881\n",
      "iteration 16592: loss: 0.22680237889289856\n",
      "iteration 16593: loss: 0.22680139541625977\n",
      "iteration 16594: loss: 0.2268003225326538\n",
      "iteration 16595: loss: 0.22679933905601501\n",
      "iteration 16596: loss: 0.22679832577705383\n",
      "iteration 16597: loss: 0.22679731249809265\n",
      "iteration 16598: loss: 0.2267962247133255\n",
      "iteration 16599: loss: 0.22679519653320312\n",
      "iteration 16600: loss: 0.22679419815540314\n",
      "iteration 16601: loss: 0.22679316997528076\n",
      "iteration 16602: loss: 0.2267921417951584\n",
      "iteration 16603: loss: 0.22679106891155243\n",
      "iteration 16604: loss: 0.22679011523723602\n",
      "iteration 16605: loss: 0.22678908705711365\n",
      "iteration 16606: loss: 0.22678792476654053\n",
      "iteration 16607: loss: 0.22678688168525696\n",
      "iteration 16608: loss: 0.22678592801094055\n",
      "iteration 16609: loss: 0.2267848551273346\n",
      "iteration 16610: loss: 0.2267839014530182\n",
      "iteration 16611: loss: 0.2267829179763794\n",
      "iteration 16612: loss: 0.22678187489509583\n",
      "iteration 16613: loss: 0.2267806977033615\n",
      "iteration 16614: loss: 0.22677966952323914\n",
      "iteration 16615: loss: 0.22677867114543915\n",
      "iteration 16616: loss: 0.22677764296531677\n",
      "iteration 16617: loss: 0.22677664458751678\n",
      "iteration 16618: loss: 0.22677557170391083\n",
      "iteration 16619: loss: 0.2267746478319168\n",
      "iteration 16620: loss: 0.22677350044250488\n",
      "iteration 16621: loss: 0.2267725020647049\n",
      "iteration 16622: loss: 0.22677159309387207\n",
      "iteration 16623: loss: 0.2267705500125885\n",
      "iteration 16624: loss: 0.22676949203014374\n",
      "iteration 16625: loss: 0.22676844894886017\n",
      "iteration 16626: loss: 0.22676733136177063\n",
      "iteration 16627: loss: 0.22676630318164825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16628: loss: 0.22676527500152588\n",
      "iteration 16629: loss: 0.2267642468214035\n",
      "iteration 16630: loss: 0.22676324844360352\n",
      "iteration 16631: loss: 0.22676225006580353\n",
      "iteration 16632: loss: 0.22676122188568115\n",
      "iteration 16633: loss: 0.2267601191997528\n",
      "iteration 16634: loss: 0.22675910592079163\n",
      "iteration 16635: loss: 0.22675809264183044\n",
      "iteration 16636: loss: 0.22675707936286926\n",
      "iteration 16637: loss: 0.2267560511827469\n",
      "iteration 16638: loss: 0.2267550230026245\n",
      "iteration 16639: loss: 0.22675399482250214\n",
      "iteration 16640: loss: 0.2267528474330902\n",
      "iteration 16641: loss: 0.2267518937587738\n",
      "iteration 16642: loss: 0.22675088047981262\n",
      "iteration 16643: loss: 0.22674985229969025\n",
      "iteration 16644: loss: 0.22674879431724548\n",
      "iteration 16645: loss: 0.22674784064292908\n",
      "iteration 16646: loss: 0.22674670815467834\n",
      "iteration 16647: loss: 0.22674565017223358\n",
      "iteration 16648: loss: 0.2267446517944336\n",
      "iteration 16649: loss: 0.2267436534166336\n",
      "iteration 16650: loss: 0.22674265503883362\n",
      "iteration 16651: loss: 0.22674164175987244\n",
      "iteration 16652: loss: 0.2267405092716217\n",
      "iteration 16653: loss: 0.2267395704984665\n",
      "iteration 16654: loss: 0.22673849761486053\n",
      "iteration 16655: loss: 0.22673752903938293\n",
      "iteration 16656: loss: 0.22673647105693817\n",
      "iteration 16657: loss: 0.22673538327217102\n",
      "iteration 16658: loss: 0.22673435509204865\n",
      "iteration 16659: loss: 0.22673335671424866\n",
      "iteration 16660: loss: 0.2267322838306427\n",
      "iteration 16661: loss: 0.2267313301563263\n",
      "iteration 16662: loss: 0.22673027217388153\n",
      "iteration 16663: loss: 0.22672924399375916\n",
      "iteration 16664: loss: 0.22672824561595917\n",
      "iteration 16665: loss: 0.2267272025346756\n",
      "iteration 16666: loss: 0.22672612965106964\n",
      "iteration 16667: loss: 0.22672510147094727\n",
      "iteration 16668: loss: 0.2267240583896637\n",
      "iteration 16669: loss: 0.22672303020954132\n",
      "iteration 16670: loss: 0.22672200202941895\n",
      "iteration 16671: loss: 0.22672104835510254\n",
      "iteration 16672: loss: 0.226719930768013\n",
      "iteration 16673: loss: 0.22671887278556824\n",
      "iteration 16674: loss: 0.22671785950660706\n",
      "iteration 16675: loss: 0.22671687602996826\n",
      "iteration 16676: loss: 0.22671587765216827\n",
      "iteration 16677: loss: 0.22671477496623993\n",
      "iteration 16678: loss: 0.22671377658843994\n",
      "iteration 16679: loss: 0.22671273350715637\n",
      "iteration 16680: loss: 0.2267117202281952\n",
      "iteration 16681: loss: 0.2267107516527176\n",
      "iteration 16682: loss: 0.22670969367027283\n",
      "iteration 16683: loss: 0.22670868039131165\n",
      "iteration 16684: loss: 0.2267075479030609\n",
      "iteration 16685: loss: 0.2267065793275833\n",
      "iteration 16686: loss: 0.22670559585094452\n",
      "iteration 16687: loss: 0.22670455276966095\n",
      "iteration 16688: loss: 0.2267034947872162\n",
      "iteration 16689: loss: 0.22670242190361023\n",
      "iteration 16690: loss: 0.22670134902000427\n",
      "iteration 16691: loss: 0.22670038044452667\n",
      "iteration 16692: loss: 0.22669938206672668\n",
      "iteration 16693: loss: 0.22669832408428192\n",
      "iteration 16694: loss: 0.22669729590415955\n",
      "iteration 16695: loss: 0.22669629752635956\n",
      "iteration 16696: loss: 0.2266952097415924\n",
      "iteration 16697: loss: 0.226694256067276\n",
      "iteration 16698: loss: 0.22669318318367004\n",
      "iteration 16699: loss: 0.22669216990470886\n",
      "iteration 16700: loss: 0.2266910970211029\n",
      "iteration 16701: loss: 0.2266901284456253\n",
      "iteration 16702: loss: 0.22668905556201935\n",
      "iteration 16703: loss: 0.22668807208538055\n",
      "iteration 16704: loss: 0.22668702900409698\n",
      "iteration 16705: loss: 0.2266860455274582\n",
      "iteration 16706: loss: 0.22668500244617462\n",
      "iteration 16707: loss: 0.22668389976024628\n",
      "iteration 16708: loss: 0.22668294608592987\n",
      "iteration 16709: loss: 0.2266819030046463\n",
      "iteration 16710: loss: 0.22668084502220154\n",
      "iteration 16711: loss: 0.22667984664440155\n",
      "iteration 16712: loss: 0.226678729057312\n",
      "iteration 16713: loss: 0.22667770087718964\n",
      "iteration 16714: loss: 0.2266767919063568\n",
      "iteration 16715: loss: 0.22667571902275085\n",
      "iteration 16716: loss: 0.22667472064495087\n",
      "iteration 16717: loss: 0.2266736924648285\n",
      "iteration 16718: loss: 0.22667260468006134\n",
      "iteration 16719: loss: 0.22667154669761658\n",
      "iteration 16720: loss: 0.2266705483198166\n",
      "iteration 16721: loss: 0.2266695201396942\n",
      "iteration 16722: loss: 0.2266685515642166\n",
      "iteration 16723: loss: 0.22666743397712708\n",
      "iteration 16724: loss: 0.2266664057970047\n",
      "iteration 16725: loss: 0.2266654074192047\n",
      "iteration 16726: loss: 0.22666437923908234\n",
      "iteration 16727: loss: 0.22666338086128235\n",
      "iteration 16728: loss: 0.22666224837303162\n",
      "iteration 16729: loss: 0.22666124999523163\n",
      "iteration 16730: loss: 0.2266603261232376\n",
      "iteration 16731: loss: 0.22665929794311523\n",
      "iteration 16732: loss: 0.2266581803560257\n",
      "iteration 16733: loss: 0.2266571819782257\n",
      "iteration 16734: loss: 0.22665615379810333\n",
      "iteration 16735: loss: 0.22665512561798096\n",
      "iteration 16736: loss: 0.22665412724018097\n",
      "iteration 16737: loss: 0.22665312886238098\n",
      "iteration 16738: loss: 0.2266521155834198\n",
      "iteration 16739: loss: 0.22665107250213623\n",
      "iteration 16740: loss: 0.2266499549150467\n",
      "iteration 16741: loss: 0.2266489565372467\n",
      "iteration 16742: loss: 0.22664792835712433\n",
      "iteration 16743: loss: 0.22664692997932434\n",
      "iteration 16744: loss: 0.22664594650268555\n",
      "iteration 16745: loss: 0.22664479911327362\n",
      "iteration 16746: loss: 0.22664380073547363\n",
      "iteration 16747: loss: 0.22664277255535126\n",
      "iteration 16748: loss: 0.22664180397987366\n",
      "iteration 16749: loss: 0.2266407459974289\n",
      "iteration 16750: loss: 0.2266397476196289\n",
      "iteration 16751: loss: 0.22663860023021698\n",
      "iteration 16752: loss: 0.2266375571489334\n",
      "iteration 16753: loss: 0.22663655877113342\n",
      "iteration 16754: loss: 0.22663560509681702\n",
      "iteration 16755: loss: 0.22663454711437225\n",
      "iteration 16756: loss: 0.2266334742307663\n",
      "iteration 16757: loss: 0.2266325056552887\n",
      "iteration 16758: loss: 0.22663149237632751\n",
      "iteration 16759: loss: 0.2266305387020111\n",
      "iteration 16760: loss: 0.22662949562072754\n",
      "iteration 16761: loss: 0.226628378033638\n",
      "iteration 16762: loss: 0.226627379655838\n",
      "iteration 16763: loss: 0.22662632167339325\n",
      "iteration 16764: loss: 0.22662532329559326\n",
      "iteration 16765: loss: 0.22662420570850372\n",
      "iteration 16766: loss: 0.22662325203418732\n",
      "iteration 16767: loss: 0.22662225365638733\n",
      "iteration 16768: loss: 0.22662119567394257\n",
      "iteration 16769: loss: 0.22662022709846497\n",
      "iteration 16770: loss: 0.22661921381950378\n",
      "iteration 16771: loss: 0.22661805152893066\n",
      "iteration 16772: loss: 0.2266170233488083\n",
      "iteration 16773: loss: 0.2266160547733307\n",
      "iteration 16774: loss: 0.2266150414943695\n",
      "iteration 16775: loss: 0.22661399841308594\n",
      "iteration 16776: loss: 0.226612851023674\n",
      "iteration 16777: loss: 0.2266119420528412\n",
      "iteration 16778: loss: 0.22661085426807404\n",
      "iteration 16779: loss: 0.22660987079143524\n",
      "iteration 16780: loss: 0.22660890221595764\n",
      "iteration 16781: loss: 0.22660787403583527\n",
      "iteration 16782: loss: 0.2266068160533905\n",
      "iteration 16783: loss: 0.22660577297210693\n",
      "iteration 16784: loss: 0.22660474479198456\n",
      "iteration 16785: loss: 0.22660374641418457\n",
      "iteration 16786: loss: 0.22660264372825623\n",
      "iteration 16787: loss: 0.22660167515277863\n",
      "iteration 16788: loss: 0.22660067677497864\n",
      "iteration 16789: loss: 0.22659964859485626\n",
      "iteration 16790: loss: 0.22659869492053986\n",
      "iteration 16791: loss: 0.22659757733345032\n",
      "iteration 16792: loss: 0.22659656405448914\n",
      "iteration 16793: loss: 0.22659549117088318\n",
      "iteration 16794: loss: 0.22659452259540558\n",
      "iteration 16795: loss: 0.2265935242176056\n",
      "iteration 16796: loss: 0.22659242153167725\n",
      "iteration 16797: loss: 0.22659137845039368\n",
      "iteration 16798: loss: 0.22659039497375488\n",
      "iteration 16799: loss: 0.2265893667936325\n",
      "iteration 16800: loss: 0.22658832371234894\n",
      "iteration 16801: loss: 0.2265872210264206\n",
      "iteration 16802: loss: 0.2265862226486206\n",
      "iteration 16803: loss: 0.2265852689743042\n",
      "iteration 16804: loss: 0.2265842705965042\n",
      "iteration 16805: loss: 0.22658315300941467\n",
      "iteration 16806: loss: 0.2265821248292923\n",
      "iteration 16807: loss: 0.2265811264514923\n",
      "iteration 16808: loss: 0.2265801727771759\n",
      "iteration 16809: loss: 0.22657914459705353\n",
      "iteration 16810: loss: 0.22657814621925354\n",
      "iteration 16811: loss: 0.2265769988298416\n",
      "iteration 16812: loss: 0.22657601535320282\n",
      "iteration 16813: loss: 0.22657501697540283\n",
      "iteration 16814: loss: 0.22657398879528046\n",
      "iteration 16815: loss: 0.2265729010105133\n",
      "iteration 16816: loss: 0.22657188773155212\n",
      "iteration 16817: loss: 0.22657088935375214\n",
      "iteration 16818: loss: 0.22656986117362976\n",
      "iteration 16819: loss: 0.22656884789466858\n",
      "iteration 16820: loss: 0.22656777501106262\n",
      "iteration 16821: loss: 0.22656676173210144\n",
      "iteration 16822: loss: 0.22656574845314026\n",
      "iteration 16823: loss: 0.22656464576721191\n",
      "iteration 16824: loss: 0.22656364738941193\n",
      "iteration 16825: loss: 0.22656266391277313\n",
      "iteration 16826: loss: 0.22656163573265076\n",
      "iteration 16827: loss: 0.22656063735485077\n",
      "iteration 16828: loss: 0.2265596091747284\n",
      "iteration 16829: loss: 0.22655852138996124\n",
      "iteration 16830: loss: 0.22655752301216125\n",
      "iteration 16831: loss: 0.22655650973320007\n",
      "iteration 16832: loss: 0.2265554666519165\n",
      "iteration 16833: loss: 0.22655439376831055\n",
      "iteration 16834: loss: 0.22655339539051056\n",
      "iteration 16835: loss: 0.22655241191387177\n",
      "iteration 16836: loss: 0.2265513837337494\n",
      "iteration 16837: loss: 0.2265504151582718\n",
      "iteration 16838: loss: 0.22654923796653748\n",
      "iteration 16839: loss: 0.2265482395887375\n",
      "iteration 16840: loss: 0.2265472412109375\n",
      "iteration 16841: loss: 0.2265462428331375\n",
      "iteration 16842: loss: 0.22654514014720917\n",
      "iteration 16843: loss: 0.22654414176940918\n",
      "iteration 16844: loss: 0.22654318809509277\n",
      "iteration 16845: loss: 0.22654218971729279\n",
      "iteration 16846: loss: 0.2265411913394928\n",
      "iteration 16847: loss: 0.22654001414775848\n",
      "iteration 16848: loss: 0.22653906047344208\n",
      "iteration 16849: loss: 0.2265380620956421\n",
      "iteration 16850: loss: 0.22653701901435852\n",
      "iteration 16851: loss: 0.22653596103191376\n",
      "iteration 16852: loss: 0.22653493285179138\n",
      "iteration 16853: loss: 0.22653397917747498\n",
      "iteration 16854: loss: 0.2265329658985138\n",
      "iteration 16855: loss: 0.22653189301490784\n",
      "iteration 16856: loss: 0.2265307903289795\n",
      "iteration 16857: loss: 0.2265298068523407\n",
      "iteration 16858: loss: 0.2265288084745407\n",
      "iteration 16859: loss: 0.22652773559093475\n",
      "iteration 16860: loss: 0.22652681171894073\n",
      "iteration 16861: loss: 0.22652582824230194\n",
      "iteration 16862: loss: 0.2265247404575348\n",
      "iteration 16863: loss: 0.22652368247509003\n",
      "iteration 16864: loss: 0.22652271389961243\n",
      "iteration 16865: loss: 0.22652170062065125\n",
      "iteration 16866: loss: 0.2265205830335617\n",
      "iteration 16867: loss: 0.22651955485343933\n",
      "iteration 16868: loss: 0.22651855647563934\n",
      "iteration 16869: loss: 0.22651758790016174\n",
      "iteration 16870: loss: 0.226516455411911\n",
      "iteration 16871: loss: 0.2265154868364334\n",
      "iteration 16872: loss: 0.22651448845863342\n",
      "iteration 16873: loss: 0.22651343047618866\n",
      "iteration 16874: loss: 0.22651246190071106\n",
      "iteration 16875: loss: 0.22651132941246033\n",
      "iteration 16876: loss: 0.22651036083698273\n",
      "iteration 16877: loss: 0.22650933265686035\n",
      "iteration 16878: loss: 0.22650833427906036\n",
      "iteration 16879: loss: 0.2265072613954544\n",
      "iteration 16880: loss: 0.22650620341300964\n",
      "iteration 16881: loss: 0.22650524973869324\n",
      "iteration 16882: loss: 0.22650423645973206\n",
      "iteration 16883: loss: 0.2265031337738037\n",
      "iteration 16884: loss: 0.22650213539600372\n",
      "iteration 16885: loss: 0.22650113701820374\n",
      "iteration 16886: loss: 0.22650015354156494\n",
      "iteration 16887: loss: 0.22649900615215302\n",
      "iteration 16888: loss: 0.22649803757667542\n",
      "iteration 16889: loss: 0.22649702429771423\n",
      "iteration 16890: loss: 0.22649602591991425\n",
      "iteration 16891: loss: 0.22649505734443665\n",
      "iteration 16892: loss: 0.2264939248561859\n",
      "iteration 16893: loss: 0.22649292647838593\n",
      "iteration 16894: loss: 0.22649192810058594\n",
      "iteration 16895: loss: 0.22649092972278595\n",
      "iteration 16896: loss: 0.2264898270368576\n",
      "iteration 16897: loss: 0.22648879885673523\n",
      "iteration 16898: loss: 0.22648780047893524\n",
      "iteration 16899: loss: 0.2264866828918457\n",
      "iteration 16900: loss: 0.2264856994152069\n",
      "iteration 16901: loss: 0.2264847755432129\n",
      "iteration 16902: loss: 0.22648374736309052\n",
      "iteration 16903: loss: 0.22648262977600098\n",
      "iteration 16904: loss: 0.226481631398201\n",
      "iteration 16905: loss: 0.2264806479215622\n",
      "iteration 16906: loss: 0.22647961974143982\n",
      "iteration 16907: loss: 0.22647854685783386\n",
      "iteration 16908: loss: 0.22647753357887268\n",
      "iteration 16909: loss: 0.22647659480571747\n",
      "iteration 16910: loss: 0.22647544741630554\n",
      "iteration 16911: loss: 0.22647452354431152\n",
      "iteration 16912: loss: 0.22647352516651154\n",
      "iteration 16913: loss: 0.22647257149219513\n",
      "iteration 16914: loss: 0.22647139430046082\n",
      "iteration 16915: loss: 0.22647042572498322\n",
      "iteration 16916: loss: 0.22646942734718323\n",
      "iteration 16917: loss: 0.2264682948589325\n",
      "iteration 16918: loss: 0.22646737098693848\n",
      "iteration 16919: loss: 0.22646622359752655\n",
      "iteration 16920: loss: 0.22646526992321014\n",
      "iteration 16921: loss: 0.22646427154541016\n",
      "iteration 16922: loss: 0.22646324336528778\n",
      "iteration 16923: loss: 0.22646215558052063\n",
      "iteration 16924: loss: 0.22646117210388184\n",
      "iteration 16925: loss: 0.22646017372608185\n",
      "iteration 16926: loss: 0.22645917534828186\n",
      "iteration 16927: loss: 0.22645816206932068\n",
      "iteration 16928: loss: 0.22645707428455353\n",
      "iteration 16929: loss: 0.22645604610443115\n",
      "iteration 16930: loss: 0.22645506262779236\n",
      "iteration 16931: loss: 0.22645406424999237\n",
      "iteration 16932: loss: 0.22645290195941925\n",
      "iteration 16933: loss: 0.22645191848278046\n",
      "iteration 16934: loss: 0.22645089030265808\n",
      "iteration 16935: loss: 0.2264498770236969\n",
      "iteration 16936: loss: 0.2264488935470581\n",
      "iteration 16937: loss: 0.22644786536693573\n",
      "iteration 16938: loss: 0.22644683718681335\n",
      "iteration 16939: loss: 0.22644582390785217\n",
      "iteration 16940: loss: 0.2264447957277298\n",
      "iteration 16941: loss: 0.2264437973499298\n",
      "iteration 16942: loss: 0.22644278407096863\n",
      "iteration 16943: loss: 0.22644181549549103\n",
      "iteration 16944: loss: 0.2264406979084015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16945: loss: 0.2264397144317627\n",
      "iteration 16946: loss: 0.22643867135047913\n",
      "iteration 16947: loss: 0.22643768787384033\n",
      "iteration 16948: loss: 0.2264365702867508\n",
      "iteration 16949: loss: 0.22643551230430603\n",
      "iteration 16950: loss: 0.22643455862998962\n",
      "iteration 16951: loss: 0.22643356025218964\n",
      "iteration 16952: loss: 0.22643251717090607\n",
      "iteration 16953: loss: 0.2264314591884613\n",
      "iteration 16954: loss: 0.2264304906129837\n",
      "iteration 16955: loss: 0.2264295071363449\n",
      "iteration 16956: loss: 0.22642838954925537\n",
      "iteration 16957: loss: 0.22642740607261658\n",
      "iteration 16958: loss: 0.2264264076948166\n",
      "iteration 16959: loss: 0.22642526030540466\n",
      "iteration 16960: loss: 0.22642426192760468\n",
      "iteration 16961: loss: 0.22642329335212708\n",
      "iteration 16962: loss: 0.22642222046852112\n",
      "iteration 16963: loss: 0.2264212667942047\n",
      "iteration 16964: loss: 0.22642028331756592\n",
      "iteration 16965: loss: 0.22641924023628235\n",
      "iteration 16966: loss: 0.22641825675964355\n",
      "iteration 16967: loss: 0.22641721367835999\n",
      "iteration 16968: loss: 0.2264162003993988\n",
      "iteration 16969: loss: 0.22641515731811523\n",
      "iteration 16970: loss: 0.2264140546321869\n",
      "iteration 16971: loss: 0.22641310095787048\n",
      "iteration 16972: loss: 0.2264120876789093\n",
      "iteration 16973: loss: 0.2264111042022705\n",
      "iteration 16974: loss: 0.22640998661518097\n",
      "iteration 16975: loss: 0.22640898823738098\n",
      "iteration 16976: loss: 0.22640803456306458\n",
      "iteration 16977: loss: 0.2264070212841034\n",
      "iteration 16978: loss: 0.22640590369701385\n",
      "iteration 16979: loss: 0.22640490531921387\n",
      "iteration 16980: loss: 0.22640390694141388\n",
      "iteration 16981: loss: 0.22640283405780792\n",
      "iteration 16982: loss: 0.22640180587768555\n",
      "iteration 16983: loss: 0.22640082240104675\n",
      "iteration 16984: loss: 0.22639982402324677\n",
      "iteration 16985: loss: 0.22639870643615723\n",
      "iteration 16986: loss: 0.22639767825603485\n",
      "iteration 16987: loss: 0.22639675438404083\n",
      "iteration 16988: loss: 0.22639569640159607\n",
      "iteration 16989: loss: 0.2263946831226349\n",
      "iteration 16990: loss: 0.22639372944831848\n",
      "iteration 16991: loss: 0.22639265656471252\n",
      "iteration 16992: loss: 0.22639170289039612\n",
      "iteration 16993: loss: 0.2263905555009842\n",
      "iteration 16994: loss: 0.22638960182666779\n",
      "iteration 16995: loss: 0.2263885736465454\n",
      "iteration 16996: loss: 0.22638759016990662\n",
      "iteration 16997: loss: 0.22638654708862305\n",
      "iteration 16998: loss: 0.22638550400733948\n",
      "iteration 16999: loss: 0.2263844907283783\n",
      "iteration 17000: loss: 0.22638340294361115\n",
      "iteration 17001: loss: 0.22638241946697235\n",
      "iteration 17002: loss: 0.22638139128684998\n",
      "iteration 17003: loss: 0.22638039290905\n",
      "iteration 17004: loss: 0.22637930512428284\n",
      "iteration 17005: loss: 0.22637835144996643\n",
      "iteration 17006: loss: 0.22637736797332764\n",
      "iteration 17007: loss: 0.2263762652873993\n",
      "iteration 17008: loss: 0.2263752520084381\n",
      "iteration 17009: loss: 0.22637423872947693\n",
      "iteration 17010: loss: 0.22637322545051575\n",
      "iteration 17011: loss: 0.22637224197387695\n",
      "iteration 17012: loss: 0.22637121379375458\n",
      "iteration 17013: loss: 0.22637024521827698\n",
      "iteration 17014: loss: 0.22636914253234863\n",
      "iteration 17015: loss: 0.22636806964874268\n",
      "iteration 17016: loss: 0.22636711597442627\n",
      "iteration 17017: loss: 0.22636613249778748\n",
      "iteration 17018: loss: 0.22636505961418152\n",
      "iteration 17019: loss: 0.22636409103870392\n",
      "iteration 17020: loss: 0.22636309266090393\n",
      "iteration 17021: loss: 0.22636201977729797\n",
      "iteration 17022: loss: 0.22636091709136963\n",
      "iteration 17023: loss: 0.22635993361473083\n",
      "iteration 17024: loss: 0.22635893523693085\n",
      "iteration 17025: loss: 0.22635796666145325\n",
      "iteration 17026: loss: 0.2263568937778473\n",
      "iteration 17027: loss: 0.2263559103012085\n",
      "iteration 17028: loss: 0.22635476291179657\n",
      "iteration 17029: loss: 0.22635383903980255\n",
      "iteration 17030: loss: 0.22635284066200256\n",
      "iteration 17031: loss: 0.22635185718536377\n",
      "iteration 17032: loss: 0.22635073959827423\n",
      "iteration 17033: loss: 0.22634968161582947\n",
      "iteration 17034: loss: 0.22634868323802948\n",
      "iteration 17035: loss: 0.22634761035442352\n",
      "iteration 17036: loss: 0.2263467013835907\n",
      "iteration 17037: loss: 0.2263457030057907\n",
      "iteration 17038: loss: 0.2263447344303131\n",
      "iteration 17039: loss: 0.22634366154670715\n",
      "iteration 17040: loss: 0.2263426035642624\n",
      "iteration 17041: loss: 0.2263416349887848\n",
      "iteration 17042: loss: 0.2263406217098236\n",
      "iteration 17043: loss: 0.22633953392505646\n",
      "iteration 17044: loss: 0.22633855044841766\n",
      "iteration 17045: loss: 0.2263375073671341\n",
      "iteration 17046: loss: 0.22633647918701172\n",
      "iteration 17047: loss: 0.22633548080921173\n",
      "iteration 17048: loss: 0.22633448243141174\n",
      "iteration 17049: loss: 0.22633342444896698\n",
      "iteration 17050: loss: 0.2263323813676834\n",
      "iteration 17051: loss: 0.22633138298988342\n",
      "iteration 17052: loss: 0.22633036971092224\n",
      "iteration 17053: loss: 0.22632932662963867\n",
      "iteration 17054: loss: 0.2263282984495163\n",
      "iteration 17055: loss: 0.22632727026939392\n",
      "iteration 17056: loss: 0.22632630169391632\n",
      "iteration 17057: loss: 0.22632519900798798\n",
      "iteration 17058: loss: 0.22632423043251038\n",
      "iteration 17059: loss: 0.226323202252388\n",
      "iteration 17060: loss: 0.22632209956645966\n",
      "iteration 17061: loss: 0.22632119059562683\n",
      "iteration 17062: loss: 0.22632019221782684\n",
      "iteration 17063: loss: 0.22631919384002686\n",
      "iteration 17064: loss: 0.2263181507587433\n",
      "iteration 17065: loss: 0.2263171374797821\n",
      "iteration 17066: loss: 0.2263161689043045\n",
      "iteration 17067: loss: 0.22631505131721497\n",
      "iteration 17068: loss: 0.22631406784057617\n",
      "iteration 17069: loss: 0.2263130396604538\n",
      "iteration 17070: loss: 0.22631196677684784\n",
      "iteration 17071: loss: 0.22631099820137024\n",
      "iteration 17072: loss: 0.22630998492240906\n",
      "iteration 17073: loss: 0.2263089120388031\n",
      "iteration 17074: loss: 0.2263079136610031\n",
      "iteration 17075: loss: 0.2263069599866867\n",
      "iteration 17076: loss: 0.22630593180656433\n",
      "iteration 17077: loss: 0.2263048142194748\n",
      "iteration 17078: loss: 0.2263038158416748\n",
      "iteration 17079: loss: 0.2263028621673584\n",
      "iteration 17080: loss: 0.22630178928375244\n",
      "iteration 17081: loss: 0.22630074620246887\n",
      "iteration 17082: loss: 0.22629976272583008\n",
      "iteration 17083: loss: 0.22629880905151367\n",
      "iteration 17084: loss: 0.22629769146442413\n",
      "iteration 17085: loss: 0.2262967824935913\n",
      "iteration 17086: loss: 0.22629575431346893\n",
      "iteration 17087: loss: 0.22629466652870178\n",
      "iteration 17088: loss: 0.226293683052063\n",
      "iteration 17089: loss: 0.226292684674263\n",
      "iteration 17090: loss: 0.22629162669181824\n",
      "iteration 17091: loss: 0.22629062831401825\n",
      "iteration 17092: loss: 0.22628962993621826\n",
      "iteration 17093: loss: 0.22628851234912872\n",
      "iteration 17094: loss: 0.22628755867481232\n",
      "iteration 17095: loss: 0.22628657519817352\n",
      "iteration 17096: loss: 0.2262854278087616\n",
      "iteration 17097: loss: 0.22628450393676758\n",
      "iteration 17098: loss: 0.226283460855484\n",
      "iteration 17099: loss: 0.22628244757652283\n",
      "iteration 17100: loss: 0.22628140449523926\n",
      "iteration 17101: loss: 0.22628040611743927\n",
      "iteration 17102: loss: 0.22627945244312286\n",
      "iteration 17103: loss: 0.2262783795595169\n",
      "iteration 17104: loss: 0.22627730667591095\n",
      "iteration 17105: loss: 0.22627635300159454\n",
      "iteration 17106: loss: 0.22627535462379456\n",
      "iteration 17107: loss: 0.2262742817401886\n",
      "iteration 17108: loss: 0.22627325356006622\n",
      "iteration 17109: loss: 0.22627227008342743\n",
      "iteration 17110: loss: 0.22627122700214386\n",
      "iteration 17111: loss: 0.22627022862434387\n",
      "iteration 17112: loss: 0.22626924514770508\n",
      "iteration 17113: loss: 0.22626817226409912\n",
      "iteration 17114: loss: 0.22626718878746033\n",
      "iteration 17115: loss: 0.22626617550849915\n",
      "iteration 17116: loss: 0.22626519203186035\n",
      "iteration 17117: loss: 0.2262641191482544\n",
      "iteration 17118: loss: 0.2262631356716156\n",
      "iteration 17119: loss: 0.226262167096138\n",
      "iteration 17120: loss: 0.2262609899044037\n",
      "iteration 17121: loss: 0.22626009583473206\n",
      "iteration 17122: loss: 0.22625908255577087\n",
      "iteration 17123: loss: 0.22625795006752014\n",
      "iteration 17124: loss: 0.22625699639320374\n",
      "iteration 17125: loss: 0.22625592350959778\n",
      "iteration 17126: loss: 0.22625494003295898\n",
      "iteration 17127: loss: 0.2262539118528366\n",
      "iteration 17128: loss: 0.226252943277359\n",
      "iteration 17129: loss: 0.22625184059143066\n",
      "iteration 17130: loss: 0.22625085711479187\n",
      "iteration 17131: loss: 0.2262498438358307\n",
      "iteration 17132: loss: 0.2262488603591919\n",
      "iteration 17133: loss: 0.22624783217906952\n",
      "iteration 17134: loss: 0.22624686360359192\n",
      "iteration 17135: loss: 0.22624583542346954\n",
      "iteration 17136: loss: 0.22624477744102478\n",
      "iteration 17137: loss: 0.2262437790632248\n",
      "iteration 17138: loss: 0.22624275088310242\n",
      "iteration 17139: loss: 0.22624167799949646\n",
      "iteration 17140: loss: 0.22624072432518005\n",
      "iteration 17141: loss: 0.22623972594738007\n",
      "iteration 17142: loss: 0.2262386530637741\n",
      "iteration 17143: loss: 0.2262376844882965\n",
      "iteration 17144: loss: 0.2262367308139801\n",
      "iteration 17145: loss: 0.22623558342456818\n",
      "iteration 17146: loss: 0.22623459994792938\n",
      "iteration 17147: loss: 0.22623363137245178\n",
      "iteration 17148: loss: 0.22623252868652344\n",
      "iteration 17149: loss: 0.22623157501220703\n",
      "iteration 17150: loss: 0.22623054683208466\n",
      "iteration 17151: loss: 0.2262294739484787\n",
      "iteration 17152: loss: 0.22622844576835632\n",
      "iteration 17153: loss: 0.22622749209403992\n",
      "iteration 17154: loss: 0.22622647881507874\n",
      "iteration 17155: loss: 0.22622542083263397\n",
      "iteration 17156: loss: 0.2262243777513504\n",
      "iteration 17157: loss: 0.226223424077034\n",
      "iteration 17158: loss: 0.2262224406003952\n",
      "iteration 17159: loss: 0.2262214720249176\n",
      "iteration 17160: loss: 0.22622033953666687\n",
      "iteration 17161: loss: 0.22621937096118927\n",
      "iteration 17162: loss: 0.22621837258338928\n",
      "iteration 17163: loss: 0.2262173891067505\n",
      "iteration 17164: loss: 0.22621627151966095\n",
      "iteration 17165: loss: 0.22621527314186096\n",
      "iteration 17166: loss: 0.22621428966522217\n",
      "iteration 17167: loss: 0.2262132465839386\n",
      "iteration 17168: loss: 0.2262122631072998\n",
      "iteration 17169: loss: 0.2262113094329834\n",
      "iteration 17170: loss: 0.22621014714241028\n",
      "iteration 17171: loss: 0.22620916366577148\n",
      "iteration 17172: loss: 0.2262081652879715\n",
      "iteration 17173: loss: 0.2262071669101715\n",
      "iteration 17174: loss: 0.22620610892772675\n",
      "iteration 17175: loss: 0.2262050360441208\n",
      "iteration 17176: loss: 0.22620411217212677\n",
      "iteration 17177: loss: 0.2262030839920044\n",
      "iteration 17178: loss: 0.22620201110839844\n",
      "iteration 17179: loss: 0.22620105743408203\n",
      "iteration 17180: loss: 0.22620005905628204\n",
      "iteration 17181: loss: 0.22619906067848206\n",
      "iteration 17182: loss: 0.2261979579925537\n",
      "iteration 17183: loss: 0.2261970043182373\n",
      "iteration 17184: loss: 0.22619593143463135\n",
      "iteration 17185: loss: 0.22619494795799255\n",
      "iteration 17186: loss: 0.22619394958019257\n",
      "iteration 17187: loss: 0.226192906498909\n",
      "iteration 17188: loss: 0.2261919230222702\n",
      "iteration 17189: loss: 0.22619089484214783\n",
      "iteration 17190: loss: 0.22618985176086426\n",
      "iteration 17191: loss: 0.22618889808654785\n",
      "iteration 17192: loss: 0.22618786990642548\n",
      "iteration 17193: loss: 0.22618675231933594\n",
      "iteration 17194: loss: 0.22618576884269714\n",
      "iteration 17195: loss: 0.22618477046489716\n",
      "iteration 17196: loss: 0.22618380188941956\n",
      "iteration 17197: loss: 0.2261827439069748\n",
      "iteration 17198: loss: 0.2261817455291748\n",
      "iteration 17199: loss: 0.22618074715137482\n",
      "iteration 17200: loss: 0.22617964446544647\n",
      "iteration 17201: loss: 0.22617869079113007\n",
      "iteration 17202: loss: 0.22617772221565247\n",
      "iteration 17203: loss: 0.22617661952972412\n",
      "iteration 17204: loss: 0.22617562115192413\n",
      "iteration 17205: loss: 0.22617468237876892\n",
      "iteration 17206: loss: 0.22617359459400177\n",
      "iteration 17207: loss: 0.22617264091968536\n",
      "iteration 17208: loss: 0.22617164254188538\n",
      "iteration 17209: loss: 0.22617053985595703\n",
      "iteration 17210: loss: 0.22616955637931824\n",
      "iteration 17211: loss: 0.22616860270500183\n",
      "iteration 17212: loss: 0.22616751492023468\n",
      "iteration 17213: loss: 0.2261665314435959\n",
      "iteration 17214: loss: 0.22616545855998993\n",
      "iteration 17215: loss: 0.22616446018218994\n",
      "iteration 17216: loss: 0.22616346180438995\n",
      "iteration 17217: loss: 0.22616243362426758\n",
      "iteration 17218: loss: 0.2261614054441452\n",
      "iteration 17219: loss: 0.2261604517698288\n",
      "iteration 17220: loss: 0.22615933418273926\n",
      "iteration 17221: loss: 0.22615838050842285\n",
      "iteration 17222: loss: 0.22615738213062286\n",
      "iteration 17223: loss: 0.2261563092470169\n",
      "iteration 17224: loss: 0.22615528106689453\n",
      "iteration 17225: loss: 0.22615428268909454\n",
      "iteration 17226: loss: 0.22615325450897217\n",
      "iteration 17227: loss: 0.22615227103233337\n",
      "iteration 17228: loss: 0.2261512577533722\n",
      "iteration 17229: loss: 0.226150244474411\n",
      "iteration 17230: loss: 0.2261492908000946\n",
      "iteration 17231: loss: 0.22614827752113342\n",
      "iteration 17232: loss: 0.22614724934101105\n",
      "iteration 17233: loss: 0.22614625096321106\n",
      "iteration 17234: loss: 0.2261451780796051\n",
      "iteration 17235: loss: 0.22614412009716034\n",
      "iteration 17236: loss: 0.22614316642284393\n",
      "iteration 17237: loss: 0.22614212334156036\n",
      "iteration 17238: loss: 0.22614113986492157\n",
      "iteration 17239: loss: 0.22614017128944397\n",
      "iteration 17240: loss: 0.226139098405838\n",
      "iteration 17241: loss: 0.22613811492919922\n",
      "iteration 17242: loss: 0.22613713145256042\n",
      "iteration 17243: loss: 0.2261359691619873\n",
      "iteration 17244: loss: 0.2261350452899933\n",
      "iteration 17245: loss: 0.2261340320110321\n",
      "iteration 17246: loss: 0.22613298892974854\n",
      "iteration 17247: loss: 0.22613206505775452\n",
      "iteration 17248: loss: 0.2261309176683426\n",
      "iteration 17249: loss: 0.22612997889518738\n",
      "iteration 17250: loss: 0.2261289358139038\n",
      "iteration 17251: loss: 0.22612790763378143\n",
      "iteration 17252: loss: 0.22612686455249786\n",
      "iteration 17253: loss: 0.22612595558166504\n",
      "iteration 17254: loss: 0.22612491250038147\n",
      "iteration 17255: loss: 0.2261238843202591\n",
      "iteration 17256: loss: 0.22612294554710388\n",
      "iteration 17257: loss: 0.22612187266349792\n",
      "iteration 17258: loss: 0.22612087428569794\n",
      "iteration 17259: loss: 0.22611984610557556\n",
      "iteration 17260: loss: 0.226118803024292\n",
      "iteration 17261: loss: 0.22611784934997559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 17262: loss: 0.22611673176288605\n",
      "iteration 17263: loss: 0.22611574828624725\n",
      "iteration 17264: loss: 0.22611479461193085\n",
      "iteration 17265: loss: 0.2261137068271637\n",
      "iteration 17266: loss: 0.2261127233505249\n",
      "iteration 17267: loss: 0.22611172497272491\n",
      "iteration 17268: loss: 0.22611065208911896\n",
      "iteration 17269: loss: 0.22610969841480255\n",
      "iteration 17270: loss: 0.2261086404323578\n",
      "iteration 17271: loss: 0.2261076420545578\n",
      "iteration 17272: loss: 0.22610659897327423\n",
      "iteration 17273: loss: 0.22610564529895782\n",
      "iteration 17274: loss: 0.22610458731651306\n",
      "iteration 17275: loss: 0.22610358893871307\n",
      "iteration 17276: loss: 0.22610263526439667\n",
      "iteration 17277: loss: 0.2261015921831131\n",
      "iteration 17278: loss: 0.22610065340995789\n",
      "iteration 17279: loss: 0.22609956562519073\n",
      "iteration 17280: loss: 0.22609856724739075\n",
      "iteration 17281: loss: 0.22609750926494598\n",
      "iteration 17282: loss: 0.22609658539295197\n",
      "iteration 17283: loss: 0.22609546780586243\n",
      "iteration 17284: loss: 0.22609452903270721\n",
      "iteration 17285: loss: 0.22609344124794006\n",
      "iteration 17286: loss: 0.22609248757362366\n",
      "iteration 17287: loss: 0.22609153389930725\n",
      "iteration 17288: loss: 0.22609038650989532\n",
      "iteration 17289: loss: 0.22608943283557892\n",
      "iteration 17290: loss: 0.22608835995197296\n",
      "iteration 17291: loss: 0.22608740627765656\n",
      "iteration 17292: loss: 0.22608645260334015\n",
      "iteration 17293: loss: 0.2260853350162506\n",
      "iteration 17294: loss: 0.2260843962430954\n",
      "iteration 17295: loss: 0.22608336806297302\n",
      "iteration 17296: loss: 0.22608232498168945\n",
      "iteration 17297: loss: 0.22608128190040588\n",
      "iteration 17298: loss: 0.2260802537202835\n",
      "iteration 17299: loss: 0.22607925534248352\n",
      "iteration 17300: loss: 0.2260783612728119\n",
      "iteration 17301: loss: 0.22607728838920593\n",
      "iteration 17302: loss: 0.22607627511024475\n",
      "iteration 17303: loss: 0.22607532143592834\n",
      "iteration 17304: loss: 0.22607421875\n",
      "iteration 17305: loss: 0.2260732650756836\n",
      "iteration 17306: loss: 0.2260722666978836\n",
      "iteration 17307: loss: 0.22607116401195526\n",
      "iteration 17308: loss: 0.22607016563415527\n",
      "iteration 17309: loss: 0.22606924176216125\n",
      "iteration 17310: loss: 0.22606810927391052\n",
      "iteration 17311: loss: 0.22606715559959412\n",
      "iteration 17312: loss: 0.22606614232063293\n",
      "iteration 17313: loss: 0.22606511414051056\n",
      "iteration 17314: loss: 0.22606408596038818\n",
      "iteration 17315: loss: 0.2260631024837494\n",
      "iteration 17316: loss: 0.22606205940246582\n",
      "iteration 17317: loss: 0.22606110572814941\n",
      "iteration 17318: loss: 0.22606010735034943\n",
      "iteration 17319: loss: 0.22605904936790466\n",
      "iteration 17320: loss: 0.2260580062866211\n",
      "iteration 17321: loss: 0.2260570079088211\n",
      "iteration 17322: loss: 0.22605600953102112\n",
      "iteration 17323: loss: 0.22605495154857635\n",
      "iteration 17324: loss: 0.22605399787425995\n",
      "iteration 17325: loss: 0.22605299949645996\n",
      "iteration 17326: loss: 0.22605197131633759\n",
      "iteration 17327: loss: 0.2260509431362152\n",
      "iteration 17328: loss: 0.22604994475841522\n",
      "iteration 17329: loss: 0.22604894638061523\n",
      "iteration 17330: loss: 0.22604794800281525\n",
      "iteration 17331: loss: 0.2260468751192093\n",
      "iteration 17332: loss: 0.22604593634605408\n",
      "iteration 17333: loss: 0.22604480385780334\n",
      "iteration 17334: loss: 0.22604389488697052\n",
      "iteration 17335: loss: 0.22604291141033173\n",
      "iteration 17336: loss: 0.22604186832904816\n",
      "iteration 17337: loss: 0.2260408103466034\n",
      "iteration 17338: loss: 0.2260398417711258\n",
      "iteration 17339: loss: 0.22603876888751984\n",
      "iteration 17340: loss: 0.22603778541088104\n",
      "iteration 17341: loss: 0.22603681683540344\n",
      "iteration 17342: loss: 0.22603578865528107\n",
      "iteration 17343: loss: 0.22603479027748108\n",
      "iteration 17344: loss: 0.2260337620973587\n",
      "iteration 17345: loss: 0.22603270411491394\n",
      "iteration 17346: loss: 0.2260318547487259\n",
      "iteration 17347: loss: 0.22603075206279755\n",
      "iteration 17348: loss: 0.22602975368499756\n",
      "iteration 17349: loss: 0.2260286509990692\n",
      "iteration 17350: loss: 0.2260276824235916\n",
      "iteration 17351: loss: 0.22602662444114685\n",
      "iteration 17352: loss: 0.22602565586566925\n",
      "iteration 17353: loss: 0.22602471709251404\n",
      "iteration 17354: loss: 0.2260235846042633\n",
      "iteration 17355: loss: 0.2260226458311081\n",
      "iteration 17356: loss: 0.2260216474533081\n",
      "iteration 17357: loss: 0.22602061927318573\n",
      "iteration 17358: loss: 0.22601962089538574\n",
      "iteration 17359: loss: 0.22601857781410217\n",
      "iteration 17360: loss: 0.226017564535141\n",
      "iteration 17361: loss: 0.2260165512561798\n",
      "iteration 17362: loss: 0.22601553797721863\n",
      "iteration 17363: loss: 0.22601458430290222\n",
      "iteration 17364: loss: 0.22601351141929626\n",
      "iteration 17365: loss: 0.22601251304149628\n",
      "iteration 17366: loss: 0.22601154446601868\n",
      "iteration 17367: loss: 0.2260105162858963\n",
      "iteration 17368: loss: 0.22600948810577393\n",
      "iteration 17369: loss: 0.22600850462913513\n",
      "iteration 17370: loss: 0.22600753605365753\n",
      "iteration 17371: loss: 0.22600646317005157\n",
      "iteration 17372: loss: 0.2260054051876068\n",
      "iteration 17373: loss: 0.2260044515132904\n",
      "iteration 17374: loss: 0.226003497838974\n",
      "iteration 17375: loss: 0.22600241005420685\n",
      "iteration 17376: loss: 0.22600145637989044\n",
      "iteration 17377: loss: 0.22600045800209045\n",
      "iteration 17378: loss: 0.2259993553161621\n",
      "iteration 17379: loss: 0.2259984016418457\n",
      "iteration 17380: loss: 0.22599740326404572\n",
      "iteration 17381: loss: 0.22599637508392334\n",
      "iteration 17382: loss: 0.22599534690380096\n",
      "iteration 17383: loss: 0.2259943187236786\n",
      "iteration 17384: loss: 0.2259933054447174\n",
      "iteration 17385: loss: 0.22599227726459503\n",
      "iteration 17386: loss: 0.22599129378795624\n",
      "iteration 17387: loss: 0.22599036991596222\n",
      "iteration 17388: loss: 0.22598929703235626\n",
      "iteration 17389: loss: 0.22598831355571747\n",
      "iteration 17390: loss: 0.22598731517791748\n",
      "iteration 17391: loss: 0.22598619759082794\n",
      "iteration 17392: loss: 0.2259853333234787\n",
      "iteration 17393: loss: 0.22598424553871155\n",
      "iteration 17394: loss: 0.22598329186439514\n",
      "iteration 17395: loss: 0.2259821891784668\n",
      "iteration 17396: loss: 0.22598126530647278\n",
      "iteration 17397: loss: 0.22598019242286682\n",
      "iteration 17398: loss: 0.22597913444042206\n",
      "iteration 17399: loss: 0.22597821056842804\n",
      "iteration 17400: loss: 0.22597713768482208\n",
      "iteration 17401: loss: 0.22597618401050568\n",
      "iteration 17402: loss: 0.2259751856327057\n",
      "iteration 17403: loss: 0.2259741723537445\n",
      "iteration 17404: loss: 0.2259732037782669\n",
      "iteration 17405: loss: 0.22597208619117737\n",
      "iteration 17406: loss: 0.22597114741802216\n",
      "iteration 17407: loss: 0.22597014904022217\n",
      "iteration 17408: loss: 0.2259691059589386\n",
      "iteration 17409: loss: 0.22596809267997742\n",
      "iteration 17410: loss: 0.22596701979637146\n",
      "iteration 17411: loss: 0.22596609592437744\n",
      "iteration 17412: loss: 0.22596506774425507\n",
      "iteration 17413: loss: 0.22596409916877747\n",
      "iteration 17414: loss: 0.22596296668052673\n",
      "iteration 17415: loss: 0.2259620875120163\n",
      "iteration 17416: loss: 0.2259611189365387\n",
      "iteration 17417: loss: 0.22595998644828796\n",
      "iteration 17418: loss: 0.22595906257629395\n",
      "iteration 17419: loss: 0.2259579598903656\n",
      "iteration 17420: loss: 0.225956991314888\n",
      "iteration 17421: loss: 0.2259560376405716\n",
      "iteration 17422: loss: 0.2259550541639328\n",
      "iteration 17423: loss: 0.22595401108264923\n",
      "iteration 17424: loss: 0.22595301270484924\n",
      "iteration 17425: loss: 0.22595198452472687\n",
      "iteration 17426: loss: 0.22595098614692688\n",
      "iteration 17427: loss: 0.22594985365867615\n",
      "iteration 17428: loss: 0.22594895958900452\n",
      "iteration 17429: loss: 0.22594794631004333\n",
      "iteration 17430: loss: 0.22594693303108215\n",
      "iteration 17431: loss: 0.22594590485095978\n",
      "iteration 17432: loss: 0.22594484686851501\n",
      "iteration 17433: loss: 0.225943922996521\n",
      "iteration 17434: loss: 0.22594289481639862\n",
      "iteration 17435: loss: 0.22594185173511505\n",
      "iteration 17436: loss: 0.22594091296195984\n",
      "iteration 17437: loss: 0.22593986988067627\n",
      "iteration 17438: loss: 0.22593888640403748\n",
      "iteration 17439: loss: 0.22593791782855988\n",
      "iteration 17440: loss: 0.22593684494495392\n",
      "iteration 17441: loss: 0.2259358912706375\n",
      "iteration 17442: loss: 0.22593478858470917\n",
      "iteration 17443: loss: 0.22593379020690918\n",
      "iteration 17444: loss: 0.22593286633491516\n",
      "iteration 17445: loss: 0.22593173384666443\n",
      "iteration 17446: loss: 0.225930854678154\n",
      "iteration 17447: loss: 0.22592973709106445\n",
      "iteration 17448: loss: 0.22592885792255402\n",
      "iteration 17449: loss: 0.22592782974243164\n",
      "iteration 17450: loss: 0.22592675685882568\n",
      "iteration 17451: loss: 0.22592580318450928\n",
      "iteration 17452: loss: 0.22592470049858093\n",
      "iteration 17453: loss: 0.2259238213300705\n",
      "iteration 17454: loss: 0.22592267394065857\n",
      "iteration 17455: loss: 0.22592172026634216\n",
      "iteration 17456: loss: 0.22592070698738098\n",
      "iteration 17457: loss: 0.2259196788072586\n",
      "iteration 17458: loss: 0.22591862082481384\n",
      "iteration 17459: loss: 0.22591769695281982\n",
      "iteration 17460: loss: 0.22591671347618103\n",
      "iteration 17461: loss: 0.22591567039489746\n",
      "iteration 17462: loss: 0.22591471672058105\n",
      "iteration 17463: loss: 0.22591373324394226\n",
      "iteration 17464: loss: 0.22591261565685272\n",
      "iteration 17465: loss: 0.22591166198253632\n",
      "iteration 17466: loss: 0.22591058909893036\n",
      "iteration 17467: loss: 0.22590968012809753\n",
      "iteration 17468: loss: 0.22590859234333038\n",
      "iteration 17469: loss: 0.2259076088666916\n",
      "iteration 17470: loss: 0.22590668499469757\n",
      "iteration 17471: loss: 0.22590558230876923\n",
      "iteration 17472: loss: 0.22590461373329163\n",
      "iteration 17473: loss: 0.22590355575084686\n",
      "iteration 17474: loss: 0.22590255737304688\n",
      "iteration 17475: loss: 0.2259015142917633\n",
      "iteration 17476: loss: 0.2259005606174469\n",
      "iteration 17477: loss: 0.2258995771408081\n",
      "iteration 17478: loss: 0.22589853405952454\n",
      "iteration 17479: loss: 0.22589758038520813\n",
      "iteration 17480: loss: 0.22589652240276337\n",
      "iteration 17481: loss: 0.22589556872844696\n",
      "iteration 17482: loss: 0.22589454054832458\n",
      "iteration 17483: loss: 0.22589357197284698\n",
      "iteration 17484: loss: 0.22589261829853058\n",
      "iteration 17485: loss: 0.22589154541492462\n",
      "iteration 17486: loss: 0.22589054703712463\n",
      "iteration 17487: loss: 0.22588953375816345\n",
      "iteration 17488: loss: 0.22588853538036346\n",
      "iteration 17489: loss: 0.2258874475955963\n",
      "iteration 17490: loss: 0.2258864939212799\n",
      "iteration 17491: loss: 0.2258855104446411\n",
      "iteration 17492: loss: 0.22588443756103516\n",
      "iteration 17493: loss: 0.22588351368904114\n",
      "iteration 17494: loss: 0.22588244080543518\n",
      "iteration 17495: loss: 0.225881427526474\n",
      "iteration 17496: loss: 0.22588053345680237\n",
      "iteration 17497: loss: 0.22587943077087402\n",
      "iteration 17498: loss: 0.22587840259075165\n",
      "iteration 17499: loss: 0.22587737441062927\n",
      "iteration 17500: loss: 0.22587645053863525\n",
      "iteration 17501: loss: 0.2258753478527069\n",
      "iteration 17502: loss: 0.2258743941783905\n",
      "iteration 17503: loss: 0.2258734256029129\n",
      "iteration 17504: loss: 0.22587236762046814\n",
      "iteration 17505: loss: 0.22587141394615173\n",
      "iteration 17506: loss: 0.22587037086486816\n",
      "iteration 17507: loss: 0.22586941719055176\n",
      "iteration 17508: loss: 0.2258683443069458\n",
      "iteration 17509: loss: 0.22586742043495178\n",
      "iteration 17510: loss: 0.22586634755134583\n",
      "iteration 17511: loss: 0.22586536407470703\n",
      "iteration 17512: loss: 0.22586436569690704\n",
      "iteration 17513: loss: 0.22586338222026825\n",
      "iteration 17514: loss: 0.22586235404014587\n",
      "iteration 17515: loss: 0.2258613109588623\n",
      "iteration 17516: loss: 0.2258603870868683\n",
      "iteration 17517: loss: 0.22585925459861755\n",
      "iteration 17518: loss: 0.22585836052894592\n",
      "iteration 17519: loss: 0.22585733234882355\n",
      "iteration 17520: loss: 0.22585633397102356\n",
      "iteration 17521: loss: 0.22585532069206238\n",
      "iteration 17522: loss: 0.2258543074131012\n",
      "iteration 17523: loss: 0.2258533537387848\n",
      "iteration 17524: loss: 0.22585225105285645\n",
      "iteration 17525: loss: 0.22585125267505646\n",
      "iteration 17526: loss: 0.22585026919841766\n",
      "iteration 17527: loss: 0.22584934532642365\n",
      "iteration 17528: loss: 0.2258482426404953\n",
      "iteration 17529: loss: 0.22584733366966248\n",
      "iteration 17530: loss: 0.2258463203907013\n",
      "iteration 17531: loss: 0.22584526240825653\n",
      "iteration 17532: loss: 0.22584426403045654\n",
      "iteration 17533: loss: 0.22584328055381775\n",
      "iteration 17534: loss: 0.22584223747253418\n",
      "iteration 17535: loss: 0.2258412390947342\n",
      "iteration 17536: loss: 0.2258402407169342\n",
      "iteration 17537: loss: 0.2258392572402954\n",
      "iteration 17538: loss: 0.22583825886249542\n",
      "iteration 17539: loss: 0.22583718597888947\n",
      "iteration 17540: loss: 0.22583620250225067\n",
      "iteration 17541: loss: 0.22583529353141785\n",
      "iteration 17542: loss: 0.2258341759443283\n",
      "iteration 17543: loss: 0.2258332073688507\n",
      "iteration 17544: loss: 0.22583213448524475\n",
      "iteration 17545: loss: 0.22583122551441193\n",
      "iteration 17546: loss: 0.22583012282848358\n",
      "iteration 17547: loss: 0.22582916915416718\n",
      "iteration 17548: loss: 0.2258281260728836\n",
      "iteration 17549: loss: 0.22582721710205078\n",
      "iteration 17550: loss: 0.2258262187242508\n",
      "iteration 17551: loss: 0.22582514584064484\n",
      "iteration 17552: loss: 0.22582420706748962\n",
      "iteration 17553: loss: 0.22582316398620605\n",
      "iteration 17554: loss: 0.22582218050956726\n",
      "iteration 17555: loss: 0.2258210927248001\n",
      "iteration 17556: loss: 0.2258201539516449\n",
      "iteration 17557: loss: 0.2258191555738449\n",
      "iteration 17558: loss: 0.22581811249256134\n",
      "iteration 17559: loss: 0.22581715881824493\n",
      "iteration 17560: loss: 0.22581616044044495\n",
      "iteration 17561: loss: 0.22581513226032257\n",
      "iteration 17562: loss: 0.2258141040802002\n",
      "iteration 17563: loss: 0.22581318020820618\n",
      "iteration 17564: loss: 0.22581210732460022\n",
      "iteration 17565: loss: 0.22581107914447784\n",
      "iteration 17566: loss: 0.22581002116203308\n",
      "iteration 17567: loss: 0.22580912709236145\n",
      "iteration 17568: loss: 0.22580811381340027\n",
      "iteration 17569: loss: 0.2258070707321167\n",
      "iteration 17570: loss: 0.2258061170578003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 17571: loss: 0.22580508887767792\n",
      "iteration 17572: loss: 0.2258041352033615\n",
      "iteration 17573: loss: 0.22580309212207794\n",
      "iteration 17574: loss: 0.22580206394195557\n",
      "iteration 17575: loss: 0.22580111026763916\n",
      "iteration 17576: loss: 0.22580011188983917\n",
      "iteration 17577: loss: 0.22579903900623322\n",
      "iteration 17578: loss: 0.22579804062843323\n",
      "iteration 17579: loss: 0.22579710185527802\n",
      "iteration 17580: loss: 0.22579610347747803\n",
      "iteration 17581: loss: 0.22579507529735565\n",
      "iteration 17582: loss: 0.2257940024137497\n",
      "iteration 17583: loss: 0.2257930487394333\n",
      "iteration 17584: loss: 0.2257920205593109\n",
      "iteration 17585: loss: 0.22579112648963928\n",
      "iteration 17586: loss: 0.22578999400138855\n",
      "iteration 17587: loss: 0.22578902542591095\n",
      "iteration 17588: loss: 0.22578799724578857\n",
      "iteration 17589: loss: 0.22578701376914978\n",
      "iteration 17590: loss: 0.2257859706878662\n",
      "iteration 17591: loss: 0.22578497231006622\n",
      "iteration 17592: loss: 0.225784033536911\n",
      "iteration 17593: loss: 0.2257830649614334\n",
      "iteration 17594: loss: 0.22578208148479462\n",
      "iteration 17595: loss: 0.22578100860118866\n",
      "iteration 17596: loss: 0.22578005492687225\n",
      "iteration 17597: loss: 0.22577905654907227\n",
      "iteration 17598: loss: 0.22577807307243347\n",
      "iteration 17599: loss: 0.22577698528766632\n",
      "iteration 17600: loss: 0.22577603161334991\n",
      "iteration 17601: loss: 0.22577497363090515\n",
      "iteration 17602: loss: 0.22577404975891113\n",
      "iteration 17603: loss: 0.22577300667762756\n",
      "iteration 17604: loss: 0.22577199339866638\n",
      "iteration 17605: loss: 0.2257709503173828\n",
      "iteration 17606: loss: 0.22576995193958282\n",
      "iteration 17607: loss: 0.22576904296875\n",
      "iteration 17608: loss: 0.22576799988746643\n",
      "iteration 17609: loss: 0.22576698660850525\n",
      "iteration 17610: loss: 0.2257659137248993\n",
      "iteration 17611: loss: 0.2257649153470993\n",
      "iteration 17612: loss: 0.22576400637626648\n",
      "iteration 17613: loss: 0.2257629632949829\n",
      "iteration 17614: loss: 0.2257620096206665\n",
      "iteration 17615: loss: 0.22576098144054413\n",
      "iteration 17616: loss: 0.22576001286506653\n",
      "iteration 17617: loss: 0.22575898468494415\n",
      "iteration 17618: loss: 0.22575803101062775\n",
      "iteration 17619: loss: 0.22575697302818298\n",
      "iteration 17620: loss: 0.2257559597492218\n",
      "iteration 17621: loss: 0.22575493156909943\n",
      "iteration 17622: loss: 0.2257539927959442\n",
      "iteration 17623: loss: 0.22575294971466064\n",
      "iteration 17624: loss: 0.22575204074382782\n",
      "iteration 17625: loss: 0.22575095295906067\n",
      "iteration 17626: loss: 0.22574999928474426\n",
      "iteration 17627: loss: 0.2257489264011383\n",
      "iteration 17628: loss: 0.2257479429244995\n",
      "iteration 17629: loss: 0.22574689984321594\n",
      "iteration 17630: loss: 0.2257460355758667\n",
      "iteration 17631: loss: 0.22574493288993835\n",
      "iteration 17632: loss: 0.22574397921562195\n",
      "iteration 17633: loss: 0.225742906332016\n",
      "iteration 17634: loss: 0.2257419377565384\n",
      "iteration 17635: loss: 0.22574086487293243\n",
      "iteration 17636: loss: 0.2257399559020996\n",
      "iteration 17637: loss: 0.2257390022277832\n",
      "iteration 17638: loss: 0.22573797404766083\n",
      "iteration 17639: loss: 0.22573700547218323\n",
      "iteration 17640: loss: 0.22573593258857727\n",
      "iteration 17641: loss: 0.22573499381542206\n",
      "iteration 17642: loss: 0.2257339507341385\n",
      "iteration 17643: loss: 0.22573301196098328\n",
      "iteration 17644: loss: 0.22573192417621613\n",
      "iteration 17645: loss: 0.22573097050189972\n",
      "iteration 17646: loss: 0.22572989761829376\n",
      "iteration 17647: loss: 0.22572898864746094\n",
      "iteration 17648: loss: 0.22572794556617737\n",
      "iteration 17649: loss: 0.22572696208953857\n",
      "iteration 17650: loss: 0.225725919008255\n",
      "iteration 17651: loss: 0.22572490572929382\n",
      "iteration 17652: loss: 0.2257239818572998\n",
      "iteration 17653: loss: 0.22572293877601624\n",
      "iteration 17654: loss: 0.22572192549705505\n",
      "iteration 17655: loss: 0.22572092711925507\n",
      "iteration 17656: loss: 0.2257198840379715\n",
      "iteration 17657: loss: 0.2257189303636551\n",
      "iteration 17658: loss: 0.22571802139282227\n",
      "iteration 17659: loss: 0.22571690380573273\n",
      "iteration 17660: loss: 0.2257160246372223\n",
      "iteration 17661: loss: 0.22571496665477753\n",
      "iteration 17662: loss: 0.22571396827697754\n",
      "iteration 17663: loss: 0.22571294009685516\n",
      "iteration 17664: loss: 0.22571201622486115\n",
      "iteration 17665: loss: 0.2257109135389328\n",
      "iteration 17666: loss: 0.2257099598646164\n",
      "iteration 17667: loss: 0.22570888698101044\n",
      "iteration 17668: loss: 0.22570793330669403\n",
      "iteration 17669: loss: 0.22570690512657166\n",
      "iteration 17670: loss: 0.22570598125457764\n",
      "iteration 17671: loss: 0.22570493817329407\n",
      "iteration 17672: loss: 0.22570395469665527\n",
      "iteration 17673: loss: 0.2257029116153717\n",
      "iteration 17674: loss: 0.2257019579410553\n",
      "iteration 17675: loss: 0.22570085525512695\n",
      "iteration 17676: loss: 0.22569990158081055\n",
      "iteration 17677: loss: 0.22569890320301056\n",
      "iteration 17678: loss: 0.22569799423217773\n",
      "iteration 17679: loss: 0.2256968915462494\n",
      "iteration 17680: loss: 0.22569596767425537\n",
      "iteration 17681: loss: 0.2256949245929718\n",
      "iteration 17682: loss: 0.2256939858198166\n",
      "iteration 17683: loss: 0.2256929576396942\n",
      "iteration 17684: loss: 0.22569191455841064\n",
      "iteration 17685: loss: 0.22569096088409424\n",
      "iteration 17686: loss: 0.22568997740745544\n",
      "iteration 17687: loss: 0.22568893432617188\n",
      "iteration 17688: loss: 0.22568795084953308\n",
      "iteration 17689: loss: 0.22568687796592712\n",
      "iteration 17690: loss: 0.2256859540939331\n",
      "iteration 17691: loss: 0.22568488121032715\n",
      "iteration 17692: loss: 0.22568392753601074\n",
      "iteration 17693: loss: 0.22568294405937195\n",
      "iteration 17694: loss: 0.22568197548389435\n",
      "iteration 17695: loss: 0.22568102180957794\n",
      "iteration 17696: loss: 0.22567994892597198\n",
      "iteration 17697: loss: 0.2256789654493332\n",
      "iteration 17698: loss: 0.22567793726921082\n",
      "iteration 17699: loss: 0.22567696869373322\n",
      "iteration 17700: loss: 0.22567586600780487\n",
      "iteration 17701: loss: 0.22567498683929443\n",
      "iteration 17702: loss: 0.22567398846149445\n",
      "iteration 17703: loss: 0.22567303478717804\n",
      "iteration 17704: loss: 0.22567197680473328\n",
      "iteration 17705: loss: 0.22567100822925568\n",
      "iteration 17706: loss: 0.2256699502468109\n",
      "iteration 17707: loss: 0.2256689965724945\n",
      "iteration 17708: loss: 0.22566798329353333\n",
      "iteration 17709: loss: 0.22566699981689453\n",
      "iteration 17710: loss: 0.22566597163677216\n",
      "iteration 17711: loss: 0.22566501796245575\n",
      "iteration 17712: loss: 0.2256639450788498\n",
      "iteration 17713: loss: 0.2256629467010498\n",
      "iteration 17714: loss: 0.22566194832324982\n",
      "iteration 17715: loss: 0.22566106915473938\n",
      "iteration 17716: loss: 0.22565999627113342\n",
      "iteration 17717: loss: 0.22565904259681702\n",
      "iteration 17718: loss: 0.22565801441669464\n",
      "iteration 17719: loss: 0.22565703094005585\n",
      "iteration 17720: loss: 0.22565598785877228\n",
      "iteration 17721: loss: 0.22565500438213348\n",
      "iteration 17722: loss: 0.22565405070781708\n",
      "iteration 17723: loss: 0.22565297782421112\n",
      "iteration 17724: loss: 0.22565202414989471\n",
      "iteration 17725: loss: 0.22565095126628876\n",
      "iteration 17726: loss: 0.22565002739429474\n",
      "iteration 17727: loss: 0.22564902901649475\n",
      "iteration 17728: loss: 0.22564801573753357\n",
      "iteration 17729: loss: 0.22564697265625\n",
      "iteration 17730: loss: 0.2256460189819336\n",
      "iteration 17731: loss: 0.2256450653076172\n",
      "iteration 17732: loss: 0.22564402222633362\n",
      "iteration 17733: loss: 0.22564300894737244\n",
      "iteration 17734: loss: 0.22564199566841125\n",
      "iteration 17735: loss: 0.22564110159873962\n",
      "iteration 17736: loss: 0.22564002871513367\n",
      "iteration 17737: loss: 0.22563907504081726\n",
      "iteration 17738: loss: 0.22563807666301727\n",
      "iteration 17739: loss: 0.22563707828521729\n",
      "iteration 17740: loss: 0.2256360501050949\n",
      "iteration 17741: loss: 0.2256350964307785\n",
      "iteration 17742: loss: 0.22563405334949493\n",
      "iteration 17743: loss: 0.22563306987285614\n",
      "iteration 17744: loss: 0.22563210129737854\n",
      "iteration 17745: loss: 0.22563108801841736\n",
      "iteration 17746: loss: 0.22563008964061737\n",
      "iteration 17747: loss: 0.2256290167570114\n",
      "iteration 17748: loss: 0.2256280481815338\n",
      "iteration 17749: loss: 0.2256270945072174\n",
      "iteration 17750: loss: 0.22562606632709503\n",
      "iteration 17751: loss: 0.22562508285045624\n",
      "iteration 17752: loss: 0.22562411427497864\n",
      "iteration 17753: loss: 0.22562316060066223\n",
      "iteration 17754: loss: 0.22562210261821747\n",
      "iteration 17755: loss: 0.2256210744380951\n",
      "iteration 17756: loss: 0.2256201207637787\n",
      "iteration 17757: loss: 0.2256191223859787\n",
      "iteration 17758: loss: 0.2256181240081787\n",
      "iteration 17759: loss: 0.2256171703338623\n",
      "iteration 17760: loss: 0.22561605274677277\n",
      "iteration 17761: loss: 0.22561511397361755\n",
      "iteration 17762: loss: 0.22561411559581757\n",
      "iteration 17763: loss: 0.225613072514534\n",
      "iteration 17764: loss: 0.2256121188402176\n",
      "iteration 17765: loss: 0.22561116516590118\n",
      "iteration 17766: loss: 0.2256101816892624\n",
      "iteration 17767: loss: 0.22560909390449524\n",
      "iteration 17768: loss: 0.22560818493366241\n",
      "iteration 17769: loss: 0.22560715675354004\n",
      "iteration 17770: loss: 0.22560615837574005\n",
      "iteration 17771: loss: 0.22560515999794006\n",
      "iteration 17772: loss: 0.22560417652130127\n",
      "iteration 17773: loss: 0.2256031334400177\n",
      "iteration 17774: loss: 0.2256021499633789\n",
      "iteration 17775: loss: 0.22560110688209534\n",
      "iteration 17776: loss: 0.22560016810894012\n",
      "iteration 17777: loss: 0.22559913992881775\n",
      "iteration 17778: loss: 0.22559817135334015\n",
      "iteration 17779: loss: 0.22559714317321777\n",
      "iteration 17780: loss: 0.22559618949890137\n",
      "iteration 17781: loss: 0.2255951464176178\n",
      "iteration 17782: loss: 0.225594162940979\n",
      "iteration 17783: loss: 0.2255931794643402\n",
      "iteration 17784: loss: 0.22559218108654022\n",
      "iteration 17785: loss: 0.22559113800525665\n",
      "iteration 17786: loss: 0.2255902737379074\n",
      "iteration 17787: loss: 0.22558920085430145\n",
      "iteration 17788: loss: 0.22558827698230743\n",
      "iteration 17789: loss: 0.22558720409870148\n",
      "iteration 17790: loss: 0.22558626532554626\n",
      "iteration 17791: loss: 0.2255852222442627\n",
      "iteration 17792: loss: 0.2255842238664627\n",
      "iteration 17793: loss: 0.2255832850933075\n",
      "iteration 17794: loss: 0.22558221220970154\n",
      "iteration 17795: loss: 0.22558125853538513\n",
      "iteration 17796: loss: 0.22558028995990753\n",
      "iteration 17797: loss: 0.22557926177978516\n",
      "iteration 17798: loss: 0.22557833790779114\n",
      "iteration 17799: loss: 0.22557727992534637\n",
      "iteration 17800: loss: 0.225576251745224\n",
      "iteration 17801: loss: 0.2255752980709076\n",
      "iteration 17802: loss: 0.22557422518730164\n",
      "iteration 17803: loss: 0.22557330131530762\n",
      "iteration 17804: loss: 0.22557227313518524\n",
      "iteration 17805: loss: 0.22557130455970764\n",
      "iteration 17806: loss: 0.22557027637958527\n",
      "iteration 17807: loss: 0.22556936740875244\n",
      "iteration 17808: loss: 0.22556832432746887\n",
      "iteration 17809: loss: 0.22556737065315247\n",
      "iteration 17810: loss: 0.2255663424730301\n",
      "iteration 17811: loss: 0.2255653589963913\n",
      "iteration 17812: loss: 0.22556433081626892\n",
      "iteration 17813: loss: 0.22556337714195251\n",
      "iteration 17814: loss: 0.22556237876415253\n",
      "iteration 17815: loss: 0.2255614548921585\n",
      "iteration 17816: loss: 0.2255602777004242\n",
      "iteration 17817: loss: 0.22555944323539734\n",
      "iteration 17818: loss: 0.2255583256483078\n",
      "iteration 17819: loss: 0.22555744647979736\n",
      "iteration 17820: loss: 0.2255563735961914\n",
      "iteration 17821: loss: 0.22555534541606903\n",
      "iteration 17822: loss: 0.225554421544075\n",
      "iteration 17823: loss: 0.22555336356163025\n",
      "iteration 17824: loss: 0.22555239498615265\n",
      "iteration 17825: loss: 0.22555139660835266\n",
      "iteration 17826: loss: 0.22555041313171387\n",
      "iteration 17827: loss: 0.22554941475391388\n",
      "iteration 17828: loss: 0.2255484163761139\n",
      "iteration 17829: loss: 0.22554747760295868\n",
      "iteration 17830: loss: 0.22554650902748108\n",
      "iteration 17831: loss: 0.2255454957485199\n",
      "iteration 17832: loss: 0.22554445266723633\n",
      "iteration 17833: loss: 0.22554349899291992\n",
      "iteration 17834: loss: 0.22554250061511993\n",
      "iteration 17835: loss: 0.22554151713848114\n",
      "iteration 17836: loss: 0.22554047405719757\n",
      "iteration 17837: loss: 0.22553952038288116\n",
      "iteration 17838: loss: 0.2255384624004364\n",
      "iteration 17839: loss: 0.22553750872612\n",
      "iteration 17840: loss: 0.22553646564483643\n",
      "iteration 17841: loss: 0.22553548216819763\n",
      "iteration 17842: loss: 0.22553451359272003\n",
      "iteration 17843: loss: 0.22553355991840363\n",
      "iteration 17844: loss: 0.22553253173828125\n",
      "iteration 17845: loss: 0.22553160786628723\n",
      "iteration 17846: loss: 0.22553054988384247\n",
      "iteration 17847: loss: 0.22552958130836487\n",
      "iteration 17848: loss: 0.22552864253520966\n",
      "iteration 17849: loss: 0.2255275696516037\n",
      "iteration 17850: loss: 0.22552664577960968\n",
      "iteration 17851: loss: 0.2255256175994873\n",
      "iteration 17852: loss: 0.2255246639251709\n",
      "iteration 17853: loss: 0.22552356123924255\n",
      "iteration 17854: loss: 0.22552268207073212\n",
      "iteration 17855: loss: 0.22552156448364258\n",
      "iteration 17856: loss: 0.22552065551280975\n",
      "iteration 17857: loss: 0.22551965713500977\n",
      "iteration 17858: loss: 0.22551870346069336\n",
      "iteration 17859: loss: 0.22551760077476501\n",
      "iteration 17860: loss: 0.22551670670509338\n",
      "iteration 17861: loss: 0.2255156934261322\n",
      "iteration 17862: loss: 0.22551465034484863\n",
      "iteration 17863: loss: 0.22551369667053223\n",
      "iteration 17864: loss: 0.22551266849040985\n",
      "iteration 17865: loss: 0.22551169991493225\n",
      "iteration 17866: loss: 0.22551068663597107\n",
      "iteration 17867: loss: 0.22550973296165466\n",
      "iteration 17868: loss: 0.2255086898803711\n",
      "iteration 17869: loss: 0.22550778090953827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 17870: loss: 0.2255067378282547\n",
      "iteration 17871: loss: 0.22550582885742188\n",
      "iteration 17872: loss: 0.22550475597381592\n",
      "iteration 17873: loss: 0.22550377249717712\n",
      "iteration 17874: loss: 0.22550275921821594\n",
      "iteration 17875: loss: 0.22550182044506073\n",
      "iteration 17876: loss: 0.22550079226493835\n",
      "iteration 17877: loss: 0.22549977898597717\n",
      "iteration 17878: loss: 0.22549882531166077\n",
      "iteration 17879: loss: 0.2254977971315384\n",
      "iteration 17880: loss: 0.22549685835838318\n",
      "iteration 17881: loss: 0.2254958599805832\n",
      "iteration 17882: loss: 0.2254948914051056\n",
      "iteration 17883: loss: 0.2254938781261444\n",
      "iteration 17884: loss: 0.2254929095506668\n",
      "iteration 17885: loss: 0.22549185156822205\n",
      "iteration 17886: loss: 0.22549080848693848\n",
      "iteration 17887: loss: 0.22548989951610565\n",
      "iteration 17888: loss: 0.22548885643482208\n",
      "iteration 17889: loss: 0.22548790276050568\n",
      "iteration 17890: loss: 0.22548691928386688\n",
      "iteration 17891: loss: 0.22548599541187286\n",
      "iteration 17892: loss: 0.2254849225282669\n",
      "iteration 17893: loss: 0.2254839688539505\n",
      "iteration 17894: loss: 0.22548291087150574\n",
      "iteration 17895: loss: 0.22548194229602814\n",
      "iteration 17896: loss: 0.22548095881938934\n",
      "iteration 17897: loss: 0.22548003494739532\n",
      "iteration 17898: loss: 0.22547896206378937\n",
      "iteration 17899: loss: 0.22547805309295654\n",
      "iteration 17900: loss: 0.22547702491283417\n",
      "iteration 17901: loss: 0.2254759967327118\n",
      "iteration 17902: loss: 0.2254750281572342\n",
      "iteration 17903: loss: 0.22547397017478943\n",
      "iteration 17904: loss: 0.2254730463027954\n",
      "iteration 17905: loss: 0.22547201812267303\n",
      "iteration 17906: loss: 0.22547109425067902\n",
      "iteration 17907: loss: 0.22547002136707306\n",
      "iteration 17908: loss: 0.22546915709972382\n",
      "iteration 17909: loss: 0.22546806931495667\n",
      "iteration 17910: loss: 0.22546708583831787\n",
      "iteration 17911: loss: 0.22546610236167908\n",
      "iteration 17912: loss: 0.22546513378620148\n",
      "iteration 17913: loss: 0.22546415030956268\n",
      "iteration 17914: loss: 0.2254631221294403\n",
      "iteration 17915: loss: 0.2254621535539627\n",
      "iteration 17916: loss: 0.2254611700773239\n",
      "iteration 17917: loss: 0.22546012699604034\n",
      "iteration 17918: loss: 0.22545918822288513\n",
      "iteration 17919: loss: 0.22545818984508514\n",
      "iteration 17920: loss: 0.22545723617076874\n",
      "iteration 17921: loss: 0.2254561483860016\n",
      "iteration 17922: loss: 0.22545525431632996\n",
      "iteration 17923: loss: 0.2254542112350464\n",
      "iteration 17924: loss: 0.22545325756072998\n",
      "iteration 17925: loss: 0.2254522293806076\n",
      "iteration 17926: loss: 0.2254512757062912\n",
      "iteration 17927: loss: 0.22545023262500763\n",
      "iteration 17928: loss: 0.22544917464256287\n",
      "iteration 17929: loss: 0.22544825077056885\n",
      "iteration 17930: loss: 0.22544725239276886\n",
      "iteration 17931: loss: 0.22544634342193604\n",
      "iteration 17932: loss: 0.22544531524181366\n",
      "iteration 17933: loss: 0.22544431686401367\n",
      "iteration 17934: loss: 0.22544331848621368\n",
      "iteration 17935: loss: 0.22544236481189728\n",
      "iteration 17936: loss: 0.22544138133525848\n",
      "iteration 17937: loss: 0.22544047236442566\n",
      "iteration 17938: loss: 0.2254393994808197\n",
      "iteration 17939: loss: 0.22543840110301971\n",
      "iteration 17940: loss: 0.2254374474287033\n",
      "iteration 17941: loss: 0.22543640434741974\n",
      "iteration 17942: loss: 0.22543540596961975\n",
      "iteration 17943: loss: 0.22543439269065857\n",
      "iteration 17944: loss: 0.22543346881866455\n",
      "iteration 17945: loss: 0.22543248534202576\n",
      "iteration 17946: loss: 0.22543147206306458\n",
      "iteration 17947: loss: 0.22543048858642578\n",
      "iteration 17948: loss: 0.2254294455051422\n",
      "iteration 17949: loss: 0.225428506731987\n",
      "iteration 17950: loss: 0.22542747855186462\n",
      "iteration 17951: loss: 0.2254265993833542\n",
      "iteration 17952: loss: 0.2254255712032318\n",
      "iteration 17953: loss: 0.22542457282543182\n",
      "iteration 17954: loss: 0.22542357444763184\n",
      "iteration 17955: loss: 0.22542253136634827\n",
      "iteration 17956: loss: 0.22542162239551544\n",
      "iteration 17957: loss: 0.22542066872119904\n",
      "iteration 17958: loss: 0.22541961073875427\n",
      "iteration 17959: loss: 0.22541861236095428\n",
      "iteration 17960: loss: 0.22541768848896027\n",
      "iteration 17961: loss: 0.2254166305065155\n",
      "iteration 17962: loss: 0.22541563212871552\n",
      "iteration 17963: loss: 0.22541466355323792\n",
      "iteration 17964: loss: 0.22541363537311554\n",
      "iteration 17965: loss: 0.22541272640228271\n",
      "iteration 17966: loss: 0.22541169822216034\n",
      "iteration 17967: loss: 0.22541069984436035\n",
      "iteration 17968: loss: 0.22540970146656036\n",
      "iteration 17969: loss: 0.225408673286438\n",
      "iteration 17970: loss: 0.22540774941444397\n",
      "iteration 17971: loss: 0.22540679574012756\n",
      "iteration 17972: loss: 0.22540584206581116\n",
      "iteration 17973: loss: 0.2254047840833664\n",
      "iteration 17974: loss: 0.2254037857055664\n",
      "iteration 17975: loss: 0.22540287673473358\n",
      "iteration 17976: loss: 0.22540180385112762\n",
      "iteration 17977: loss: 0.22540083527565002\n",
      "iteration 17978: loss: 0.22539980709552765\n",
      "iteration 17979: loss: 0.22539889812469482\n",
      "iteration 17980: loss: 0.22539789974689484\n",
      "iteration 17981: loss: 0.22539690136909485\n",
      "iteration 17982: loss: 0.22539587318897247\n",
      "iteration 17983: loss: 0.22539491951465607\n",
      "iteration 17984: loss: 0.2253938913345337\n",
      "iteration 17985: loss: 0.2253929078578949\n",
      "iteration 17986: loss: 0.2253919094800949\n",
      "iteration 17987: loss: 0.2253909856081009\n",
      "iteration 17988: loss: 0.22538995742797852\n",
      "iteration 17989: loss: 0.22538892924785614\n",
      "iteration 17990: loss: 0.22538796067237854\n",
      "iteration 17991: loss: 0.22538697719573975\n",
      "iteration 17992: loss: 0.2253860980272293\n",
      "iteration 17993: loss: 0.22538509964942932\n",
      "iteration 17994: loss: 0.22538407146930695\n",
      "iteration 17995: loss: 0.22538307309150696\n",
      "iteration 17996: loss: 0.22538205981254578\n",
      "iteration 17997: loss: 0.22538110613822937\n",
      "iteration 17998: loss: 0.22538010776042938\n",
      "iteration 17999: loss: 0.225379079580307\n",
      "iteration 18000: loss: 0.22537818551063538\n",
      "iteration 18001: loss: 0.22537711262702942\n",
      "iteration 18002: loss: 0.2253762036561966\n",
      "iteration 18003: loss: 0.2253752052783966\n",
      "iteration 18004: loss: 0.22537414729595184\n",
      "iteration 18005: loss: 0.22537319362163544\n",
      "iteration 18006: loss: 0.22537219524383545\n",
      "iteration 18007: loss: 0.22537121176719666\n",
      "iteration 18008: loss: 0.22537024319171906\n",
      "iteration 18009: loss: 0.22536925971508026\n",
      "iteration 18010: loss: 0.22536829113960266\n",
      "iteration 18011: loss: 0.2253672182559967\n",
      "iteration 18012: loss: 0.22536630928516388\n",
      "iteration 18013: loss: 0.2253653109073639\n",
      "iteration 18014: loss: 0.22536441683769226\n",
      "iteration 18015: loss: 0.2253633439540863\n",
      "iteration 18016: loss: 0.2253623753786087\n",
      "iteration 18017: loss: 0.2253613919019699\n",
      "iteration 18018: loss: 0.22536031901836395\n",
      "iteration 18019: loss: 0.22535939514636993\n",
      "iteration 18020: loss: 0.22535841166973114\n",
      "iteration 18021: loss: 0.22535744309425354\n",
      "iteration 18022: loss: 0.22535645961761475\n",
      "iteration 18023: loss: 0.22535543143749237\n",
      "iteration 18024: loss: 0.22535447776317596\n",
      "iteration 18025: loss: 0.2253534346818924\n",
      "iteration 18026: loss: 0.2253524363040924\n",
      "iteration 18027: loss: 0.2253514975309372\n",
      "iteration 18028: loss: 0.22535046935081482\n",
      "iteration 18029: loss: 0.2253495752811432\n",
      "iteration 18030: loss: 0.225348562002182\n",
      "iteration 18031: loss: 0.22534754872322083\n",
      "iteration 18032: loss: 0.225346639752388\n",
      "iteration 18033: loss: 0.22534556686878204\n",
      "iteration 18034: loss: 0.2253446877002716\n",
      "iteration 18035: loss: 0.22534365952014923\n",
      "iteration 18036: loss: 0.22534267604351044\n",
      "iteration 18037: loss: 0.22534164786338806\n",
      "iteration 18038: loss: 0.2253406047821045\n",
      "iteration 18039: loss: 0.22533969581127167\n",
      "iteration 18040: loss: 0.2253386676311493\n",
      "iteration 18041: loss: 0.22533774375915527\n",
      "iteration 18042: loss: 0.2253367155790329\n",
      "iteration 18043: loss: 0.22533579170703888\n",
      "iteration 18044: loss: 0.2253347635269165\n",
      "iteration 18045: loss: 0.22533385455608368\n",
      "iteration 18046: loss: 0.22533278167247772\n",
      "iteration 18047: loss: 0.22533175349235535\n",
      "iteration 18048: loss: 0.2253308743238449\n",
      "iteration 18049: loss: 0.22532980144023895\n",
      "iteration 18050: loss: 0.22532883286476135\n",
      "iteration 18051: loss: 0.22532787919044495\n",
      "iteration 18052: loss: 0.22532689571380615\n",
      "iteration 18053: loss: 0.22532594203948975\n",
      "iteration 18054: loss: 0.22532494366168976\n",
      "iteration 18055: loss: 0.22532396018505096\n",
      "iteration 18056: loss: 0.22532300651073456\n",
      "iteration 18057: loss: 0.225321963429451\n",
      "iteration 18058: loss: 0.22532102465629578\n",
      "iteration 18059: loss: 0.2253199815750122\n",
      "iteration 18060: loss: 0.22531898319721222\n",
      "iteration 18061: loss: 0.225318044424057\n",
      "iteration 18062: loss: 0.2253170758485794\n",
      "iteration 18063: loss: 0.22531600296497345\n",
      "iteration 18064: loss: 0.22531509399414062\n",
      "iteration 18065: loss: 0.22531406581401825\n",
      "iteration 18066: loss: 0.22531311213970184\n",
      "iteration 18067: loss: 0.22531211376190186\n",
      "iteration 18068: loss: 0.22531108558177948\n",
      "iteration 18069: loss: 0.22531016170978546\n",
      "iteration 18070: loss: 0.22530913352966309\n",
      "iteration 18071: loss: 0.22530817985534668\n",
      "iteration 18072: loss: 0.22530722618103027\n",
      "iteration 18073: loss: 0.2253061980009079\n",
      "iteration 18074: loss: 0.2253052294254303\n",
      "iteration 18075: loss: 0.2253042459487915\n",
      "iteration 18076: loss: 0.22530324757099152\n",
      "iteration 18077: loss: 0.2253023087978363\n",
      "iteration 18078: loss: 0.22530129551887512\n",
      "iteration 18079: loss: 0.2253003567457199\n",
      "iteration 18080: loss: 0.22529928386211395\n",
      "iteration 18081: loss: 0.22529835999011993\n",
      "iteration 18082: loss: 0.22529737651348114\n",
      "iteration 18083: loss: 0.22529634833335876\n",
      "iteration 18084: loss: 0.22529542446136475\n",
      "iteration 18085: loss: 0.22529442608356476\n",
      "iteration 18086: loss: 0.22529342770576477\n",
      "iteration 18087: loss: 0.2252923995256424\n",
      "iteration 18088: loss: 0.225291445851326\n",
      "iteration 18089: loss: 0.22529049217700958\n",
      "iteration 18090: loss: 0.2252894639968872\n",
      "iteration 18091: loss: 0.22528859972953796\n",
      "iteration 18092: loss: 0.2252875566482544\n",
      "iteration 18093: loss: 0.22528652846813202\n",
      "iteration 18094: loss: 0.22528564929962158\n",
      "iteration 18095: loss: 0.22528454661369324\n",
      "iteration 18096: loss: 0.2252836674451828\n",
      "iteration 18097: loss: 0.22528263926506042\n",
      "iteration 18098: loss: 0.22528156638145447\n",
      "iteration 18099: loss: 0.22528067231178284\n",
      "iteration 18100: loss: 0.22527968883514404\n",
      "iteration 18101: loss: 0.22527870535850525\n",
      "iteration 18102: loss: 0.22527766227722168\n",
      "iteration 18103: loss: 0.22527675330638885\n",
      "iteration 18104: loss: 0.22527575492858887\n",
      "iteration 18105: loss: 0.22527475655078888\n",
      "iteration 18106: loss: 0.2252737581729889\n",
      "iteration 18107: loss: 0.22527280449867249\n",
      "iteration 18108: loss: 0.2252717763185501\n",
      "iteration 18109: loss: 0.22527079284191132\n",
      "iteration 18110: loss: 0.2252698391675949\n",
      "iteration 18111: loss: 0.22526893019676208\n",
      "iteration 18112: loss: 0.2252679169178009\n",
      "iteration 18113: loss: 0.22526688873767853\n",
      "iteration 18114: loss: 0.2252660095691681\n",
      "iteration 18115: loss: 0.22526493668556213\n",
      "iteration 18116: loss: 0.22526392340660095\n",
      "iteration 18117: loss: 0.22526296973228455\n",
      "iteration 18118: loss: 0.22526197135448456\n",
      "iteration 18119: loss: 0.22526104748249054\n",
      "iteration 18120: loss: 0.22525998950004578\n",
      "iteration 18121: loss: 0.22525906562805176\n",
      "iteration 18122: loss: 0.22525811195373535\n",
      "iteration 18123: loss: 0.22525711357593536\n",
      "iteration 18124: loss: 0.225256085395813\n",
      "iteration 18125: loss: 0.22525516152381897\n",
      "iteration 18126: loss: 0.2252541035413742\n",
      "iteration 18127: loss: 0.22525307536125183\n",
      "iteration 18128: loss: 0.2252521812915802\n",
      "iteration 18129: loss: 0.22525116801261902\n",
      "iteration 18130: loss: 0.225250244140625\n",
      "iteration 18131: loss: 0.2252492904663086\n",
      "iteration 18132: loss: 0.2252482920885086\n",
      "iteration 18133: loss: 0.2252473086118698\n",
      "iteration 18134: loss: 0.22524628043174744\n",
      "iteration 18135: loss: 0.22524526715278625\n",
      "iteration 18136: loss: 0.22524435818195343\n",
      "iteration 18137: loss: 0.22524337470531464\n",
      "iteration 18138: loss: 0.22524242103099823\n",
      "iteration 18139: loss: 0.22524145245552063\n",
      "iteration 18140: loss: 0.22524039447307587\n",
      "iteration 18141: loss: 0.22523947060108185\n",
      "iteration 18142: loss: 0.22523841261863708\n",
      "iteration 18143: loss: 0.22523756325244904\n",
      "iteration 18144: loss: 0.2252364605665207\n",
      "iteration 18145: loss: 0.2252354621887207\n",
      "iteration 18146: loss: 0.22523455321788788\n",
      "iteration 18147: loss: 0.2252335250377655\n",
      "iteration 18148: loss: 0.22523260116577148\n",
      "iteration 18149: loss: 0.22523155808448792\n",
      "iteration 18150: loss: 0.2252306044101715\n",
      "iteration 18151: loss: 0.22522969543933868\n",
      "iteration 18152: loss: 0.22522863745689392\n",
      "iteration 18153: loss: 0.2252277433872223\n",
      "iteration 18154: loss: 0.22522667050361633\n",
      "iteration 18155: loss: 0.22522571682929993\n",
      "iteration 18156: loss: 0.2252248227596283\n",
      "iteration 18157: loss: 0.22522374987602234\n",
      "iteration 18158: loss: 0.22522273659706116\n",
      "iteration 18159: loss: 0.22522178292274475\n",
      "iteration 18160: loss: 0.22522082924842834\n",
      "iteration 18161: loss: 0.22521980106830597\n",
      "iteration 18162: loss: 0.22521886229515076\n",
      "iteration 18163: loss: 0.2252178192138672\n",
      "iteration 18164: loss: 0.22521691024303436\n",
      "iteration 18165: loss: 0.2252158671617508\n",
      "iteration 18166: loss: 0.2252148687839508\n",
      "iteration 18167: loss: 0.22521397471427917\n",
      "iteration 18168: loss: 0.2252129316329956\n",
      "iteration 18169: loss: 0.2252120077610016\n",
      "iteration 18170: loss: 0.22521105408668518\n",
      "iteration 18171: loss: 0.225210040807724\n",
      "iteration 18172: loss: 0.225209042429924\n",
      "iteration 18173: loss: 0.22520811855793\n",
      "iteration 18174: loss: 0.22520709037780762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 18175: loss: 0.22520604729652405\n",
      "iteration 18176: loss: 0.2252051830291748\n",
      "iteration 18177: loss: 0.22520413994789124\n",
      "iteration 18178: loss: 0.22520315647125244\n",
      "iteration 18179: loss: 0.22520223259925842\n",
      "iteration 18180: loss: 0.22520121932029724\n",
      "iteration 18181: loss: 0.22520020604133606\n",
      "iteration 18182: loss: 0.22519925236701965\n",
      "iteration 18183: loss: 0.22519823908805847\n",
      "iteration 18184: loss: 0.22519724071025848\n",
      "iteration 18185: loss: 0.22519631683826447\n",
      "iteration 18186: loss: 0.22519536316394806\n",
      "iteration 18187: loss: 0.22519426047801971\n",
      "iteration 18188: loss: 0.2251933515071869\n",
      "iteration 18189: loss: 0.22519239783287048\n",
      "iteration 18190: loss: 0.22519150376319885\n",
      "iteration 18191: loss: 0.22519049048423767\n",
      "iteration 18192: loss: 0.2251894474029541\n",
      "iteration 18193: loss: 0.22518853843212128\n",
      "iteration 18194: loss: 0.2251874953508377\n",
      "iteration 18195: loss: 0.2251865565776825\n",
      "iteration 18196: loss: 0.2251855880022049\n",
      "iteration 18197: loss: 0.22518455982208252\n",
      "iteration 18198: loss: 0.2251836359500885\n",
      "iteration 18199: loss: 0.22518260776996613\n",
      "iteration 18200: loss: 0.22518165409564972\n",
      "iteration 18201: loss: 0.22518067061901093\n",
      "iteration 18202: loss: 0.22517971694469452\n",
      "iteration 18203: loss: 0.2251787632703781\n",
      "iteration 18204: loss: 0.22517767548561096\n",
      "iteration 18205: loss: 0.22517676651477814\n",
      "iteration 18206: loss: 0.22517573833465576\n",
      "iteration 18207: loss: 0.22517478466033936\n",
      "iteration 18208: loss: 0.22517386078834534\n",
      "iteration 18209: loss: 0.22517287731170654\n",
      "iteration 18210: loss: 0.22517189383506775\n",
      "iteration 18211: loss: 0.22517094016075134\n",
      "iteration 18212: loss: 0.22516994178295135\n",
      "iteration 18213: loss: 0.22516891360282898\n",
      "iteration 18214: loss: 0.22516794502735138\n",
      "iteration 18215: loss: 0.22516699135303497\n",
      "iteration 18216: loss: 0.22516605257987976\n",
      "iteration 18217: loss: 0.22516503930091858\n",
      "iteration 18218: loss: 0.22516408562660217\n",
      "iteration 18219: loss: 0.22516310214996338\n",
      "iteration 18220: loss: 0.22516211867332458\n",
      "iteration 18221: loss: 0.22516107559204102\n",
      "iteration 18222: loss: 0.2251601219177246\n",
      "iteration 18223: loss: 0.22515913844108582\n",
      "iteration 18224: loss: 0.2251581847667694\n",
      "iteration 18225: loss: 0.22515714168548584\n",
      "iteration 18226: loss: 0.22515623271465302\n",
      "iteration 18227: loss: 0.2251552790403366\n",
      "iteration 18228: loss: 0.2251543551683426\n",
      "iteration 18229: loss: 0.22515340149402618\n",
      "iteration 18230: loss: 0.2251523733139038\n",
      "iteration 18231: loss: 0.22515133023262024\n",
      "iteration 18232: loss: 0.22515039145946503\n",
      "iteration 18233: loss: 0.22514936327934265\n",
      "iteration 18234: loss: 0.22514843940734863\n",
      "iteration 18235: loss: 0.22514745593070984\n",
      "iteration 18236: loss: 0.22514648735523224\n",
      "iteration 18237: loss: 0.22514548897743225\n",
      "iteration 18238: loss: 0.22514450550079346\n",
      "iteration 18239: loss: 0.22514352202415466\n",
      "iteration 18240: loss: 0.22514252364635468\n",
      "iteration 18241: loss: 0.2251415252685547\n",
      "iteration 18242: loss: 0.22514061629772186\n",
      "iteration 18243: loss: 0.22513966262340546\n",
      "iteration 18244: loss: 0.22513866424560547\n",
      "iteration 18245: loss: 0.22513766586780548\n",
      "iteration 18246: loss: 0.2251366376876831\n",
      "iteration 18247: loss: 0.2251357138156891\n",
      "iteration 18248: loss: 0.22513480484485626\n",
      "iteration 18249: loss: 0.22513380646705627\n",
      "iteration 18250: loss: 0.22513285279273987\n",
      "iteration 18251: loss: 0.2251318395137787\n",
      "iteration 18252: loss: 0.2251308411359787\n",
      "iteration 18253: loss: 0.22512993216514587\n",
      "iteration 18254: loss: 0.2251288890838623\n",
      "iteration 18255: loss: 0.2251279354095459\n",
      "iteration 18256: loss: 0.2251269519329071\n",
      "iteration 18257: loss: 0.2251259833574295\n",
      "iteration 18258: loss: 0.2251249998807907\n",
      "iteration 18259: loss: 0.22512395679950714\n",
      "iteration 18260: loss: 0.22512301802635193\n",
      "iteration 18261: loss: 0.22512204945087433\n",
      "iteration 18262: loss: 0.2251211404800415\n",
      "iteration 18263: loss: 0.22512009739875793\n",
      "iteration 18264: loss: 0.22511915862560272\n",
      "iteration 18265: loss: 0.22511816024780273\n",
      "iteration 18266: loss: 0.22511723637580872\n",
      "iteration 18267: loss: 0.2251162976026535\n",
      "iteration 18268: loss: 0.22511526942253113\n",
      "iteration 18269: loss: 0.22511427104473114\n",
      "iteration 18270: loss: 0.22511334717273712\n",
      "iteration 18271: loss: 0.22511234879493713\n",
      "iteration 18272: loss: 0.22511129081249237\n",
      "iteration 18273: loss: 0.22511038184165955\n",
      "iteration 18274: loss: 0.22510941326618195\n",
      "iteration 18275: loss: 0.22510847449302673\n",
      "iteration 18276: loss: 0.22510746121406555\n",
      "iteration 18277: loss: 0.22510647773742676\n",
      "iteration 18278: loss: 0.22510552406311035\n",
      "iteration 18279: loss: 0.22510452568531036\n",
      "iteration 18280: loss: 0.22510354220867157\n",
      "iteration 18281: loss: 0.22510258853435516\n",
      "iteration 18282: loss: 0.22510161995887756\n",
      "iteration 18283: loss: 0.22510060667991638\n",
      "iteration 18284: loss: 0.22509965300559998\n",
      "iteration 18285: loss: 0.22509869933128357\n",
      "iteration 18286: loss: 0.2250976860523224\n",
      "iteration 18287: loss: 0.2250967025756836\n",
      "iteration 18288: loss: 0.22509579360485077\n",
      "iteration 18289: loss: 0.22509479522705078\n",
      "iteration 18290: loss: 0.2250937968492508\n",
      "iteration 18291: loss: 0.22509291768074036\n",
      "iteration 18292: loss: 0.2250918447971344\n",
      "iteration 18293: loss: 0.22509083151817322\n",
      "iteration 18294: loss: 0.22508995234966278\n",
      "iteration 18295: loss: 0.2250889092683792\n",
      "iteration 18296: loss: 0.22508791089057922\n",
      "iteration 18297: loss: 0.2250870168209076\n",
      "iteration 18298: loss: 0.2250860035419464\n",
      "iteration 18299: loss: 0.22508504986763\n",
      "iteration 18300: loss: 0.2250840961933136\n",
      "iteration 18301: loss: 0.2250831127166748\n",
      "iteration 18302: loss: 0.22508206963539124\n",
      "iteration 18303: loss: 0.2250811606645584\n",
      "iteration 18304: loss: 0.225080206990242\n",
      "iteration 18305: loss: 0.22507917881011963\n",
      "iteration 18306: loss: 0.2250782549381256\n",
      "iteration 18307: loss: 0.22507724165916443\n",
      "iteration 18308: loss: 0.22507624328136444\n",
      "iteration 18309: loss: 0.22507533431053162\n",
      "iteration 18310: loss: 0.22507426142692566\n",
      "iteration 18311: loss: 0.2250734120607376\n",
      "iteration 18312: loss: 0.22507241368293762\n",
      "iteration 18313: loss: 0.22507138550281525\n",
      "iteration 18314: loss: 0.22507047653198242\n",
      "iteration 18315: loss: 0.22506943345069885\n",
      "iteration 18316: loss: 0.22506847977638245\n",
      "iteration 18317: loss: 0.22506752610206604\n",
      "iteration 18318: loss: 0.22506654262542725\n",
      "iteration 18319: loss: 0.22506563365459442\n",
      "iteration 18320: loss: 0.22506460547447205\n",
      "iteration 18321: loss: 0.22506365180015564\n",
      "iteration 18322: loss: 0.22506260871887207\n",
      "iteration 18323: loss: 0.22506172955036163\n",
      "iteration 18324: loss: 0.22506074607372284\n",
      "iteration 18325: loss: 0.22505974769592285\n",
      "iteration 18326: loss: 0.2250588834285736\n",
      "iteration 18327: loss: 0.22505779564380646\n",
      "iteration 18328: loss: 0.22505681216716766\n",
      "iteration 18329: loss: 0.22505593299865723\n",
      "iteration 18330: loss: 0.22505493462085724\n",
      "iteration 18331: loss: 0.22505387663841248\n",
      "iteration 18332: loss: 0.22505302727222443\n",
      "iteration 18333: loss: 0.22505196928977966\n",
      "iteration 18334: loss: 0.22505100071430206\n",
      "iteration 18335: loss: 0.22505004703998566\n",
      "iteration 18336: loss: 0.22504904866218567\n",
      "iteration 18337: loss: 0.22504806518554688\n",
      "iteration 18338: loss: 0.22504714131355286\n",
      "iteration 18339: loss: 0.22504611313343048\n",
      "iteration 18340: loss: 0.22504515945911407\n",
      "iteration 18341: loss: 0.22504422068595886\n",
      "iteration 18342: loss: 0.22504322230815887\n",
      "iteration 18343: loss: 0.22504225373268127\n",
      "iteration 18344: loss: 0.2250412404537201\n",
      "iteration 18345: loss: 0.22504034638404846\n",
      "iteration 18346: loss: 0.22503936290740967\n",
      "iteration 18347: loss: 0.2250383198261261\n",
      "iteration 18348: loss: 0.2250373810529709\n",
      "iteration 18349: loss: 0.2250363826751709\n",
      "iteration 18350: loss: 0.22503551840782166\n",
      "iteration 18351: loss: 0.22503450512886047\n",
      "iteration 18352: loss: 0.22503352165222168\n",
      "iteration 18353: loss: 0.2250325232744217\n",
      "iteration 18354: loss: 0.2250315397977829\n",
      "iteration 18355: loss: 0.22503061592578888\n",
      "iteration 18356: loss: 0.22502963244915009\n",
      "iteration 18357: loss: 0.22502867877483368\n",
      "iteration 18358: loss: 0.2250276803970337\n",
      "iteration 18359: loss: 0.2250266820192337\n",
      "iteration 18360: loss: 0.22502577304840088\n",
      "iteration 18361: loss: 0.22502481937408447\n",
      "iteration 18362: loss: 0.22502374649047852\n",
      "iteration 18363: loss: 0.22502291202545166\n",
      "iteration 18364: loss: 0.22502191364765167\n",
      "iteration 18365: loss: 0.2250208854675293\n",
      "iteration 18366: loss: 0.2250199019908905\n",
      "iteration 18367: loss: 0.2250189334154129\n",
      "iteration 18368: loss: 0.2250179499387741\n",
      "iteration 18369: loss: 0.2250170260667801\n",
      "iteration 18370: loss: 0.22501607239246368\n",
      "iteration 18371: loss: 0.22501501441001892\n",
      "iteration 18372: loss: 0.22501416504383087\n",
      "iteration 18373: loss: 0.2250131070613861\n",
      "iteration 18374: loss: 0.22501206398010254\n",
      "iteration 18375: loss: 0.2250111848115921\n",
      "iteration 18376: loss: 0.2250102460384369\n",
      "iteration 18377: loss: 0.22500920295715332\n",
      "iteration 18378: loss: 0.2250082939863205\n",
      "iteration 18379: loss: 0.22500737011432648\n",
      "iteration 18380: loss: 0.22500629723072052\n",
      "iteration 18381: loss: 0.2250053882598877\n",
      "iteration 18382: loss: 0.2250044345855713\n",
      "iteration 18383: loss: 0.2250034064054489\n",
      "iteration 18384: loss: 0.2250024825334549\n",
      "iteration 18385: loss: 0.2250014841556549\n",
      "iteration 18386: loss: 0.2250005304813385\n",
      "iteration 18387: loss: 0.2249995917081833\n",
      "iteration 18388: loss: 0.22499851882457733\n",
      "iteration 18389: loss: 0.2249975949525833\n",
      "iteration 18390: loss: 0.22499659657478333\n",
      "iteration 18391: loss: 0.2249956578016281\n",
      "iteration 18392: loss: 0.2249947041273117\n",
      "iteration 18393: loss: 0.22499367594718933\n",
      "iteration 18394: loss: 0.22499267756938934\n",
      "iteration 18395: loss: 0.2249917984008789\n",
      "iteration 18396: loss: 0.2249908149242401\n",
      "iteration 18397: loss: 0.22498977184295654\n",
      "iteration 18398: loss: 0.2249889373779297\n",
      "iteration 18399: loss: 0.2249879390001297\n",
      "iteration 18400: loss: 0.2249869555234909\n",
      "iteration 18401: loss: 0.2249859869480133\n",
      "iteration 18402: loss: 0.22498509287834167\n",
      "iteration 18403: loss: 0.2249840497970581\n",
      "iteration 18404: loss: 0.2249830663204193\n",
      "iteration 18405: loss: 0.2249820977449417\n",
      "iteration 18406: loss: 0.2249811589717865\n",
      "iteration 18407: loss: 0.2249801605939865\n",
      "iteration 18408: loss: 0.22497916221618652\n",
      "iteration 18409: loss: 0.2249782830476761\n",
      "iteration 18410: loss: 0.2249772548675537\n",
      "iteration 18411: loss: 0.22497625648975372\n",
      "iteration 18412: loss: 0.22497539222240448\n",
      "iteration 18413: loss: 0.22497442364692688\n",
      "iteration 18414: loss: 0.2249733954668045\n",
      "iteration 18415: loss: 0.22497233748435974\n",
      "iteration 18416: loss: 0.2249714583158493\n",
      "iteration 18417: loss: 0.2249704897403717\n",
      "iteration 18418: loss: 0.2249695062637329\n",
      "iteration 18419: loss: 0.2249685823917389\n",
      "iteration 18420: loss: 0.2249675989151001\n",
      "iteration 18421: loss: 0.2249666154384613\n",
      "iteration 18422: loss: 0.2249656468629837\n",
      "iteration 18423: loss: 0.2249646633863449\n",
      "iteration 18424: loss: 0.2249637395143509\n",
      "iteration 18425: loss: 0.22496278584003448\n",
      "iteration 18426: loss: 0.2249617874622345\n",
      "iteration 18427: loss: 0.22496077418327332\n",
      "iteration 18428: loss: 0.22495989501476288\n",
      "iteration 18429: loss: 0.2249588966369629\n",
      "iteration 18430: loss: 0.22495786845684052\n",
      "iteration 18431: loss: 0.22495684027671814\n",
      "iteration 18432: loss: 0.2249559909105301\n",
      "iteration 18433: loss: 0.2249549925327301\n",
      "iteration 18434: loss: 0.2249540388584137\n",
      "iteration 18435: loss: 0.22495312988758087\n",
      "iteration 18436: loss: 0.2249520719051361\n",
      "iteration 18437: loss: 0.2249511480331421\n",
      "iteration 18438: loss: 0.22495022416114807\n",
      "iteration 18439: loss: 0.2249491661787033\n",
      "iteration 18440: loss: 0.22494825720787048\n",
      "iteration 18441: loss: 0.22494728863239288\n",
      "iteration 18442: loss: 0.2249463051557541\n",
      "iteration 18443: loss: 0.2249453067779541\n",
      "iteration 18444: loss: 0.22494442760944366\n",
      "iteration 18445: loss: 0.2249433994293213\n",
      "iteration 18446: loss: 0.22494244575500488\n",
      "iteration 18447: loss: 0.2249414175748825\n",
      "iteration 18448: loss: 0.2249404937028885\n",
      "iteration 18449: loss: 0.22493954002857208\n",
      "iteration 18450: loss: 0.2249385118484497\n",
      "iteration 18451: loss: 0.2249375283718109\n",
      "iteration 18452: loss: 0.22493663430213928\n",
      "iteration 18453: loss: 0.22493569552898407\n",
      "iteration 18454: loss: 0.22493469715118408\n",
      "iteration 18455: loss: 0.2249336987733841\n",
      "iteration 18456: loss: 0.22493278980255127\n",
      "iteration 18457: loss: 0.22493180632591248\n",
      "iteration 18458: loss: 0.2249308079481125\n",
      "iteration 18459: loss: 0.22492988407611847\n",
      "iteration 18460: loss: 0.22492890059947968\n",
      "iteration 18461: loss: 0.22492794692516327\n",
      "iteration 18462: loss: 0.22492703795433044\n",
      "iteration 18463: loss: 0.22492602467536926\n",
      "iteration 18464: loss: 0.22492504119873047\n",
      "iteration 18465: loss: 0.22492404282093048\n",
      "iteration 18466: loss: 0.22492308914661407\n",
      "iteration 18467: loss: 0.22492210566997528\n",
      "iteration 18468: loss: 0.22492115199565887\n",
      "iteration 18469: loss: 0.22492022812366486\n",
      "iteration 18470: loss: 0.2249191701412201\n",
      "iteration 18471: loss: 0.22491824626922607\n",
      "iteration 18472: loss: 0.22491736710071564\n",
      "iteration 18473: loss: 0.22491636872291565\n",
      "iteration 18474: loss: 0.22491541504859924\n",
      "iteration 18475: loss: 0.22491435706615448\n",
      "iteration 18476: loss: 0.22491343319416046\n",
      "iteration 18477: loss: 0.22491252422332764\n",
      "iteration 18478: loss: 0.22491149604320526\n",
      "iteration 18479: loss: 0.22491052746772766\n",
      "iteration 18480: loss: 0.22490963339805603\n",
      "iteration 18481: loss: 0.22490863502025604\n",
      "iteration 18482: loss: 0.22490763664245605\n",
      "iteration 18483: loss: 0.22490672767162323\n",
      "iteration 18484: loss: 0.22490575909614563\n",
      "iteration 18485: loss: 0.22490474581718445\n",
      "iteration 18486: loss: 0.22490379214286804\n",
      "iteration 18487: loss: 0.22490286827087402\n",
      "iteration 18488: loss: 0.22490186989307404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 18489: loss: 0.22490087151527405\n",
      "iteration 18490: loss: 0.22489997744560242\n",
      "iteration 18491: loss: 0.2248990535736084\n",
      "iteration 18492: loss: 0.2248980551958084\n",
      "iteration 18493: loss: 0.22489705681800842\n",
      "iteration 18494: loss: 0.2248961180448532\n",
      "iteration 18495: loss: 0.22489511966705322\n",
      "iteration 18496: loss: 0.22489416599273682\n",
      "iteration 18497: loss: 0.22489318251609802\n",
      "iteration 18498: loss: 0.224892258644104\n",
      "iteration 18499: loss: 0.22489123046398163\n",
      "iteration 18500: loss: 0.22489023208618164\n",
      "iteration 18501: loss: 0.22488930821418762\n",
      "iteration 18502: loss: 0.22488835453987122\n",
      "iteration 18503: loss: 0.22488737106323242\n",
      "iteration 18504: loss: 0.22488649189472198\n",
      "iteration 18505: loss: 0.224885493516922\n",
      "iteration 18506: loss: 0.2248845100402832\n",
      "iteration 18507: loss: 0.2248835563659668\n",
      "iteration 18508: loss: 0.22488263249397278\n",
      "iteration 18509: loss: 0.2248816192150116\n",
      "iteration 18510: loss: 0.22488072514533997\n",
      "iteration 18511: loss: 0.2248796969652176\n",
      "iteration 18512: loss: 0.22487878799438477\n",
      "iteration 18513: loss: 0.22487778961658478\n",
      "iteration 18514: loss: 0.2248767912387848\n",
      "iteration 18515: loss: 0.22487595677375793\n",
      "iteration 18516: loss: 0.22487497329711914\n",
      "iteration 18517: loss: 0.22487394511699677\n",
      "iteration 18518: loss: 0.2248729169368744\n",
      "iteration 18519: loss: 0.22487202286720276\n",
      "iteration 18520: loss: 0.22487100958824158\n",
      "iteration 18521: loss: 0.22487004101276398\n",
      "iteration 18522: loss: 0.22486910223960876\n",
      "iteration 18523: loss: 0.22486814856529236\n",
      "iteration 18524: loss: 0.22486717998981476\n",
      "iteration 18525: loss: 0.22486618161201477\n",
      "iteration 18526: loss: 0.22486531734466553\n",
      "iteration 18527: loss: 0.22486433386802673\n",
      "iteration 18528: loss: 0.22486333549022675\n",
      "iteration 18529: loss: 0.22486238181591034\n",
      "iteration 18530: loss: 0.22486147284507751\n",
      "iteration 18531: loss: 0.22486050426959991\n",
      "iteration 18532: loss: 0.22485950589179993\n",
      "iteration 18533: loss: 0.22485849261283875\n",
      "iteration 18534: loss: 0.22485752403736115\n",
      "iteration 18535: loss: 0.22485658526420593\n",
      "iteration 18536: loss: 0.22485563158988953\n",
      "iteration 18537: loss: 0.2248547077178955\n",
      "iteration 18538: loss: 0.2248537242412567\n",
      "iteration 18539: loss: 0.22485271096229553\n",
      "iteration 18540: loss: 0.22485175728797913\n",
      "iteration 18541: loss: 0.2248508185148239\n",
      "iteration 18542: loss: 0.22484982013702393\n",
      "iteration 18543: loss: 0.22484886646270752\n",
      "iteration 18544: loss: 0.2248479574918747\n",
      "iteration 18545: loss: 0.2248469889163971\n",
      "iteration 18546: loss: 0.2248460352420807\n",
      "iteration 18547: loss: 0.2248450517654419\n",
      "iteration 18548: loss: 0.22484412789344788\n",
      "iteration 18549: loss: 0.2248431146144867\n",
      "iteration 18550: loss: 0.22484219074249268\n",
      "iteration 18551: loss: 0.22484120726585388\n",
      "iteration 18552: loss: 0.22484025359153748\n",
      "iteration 18553: loss: 0.2248392403125763\n",
      "iteration 18554: loss: 0.22483833134174347\n",
      "iteration 18555: loss: 0.22483737766742706\n",
      "iteration 18556: loss: 0.22483639419078827\n",
      "iteration 18557: loss: 0.22483539581298828\n",
      "iteration 18558: loss: 0.22483447194099426\n",
      "iteration 18559: loss: 0.22483353316783905\n",
      "iteration 18560: loss: 0.22483253479003906\n",
      "iteration 18561: loss: 0.2248315066099167\n",
      "iteration 18562: loss: 0.22483067214488983\n",
      "iteration 18563: loss: 0.22482971847057343\n",
      "iteration 18564: loss: 0.22482867538928986\n",
      "iteration 18565: loss: 0.22482779622077942\n",
      "iteration 18566: loss: 0.22482681274414062\n",
      "iteration 18567: loss: 0.22482581436634064\n",
      "iteration 18568: loss: 0.22482486069202423\n",
      "iteration 18569: loss: 0.2248239517211914\n",
      "iteration 18570: loss: 0.2248229682445526\n",
      "iteration 18571: loss: 0.22482196986675262\n",
      "iteration 18572: loss: 0.22482100129127502\n",
      "iteration 18573: loss: 0.2248200625181198\n",
      "iteration 18574: loss: 0.2248190939426422\n",
      "iteration 18575: loss: 0.22481808066368103\n",
      "iteration 18576: loss: 0.22481712698936462\n",
      "iteration 18577: loss: 0.2248162478208542\n",
      "iteration 18578: loss: 0.2248152792453766\n",
      "iteration 18579: loss: 0.2248142659664154\n",
      "iteration 18580: loss: 0.22481341660022736\n",
      "iteration 18581: loss: 0.22481241822242737\n",
      "iteration 18582: loss: 0.22481143474578857\n",
      "iteration 18583: loss: 0.22481045126914978\n",
      "iteration 18584: loss: 0.2248094528913498\n",
      "iteration 18585: loss: 0.22480852901935577\n",
      "iteration 18586: loss: 0.22480762004852295\n",
      "iteration 18587: loss: 0.22480659186840057\n",
      "iteration 18588: loss: 0.2248055636882782\n",
      "iteration 18589: loss: 0.22480472922325134\n",
      "iteration 18590: loss: 0.22480376064777374\n",
      "iteration 18591: loss: 0.22480276226997375\n",
      "iteration 18592: loss: 0.22480180859565735\n",
      "iteration 18593: loss: 0.22480089962482452\n",
      "iteration 18594: loss: 0.22479991614818573\n",
      "iteration 18595: loss: 0.22479894757270813\n",
      "iteration 18596: loss: 0.22479791939258575\n",
      "iteration 18597: loss: 0.22479698061943054\n",
      "iteration 18598: loss: 0.2247960865497589\n",
      "iteration 18599: loss: 0.22479510307312012\n",
      "iteration 18600: loss: 0.22479411959648132\n",
      "iteration 18601: loss: 0.2247932404279709\n",
      "iteration 18602: loss: 0.2247922122478485\n",
      "iteration 18603: loss: 0.2247912436723709\n",
      "iteration 18604: loss: 0.22479033470153809\n",
      "iteration 18605: loss: 0.22478938102722168\n",
      "iteration 18606: loss: 0.2247883826494217\n",
      "iteration 18607: loss: 0.2247873842716217\n",
      "iteration 18608: loss: 0.22478647530078888\n",
      "iteration 18609: loss: 0.22478552162647247\n",
      "iteration 18610: loss: 0.2247844934463501\n",
      "iteration 18611: loss: 0.22478358447551727\n",
      "iteration 18612: loss: 0.22478261590003967\n",
      "iteration 18613: loss: 0.22478166222572327\n",
      "iteration 18614: loss: 0.22478064894676208\n",
      "iteration 18615: loss: 0.22477972507476807\n",
      "iteration 18616: loss: 0.22477884590625763\n",
      "iteration 18617: loss: 0.22477786242961884\n",
      "iteration 18618: loss: 0.22477689385414124\n",
      "iteration 18619: loss: 0.2247759997844696\n",
      "iteration 18620: loss: 0.22477500140666962\n",
      "iteration 18621: loss: 0.22477403283119202\n",
      "iteration 18622: loss: 0.22477304935455322\n",
      "iteration 18623: loss: 0.2247721254825592\n",
      "iteration 18624: loss: 0.22477109730243683\n",
      "iteration 18625: loss: 0.22477015852928162\n",
      "iteration 18626: loss: 0.2247692048549652\n",
      "iteration 18627: loss: 0.22476831078529358\n",
      "iteration 18628: loss: 0.2247672975063324\n",
      "iteration 18629: loss: 0.2247663289308548\n",
      "iteration 18630: loss: 0.2247653752565384\n",
      "iteration 18631: loss: 0.2247643917798996\n",
      "iteration 18632: loss: 0.22476348280906677\n",
      "iteration 18633: loss: 0.22476251423358917\n",
      "iteration 18634: loss: 0.22476160526275635\n",
      "iteration 18635: loss: 0.22476062178611755\n",
      "iteration 18636: loss: 0.22475974261760712\n",
      "iteration 18637: loss: 0.22475869953632355\n",
      "iteration 18638: loss: 0.2247578203678131\n",
      "iteration 18639: loss: 0.22475679218769073\n",
      "iteration 18640: loss: 0.22475579380989075\n",
      "iteration 18641: loss: 0.22475485503673553\n",
      "iteration 18642: loss: 0.22475385665893555\n",
      "iteration 18643: loss: 0.22475290298461914\n",
      "iteration 18644: loss: 0.22475199401378632\n",
      "iteration 18645: loss: 0.22475102543830872\n",
      "iteration 18646: loss: 0.2247500866651535\n",
      "iteration 18647: loss: 0.22474908828735352\n",
      "iteration 18648: loss: 0.2247481793165207\n",
      "iteration 18649: loss: 0.22474722564220428\n",
      "iteration 18650: loss: 0.2247462272644043\n",
      "iteration 18651: loss: 0.22474530339241028\n",
      "iteration 18652: loss: 0.22474431991577148\n",
      "iteration 18653: loss: 0.22474341094493866\n",
      "iteration 18654: loss: 0.22474244236946106\n",
      "iteration 18655: loss: 0.22474148869514465\n",
      "iteration 18656: loss: 0.22474050521850586\n",
      "iteration 18657: loss: 0.22473962604999542\n",
      "iteration 18658: loss: 0.22473864257335663\n",
      "iteration 18659: loss: 0.22473764419555664\n",
      "iteration 18660: loss: 0.22473672032356262\n",
      "iteration 18661: loss: 0.2247357815504074\n",
      "iteration 18662: loss: 0.22473478317260742\n",
      "iteration 18663: loss: 0.22473378479480743\n",
      "iteration 18664: loss: 0.22473280131816864\n",
      "iteration 18665: loss: 0.2247319221496582\n",
      "iteration 18666: loss: 0.2247309386730194\n",
      "iteration 18667: loss: 0.224729984998703\n",
      "iteration 18668: loss: 0.2247290164232254\n",
      "iteration 18669: loss: 0.22472813725471497\n",
      "iteration 18670: loss: 0.22472715377807617\n",
      "iteration 18671: loss: 0.22472620010375977\n",
      "iteration 18672: loss: 0.22472521662712097\n",
      "iteration 18673: loss: 0.22472432255744934\n",
      "iteration 18674: loss: 0.22472330927848816\n",
      "iteration 18675: loss: 0.22472235560417175\n",
      "iteration 18676: loss: 0.22472135722637177\n",
      "iteration 18677: loss: 0.2247205227613449\n",
      "iteration 18678: loss: 0.22471952438354492\n",
      "iteration 18679: loss: 0.22471857070922852\n",
      "iteration 18680: loss: 0.22471754252910614\n",
      "iteration 18681: loss: 0.22471657395362854\n",
      "iteration 18682: loss: 0.22471563518047333\n",
      "iteration 18683: loss: 0.2247147560119629\n",
      "iteration 18684: loss: 0.2247137576341629\n",
      "iteration 18685: loss: 0.2247127741575241\n",
      "iteration 18686: loss: 0.2247118055820465\n",
      "iteration 18687: loss: 0.22471097111701965\n",
      "iteration 18688: loss: 0.22470994293689728\n",
      "iteration 18689: loss: 0.22470898926258087\n",
      "iteration 18690: loss: 0.22470799088478088\n",
      "iteration 18691: loss: 0.2247070074081421\n",
      "iteration 18692: loss: 0.22470609843730927\n",
      "iteration 18693: loss: 0.22470514476299286\n",
      "iteration 18694: loss: 0.22470417618751526\n",
      "iteration 18695: loss: 0.22470322251319885\n",
      "iteration 18696: loss: 0.22470231354236603\n",
      "iteration 18697: loss: 0.22470135986804962\n",
      "iteration 18698: loss: 0.22470040619373322\n",
      "iteration 18699: loss: 0.22469942271709442\n",
      "iteration 18700: loss: 0.22469846904277802\n",
      "iteration 18701: loss: 0.22469758987426758\n",
      "iteration 18702: loss: 0.22469660639762878\n",
      "iteration 18703: loss: 0.2246955931186676\n",
      "iteration 18704: loss: 0.2246946543455124\n",
      "iteration 18705: loss: 0.22469362616539001\n",
      "iteration 18706: loss: 0.22469279170036316\n",
      "iteration 18707: loss: 0.22469182312488556\n",
      "iteration 18708: loss: 0.22469083964824677\n",
      "iteration 18709: loss: 0.22468988597393036\n",
      "iteration 18710: loss: 0.22468896210193634\n",
      "iteration 18711: loss: 0.22468800842761993\n",
      "iteration 18712: loss: 0.22468698024749756\n",
      "iteration 18713: loss: 0.22468610107898712\n",
      "iteration 18714: loss: 0.22468511760234833\n",
      "iteration 18715: loss: 0.22468416392803192\n",
      "iteration 18716: loss: 0.22468316555023193\n",
      "iteration 18717: loss: 0.2246823012828827\n",
      "iteration 18718: loss: 0.22468134760856628\n",
      "iteration 18719: loss: 0.2246803492307663\n",
      "iteration 18720: loss: 0.2246793508529663\n",
      "iteration 18721: loss: 0.2246783971786499\n",
      "iteration 18722: loss: 0.22467760741710663\n",
      "iteration 18723: loss: 0.22467665374279022\n",
      "iteration 18724: loss: 0.22467561066150665\n",
      "iteration 18725: loss: 0.22467465698719025\n",
      "iteration 18726: loss: 0.2246737778186798\n",
      "iteration 18727: loss: 0.22467276453971863\n",
      "iteration 18728: loss: 0.22467179596424103\n",
      "iteration 18729: loss: 0.22467079758644104\n",
      "iteration 18730: loss: 0.22466988861560822\n",
      "iteration 18731: loss: 0.22466886043548584\n",
      "iteration 18732: loss: 0.2246679961681366\n",
      "iteration 18733: loss: 0.2246670424938202\n",
      "iteration 18734: loss: 0.2246660739183426\n",
      "iteration 18735: loss: 0.2246650755405426\n",
      "iteration 18736: loss: 0.22466424107551575\n",
      "iteration 18737: loss: 0.22466322779655457\n",
      "iteration 18738: loss: 0.22466222941875458\n",
      "iteration 18739: loss: 0.22466132044792175\n",
      "iteration 18740: loss: 0.22466032207012177\n",
      "iteration 18741: loss: 0.22465939819812775\n",
      "iteration 18742: loss: 0.22465848922729492\n",
      "iteration 18743: loss: 0.22465753555297852\n",
      "iteration 18744: loss: 0.22465655207633972\n",
      "iteration 18745: loss: 0.2246556282043457\n",
      "iteration 18746: loss: 0.22465470433235168\n",
      "iteration 18747: loss: 0.2246536761522293\n",
      "iteration 18748: loss: 0.2246527224779129\n",
      "iteration 18749: loss: 0.22465172410011292\n",
      "iteration 18750: loss: 0.2246507704257965\n",
      "iteration 18751: loss: 0.22464990615844727\n",
      "iteration 18752: loss: 0.22464890778064728\n",
      "iteration 18753: loss: 0.22464799880981445\n",
      "iteration 18754: loss: 0.22464695572853088\n",
      "iteration 18755: loss: 0.22464601695537567\n",
      "iteration 18756: loss: 0.22464513778686523\n",
      "iteration 18757: loss: 0.22464421391487122\n",
      "iteration 18758: loss: 0.224643275141716\n",
      "iteration 18759: loss: 0.2246423065662384\n",
      "iteration 18760: loss: 0.224641352891922\n",
      "iteration 18761: loss: 0.2246403992176056\n",
      "iteration 18762: loss: 0.2246393859386444\n",
      "iteration 18763: loss: 0.22463850677013397\n",
      "iteration 18764: loss: 0.22463755309581757\n",
      "iteration 18765: loss: 0.22463658452033997\n",
      "iteration 18766: loss: 0.22463560104370117\n",
      "iteration 18767: loss: 0.22463461756706238\n",
      "iteration 18768: loss: 0.22463376820087433\n",
      "iteration 18769: loss: 0.22463274002075195\n",
      "iteration 18770: loss: 0.22463178634643555\n",
      "iteration 18771: loss: 0.22463083267211914\n",
      "iteration 18772: loss: 0.22462987899780273\n",
      "iteration 18773: loss: 0.22462904453277588\n",
      "iteration 18774: loss: 0.22462806105613708\n",
      "iteration 18775: loss: 0.2246270626783371\n",
      "iteration 18776: loss: 0.2246261090040207\n",
      "iteration 18777: loss: 0.22462520003318787\n",
      "iteration 18778: loss: 0.22462427616119385\n",
      "iteration 18779: loss: 0.22462327778339386\n",
      "iteration 18780: loss: 0.22462236881256104\n",
      "iteration 18781: loss: 0.22462138533592224\n",
      "iteration 18782: loss: 0.22462043166160583\n",
      "iteration 18783: loss: 0.22461946308612823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 18784: loss: 0.22461852431297302\n",
      "iteration 18785: loss: 0.22461757063865662\n",
      "iteration 18786: loss: 0.22461660206317902\n",
      "iteration 18787: loss: 0.2246156483888626\n",
      "iteration 18788: loss: 0.22461473941802979\n",
      "iteration 18789: loss: 0.224613755941391\n",
      "iteration 18790: loss: 0.22461283206939697\n",
      "iteration 18791: loss: 0.22461187839508057\n",
      "iteration 18792: loss: 0.22461089491844177\n",
      "iteration 18793: loss: 0.22461001574993134\n",
      "iteration 18794: loss: 0.22460904717445374\n",
      "iteration 18795: loss: 0.22460810840129852\n",
      "iteration 18796: loss: 0.22460713982582092\n",
      "iteration 18797: loss: 0.22460615634918213\n",
      "iteration 18798: loss: 0.2246052473783493\n",
      "iteration 18799: loss: 0.22460432350635529\n",
      "iteration 18800: loss: 0.2246033400297165\n",
      "iteration 18801: loss: 0.2246023416519165\n",
      "iteration 18802: loss: 0.22460143268108368\n",
      "iteration 18803: loss: 0.22460046410560608\n",
      "iteration 18804: loss: 0.22459952533245087\n",
      "iteration 18805: loss: 0.22459855675697327\n",
      "iteration 18806: loss: 0.22459769248962402\n",
      "iteration 18807: loss: 0.22459666430950165\n",
      "iteration 18808: loss: 0.22459569573402405\n",
      "iteration 18809: loss: 0.22459478676319122\n",
      "iteration 18810: loss: 0.22459392249584198\n",
      "iteration 18811: loss: 0.224592924118042\n",
      "iteration 18812: loss: 0.22459197044372559\n",
      "iteration 18813: loss: 0.22459101676940918\n",
      "iteration 18814: loss: 0.2245900183916092\n",
      "iteration 18815: loss: 0.22458915412425995\n",
      "iteration 18816: loss: 0.22458815574645996\n",
      "iteration 18817: loss: 0.22458720207214355\n",
      "iteration 18818: loss: 0.22458621859550476\n",
      "iteration 18819: loss: 0.22458525002002716\n",
      "iteration 18820: loss: 0.22458431124687195\n",
      "iteration 18821: loss: 0.22458334267139435\n",
      "iteration 18822: loss: 0.2245824635028839\n",
      "iteration 18823: loss: 0.2245815098285675\n",
      "iteration 18824: loss: 0.2245805263519287\n",
      "iteration 18825: loss: 0.2245796173810959\n",
      "iteration 18826: loss: 0.22457876801490784\n",
      "iteration 18827: loss: 0.22457775473594666\n",
      "iteration 18828: loss: 0.22457678616046906\n",
      "iteration 18829: loss: 0.22457583248615265\n",
      "iteration 18830: loss: 0.22457484900951385\n",
      "iteration 18831: loss: 0.22457396984100342\n",
      "iteration 18832: loss: 0.224573016166687\n",
      "iteration 18833: loss: 0.2245720624923706\n",
      "iteration 18834: loss: 0.2245711088180542\n",
      "iteration 18835: loss: 0.22457008063793182\n",
      "iteration 18836: loss: 0.224569171667099\n",
      "iteration 18837: loss: 0.22456827759742737\n",
      "iteration 18838: loss: 0.22456736862659454\n",
      "iteration 18839: loss: 0.22456641495227814\n",
      "iteration 18840: loss: 0.22456541657447815\n",
      "iteration 18841: loss: 0.22456443309783936\n",
      "iteration 18842: loss: 0.22456350922584534\n",
      "iteration 18843: loss: 0.22456257045269012\n",
      "iteration 18844: loss: 0.2245616912841797\n",
      "iteration 18845: loss: 0.22456073760986328\n",
      "iteration 18846: loss: 0.2245597392320633\n",
      "iteration 18847: loss: 0.2245587855577469\n",
      "iteration 18848: loss: 0.2245578020811081\n",
      "iteration 18849: loss: 0.22455696761608124\n",
      "iteration 18850: loss: 0.22455599904060364\n",
      "iteration 18851: loss: 0.22455504536628723\n",
      "iteration 18852: loss: 0.22455401718616486\n",
      "iteration 18853: loss: 0.22455303370952606\n",
      "iteration 18854: loss: 0.224552184343338\n",
      "iteration 18855: loss: 0.2245512753725052\n",
      "iteration 18856: loss: 0.2245502471923828\n",
      "iteration 18857: loss: 0.2245492935180664\n",
      "iteration 18858: loss: 0.22454842925071716\n",
      "iteration 18859: loss: 0.22454741597175598\n",
      "iteration 18860: loss: 0.22454652190208435\n",
      "iteration 18861: loss: 0.22454556822776794\n",
      "iteration 18862: loss: 0.22454464435577393\n",
      "iteration 18863: loss: 0.2245437204837799\n",
      "iteration 18864: loss: 0.22454270720481873\n",
      "iteration 18865: loss: 0.2245417833328247\n",
      "iteration 18866: loss: 0.2245407998561859\n",
      "iteration 18867: loss: 0.2245398759841919\n",
      "iteration 18868: loss: 0.2245388925075531\n",
      "iteration 18869: loss: 0.22453801333904266\n",
      "iteration 18870: loss: 0.22453704476356506\n",
      "iteration 18871: loss: 0.22453610599040985\n",
      "iteration 18872: loss: 0.22453510761260986\n",
      "iteration 18873: loss: 0.22453418374061584\n",
      "iteration 18874: loss: 0.22453320026397705\n",
      "iteration 18875: loss: 0.2245323359966278\n",
      "iteration 18876: loss: 0.22453133761882782\n",
      "iteration 18877: loss: 0.22453045845031738\n",
      "iteration 18878: loss: 0.22452950477600098\n",
      "iteration 18879: loss: 0.224528506398201\n",
      "iteration 18880: loss: 0.22452759742736816\n",
      "iteration 18881: loss: 0.22452668845653534\n",
      "iteration 18882: loss: 0.22452566027641296\n",
      "iteration 18883: loss: 0.22452476620674133\n",
      "iteration 18884: loss: 0.22452382743358612\n",
      "iteration 18885: loss: 0.22452287375926971\n",
      "iteration 18886: loss: 0.2245219200849533\n",
      "iteration 18887: loss: 0.2245209962129593\n",
      "iteration 18888: loss: 0.2245199978351593\n",
      "iteration 18889: loss: 0.2245190590620041\n",
      "iteration 18890: loss: 0.22451817989349365\n",
      "iteration 18891: loss: 0.22451718151569366\n",
      "iteration 18892: loss: 0.22451630234718323\n",
      "iteration 18893: loss: 0.22451536357402802\n",
      "iteration 18894: loss: 0.22451433539390564\n",
      "iteration 18895: loss: 0.2245134562253952\n",
      "iteration 18896: loss: 0.22451253235340118\n",
      "iteration 18897: loss: 0.22451157867908478\n",
      "iteration 18898: loss: 0.22451059520244598\n",
      "iteration 18899: loss: 0.22450967133045197\n",
      "iteration 18900: loss: 0.2245086431503296\n",
      "iteration 18901: loss: 0.22450780868530273\n",
      "iteration 18902: loss: 0.22450682520866394\n",
      "iteration 18903: loss: 0.22450585663318634\n",
      "iteration 18904: loss: 0.22450485825538635\n",
      "iteration 18905: loss: 0.22450394928455353\n",
      "iteration 18906: loss: 0.2245030701160431\n",
      "iteration 18907: loss: 0.22450211644172668\n",
      "iteration 18908: loss: 0.22450117766857147\n",
      "iteration 18909: loss: 0.22450017929077148\n",
      "iteration 18910: loss: 0.22449925541877747\n",
      "iteration 18911: loss: 0.22449834644794464\n",
      "iteration 18912: loss: 0.22449734807014465\n",
      "iteration 18913: loss: 0.2244965136051178\n",
      "iteration 18914: loss: 0.22449550032615662\n",
      "iteration 18915: loss: 0.2244945764541626\n",
      "iteration 18916: loss: 0.2244935929775238\n",
      "iteration 18917: loss: 0.2244926393032074\n",
      "iteration 18918: loss: 0.22449171543121338\n",
      "iteration 18919: loss: 0.22449079155921936\n",
      "iteration 18920: loss: 0.22448983788490295\n",
      "iteration 18921: loss: 0.22448892891407013\n",
      "iteration 18922: loss: 0.22448797523975372\n",
      "iteration 18923: loss: 0.22448697686195374\n",
      "iteration 18924: loss: 0.22448602318763733\n",
      "iteration 18925: loss: 0.22448520362377167\n",
      "iteration 18926: loss: 0.2244841605424881\n",
      "iteration 18927: loss: 0.22448325157165527\n",
      "iteration 18928: loss: 0.22448232769966125\n",
      "iteration 18929: loss: 0.22448134422302246\n",
      "iteration 18930: loss: 0.22448042035102844\n",
      "iteration 18931: loss: 0.2244795262813568\n",
      "iteration 18932: loss: 0.2244785726070404\n",
      "iteration 18933: loss: 0.224477618932724\n",
      "iteration 18934: loss: 0.2244766652584076\n",
      "iteration 18935: loss: 0.2244756519794464\n",
      "iteration 18936: loss: 0.22447475790977478\n",
      "iteration 18937: loss: 0.22447387874126434\n",
      "iteration 18938: loss: 0.22447283565998077\n",
      "iteration 18939: loss: 0.22447200119495392\n",
      "iteration 18940: loss: 0.22447100281715393\n",
      "iteration 18941: loss: 0.22447004914283752\n",
      "iteration 18942: loss: 0.22446906566619873\n",
      "iteration 18943: loss: 0.22446808218955994\n",
      "iteration 18944: loss: 0.22446727752685547\n",
      "iteration 18945: loss: 0.22446636855602264\n",
      "iteration 18946: loss: 0.22446539998054504\n",
      "iteration 18947: loss: 0.22446441650390625\n",
      "iteration 18948: loss: 0.22446343302726746\n",
      "iteration 18949: loss: 0.2244625836610794\n",
      "iteration 18950: loss: 0.2244616448879242\n",
      "iteration 18951: loss: 0.2244606465101242\n",
      "iteration 18952: loss: 0.22445973753929138\n",
      "iteration 18953: loss: 0.2244587242603302\n",
      "iteration 18954: loss: 0.2244577407836914\n",
      "iteration 18955: loss: 0.22445693612098694\n",
      "iteration 18956: loss: 0.22445595264434814\n",
      "iteration 18957: loss: 0.22445496916770935\n",
      "iteration 18958: loss: 0.2244540899991989\n",
      "iteration 18959: loss: 0.22445309162139893\n",
      "iteration 18960: loss: 0.2244521826505661\n",
      "iteration 18961: loss: 0.22445125877857208\n",
      "iteration 18962: loss: 0.22445030510425568\n",
      "iteration 18963: loss: 0.22444936633110046\n",
      "iteration 18964: loss: 0.22444839775562286\n",
      "iteration 18965: loss: 0.22444748878479004\n",
      "iteration 18966: loss: 0.2244466245174408\n",
      "iteration 18967: loss: 0.22444561123847961\n",
      "iteration 18968: loss: 0.2244446724653244\n",
      "iteration 18969: loss: 0.22444376349449158\n",
      "iteration 18970: loss: 0.224442720413208\n",
      "iteration 18971: loss: 0.22444181144237518\n",
      "iteration 18972: loss: 0.2244408130645752\n",
      "iteration 18973: loss: 0.22443997859954834\n",
      "iteration 18974: loss: 0.22443902492523193\n",
      "iteration 18975: loss: 0.22443804144859314\n",
      "iteration 18976: loss: 0.2244371622800827\n",
      "iteration 18977: loss: 0.2244361937046051\n",
      "iteration 18978: loss: 0.22443528473377228\n",
      "iteration 18979: loss: 0.22443430125713348\n",
      "iteration 18980: loss: 0.22443345189094543\n",
      "iteration 18981: loss: 0.22443249821662903\n",
      "iteration 18982: loss: 0.22443151473999023\n",
      "iteration 18983: loss: 0.22443056106567383\n",
      "iteration 18984: loss: 0.22442960739135742\n",
      "iteration 18985: loss: 0.2244286984205246\n",
      "iteration 18986: loss: 0.22442777454853058\n",
      "iteration 18987: loss: 0.22442683577537537\n",
      "iteration 18988: loss: 0.22442588210105896\n",
      "iteration 18989: loss: 0.22442491352558136\n",
      "iteration 18990: loss: 0.22442397475242615\n",
      "iteration 18991: loss: 0.22442308068275452\n",
      "iteration 18992: loss: 0.2244221717119217\n",
      "iteration 18993: loss: 0.2244211733341217\n",
      "iteration 18994: loss: 0.2244202196598053\n",
      "iteration 18995: loss: 0.2244192659854889\n",
      "iteration 18996: loss: 0.22441837191581726\n",
      "iteration 18997: loss: 0.22441749274730682\n",
      "iteration 18998: loss: 0.22441652417182922\n",
      "iteration 18999: loss: 0.22441557049751282\n",
      "iteration 19000: loss: 0.22441458702087402\n",
      "iteration 19001: loss: 0.22441363334655762\n",
      "iteration 19002: loss: 0.2244127243757248\n",
      "iteration 19003: loss: 0.22441184520721436\n",
      "iteration 19004: loss: 0.22441092133522034\n",
      "iteration 19005: loss: 0.22440990805625916\n",
      "iteration 19006: loss: 0.22440898418426514\n",
      "iteration 19007: loss: 0.22440806031227112\n",
      "iteration 19008: loss: 0.22440704703330994\n",
      "iteration 19009: loss: 0.22440609335899353\n",
      "iteration 19010: loss: 0.22440525889396667\n",
      "iteration 19011: loss: 0.22440433502197266\n",
      "iteration 19012: loss: 0.22440338134765625\n",
      "iteration 19013: loss: 0.22440238296985626\n",
      "iteration 19014: loss: 0.2244015634059906\n",
      "iteration 19015: loss: 0.22440055012702942\n",
      "iteration 19016: loss: 0.2243996113538742\n",
      "iteration 19017: loss: 0.2243986874818802\n",
      "iteration 19018: loss: 0.22439773380756378\n",
      "iteration 19019: loss: 0.22439685463905334\n",
      "iteration 19020: loss: 0.22439582645893097\n",
      "iteration 19021: loss: 0.22439494729042053\n",
      "iteration 19022: loss: 0.2243940383195877\n",
      "iteration 19023: loss: 0.2243930548429489\n",
      "iteration 19024: loss: 0.2243921309709549\n",
      "iteration 19025: loss: 0.2243911325931549\n",
      "iteration 19026: loss: 0.22439023852348328\n",
      "iteration 19027: loss: 0.22438926994800568\n",
      "iteration 19028: loss: 0.22438833117485046\n",
      "iteration 19029: loss: 0.22438740730285645\n",
      "iteration 19030: loss: 0.22438649833202362\n",
      "iteration 19031: loss: 0.2243855893611908\n",
      "iteration 19032: loss: 0.22438466548919678\n",
      "iteration 19033: loss: 0.22438374161720276\n",
      "iteration 19034: loss: 0.22438272833824158\n",
      "iteration 19035: loss: 0.22438177466392517\n",
      "iteration 19036: loss: 0.22438080608844757\n",
      "iteration 19037: loss: 0.22437997162342072\n",
      "iteration 19038: loss: 0.22437898814678192\n",
      "iteration 19039: loss: 0.2243780791759491\n",
      "iteration 19040: loss: 0.22437706589698792\n",
      "iteration 19041: loss: 0.22437623143196106\n",
      "iteration 19042: loss: 0.22437520325183868\n",
      "iteration 19043: loss: 0.22437426447868347\n",
      "iteration 19044: loss: 0.22437341511249542\n",
      "iteration 19045: loss: 0.2243725061416626\n",
      "iteration 19046: loss: 0.2243715226650238\n",
      "iteration 19047: loss: 0.2243705689907074\n",
      "iteration 19048: loss: 0.22436967492103577\n",
      "iteration 19049: loss: 0.22436872124671936\n",
      "iteration 19050: loss: 0.22436773777008057\n",
      "iteration 19051: loss: 0.22436678409576416\n",
      "iteration 19052: loss: 0.2243659496307373\n",
      "iteration 19053: loss: 0.22436495125293732\n",
      "iteration 19054: loss: 0.22436411678791046\n",
      "iteration 19055: loss: 0.22436311841011047\n",
      "iteration 19056: loss: 0.22436217963695526\n",
      "iteration 19057: loss: 0.22436121106147766\n",
      "iteration 19058: loss: 0.22436030209064484\n",
      "iteration 19059: loss: 0.22435936331748962\n",
      "iteration 19060: loss: 0.22435840964317322\n",
      "iteration 19061: loss: 0.2243575155735016\n",
      "iteration 19062: loss: 0.22435660660266876\n",
      "iteration 19063: loss: 0.22435560822486877\n",
      "iteration 19064: loss: 0.22435462474822998\n",
      "iteration 19065: loss: 0.22435379028320312\n",
      "iteration 19066: loss: 0.22435280680656433\n",
      "iteration 19067: loss: 0.2243518829345703\n",
      "iteration 19068: loss: 0.2243509292602539\n",
      "iteration 19069: loss: 0.22435002028942108\n",
      "iteration 19070: loss: 0.2243490219116211\n",
      "iteration 19071: loss: 0.22434809803962708\n",
      "iteration 19072: loss: 0.22434718906879425\n",
      "iteration 19073: loss: 0.2243463099002838\n",
      "iteration 19074: loss: 0.224345400929451\n",
      "iteration 19075: loss: 0.224344402551651\n",
      "iteration 19076: loss: 0.22434349358081818\n",
      "iteration 19077: loss: 0.22434251010417938\n",
      "iteration 19078: loss: 0.22434160113334656\n",
      "iteration 19079: loss: 0.22434067726135254\n",
      "iteration 19080: loss: 0.22433975338935852\n",
      "iteration 19081: loss: 0.22433874011039734\n",
      "iteration 19082: loss: 0.22433790564537048\n",
      "iteration 19083: loss: 0.22433695197105408\n",
      "iteration 19084: loss: 0.22433599829673767\n",
      "iteration 19085: loss: 0.22433504462242126\n",
      "iteration 19086: loss: 0.22433412075042725\n",
      "iteration 19087: loss: 0.224333256483078\n",
      "iteration 19088: loss: 0.22433225810527802\n",
      "iteration 19089: loss: 0.224331334233284\n",
      "iteration 19090: loss: 0.2243303805589676\n",
      "iteration 19091: loss: 0.22432942688465118\n",
      "iteration 19092: loss: 0.22432848811149597\n",
      "iteration 19093: loss: 0.22432760894298553\n",
      "iteration 19094: loss: 0.22432665526866913\n",
      "iteration 19095: loss: 0.22432568669319153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19096: loss: 0.2243247777223587\n",
      "iteration 19097: loss: 0.22432394325733185\n",
      "iteration 19098: loss: 0.22432298958301544\n",
      "iteration 19099: loss: 0.22432203590869904\n",
      "iteration 19100: loss: 0.22432105243206024\n",
      "iteration 19101: loss: 0.22432012856006622\n",
      "iteration 19102: loss: 0.22431913018226624\n",
      "iteration 19103: loss: 0.22431831061840057\n",
      "iteration 19104: loss: 0.22431735694408417\n",
      "iteration 19105: loss: 0.22431643307209015\n",
      "iteration 19106: loss: 0.22431547939777374\n",
      "iteration 19107: loss: 0.22431449592113495\n",
      "iteration 19108: loss: 0.22431358695030212\n",
      "iteration 19109: loss: 0.22431263327598572\n",
      "iteration 19110: loss: 0.22431175410747528\n",
      "iteration 19111: loss: 0.22431084513664246\n",
      "iteration 19112: loss: 0.22430992126464844\n",
      "iteration 19113: loss: 0.22430893778800964\n",
      "iteration 19114: loss: 0.22430801391601562\n",
      "iteration 19115: loss: 0.22430706024169922\n",
      "iteration 19116: loss: 0.22430618107318878\n",
      "iteration 19117: loss: 0.22430524230003357\n",
      "iteration 19118: loss: 0.22430434823036194\n",
      "iteration 19119: loss: 0.22430339455604553\n",
      "iteration 19120: loss: 0.22430245578289032\n",
      "iteration 19121: loss: 0.2243015021085739\n",
      "iteration 19122: loss: 0.2243005484342575\n",
      "iteration 19123: loss: 0.2242995947599411\n",
      "iteration 19124: loss: 0.22429867088794708\n",
      "iteration 19125: loss: 0.22429773211479187\n",
      "iteration 19126: loss: 0.22429680824279785\n",
      "iteration 19127: loss: 0.224295973777771\n",
      "iteration 19128: loss: 0.2242950201034546\n",
      "iteration 19129: loss: 0.22429409623146057\n",
      "iteration 19130: loss: 0.22429314255714417\n",
      "iteration 19131: loss: 0.22429220378398895\n",
      "iteration 19132: loss: 0.22429125010967255\n",
      "iteration 19133: loss: 0.22429034113883972\n",
      "iteration 19134: loss: 0.22428938746452332\n",
      "iteration 19135: loss: 0.2242884337902069\n",
      "iteration 19136: loss: 0.22428755462169647\n",
      "iteration 19137: loss: 0.22428663074970245\n",
      "iteration 19138: loss: 0.22428564727306366\n",
      "iteration 19139: loss: 0.22428472340106964\n",
      "iteration 19140: loss: 0.22428376972675323\n",
      "iteration 19141: loss: 0.22428283095359802\n",
      "iteration 19142: loss: 0.224281907081604\n",
      "iteration 19143: loss: 0.22428110241889954\n",
      "iteration 19144: loss: 0.22428016364574432\n",
      "iteration 19145: loss: 0.2242792546749115\n",
      "iteration 19146: loss: 0.2242782562971115\n",
      "iteration 19147: loss: 0.2242773026227951\n",
      "iteration 19148: loss: 0.22427639365196228\n",
      "iteration 19149: loss: 0.22427546977996826\n",
      "iteration 19150: loss: 0.22427448630332947\n",
      "iteration 19151: loss: 0.22427363693714142\n",
      "iteration 19152: loss: 0.22427260875701904\n",
      "iteration 19153: loss: 0.2242717295885086\n",
      "iteration 19154: loss: 0.22427082061767578\n",
      "iteration 19155: loss: 0.22426986694335938\n",
      "iteration 19156: loss: 0.22426891326904297\n",
      "iteration 19157: loss: 0.22426795959472656\n",
      "iteration 19158: loss: 0.22426709532737732\n",
      "iteration 19159: loss: 0.2242661714553833\n",
      "iteration 19160: loss: 0.22426524758338928\n",
      "iteration 19161: loss: 0.22426429390907288\n",
      "iteration 19162: loss: 0.22426339983940125\n",
      "iteration 19163: loss: 0.22426250576972961\n",
      "iteration 19164: loss: 0.22426152229309082\n",
      "iteration 19165: loss: 0.22426056861877441\n",
      "iteration 19166: loss: 0.2242596596479416\n",
      "iteration 19167: loss: 0.224258691072464\n",
      "iteration 19168: loss: 0.22425779700279236\n",
      "iteration 19169: loss: 0.22425687313079834\n",
      "iteration 19170: loss: 0.22425594925880432\n",
      "iteration 19171: loss: 0.2242550402879715\n",
      "iteration 19172: loss: 0.2242540419101715\n",
      "iteration 19173: loss: 0.22425313293933868\n",
      "iteration 19174: loss: 0.22425222396850586\n",
      "iteration 19175: loss: 0.22425131499767303\n",
      "iteration 19176: loss: 0.2242504060268402\n",
      "iteration 19177: loss: 0.2242494821548462\n",
      "iteration 19178: loss: 0.2242484986782074\n",
      "iteration 19179: loss: 0.22424761950969696\n",
      "iteration 19180: loss: 0.22424669563770294\n",
      "iteration 19181: loss: 0.22424566745758057\n",
      "iteration 19182: loss: 0.22424480319023132\n",
      "iteration 19183: loss: 0.2242438793182373\n",
      "iteration 19184: loss: 0.2242429256439209\n",
      "iteration 19185: loss: 0.22424204647541046\n",
      "iteration 19186: loss: 0.22424101829528809\n",
      "iteration 19187: loss: 0.22424015402793884\n",
      "iteration 19188: loss: 0.22423920035362244\n",
      "iteration 19189: loss: 0.224238321185112\n",
      "iteration 19190: loss: 0.2242373675107956\n",
      "iteration 19191: loss: 0.22423644363880157\n",
      "iteration 19192: loss: 0.22423548996448517\n",
      "iteration 19193: loss: 0.22423461079597473\n",
      "iteration 19194: loss: 0.22423359751701355\n",
      "iteration 19195: loss: 0.2242327630519867\n",
      "iteration 19196: loss: 0.2242318093776703\n",
      "iteration 19197: loss: 0.22423091530799866\n",
      "iteration 19198: loss: 0.22423000633716583\n",
      "iteration 19199: loss: 0.22422900795936584\n",
      "iteration 19200: loss: 0.22422809898853302\n",
      "iteration 19201: loss: 0.2242271602153778\n",
      "iteration 19202: loss: 0.2242262363433838\n",
      "iteration 19203: loss: 0.22422528266906738\n",
      "iteration 19204: loss: 0.22422437369823456\n",
      "iteration 19205: loss: 0.22422349452972412\n",
      "iteration 19206: loss: 0.22422249615192413\n",
      "iteration 19207: loss: 0.22422173619270325\n",
      "iteration 19208: loss: 0.22422070801258087\n",
      "iteration 19209: loss: 0.22421979904174805\n",
      "iteration 19210: loss: 0.22421884536743164\n",
      "iteration 19211: loss: 0.22421793639659882\n",
      "iteration 19212: loss: 0.2242169827222824\n",
      "iteration 19213: loss: 0.22421610355377197\n",
      "iteration 19214: loss: 0.22421510517597198\n",
      "iteration 19215: loss: 0.22421422600746155\n",
      "iteration 19216: loss: 0.22421327233314514\n",
      "iteration 19217: loss: 0.22421233355998993\n",
      "iteration 19218: loss: 0.2242114096879959\n",
      "iteration 19219: loss: 0.2242104560136795\n",
      "iteration 19220: loss: 0.2242095023393631\n",
      "iteration 19221: loss: 0.22420862317085266\n",
      "iteration 19222: loss: 0.22420768439769745\n",
      "iteration 19223: loss: 0.224206805229187\n",
      "iteration 19224: loss: 0.224205881357193\n",
      "iteration 19225: loss: 0.22420498728752136\n",
      "iteration 19226: loss: 0.22420406341552734\n",
      "iteration 19227: loss: 0.22420310974121094\n",
      "iteration 19228: loss: 0.2242022007703781\n",
      "iteration 19229: loss: 0.2242012321949005\n",
      "iteration 19230: loss: 0.2242002934217453\n",
      "iteration 19231: loss: 0.22419944405555725\n",
      "iteration 19232: loss: 0.22419846057891846\n",
      "iteration 19233: loss: 0.22419755160808563\n",
      "iteration 19234: loss: 0.2241966724395752\n",
      "iteration 19235: loss: 0.2241957187652588\n",
      "iteration 19236: loss: 0.22419483959674835\n",
      "iteration 19237: loss: 0.22419385612010956\n",
      "iteration 19238: loss: 0.22419288754463196\n",
      "iteration 19239: loss: 0.2241920530796051\n",
      "iteration 19240: loss: 0.22419103980064392\n",
      "iteration 19241: loss: 0.2241901457309723\n",
      "iteration 19242: loss: 0.22418923676013947\n",
      "iteration 19243: loss: 0.22418828308582306\n",
      "iteration 19244: loss: 0.22418737411499023\n",
      "iteration 19245: loss: 0.2241864651441574\n",
      "iteration 19246: loss: 0.22418555617332458\n",
      "iteration 19247: loss: 0.22418460249900818\n",
      "iteration 19248: loss: 0.2241836041212082\n",
      "iteration 19249: loss: 0.22418272495269775\n",
      "iteration 19250: loss: 0.22418181598186493\n",
      "iteration 19251: loss: 0.2241809070110321\n",
      "iteration 19252: loss: 0.22417998313903809\n",
      "iteration 19253: loss: 0.2241789996623993\n",
      "iteration 19254: loss: 0.22417812049388885\n",
      "iteration 19255: loss: 0.22417721152305603\n",
      "iteration 19256: loss: 0.224176287651062\n",
      "iteration 19257: loss: 0.22417540848255157\n",
      "iteration 19258: loss: 0.2241743803024292\n",
      "iteration 19259: loss: 0.22417350113391876\n",
      "iteration 19260: loss: 0.22417263686656952\n",
      "iteration 19261: loss: 0.2241717129945755\n",
      "iteration 19262: loss: 0.2241707146167755\n",
      "iteration 19263: loss: 0.2241697758436203\n",
      "iteration 19264: loss: 0.22416885197162628\n",
      "iteration 19265: loss: 0.22416801750659943\n",
      "iteration 19266: loss: 0.22416706383228302\n",
      "iteration 19267: loss: 0.2241661250591278\n",
      "iteration 19268: loss: 0.2241652011871338\n",
      "iteration 19269: loss: 0.22416432201862335\n",
      "iteration 19270: loss: 0.22416336834430695\n",
      "iteration 19271: loss: 0.22416242957115173\n",
      "iteration 19272: loss: 0.2241615355014801\n",
      "iteration 19273: loss: 0.22416064143180847\n",
      "iteration 19274: loss: 0.22415968775749207\n",
      "iteration 19275: loss: 0.22415876388549805\n",
      "iteration 19276: loss: 0.22415781021118164\n",
      "iteration 19277: loss: 0.22415685653686523\n",
      "iteration 19278: loss: 0.224155992269516\n",
      "iteration 19279: loss: 0.22415503859519958\n",
      "iteration 19280: loss: 0.22415418922901154\n",
      "iteration 19281: loss: 0.22415320575237274\n",
      "iteration 19282: loss: 0.22415228188037872\n",
      "iteration 19283: loss: 0.2241513431072235\n",
      "iteration 19284: loss: 0.22415044903755188\n",
      "iteration 19285: loss: 0.22414951026439667\n",
      "iteration 19286: loss: 0.22414866089820862\n",
      "iteration 19287: loss: 0.22414764761924744\n",
      "iteration 19288: loss: 0.224146768450737\n",
      "iteration 19289: loss: 0.22414584457874298\n",
      "iteration 19290: loss: 0.22414489090442657\n",
      "iteration 19291: loss: 0.22414402663707733\n",
      "iteration 19292: loss: 0.22414305806159973\n",
      "iteration 19293: loss: 0.2241421937942505\n",
      "iteration 19294: loss: 0.22414124011993408\n",
      "iteration 19295: loss: 0.22414028644561768\n",
      "iteration 19296: loss: 0.22413940727710724\n",
      "iteration 19297: loss: 0.22413845360279083\n",
      "iteration 19298: loss: 0.2241375893354416\n",
      "iteration 19299: loss: 0.22413663566112518\n",
      "iteration 19300: loss: 0.22413568198680878\n",
      "iteration 19301: loss: 0.22413472831249237\n",
      "iteration 19302: loss: 0.22413389384746552\n",
      "iteration 19303: loss: 0.2241329848766327\n",
      "iteration 19304: loss: 0.2241319864988327\n",
      "iteration 19305: loss: 0.22413107752799988\n",
      "iteration 19306: loss: 0.22413018345832825\n",
      "iteration 19307: loss: 0.22412922978401184\n",
      "iteration 19308: loss: 0.22412827610969543\n",
      "iteration 19309: loss: 0.2241273671388626\n",
      "iteration 19310: loss: 0.2241264283657074\n",
      "iteration 19311: loss: 0.22412554919719696\n",
      "iteration 19312: loss: 0.22412467002868652\n",
      "iteration 19313: loss: 0.2241237610578537\n",
      "iteration 19314: loss: 0.2241228073835373\n",
      "iteration 19315: loss: 0.2241218537092209\n",
      "iteration 19316: loss: 0.22412094473838806\n",
      "iteration 19317: loss: 0.22412002086639404\n",
      "iteration 19318: loss: 0.2241191416978836\n",
      "iteration 19319: loss: 0.2241182029247284\n",
      "iteration 19320: loss: 0.22411727905273438\n",
      "iteration 19321: loss: 0.22411629557609558\n",
      "iteration 19322: loss: 0.2241154909133911\n",
      "iteration 19323: loss: 0.22411449253559113\n",
      "iteration 19324: loss: 0.22411362826824188\n",
      "iteration 19325: loss: 0.22411271929740906\n",
      "iteration 19326: loss: 0.22411172091960907\n",
      "iteration 19327: loss: 0.22411087155342102\n",
      "iteration 19328: loss: 0.22410991787910461\n",
      "iteration 19329: loss: 0.2241089642047882\n",
      "iteration 19330: loss: 0.224108025431633\n",
      "iteration 19331: loss: 0.22410717606544495\n",
      "iteration 19332: loss: 0.22410626709461212\n",
      "iteration 19333: loss: 0.2241053581237793\n",
      "iteration 19334: loss: 0.22410443425178528\n",
      "iteration 19335: loss: 0.22410349547863007\n",
      "iteration 19336: loss: 0.22410254180431366\n",
      "iteration 19337: loss: 0.22410158812999725\n",
      "iteration 19338: loss: 0.2241007387638092\n",
      "iteration 19339: loss: 0.22409987449645996\n",
      "iteration 19340: loss: 0.22409895062446594\n",
      "iteration 19341: loss: 0.22409796714782715\n",
      "iteration 19342: loss: 0.22409705817699432\n",
      "iteration 19343: loss: 0.2240961492061615\n",
      "iteration 19344: loss: 0.22409525513648987\n",
      "iteration 19345: loss: 0.22409431636333466\n",
      "iteration 19346: loss: 0.22409339249134064\n",
      "iteration 19347: loss: 0.22409243881702423\n",
      "iteration 19348: loss: 0.2240915298461914\n",
      "iteration 19349: loss: 0.22409066557884216\n",
      "iteration 19350: loss: 0.22408974170684814\n",
      "iteration 19351: loss: 0.22408881783485413\n",
      "iteration 19352: loss: 0.2240878790616989\n",
      "iteration 19353: loss: 0.22408688068389893\n",
      "iteration 19354: loss: 0.22408607602119446\n",
      "iteration 19355: loss: 0.22408516705036163\n",
      "iteration 19356: loss: 0.22408422827720642\n",
      "iteration 19357: loss: 0.2240833342075348\n",
      "iteration 19358: loss: 0.22408238053321838\n",
      "iteration 19359: loss: 0.22408142685890198\n",
      "iteration 19360: loss: 0.22408048808574677\n",
      "iteration 19361: loss: 0.22407956421375275\n",
      "iteration 19362: loss: 0.2240786999464035\n",
      "iteration 19363: loss: 0.2240777462720871\n",
      "iteration 19364: loss: 0.2240767925977707\n",
      "iteration 19365: loss: 0.22407600283622742\n",
      "iteration 19366: loss: 0.2240750789642334\n",
      "iteration 19367: loss: 0.224074125289917\n",
      "iteration 19368: loss: 0.22407321631908417\n",
      "iteration 19369: loss: 0.22407236695289612\n",
      "iteration 19370: loss: 0.2240714132785797\n",
      "iteration 19371: loss: 0.2240704596042633\n",
      "iteration 19372: loss: 0.2240695059299469\n",
      "iteration 19373: loss: 0.22406864166259766\n",
      "iteration 19374: loss: 0.22406771779060364\n",
      "iteration 19375: loss: 0.224066823720932\n",
      "iteration 19376: loss: 0.22406582534313202\n",
      "iteration 19377: loss: 0.2240649163722992\n",
      "iteration 19378: loss: 0.22406402230262756\n",
      "iteration 19379: loss: 0.22406303882598877\n",
      "iteration 19380: loss: 0.22406215965747833\n",
      "iteration 19381: loss: 0.2240612804889679\n",
      "iteration 19382: loss: 0.22406037151813507\n",
      "iteration 19383: loss: 0.22405946254730225\n",
      "iteration 19384: loss: 0.2240585833787918\n",
      "iteration 19385: loss: 0.22405767440795898\n",
      "iteration 19386: loss: 0.22405675053596497\n",
      "iteration 19387: loss: 0.22405579686164856\n",
      "iteration 19388: loss: 0.22405485808849335\n",
      "iteration 19389: loss: 0.22405393421649933\n",
      "iteration 19390: loss: 0.22405299544334412\n",
      "iteration 19391: loss: 0.22405210137367249\n",
      "iteration 19392: loss: 0.22405126690864563\n",
      "iteration 19393: loss: 0.22405031323432922\n",
      "iteration 19394: loss: 0.22404935956001282\n",
      "iteration 19395: loss: 0.22404849529266357\n",
      "iteration 19396: loss: 0.22404757142066956\n",
      "iteration 19397: loss: 0.22404663264751434\n",
      "iteration 19398: loss: 0.2240457534790039\n",
      "iteration 19399: loss: 0.2240447998046875\n",
      "iteration 19400: loss: 0.22404387593269348\n",
      "iteration 19401: loss: 0.22404296696186066\n",
      "iteration 19402: loss: 0.22404202818870544\n",
      "iteration 19403: loss: 0.22404110431671143\n",
      "iteration 19404: loss: 0.22404015064239502\n",
      "iteration 19405: loss: 0.2240392416715622\n",
      "iteration 19406: loss: 0.22403831779956818\n",
      "iteration 19407: loss: 0.22403745353221893\n",
      "iteration 19408: loss: 0.22403652966022491\n",
      "iteration 19409: loss: 0.2240356206893921\n",
      "iteration 19410: loss: 0.22403471171855927\n",
      "iteration 19411: loss: 0.22403378784656525\n",
      "iteration 19412: loss: 0.22403284907341003\n",
      "iteration 19413: loss: 0.2240319550037384\n",
      "iteration 19414: loss: 0.22403106093406677\n",
      "iteration 19415: loss: 0.22403016686439514\n",
      "iteration 19416: loss: 0.22402921319007874\n",
      "iteration 19417: loss: 0.22402825951576233\n",
      "iteration 19418: loss: 0.22402743995189667\n",
      "iteration 19419: loss: 0.22402653098106384\n",
      "iteration 19420: loss: 0.22402560710906982\n",
      "iteration 19421: loss: 0.22402465343475342\n",
      "iteration 19422: loss: 0.22402377426624298\n",
      "iteration 19423: loss: 0.22402283549308777\n",
      "iteration 19424: loss: 0.22402195632457733\n",
      "iteration 19425: loss: 0.22402100265026093\n",
      "iteration 19426: loss: 0.22402004897594452\n",
      "iteration 19427: loss: 0.22401919960975647\n",
      "iteration 19428: loss: 0.22401829063892365\n",
      "iteration 19429: loss: 0.22401730716228485\n",
      "iteration 19430: loss: 0.22401642799377441\n",
      "iteration 19431: loss: 0.2240155190229416\n",
      "iteration 19432: loss: 0.22401456534862518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19433: loss: 0.22401371598243713\n",
      "iteration 19434: loss: 0.2240128517150879\n",
      "iteration 19435: loss: 0.22401192784309387\n",
      "iteration 19436: loss: 0.22401101887226105\n",
      "iteration 19437: loss: 0.22401003539562225\n",
      "iteration 19438: loss: 0.22400912642478943\n",
      "iteration 19439: loss: 0.2240082025527954\n",
      "iteration 19440: loss: 0.22400732338428497\n",
      "iteration 19441: loss: 0.22400644421577454\n",
      "iteration 19442: loss: 0.2240055352449417\n",
      "iteration 19443: loss: 0.2240045964717865\n",
      "iteration 19444: loss: 0.22400370240211487\n",
      "iteration 19445: loss: 0.22400271892547607\n",
      "iteration 19446: loss: 0.22400183975696564\n",
      "iteration 19447: loss: 0.2240009605884552\n",
      "iteration 19448: loss: 0.2240000218153\n",
      "iteration 19449: loss: 0.22399909794330597\n",
      "iteration 19450: loss: 0.22399814426898956\n",
      "iteration 19451: loss: 0.22399726510047913\n",
      "iteration 19452: loss: 0.2239963561296463\n",
      "iteration 19453: loss: 0.2239954173564911\n",
      "iteration 19454: loss: 0.22399446368217468\n",
      "iteration 19455: loss: 0.22399358451366425\n",
      "iteration 19456: loss: 0.22399267554283142\n",
      "iteration 19457: loss: 0.22399182617664337\n",
      "iteration 19458: loss: 0.22399087250232697\n",
      "iteration 19459: loss: 0.2239900529384613\n",
      "iteration 19460: loss: 0.22398903965950012\n",
      "iteration 19461: loss: 0.2239881008863449\n",
      "iteration 19462: loss: 0.22398725152015686\n",
      "iteration 19463: loss: 0.22398631274700165\n",
      "iteration 19464: loss: 0.22398538887500763\n",
      "iteration 19465: loss: 0.2239845097064972\n",
      "iteration 19466: loss: 0.2239835560321808\n",
      "iteration 19467: loss: 0.22398266196250916\n",
      "iteration 19468: loss: 0.22398173809051514\n",
      "iteration 19469: loss: 0.2239808589220047\n",
      "iteration 19470: loss: 0.2239799201488495\n",
      "iteration 19471: loss: 0.22397902607917786\n",
      "iteration 19472: loss: 0.22397807240486145\n",
      "iteration 19473: loss: 0.2239772081375122\n",
      "iteration 19474: loss: 0.22397632896900177\n",
      "iteration 19475: loss: 0.22397539019584656\n",
      "iteration 19476: loss: 0.22397446632385254\n",
      "iteration 19477: loss: 0.22397355735301971\n",
      "iteration 19478: loss: 0.22397272288799286\n",
      "iteration 19479: loss: 0.22397172451019287\n",
      "iteration 19480: loss: 0.22397081553936005\n",
      "iteration 19481: loss: 0.223969966173172\n",
      "iteration 19482: loss: 0.2239690124988556\n",
      "iteration 19483: loss: 0.22396807372570038\n",
      "iteration 19484: loss: 0.22396722435951233\n",
      "iteration 19485: loss: 0.22396628558635712\n",
      "iteration 19486: loss: 0.2239653766155243\n",
      "iteration 19487: loss: 0.22396449744701385\n",
      "iteration 19488: loss: 0.22396358847618103\n",
      "iteration 19489: loss: 0.2239626944065094\n",
      "iteration 19490: loss: 0.22396178543567657\n",
      "iteration 19491: loss: 0.22396084666252136\n",
      "iteration 19492: loss: 0.22395995259284973\n",
      "iteration 19493: loss: 0.2239590436220169\n",
      "iteration 19494: loss: 0.22395813465118408\n",
      "iteration 19495: loss: 0.22395721077919006\n",
      "iteration 19496: loss: 0.22395625710487366\n",
      "iteration 19497: loss: 0.22395539283752441\n",
      "iteration 19498: loss: 0.2239544838666916\n",
      "iteration 19499: loss: 0.22395353019237518\n",
      "iteration 19500: loss: 0.22395268082618713\n",
      "iteration 19501: loss: 0.22395169734954834\n",
      "iteration 19502: loss: 0.22395086288452148\n",
      "iteration 19503: loss: 0.22394999861717224\n",
      "iteration 19504: loss: 0.22394903004169464\n",
      "iteration 19505: loss: 0.2239481508731842\n",
      "iteration 19506: loss: 0.2239471971988678\n",
      "iteration 19507: loss: 0.22394628822803497\n",
      "iteration 19508: loss: 0.22394537925720215\n",
      "iteration 19509: loss: 0.22394447028636932\n",
      "iteration 19510: loss: 0.2239435613155365\n",
      "iteration 19511: loss: 0.22394263744354248\n",
      "iteration 19512: loss: 0.22394172847270966\n",
      "iteration 19513: loss: 0.22394084930419922\n",
      "iteration 19514: loss: 0.2239399403333664\n",
      "iteration 19515: loss: 0.22393898665905\n",
      "iteration 19516: loss: 0.22393807768821716\n",
      "iteration 19517: loss: 0.22393712401390076\n",
      "iteration 19518: loss: 0.2239362746477127\n",
      "iteration 19519: loss: 0.22393539547920227\n",
      "iteration 19520: loss: 0.22393444180488586\n",
      "iteration 19521: loss: 0.223933607339859\n",
      "iteration 19522: loss: 0.22393269836902618\n",
      "iteration 19523: loss: 0.22393178939819336\n",
      "iteration 19524: loss: 0.22393092513084412\n",
      "iteration 19525: loss: 0.22392988204956055\n",
      "iteration 19526: loss: 0.22392907738685608\n",
      "iteration 19527: loss: 0.22392813861370087\n",
      "iteration 19528: loss: 0.22392725944519043\n",
      "iteration 19529: loss: 0.22392627596855164\n",
      "iteration 19530: loss: 0.22392544150352478\n",
      "iteration 19531: loss: 0.22392451763153076\n",
      "iteration 19532: loss: 0.22392363846302032\n",
      "iteration 19533: loss: 0.2239226996898651\n",
      "iteration 19534: loss: 0.22392185032367706\n",
      "iteration 19535: loss: 0.22392089664936066\n",
      "iteration 19536: loss: 0.22391991317272186\n",
      "iteration 19537: loss: 0.2239191085100174\n",
      "iteration 19538: loss: 0.22391816973686218\n",
      "iteration 19539: loss: 0.22391727566719055\n",
      "iteration 19540: loss: 0.22391633689403534\n",
      "iteration 19541: loss: 0.22391542792320251\n",
      "iteration 19542: loss: 0.2239144742488861\n",
      "iteration 19543: loss: 0.22391358017921448\n",
      "iteration 19544: loss: 0.22391267120838165\n",
      "iteration 19545: loss: 0.22391179203987122\n",
      "iteration 19546: loss: 0.2239108830690384\n",
      "iteration 19547: loss: 0.22390994429588318\n",
      "iteration 19548: loss: 0.22390909492969513\n",
      "iteration 19549: loss: 0.2239081859588623\n",
      "iteration 19550: loss: 0.2239072620868683\n",
      "iteration 19551: loss: 0.22390635311603546\n",
      "iteration 19552: loss: 0.22390548884868622\n",
      "iteration 19553: loss: 0.22390449047088623\n",
      "iteration 19554: loss: 0.2239036113023758\n",
      "iteration 19555: loss: 0.22390267252922058\n",
      "iteration 19556: loss: 0.22390179336071014\n",
      "iteration 19557: loss: 0.22390088438987732\n",
      "iteration 19558: loss: 0.2238999903202057\n",
      "iteration 19559: loss: 0.22389908134937286\n",
      "iteration 19560: loss: 0.22389814257621765\n",
      "iteration 19561: loss: 0.22389721870422363\n",
      "iteration 19562: loss: 0.22389641404151917\n",
      "iteration 19563: loss: 0.22389551997184753\n",
      "iteration 19564: loss: 0.2238946259021759\n",
      "iteration 19565: loss: 0.22389371693134308\n",
      "iteration 19566: loss: 0.22389283776283264\n",
      "iteration 19567: loss: 0.22389188408851624\n",
      "iteration 19568: loss: 0.2238909751176834\n",
      "iteration 19569: loss: 0.223890021443367\n",
      "iteration 19570: loss: 0.2238890826702118\n",
      "iteration 19571: loss: 0.22388820350170135\n",
      "iteration 19572: loss: 0.2238873541355133\n",
      "iteration 19573: loss: 0.22388648986816406\n",
      "iteration 19574: loss: 0.22388550639152527\n",
      "iteration 19575: loss: 0.22388462722301483\n",
      "iteration 19576: loss: 0.2238837480545044\n",
      "iteration 19577: loss: 0.223882794380188\n",
      "iteration 19578: loss: 0.22388191521167755\n",
      "iteration 19579: loss: 0.22388103604316711\n",
      "iteration 19580: loss: 0.2238800972700119\n",
      "iteration 19581: loss: 0.22387924790382385\n",
      "iteration 19582: loss: 0.22387829422950745\n",
      "iteration 19583: loss: 0.22387740015983582\n",
      "iteration 19584: loss: 0.2238764762878418\n",
      "iteration 19585: loss: 0.22387559711933136\n",
      "iteration 19586: loss: 0.22387468814849854\n",
      "iteration 19587: loss: 0.2238738089799881\n",
      "iteration 19588: loss: 0.22387290000915527\n",
      "iteration 19589: loss: 0.22387194633483887\n",
      "iteration 19590: loss: 0.22387102246284485\n",
      "iteration 19591: loss: 0.22387011349201202\n",
      "iteration 19592: loss: 0.2238692343235016\n",
      "iteration 19593: loss: 0.22386832535266876\n",
      "iteration 19594: loss: 0.22386746108531952\n",
      "iteration 19595: loss: 0.2238665372133255\n",
      "iteration 19596: loss: 0.22386565804481506\n",
      "iteration 19597: loss: 0.22386471927165985\n",
      "iteration 19598: loss: 0.22386384010314941\n",
      "iteration 19599: loss: 0.22386297583580017\n",
      "iteration 19600: loss: 0.22386202216148376\n",
      "iteration 19601: loss: 0.22386109828948975\n",
      "iteration 19602: loss: 0.22386018931865692\n",
      "iteration 19603: loss: 0.2238592803478241\n",
      "iteration 19604: loss: 0.22385835647583008\n",
      "iteration 19605: loss: 0.22385747730731964\n",
      "iteration 19606: loss: 0.2238565981388092\n",
      "iteration 19607: loss: 0.22385570406913757\n",
      "iteration 19608: loss: 0.22385482490062714\n",
      "iteration 19609: loss: 0.2238539457321167\n",
      "iteration 19610: loss: 0.22385306656360626\n",
      "iteration 19611: loss: 0.22385215759277344\n",
      "iteration 19612: loss: 0.22385123372077942\n",
      "iteration 19613: loss: 0.2238503247499466\n",
      "iteration 19614: loss: 0.22384941577911377\n",
      "iteration 19615: loss: 0.22384850680828094\n",
      "iteration 19616: loss: 0.22384758293628693\n",
      "iteration 19617: loss: 0.2238466739654541\n",
      "iteration 19618: loss: 0.22384579479694366\n",
      "iteration 19619: loss: 0.22384488582611084\n",
      "iteration 19620: loss: 0.22384397685527802\n",
      "iteration 19621: loss: 0.22384309768676758\n",
      "iteration 19622: loss: 0.22384214401245117\n",
      "iteration 19623: loss: 0.22384123504161835\n",
      "iteration 19624: loss: 0.22384044528007507\n",
      "iteration 19625: loss: 0.22383944690227509\n",
      "iteration 19626: loss: 0.22383861243724823\n",
      "iteration 19627: loss: 0.2238376885652542\n",
      "iteration 19628: loss: 0.22383680939674377\n",
      "iteration 19629: loss: 0.22383587062358856\n",
      "iteration 19630: loss: 0.22383499145507812\n",
      "iteration 19631: loss: 0.22383400797843933\n",
      "iteration 19632: loss: 0.22383315861225128\n",
      "iteration 19633: loss: 0.22383224964141846\n",
      "iteration 19634: loss: 0.22383137047290802\n",
      "iteration 19635: loss: 0.2238304167985916\n",
      "iteration 19636: loss: 0.22382953763008118\n",
      "iteration 19637: loss: 0.22382859885692596\n",
      "iteration 19638: loss: 0.22382774949073792\n",
      "iteration 19639: loss: 0.2238268405199051\n",
      "iteration 19640: loss: 0.22382593154907227\n",
      "iteration 19641: loss: 0.22382506728172302\n",
      "iteration 19642: loss: 0.223824143409729\n",
      "iteration 19643: loss: 0.22382323443889618\n",
      "iteration 19644: loss: 0.22382235527038574\n",
      "iteration 19645: loss: 0.2238214910030365\n",
      "iteration 19646: loss: 0.2238205373287201\n",
      "iteration 19647: loss: 0.22381964325904846\n",
      "iteration 19648: loss: 0.22381874918937683\n",
      "iteration 19649: loss: 0.2238178551197052\n",
      "iteration 19650: loss: 0.22381699085235596\n",
      "iteration 19651: loss: 0.22381603717803955\n",
      "iteration 19652: loss: 0.22381511330604553\n",
      "iteration 19653: loss: 0.2238142192363739\n",
      "iteration 19654: loss: 0.22381338477134705\n",
      "iteration 19655: loss: 0.2238125056028366\n",
      "iteration 19656: loss: 0.22381162643432617\n",
      "iteration 19657: loss: 0.22381067276000977\n",
      "iteration 19658: loss: 0.22380976378917694\n",
      "iteration 19659: loss: 0.2238089144229889\n",
      "iteration 19660: loss: 0.22380796074867249\n",
      "iteration 19661: loss: 0.22380706667900085\n",
      "iteration 19662: loss: 0.22380618751049042\n",
      "iteration 19663: loss: 0.223805233836174\n",
      "iteration 19664: loss: 0.2238043248653412\n",
      "iteration 19665: loss: 0.22380347549915314\n",
      "iteration 19666: loss: 0.22380252182483673\n",
      "iteration 19667: loss: 0.2238016575574875\n",
      "iteration 19668: loss: 0.22380074858665466\n",
      "iteration 19669: loss: 0.22379986941814423\n",
      "iteration 19670: loss: 0.2237989902496338\n",
      "iteration 19671: loss: 0.22379812598228455\n",
      "iteration 19672: loss: 0.22379717230796814\n",
      "iteration 19673: loss: 0.2237962931394577\n",
      "iteration 19674: loss: 0.22379541397094727\n",
      "iteration 19675: loss: 0.22379449009895325\n",
      "iteration 19676: loss: 0.22379358112812042\n",
      "iteration 19677: loss: 0.2237926423549652\n",
      "iteration 19678: loss: 0.22379174828529358\n",
      "iteration 19679: loss: 0.22379088401794434\n",
      "iteration 19680: loss: 0.2237899750471115\n",
      "iteration 19681: loss: 0.22378914058208466\n",
      "iteration 19682: loss: 0.22378817200660706\n",
      "iteration 19683: loss: 0.22378726303577423\n",
      "iteration 19684: loss: 0.22378647327423096\n",
      "iteration 19685: loss: 0.2237854301929474\n",
      "iteration 19686: loss: 0.22378453612327576\n",
      "iteration 19687: loss: 0.22378365695476532\n",
      "iteration 19688: loss: 0.22378280758857727\n",
      "iteration 19689: loss: 0.22378189861774445\n",
      "iteration 19690: loss: 0.22378095984458923\n",
      "iteration 19691: loss: 0.2237800806760788\n",
      "iteration 19692: loss: 0.22377920150756836\n",
      "iteration 19693: loss: 0.22377827763557434\n",
      "iteration 19694: loss: 0.2237774133682251\n",
      "iteration 19695: loss: 0.22377648949623108\n",
      "iteration 19696: loss: 0.22377558052539825\n",
      "iteration 19697: loss: 0.2237747460603714\n",
      "iteration 19698: loss: 0.22377383708953857\n",
      "iteration 19699: loss: 0.22377292811870575\n",
      "iteration 19700: loss: 0.22377200424671173\n",
      "iteration 19701: loss: 0.2237711250782013\n",
      "iteration 19702: loss: 0.22377026081085205\n",
      "iteration 19703: loss: 0.22376930713653564\n",
      "iteration 19704: loss: 0.2237684428691864\n",
      "iteration 19705: loss: 0.22376754879951477\n",
      "iteration 19706: loss: 0.22376668453216553\n",
      "iteration 19707: loss: 0.2237657606601715\n",
      "iteration 19708: loss: 0.22376485168933868\n",
      "iteration 19709: loss: 0.22376397252082825\n",
      "iteration 19710: loss: 0.22376307845115662\n",
      "iteration 19711: loss: 0.22376218438148499\n",
      "iteration 19712: loss: 0.22376124560832977\n",
      "iteration 19713: loss: 0.2237604558467865\n",
      "iteration 19714: loss: 0.2237594574689865\n",
      "iteration 19715: loss: 0.22375857830047607\n",
      "iteration 19716: loss: 0.22375769913196564\n",
      "iteration 19717: loss: 0.2237567901611328\n",
      "iteration 19718: loss: 0.2237558364868164\n",
      "iteration 19719: loss: 0.22375497221946716\n",
      "iteration 19720: loss: 0.22375409305095673\n",
      "iteration 19721: loss: 0.22375325858592987\n",
      "iteration 19722: loss: 0.22375233471393585\n",
      "iteration 19723: loss: 0.22375142574310303\n",
      "iteration 19724: loss: 0.22375056147575378\n",
      "iteration 19725: loss: 0.22374963760375977\n",
      "iteration 19726: loss: 0.22374872863292694\n",
      "iteration 19727: loss: 0.22374781966209412\n",
      "iteration 19728: loss: 0.22374697029590607\n",
      "iteration 19729: loss: 0.22374603152275085\n",
      "iteration 19730: loss: 0.22374510765075684\n",
      "iteration 19731: loss: 0.2237442284822464\n",
      "iteration 19732: loss: 0.22374339401721954\n",
      "iteration 19733: loss: 0.22374241054058075\n",
      "iteration 19734: loss: 0.22374162077903748\n",
      "iteration 19735: loss: 0.22374069690704346\n",
      "iteration 19736: loss: 0.22373981773853302\n",
      "iteration 19737: loss: 0.22373883426189423\n",
      "iteration 19738: loss: 0.22373798489570618\n",
      "iteration 19739: loss: 0.22373709082603455\n",
      "iteration 19740: loss: 0.22373616695404053\n",
      "iteration 19741: loss: 0.2237352430820465\n",
      "iteration 19742: loss: 0.22373434901237488\n",
      "iteration 19743: loss: 0.22373349964618683\n",
      "iteration 19744: loss: 0.2237326204776764\n",
      "iteration 19745: loss: 0.22373172640800476\n",
      "iteration 19746: loss: 0.22373084723949432\n",
      "iteration 19747: loss: 0.2237299680709839\n",
      "iteration 19748: loss: 0.22372905910015106\n",
      "iteration 19749: loss: 0.22372817993164062\n",
      "iteration 19750: loss: 0.22372718155384064\n",
      "iteration 19751: loss: 0.22372636198997498\n",
      "iteration 19752: loss: 0.22372543811798096\n",
      "iteration 19753: loss: 0.22372455894947052\n",
      "iteration 19754: loss: 0.2237236201763153\n",
      "iteration 19755: loss: 0.22372274100780487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19756: loss: 0.22372189164161682\n",
      "iteration 19757: loss: 0.2237209975719452\n",
      "iteration 19758: loss: 0.22372011840343475\n",
      "iteration 19759: loss: 0.22371920943260193\n",
      "iteration 19760: loss: 0.2237183302640915\n",
      "iteration 19761: loss: 0.22371748089790344\n",
      "iteration 19762: loss: 0.2237165868282318\n",
      "iteration 19763: loss: 0.2237156629562378\n",
      "iteration 19764: loss: 0.22371478378772736\n",
      "iteration 19765: loss: 0.22371380031108856\n",
      "iteration 19766: loss: 0.22371292114257812\n",
      "iteration 19767: loss: 0.22371208667755127\n",
      "iteration 19768: loss: 0.22371116280555725\n",
      "iteration 19769: loss: 0.22371020913124084\n",
      "iteration 19770: loss: 0.2237093448638916\n",
      "iteration 19771: loss: 0.22370846569538116\n",
      "iteration 19772: loss: 0.2237076461315155\n",
      "iteration 19773: loss: 0.22370676696300507\n",
      "iteration 19774: loss: 0.22370581328868866\n",
      "iteration 19775: loss: 0.22370493412017822\n",
      "iteration 19776: loss: 0.22370409965515137\n",
      "iteration 19777: loss: 0.22370311617851257\n",
      "iteration 19778: loss: 0.22370223701000214\n",
      "iteration 19779: loss: 0.22370131313800812\n",
      "iteration 19780: loss: 0.22370043396949768\n",
      "iteration 19781: loss: 0.22369956970214844\n",
      "iteration 19782: loss: 0.22369864583015442\n",
      "iteration 19783: loss: 0.2236977517604828\n",
      "iteration 19784: loss: 0.22369690239429474\n",
      "iteration 19785: loss: 0.2236960232257843\n",
      "iteration 19786: loss: 0.22369512915611267\n",
      "iteration 19787: loss: 0.22369423508644104\n",
      "iteration 19788: loss: 0.2236933410167694\n",
      "iteration 19789: loss: 0.22369244694709778\n",
      "iteration 19790: loss: 0.22369155287742615\n",
      "iteration 19791: loss: 0.2236906737089157\n",
      "iteration 19792: loss: 0.2236897498369217\n",
      "iteration 19793: loss: 0.22368887066841125\n",
      "iteration 19794: loss: 0.223688006401062\n",
      "iteration 19795: loss: 0.223687082529068\n",
      "iteration 19796: loss: 0.22368621826171875\n",
      "iteration 19797: loss: 0.2236853539943695\n",
      "iteration 19798: loss: 0.2236843854188919\n",
      "iteration 19799: loss: 0.22368347644805908\n",
      "iteration 19800: loss: 0.22368267178535461\n",
      "iteration 19801: loss: 0.2236817330121994\n",
      "iteration 19802: loss: 0.22368088364601135\n",
      "iteration 19803: loss: 0.2236800193786621\n",
      "iteration 19804: loss: 0.2236790657043457\n",
      "iteration 19805: loss: 0.22367826104164124\n",
      "iteration 19806: loss: 0.2236773520708084\n",
      "iteration 19807: loss: 0.223676398396492\n",
      "iteration 19808: loss: 0.22367548942565918\n",
      "iteration 19809: loss: 0.22367462515830994\n",
      "iteration 19810: loss: 0.22367370128631592\n",
      "iteration 19811: loss: 0.22367283701896667\n",
      "iteration 19812: loss: 0.22367191314697266\n",
      "iteration 19813: loss: 0.2236710786819458\n",
      "iteration 19814: loss: 0.22367021441459656\n",
      "iteration 19815: loss: 0.22366929054260254\n",
      "iteration 19816: loss: 0.2236684113740921\n",
      "iteration 19817: loss: 0.22366754710674286\n",
      "iteration 19818: loss: 0.22366663813591003\n",
      "iteration 19819: loss: 0.2236657589673996\n",
      "iteration 19820: loss: 0.22366483509540558\n",
      "iteration 19821: loss: 0.22366397082805634\n",
      "iteration 19822: loss: 0.2236630916595459\n",
      "iteration 19823: loss: 0.22366218268871307\n",
      "iteration 19824: loss: 0.22366130352020264\n",
      "iteration 19825: loss: 0.22366037964820862\n",
      "iteration 19826: loss: 0.22365951538085938\n",
      "iteration 19827: loss: 0.22365863621234894\n",
      "iteration 19828: loss: 0.22365769743919373\n",
      "iteration 19829: loss: 0.2236568182706833\n",
      "iteration 19830: loss: 0.22365602850914001\n",
      "iteration 19831: loss: 0.22365513443946838\n",
      "iteration 19832: loss: 0.22365419566631317\n",
      "iteration 19833: loss: 0.22365331649780273\n",
      "iteration 19834: loss: 0.2236524373292923\n",
      "iteration 19835: loss: 0.22365152835845947\n",
      "iteration 19836: loss: 0.22365064918994904\n",
      "iteration 19837: loss: 0.22364971041679382\n",
      "iteration 19838: loss: 0.2236488312482834\n",
      "iteration 19839: loss: 0.22364798188209534\n",
      "iteration 19840: loss: 0.2236470729112625\n",
      "iteration 19841: loss: 0.22364619374275208\n",
      "iteration 19842: loss: 0.22364529967308044\n",
      "iteration 19843: loss: 0.2236444056034088\n",
      "iteration 19844: loss: 0.22364354133605957\n",
      "iteration 19845: loss: 0.22364263236522675\n",
      "iteration 19846: loss: 0.2236417979001999\n",
      "iteration 19847: loss: 0.22364088892936707\n",
      "iteration 19848: loss: 0.22364000976085663\n",
      "iteration 19849: loss: 0.2236391007900238\n",
      "iteration 19850: loss: 0.22363825142383575\n",
      "iteration 19851: loss: 0.22363729774951935\n",
      "iteration 19852: loss: 0.2236364632844925\n",
      "iteration 19853: loss: 0.22363558411598206\n",
      "iteration 19854: loss: 0.22363467514514923\n",
      "iteration 19855: loss: 0.2236338108778\n",
      "iteration 19856: loss: 0.22363285720348358\n",
      "iteration 19857: loss: 0.22363193333148956\n",
      "iteration 19858: loss: 0.2236310988664627\n",
      "iteration 19859: loss: 0.22363021969795227\n",
      "iteration 19860: loss: 0.22362931072711945\n",
      "iteration 19861: loss: 0.22362849116325378\n",
      "iteration 19862: loss: 0.22362752258777618\n",
      "iteration 19863: loss: 0.22362664341926575\n",
      "iteration 19864: loss: 0.22362582385540009\n",
      "iteration 19865: loss: 0.22362489998340607\n",
      "iteration 19866: loss: 0.2236240804195404\n",
      "iteration 19867: loss: 0.22362318634986877\n",
      "iteration 19868: loss: 0.22362220287322998\n",
      "iteration 19869: loss: 0.22362136840820312\n",
      "iteration 19870: loss: 0.2236204594373703\n",
      "iteration 19871: loss: 0.22361961007118225\n",
      "iteration 19872: loss: 0.22361867129802704\n",
      "iteration 19873: loss: 0.22361786663532257\n",
      "iteration 19874: loss: 0.22361692786216736\n",
      "iteration 19875: loss: 0.2236160784959793\n",
      "iteration 19876: loss: 0.2236151248216629\n",
      "iteration 19877: loss: 0.22361424565315247\n",
      "iteration 19878: loss: 0.22361333668231964\n",
      "iteration 19879: loss: 0.22361250221729279\n",
      "iteration 19880: loss: 0.22361162304878235\n",
      "iteration 19881: loss: 0.2236107587814331\n",
      "iteration 19882: loss: 0.2236098051071167\n",
      "iteration 19883: loss: 0.22360897064208984\n",
      "iteration 19884: loss: 0.22360804677009583\n",
      "iteration 19885: loss: 0.2236071527004242\n",
      "iteration 19886: loss: 0.22360631823539734\n",
      "iteration 19887: loss: 0.2236054390668869\n",
      "iteration 19888: loss: 0.22360453009605408\n",
      "iteration 19889: loss: 0.22360365092754364\n",
      "iteration 19890: loss: 0.2236028015613556\n",
      "iteration 19891: loss: 0.22360186278820038\n",
      "iteration 19892: loss: 0.22360101342201233\n",
      "iteration 19893: loss: 0.22360014915466309\n",
      "iteration 19894: loss: 0.2235991656780243\n",
      "iteration 19895: loss: 0.22359828650951385\n",
      "iteration 19896: loss: 0.223597452044487\n",
      "iteration 19897: loss: 0.22359652817249298\n",
      "iteration 19898: loss: 0.22359564900398254\n",
      "iteration 19899: loss: 0.2235948145389557\n",
      "iteration 19900: loss: 0.22359392046928406\n",
      "iteration 19901: loss: 0.22359304130077362\n",
      "iteration 19902: loss: 0.2235921323299408\n",
      "iteration 19903: loss: 0.22359128296375275\n",
      "iteration 19904: loss: 0.2235904186964035\n",
      "iteration 19905: loss: 0.2235894650220871\n",
      "iteration 19906: loss: 0.22358866035938263\n",
      "iteration 19907: loss: 0.22358770668506622\n",
      "iteration 19908: loss: 0.22358687222003937\n",
      "iteration 19909: loss: 0.22358593344688416\n",
      "iteration 19910: loss: 0.2235851287841797\n",
      "iteration 19911: loss: 0.22358420491218567\n",
      "iteration 19912: loss: 0.22358329594135284\n",
      "iteration 19913: loss: 0.2235824316740036\n",
      "iteration 19914: loss: 0.22358155250549316\n",
      "iteration 19915: loss: 0.2235807180404663\n",
      "iteration 19916: loss: 0.22357980906963348\n",
      "iteration 19917: loss: 0.22357892990112305\n",
      "iteration 19918: loss: 0.2235780656337738\n",
      "iteration 19919: loss: 0.22357718646526337\n",
      "iteration 19920: loss: 0.22357623279094696\n",
      "iteration 19921: loss: 0.22357535362243652\n",
      "iteration 19922: loss: 0.22357451915740967\n",
      "iteration 19923: loss: 0.22357359528541565\n",
      "iteration 19924: loss: 0.2235727310180664\n",
      "iteration 19925: loss: 0.22357186675071716\n",
      "iteration 19926: loss: 0.22357097268104553\n",
      "iteration 19927: loss: 0.22357003390789032\n",
      "iteration 19928: loss: 0.22356918454170227\n",
      "iteration 19929: loss: 0.22356835007667542\n",
      "iteration 19930: loss: 0.22356748580932617\n",
      "iteration 19931: loss: 0.22356660664081573\n",
      "iteration 19932: loss: 0.2235657274723053\n",
      "iteration 19933: loss: 0.22356471419334412\n",
      "iteration 19934: loss: 0.22356390953063965\n",
      "iteration 19935: loss: 0.2235630452632904\n",
      "iteration 19936: loss: 0.2235621213912964\n",
      "iteration 19937: loss: 0.22356121242046356\n",
      "iteration 19938: loss: 0.2235604077577591\n",
      "iteration 19939: loss: 0.22355949878692627\n",
      "iteration 19940: loss: 0.22355858981609344\n",
      "iteration 19941: loss: 0.2235577404499054\n",
      "iteration 19942: loss: 0.22355684638023376\n",
      "iteration 19943: loss: 0.22355595231056213\n",
      "iteration 19944: loss: 0.2235550880432129\n",
      "iteration 19945: loss: 0.22355417907238007\n",
      "iteration 19946: loss: 0.2235533744096756\n",
      "iteration 19947: loss: 0.22355246543884277\n",
      "iteration 19948: loss: 0.22355151176452637\n",
      "iteration 19949: loss: 0.22355063259601593\n",
      "iteration 19950: loss: 0.22354984283447266\n",
      "iteration 19951: loss: 0.22354896366596222\n",
      "iteration 19952: loss: 0.22354798018932343\n",
      "iteration 19953: loss: 0.22354714572429657\n",
      "iteration 19954: loss: 0.22354623675346375\n",
      "iteration 19955: loss: 0.2235453873872757\n",
      "iteration 19956: loss: 0.22354450821876526\n",
      "iteration 19957: loss: 0.22354361414909363\n",
      "iteration 19958: loss: 0.22354277968406677\n",
      "iteration 19959: loss: 0.22354188561439514\n",
      "iteration 19960: loss: 0.2235410213470459\n",
      "iteration 19961: loss: 0.2235400676727295\n",
      "iteration 19962: loss: 0.22353926301002502\n",
      "iteration 19963: loss: 0.2235383689403534\n",
      "iteration 19964: loss: 0.22353748977184296\n",
      "iteration 19965: loss: 0.22353653609752655\n",
      "iteration 19966: loss: 0.22353574633598328\n",
      "iteration 19967: loss: 0.22353485226631165\n",
      "iteration 19968: loss: 0.22353389859199524\n",
      "iteration 19969: loss: 0.22353306412696838\n",
      "iteration 19970: loss: 0.22353217005729675\n",
      "iteration 19971: loss: 0.2235313206911087\n",
      "iteration 19972: loss: 0.22353045642375946\n",
      "iteration 19973: loss: 0.22352954745292664\n",
      "iteration 19974: loss: 0.22352871298789978\n",
      "iteration 19975: loss: 0.2235277146100998\n",
      "iteration 19976: loss: 0.22352686524391174\n",
      "iteration 19977: loss: 0.22352609038352966\n",
      "iteration 19978: loss: 0.22352521121501923\n",
      "iteration 19979: loss: 0.2235242873430252\n",
      "iteration 19980: loss: 0.22352337837219238\n",
      "iteration 19981: loss: 0.22352251410484314\n",
      "iteration 19982: loss: 0.2235216349363327\n",
      "iteration 19983: loss: 0.22352071106433868\n",
      "iteration 19984: loss: 0.22351984679698944\n",
      "iteration 19985: loss: 0.22351889312267303\n",
      "iteration 19986: loss: 0.22351816296577454\n",
      "iteration 19987: loss: 0.2235172688961029\n",
      "iteration 19988: loss: 0.22351638972759247\n",
      "iteration 19989: loss: 0.22351548075675964\n",
      "iteration 19990: loss: 0.2235146313905716\n",
      "iteration 19991: loss: 0.22351375222206116\n",
      "iteration 19992: loss: 0.22351281344890594\n",
      "iteration 19993: loss: 0.2235119789838791\n",
      "iteration 19994: loss: 0.22351107001304626\n",
      "iteration 19995: loss: 0.22351022064685822\n",
      "iteration 19996: loss: 0.2235093116760254\n",
      "iteration 19997: loss: 0.22350850701332092\n",
      "iteration 19998: loss: 0.22350752353668213\n",
      "iteration 19999: loss: 0.22350668907165527\n",
      "iteration 20000: loss: 0.2235058844089508\n",
      "iteration 20001: loss: 0.2235049456357956\n",
      "iteration 20002: loss: 0.22350406646728516\n",
      "iteration 20003: loss: 0.2235032021999359\n",
      "iteration 20004: loss: 0.22350236773490906\n",
      "iteration 20005: loss: 0.22350148856639862\n",
      "iteration 20006: loss: 0.22350060939788818\n",
      "iteration 20007: loss: 0.22349973022937775\n",
      "iteration 20008: loss: 0.22349877655506134\n",
      "iteration 20009: loss: 0.2234979122877121\n",
      "iteration 20010: loss: 0.22349706292152405\n",
      "iteration 20011: loss: 0.2234961986541748\n",
      "iteration 20012: loss: 0.22349531948566437\n",
      "iteration 20013: loss: 0.22349445521831512\n",
      "iteration 20014: loss: 0.22349362075328827\n",
      "iteration 20015: loss: 0.22349265217781067\n",
      "iteration 20016: loss: 0.22349175810813904\n",
      "iteration 20017: loss: 0.2234908640384674\n",
      "iteration 20018: loss: 0.22349007427692413\n",
      "iteration 20019: loss: 0.2234891653060913\n",
      "iteration 20020: loss: 0.22348836064338684\n",
      "iteration 20021: loss: 0.22348742187023163\n",
      "iteration 20022: loss: 0.22348657250404358\n",
      "iteration 20023: loss: 0.22348567843437195\n",
      "iteration 20024: loss: 0.2234847992658615\n",
      "iteration 20025: loss: 0.22348394989967346\n",
      "iteration 20026: loss: 0.22348301112651825\n",
      "iteration 20027: loss: 0.22348210215568542\n",
      "iteration 20028: loss: 0.22348129749298096\n",
      "iteration 20029: loss: 0.22348037362098694\n",
      "iteration 20030: loss: 0.22347958385944366\n",
      "iteration 20031: loss: 0.22347863018512726\n",
      "iteration 20032: loss: 0.2234778106212616\n",
      "iteration 20033: loss: 0.22347691655158997\n",
      "iteration 20034: loss: 0.22347597777843475\n",
      "iteration 20035: loss: 0.22347509860992432\n",
      "iteration 20036: loss: 0.22347429394721985\n",
      "iteration 20037: loss: 0.22347335517406464\n",
      "iteration 20038: loss: 0.2234725058078766\n",
      "iteration 20039: loss: 0.22347167134284973\n",
      "iteration 20040: loss: 0.2234707623720169\n",
      "iteration 20041: loss: 0.22346989810466766\n",
      "iteration 20042: loss: 0.2234690636396408\n",
      "iteration 20043: loss: 0.22346821427345276\n",
      "iteration 20044: loss: 0.22346730530261993\n",
      "iteration 20045: loss: 0.2234664410352707\n",
      "iteration 20046: loss: 0.22346556186676025\n",
      "iteration 20047: loss: 0.22346463799476624\n",
      "iteration 20048: loss: 0.223463773727417\n",
      "iteration 20049: loss: 0.22346296906471252\n",
      "iteration 20050: loss: 0.2234620749950409\n",
      "iteration 20051: loss: 0.22346118092536926\n",
      "iteration 20052: loss: 0.22346024215221405\n",
      "iteration 20053: loss: 0.223459392786026\n",
      "iteration 20054: loss: 0.22345848381519318\n",
      "iteration 20055: loss: 0.2234576940536499\n",
      "iteration 20056: loss: 0.22345678508281708\n",
      "iteration 20057: loss: 0.22345595061779022\n",
      "iteration 20058: loss: 0.2234550416469574\n",
      "iteration 20059: loss: 0.22345420718193054\n",
      "iteration 20060: loss: 0.22345325350761414\n",
      "iteration 20061: loss: 0.2234523743391037\n",
      "iteration 20062: loss: 0.22345152497291565\n",
      "iteration 20063: loss: 0.2234506607055664\n",
      "iteration 20064: loss: 0.22344979643821716\n",
      "iteration 20065: loss: 0.22344890236854553\n",
      "iteration 20066: loss: 0.22344806790351868\n",
      "iteration 20067: loss: 0.22344717383384705\n",
      "iteration 20068: loss: 0.2234462946653366\n",
      "iteration 20069: loss: 0.22344549000263214\n",
      "iteration 20070: loss: 0.22344453632831573\n",
      "iteration 20071: loss: 0.2234436720609665\n",
      "iteration 20072: loss: 0.22344279289245605\n",
      "iteration 20073: loss: 0.2234419584274292\n",
      "iteration 20074: loss: 0.22344107925891876\n",
      "iteration 20075: loss: 0.22344021499156952\n",
      "iteration 20076: loss: 0.2234393060207367\n",
      "iteration 20077: loss: 0.22343845665454865\n",
      "iteration 20078: loss: 0.22343754768371582\n",
      "iteration 20079: loss: 0.22343674302101135\n",
      "iteration 20080: loss: 0.22343580424785614\n",
      "iteration 20081: loss: 0.22343496978282928\n",
      "iteration 20082: loss: 0.22343406081199646\n",
      "iteration 20083: loss: 0.22343328595161438\n",
      "iteration 20084: loss: 0.22343234717845917\n",
      "iteration 20085: loss: 0.2234315425157547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 20086: loss: 0.2234305888414383\n",
      "iteration 20087: loss: 0.22342976927757263\n",
      "iteration 20088: loss: 0.22342881560325623\n",
      "iteration 20089: loss: 0.22342801094055176\n",
      "iteration 20090: loss: 0.22342713177204132\n",
      "iteration 20091: loss: 0.22342626750469208\n",
      "iteration 20092: loss: 0.22342535853385925\n",
      "iteration 20093: loss: 0.2234245240688324\n",
      "iteration 20094: loss: 0.22342364490032196\n",
      "iteration 20095: loss: 0.22342272102832794\n",
      "iteration 20096: loss: 0.2234218567609787\n",
      "iteration 20097: loss: 0.22342097759246826\n",
      "iteration 20098: loss: 0.2234201431274414\n",
      "iteration 20099: loss: 0.22341927886009216\n",
      "iteration 20100: loss: 0.22341838479042053\n",
      "iteration 20101: loss: 0.22341755032539368\n",
      "iteration 20102: loss: 0.22341661155223846\n",
      "iteration 20103: loss: 0.2234157770872116\n",
      "iteration 20104: loss: 0.22341492772102356\n",
      "iteration 20105: loss: 0.22341403365135193\n",
      "iteration 20106: loss: 0.22341318428516388\n",
      "iteration 20107: loss: 0.22341227531433105\n",
      "iteration 20108: loss: 0.2234114110469818\n",
      "iteration 20109: loss: 0.22341053187847137\n",
      "iteration 20110: loss: 0.22340968251228333\n",
      "iteration 20111: loss: 0.22340884804725647\n",
      "iteration 20112: loss: 0.22340798377990723\n",
      "iteration 20113: loss: 0.2234070748090744\n",
      "iteration 20114: loss: 0.22340622544288635\n",
      "iteration 20115: loss: 0.2234053909778595\n",
      "iteration 20116: loss: 0.2234044373035431\n",
      "iteration 20117: loss: 0.22340361773967743\n",
      "iteration 20118: loss: 0.22340276837348938\n",
      "iteration 20119: loss: 0.22340182960033417\n",
      "iteration 20120: loss: 0.22340098023414612\n",
      "iteration 20121: loss: 0.22340011596679688\n",
      "iteration 20122: loss: 0.22339928150177002\n",
      "iteration 20123: loss: 0.2233983725309372\n",
      "iteration 20124: loss: 0.22339749336242676\n",
      "iteration 20125: loss: 0.22339670360088348\n",
      "iteration 20126: loss: 0.22339582443237305\n",
      "iteration 20127: loss: 0.22339491546154022\n",
      "iteration 20128: loss: 0.22339403629302979\n",
      "iteration 20129: loss: 0.22339317202568054\n",
      "iteration 20130: loss: 0.22339224815368652\n",
      "iteration 20131: loss: 0.22339138388633728\n",
      "iteration 20132: loss: 0.22339054942131042\n",
      "iteration 20133: loss: 0.2233896702528\n",
      "iteration 20134: loss: 0.22338882088661194\n",
      "iteration 20135: loss: 0.2233879119157791\n",
      "iteration 20136: loss: 0.22338704764842987\n",
      "iteration 20137: loss: 0.2233862429857254\n",
      "iteration 20138: loss: 0.22338533401489258\n",
      "iteration 20139: loss: 0.22338449954986572\n",
      "iteration 20140: loss: 0.22338363528251648\n",
      "iteration 20141: loss: 0.22338275611400604\n",
      "iteration 20142: loss: 0.223381906747818\n",
      "iteration 20143: loss: 0.22338104248046875\n",
      "iteration 20144: loss: 0.2233801633119583\n",
      "iteration 20145: loss: 0.22337929904460907\n",
      "iteration 20146: loss: 0.22337841987609863\n",
      "iteration 20147: loss: 0.2233775109052658\n",
      "iteration 20148: loss: 0.22337667644023895\n",
      "iteration 20149: loss: 0.22337579727172852\n",
      "iteration 20150: loss: 0.22337493300437927\n",
      "iteration 20151: loss: 0.22337408363819122\n",
      "iteration 20152: loss: 0.223373144865036\n",
      "iteration 20153: loss: 0.22337231040000916\n",
      "iteration 20154: loss: 0.2233714610338211\n",
      "iteration 20155: loss: 0.22337064146995544\n",
      "iteration 20156: loss: 0.2233697474002838\n",
      "iteration 20157: loss: 0.22336885333061218\n",
      "iteration 20158: loss: 0.22336797416210175\n",
      "iteration 20159: loss: 0.22336706519126892\n",
      "iteration 20160: loss: 0.22336623072624207\n",
      "iteration 20161: loss: 0.2233653962612152\n",
      "iteration 20162: loss: 0.22336454689502716\n",
      "iteration 20163: loss: 0.22336366772651672\n",
      "iteration 20164: loss: 0.2233627587556839\n",
      "iteration 20165: loss: 0.22336189448833466\n",
      "iteration 20166: loss: 0.22336101531982422\n",
      "iteration 20167: loss: 0.22336021065711975\n",
      "iteration 20168: loss: 0.2233593463897705\n",
      "iteration 20169: loss: 0.22335848212242126\n",
      "iteration 20170: loss: 0.22335763275623322\n",
      "iteration 20171: loss: 0.22335679829120636\n",
      "iteration 20172: loss: 0.22335588932037354\n",
      "iteration 20173: loss: 0.22335496544837952\n",
      "iteration 20174: loss: 0.22335414588451385\n",
      "iteration 20175: loss: 0.22335323691368103\n",
      "iteration 20176: loss: 0.2233523577451706\n",
      "iteration 20177: loss: 0.22335152328014374\n",
      "iteration 20178: loss: 0.2233506441116333\n",
      "iteration 20179: loss: 0.22334976494312286\n",
      "iteration 20180: loss: 0.223348930478096\n",
      "iteration 20181: loss: 0.22334814071655273\n",
      "iteration 20182: loss: 0.2233472317457199\n",
      "iteration 20183: loss: 0.2233463078737259\n",
      "iteration 20184: loss: 0.22334547340869904\n",
      "iteration 20185: loss: 0.2233446091413498\n",
      "iteration 20186: loss: 0.22334370017051697\n",
      "iteration 20187: loss: 0.22334282100200653\n",
      "iteration 20188: loss: 0.22334198653697968\n",
      "iteration 20189: loss: 0.22334113717079163\n",
      "iteration 20190: loss: 0.22334030270576477\n",
      "iteration 20191: loss: 0.22333940863609314\n",
      "iteration 20192: loss: 0.2233385294675827\n",
      "iteration 20193: loss: 0.22333769500255585\n",
      "iteration 20194: loss: 0.2233368158340454\n",
      "iteration 20195: loss: 0.22333598136901855\n",
      "iteration 20196: loss: 0.2233351171016693\n",
      "iteration 20197: loss: 0.22333426773548126\n",
      "iteration 20198: loss: 0.22333335876464844\n",
      "iteration 20199: loss: 0.2233324944972992\n",
      "iteration 20200: loss: 0.22333166003227234\n",
      "iteration 20201: loss: 0.2233307808637619\n",
      "iteration 20202: loss: 0.22332990169525146\n",
      "iteration 20203: loss: 0.22332899272441864\n",
      "iteration 20204: loss: 0.22332820296287537\n",
      "iteration 20205: loss: 0.22332727909088135\n",
      "iteration 20206: loss: 0.2233264446258545\n",
      "iteration 20207: loss: 0.22332556545734406\n",
      "iteration 20208: loss: 0.2233247309923172\n",
      "iteration 20209: loss: 0.22332386672496796\n",
      "iteration 20210: loss: 0.22332307696342468\n",
      "iteration 20211: loss: 0.22332218289375305\n",
      "iteration 20212: loss: 0.22332128882408142\n",
      "iteration 20213: loss: 0.22332033514976501\n",
      "iteration 20214: loss: 0.22331953048706055\n",
      "iteration 20215: loss: 0.2233186513185501\n",
      "iteration 20216: loss: 0.22331781685352325\n",
      "iteration 20217: loss: 0.2233169972896576\n",
      "iteration 20218: loss: 0.22331610321998596\n",
      "iteration 20219: loss: 0.22331523895263672\n",
      "iteration 20220: loss: 0.22331435978412628\n",
      "iteration 20221: loss: 0.223313570022583\n",
      "iteration 20222: loss: 0.22331266105175018\n",
      "iteration 20223: loss: 0.22331178188323975\n",
      "iteration 20224: loss: 0.22331099212169647\n",
      "iteration 20225: loss: 0.22331008315086365\n",
      "iteration 20226: loss: 0.2233092337846756\n",
      "iteration 20227: loss: 0.22330832481384277\n",
      "iteration 20228: loss: 0.22330746054649353\n",
      "iteration 20229: loss: 0.2233065813779831\n",
      "iteration 20230: loss: 0.22330574691295624\n",
      "iteration 20231: loss: 0.22330491244792938\n",
      "iteration 20232: loss: 0.22330406308174133\n",
      "iteration 20233: loss: 0.22330307960510254\n",
      "iteration 20234: loss: 0.22330227494239807\n",
      "iteration 20235: loss: 0.2233014553785324\n",
      "iteration 20236: loss: 0.22330057621002197\n",
      "iteration 20237: loss: 0.22329974174499512\n",
      "iteration 20238: loss: 0.22329886257648468\n",
      "iteration 20239: loss: 0.22329802811145782\n",
      "iteration 20240: loss: 0.223297119140625\n",
      "iteration 20241: loss: 0.22329625487327576\n",
      "iteration 20242: loss: 0.22329536080360413\n",
      "iteration 20243: loss: 0.22329452633857727\n",
      "iteration 20244: loss: 0.22329366207122803\n",
      "iteration 20245: loss: 0.2232927829027176\n",
      "iteration 20246: loss: 0.22329191863536835\n",
      "iteration 20247: loss: 0.22329111397266388\n",
      "iteration 20248: loss: 0.22329024970531464\n",
      "iteration 20249: loss: 0.22328941524028778\n",
      "iteration 20250: loss: 0.22328844666481018\n",
      "iteration 20251: loss: 0.22328758239746094\n",
      "iteration 20252: loss: 0.2232867181301117\n",
      "iteration 20253: loss: 0.22328588366508484\n",
      "iteration 20254: loss: 0.22328504920005798\n",
      "iteration 20255: loss: 0.22328424453735352\n",
      "iteration 20256: loss: 0.22328336536884308\n",
      "iteration 20257: loss: 0.22328250110149384\n",
      "iteration 20258: loss: 0.22328157722949982\n",
      "iteration 20259: loss: 0.22328071296215057\n",
      "iteration 20260: loss: 0.22327986359596252\n",
      "iteration 20261: loss: 0.22327902913093567\n",
      "iteration 20262: loss: 0.2232782393693924\n",
      "iteration 20263: loss: 0.22327733039855957\n",
      "iteration 20264: loss: 0.22327646613121033\n",
      "iteration 20265: loss: 0.22327561676502228\n",
      "iteration 20266: loss: 0.22327470779418945\n",
      "iteration 20267: loss: 0.2232738435268402\n",
      "iteration 20268: loss: 0.22327299416065216\n",
      "iteration 20269: loss: 0.2232721745967865\n",
      "iteration 20270: loss: 0.22327128052711487\n",
      "iteration 20271: loss: 0.2232704907655716\n",
      "iteration 20272: loss: 0.22326955199241638\n",
      "iteration 20273: loss: 0.22326874732971191\n",
      "iteration 20274: loss: 0.2232678383588791\n",
      "iteration 20275: loss: 0.22326703369617462\n",
      "iteration 20276: loss: 0.22326619923114777\n",
      "iteration 20277: loss: 0.22326533496379852\n",
      "iteration 20278: loss: 0.22326445579528809\n",
      "iteration 20279: loss: 0.22326354682445526\n",
      "iteration 20280: loss: 0.2232626974582672\n",
      "iteration 20281: loss: 0.22326190769672394\n",
      "iteration 20282: loss: 0.2232610434293747\n",
      "iteration 20283: loss: 0.22326020896434784\n",
      "iteration 20284: loss: 0.22325928509235382\n",
      "iteration 20285: loss: 0.22325842082500458\n",
      "iteration 20286: loss: 0.22325751185417175\n",
      "iteration 20287: loss: 0.2232566624879837\n",
      "iteration 20288: loss: 0.22325584292411804\n",
      "iteration 20289: loss: 0.22325506806373596\n",
      "iteration 20290: loss: 0.22325417399406433\n",
      "iteration 20291: loss: 0.22325332462787628\n",
      "iteration 20292: loss: 0.22325246036052704\n",
      "iteration 20293: loss: 0.223251610994339\n",
      "iteration 20294: loss: 0.22325070202350616\n",
      "iteration 20295: loss: 0.22324983775615692\n",
      "iteration 20296: loss: 0.22324900329113007\n",
      "iteration 20297: loss: 0.22324812412261963\n",
      "iteration 20298: loss: 0.22324728965759277\n",
      "iteration 20299: loss: 0.22324641048908234\n",
      "iteration 20300: loss: 0.22324557602405548\n",
      "iteration 20301: loss: 0.22324466705322266\n",
      "iteration 20302: loss: 0.2232438623905182\n",
      "iteration 20303: loss: 0.22324295341968536\n",
      "iteration 20304: loss: 0.22324208915233612\n",
      "iteration 20305: loss: 0.22324128448963165\n",
      "iteration 20306: loss: 0.22324040532112122\n",
      "iteration 20307: loss: 0.22323957085609436\n",
      "iteration 20308: loss: 0.22323867678642273\n",
      "iteration 20309: loss: 0.22323784232139587\n",
      "iteration 20310: loss: 0.22323694825172424\n",
      "iteration 20311: loss: 0.2232360541820526\n",
      "iteration 20312: loss: 0.22323521971702576\n",
      "iteration 20313: loss: 0.2232344150543213\n",
      "iteration 20314: loss: 0.22323353588581085\n",
      "iteration 20315: loss: 0.223232701420784\n",
      "iteration 20316: loss: 0.22323188185691833\n",
      "iteration 20317: loss: 0.22323103249073029\n",
      "iteration 20318: loss: 0.22323007881641388\n",
      "iteration 20319: loss: 0.22322925925254822\n",
      "iteration 20320: loss: 0.22322845458984375\n",
      "iteration 20321: loss: 0.2232275903224945\n",
      "iteration 20322: loss: 0.22322671115398407\n",
      "iteration 20323: loss: 0.22322587668895721\n",
      "iteration 20324: loss: 0.22322502732276917\n",
      "iteration 20325: loss: 0.22322416305541992\n",
      "iteration 20326: loss: 0.2232232540845871\n",
      "iteration 20327: loss: 0.22322240471839905\n",
      "iteration 20328: loss: 0.2232215404510498\n",
      "iteration 20329: loss: 0.22322070598602295\n",
      "iteration 20330: loss: 0.22321990132331848\n",
      "iteration 20331: loss: 0.22321900725364685\n",
      "iteration 20332: loss: 0.22321811318397522\n",
      "iteration 20333: loss: 0.22321732342243195\n",
      "iteration 20334: loss: 0.22321638464927673\n",
      "iteration 20335: loss: 0.22321555018424988\n",
      "iteration 20336: loss: 0.22321471571922302\n",
      "iteration 20337: loss: 0.22321388125419617\n",
      "iteration 20338: loss: 0.22321303188800812\n",
      "iteration 20339: loss: 0.22321216762065887\n",
      "iteration 20340: loss: 0.22321125864982605\n",
      "iteration 20341: loss: 0.2232104241847992\n",
      "iteration 20342: loss: 0.22320958971977234\n",
      "iteration 20343: loss: 0.2232087105512619\n",
      "iteration 20344: loss: 0.22320790588855743\n",
      "iteration 20345: loss: 0.2232070416212082\n",
      "iteration 20346: loss: 0.22320619225502014\n",
      "iteration 20347: loss: 0.2232053577899933\n",
      "iteration 20348: loss: 0.22320452332496643\n",
      "iteration 20349: loss: 0.22320358455181122\n",
      "iteration 20350: loss: 0.22320270538330078\n",
      "iteration 20351: loss: 0.2232019007205963\n",
      "iteration 20352: loss: 0.22320103645324707\n",
      "iteration 20353: loss: 0.22320015728473663\n",
      "iteration 20354: loss: 0.2231992930173874\n",
      "iteration 20355: loss: 0.22319850325584412\n",
      "iteration 20356: loss: 0.22319774329662323\n",
      "iteration 20357: loss: 0.22319678962230682\n",
      "iteration 20358: loss: 0.223195880651474\n",
      "iteration 20359: loss: 0.22319507598876953\n",
      "iteration 20360: loss: 0.22319427132606506\n",
      "iteration 20361: loss: 0.22319333255290985\n",
      "iteration 20362: loss: 0.22319252789020538\n",
      "iteration 20363: loss: 0.22319169342517853\n",
      "iteration 20364: loss: 0.22319075465202332\n",
      "iteration 20365: loss: 0.22318990528583527\n",
      "iteration 20366: loss: 0.22318904101848602\n",
      "iteration 20367: loss: 0.22318820655345917\n",
      "iteration 20368: loss: 0.2231874018907547\n",
      "iteration 20369: loss: 0.22318661212921143\n",
      "iteration 20370: loss: 0.2231857031583786\n",
      "iteration 20371: loss: 0.22318485379219055\n",
      "iteration 20372: loss: 0.2231840193271637\n",
      "iteration 20373: loss: 0.22318312525749207\n",
      "iteration 20374: loss: 0.22318227589130402\n",
      "iteration 20375: loss: 0.22318141162395477\n",
      "iteration 20376: loss: 0.2231806069612503\n",
      "iteration 20377: loss: 0.22317977249622345\n",
      "iteration 20378: loss: 0.2231789082288742\n",
      "iteration 20379: loss: 0.22317799925804138\n",
      "iteration 20380: loss: 0.22317716479301453\n",
      "iteration 20381: loss: 0.2231762856245041\n",
      "iteration 20382: loss: 0.22317545115947723\n",
      "iteration 20383: loss: 0.22317469120025635\n",
      "iteration 20384: loss: 0.22317378222942352\n",
      "iteration 20385: loss: 0.22317290306091309\n",
      "iteration 20386: loss: 0.22317209839820862\n",
      "iteration 20387: loss: 0.223171204328537\n",
      "iteration 20388: loss: 0.22317031025886536\n",
      "iteration 20389: loss: 0.2231694757938385\n",
      "iteration 20390: loss: 0.22316865622997284\n",
      "iteration 20391: loss: 0.2231677770614624\n",
      "iteration 20392: loss: 0.22316697239875793\n",
      "iteration 20393: loss: 0.22316613793373108\n",
      "iteration 20394: loss: 0.22316530346870422\n",
      "iteration 20395: loss: 0.22316443920135498\n",
      "iteration 20396: loss: 0.22316356003284454\n",
      "iteration 20397: loss: 0.22316274046897888\n",
      "iteration 20398: loss: 0.22316186130046844\n",
      "iteration 20399: loss: 0.223160982131958\n",
      "iteration 20400: loss: 0.22316014766693115\n",
      "iteration 20401: loss: 0.2231592833995819\n",
      "iteration 20402: loss: 0.22315843403339386\n",
      "iteration 20403: loss: 0.22315764427185059\n",
      "iteration 20404: loss: 0.22315678000450134\n",
      "iteration 20405: loss: 0.2231559455394745\n",
      "iteration 20406: loss: 0.22315499186515808\n",
      "iteration 20407: loss: 0.223154217004776\n",
      "iteration 20408: loss: 0.22315338253974915\n",
      "iteration 20409: loss: 0.2231525480747223\n",
      "iteration 20410: loss: 0.22315171360969543\n",
      "iteration 20411: loss: 0.22315077483654022\n",
      "iteration 20412: loss: 0.22314997017383575\n",
      "iteration 20413: loss: 0.2231491059064865\n",
      "iteration 20414: loss: 0.22314822673797607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 20415: loss: 0.2231474220752716\n",
      "iteration 20416: loss: 0.22314655780792236\n",
      "iteration 20417: loss: 0.22314564883708954\n",
      "iteration 20418: loss: 0.22314481437206268\n",
      "iteration 20419: loss: 0.22314396500587463\n",
      "iteration 20420: loss: 0.22314317524433136\n",
      "iteration 20421: loss: 0.22314229607582092\n",
      "iteration 20422: loss: 0.22314147651195526\n",
      "iteration 20423: loss: 0.2231406271457672\n",
      "iteration 20424: loss: 0.22313973307609558\n",
      "iteration 20425: loss: 0.22313889861106873\n",
      "iteration 20426: loss: 0.22313804924488068\n",
      "iteration 20427: loss: 0.22313718497753143\n",
      "iteration 20428: loss: 0.22313639521598816\n",
      "iteration 20429: loss: 0.22313551604747772\n",
      "iteration 20430: loss: 0.22313471138477325\n",
      "iteration 20431: loss: 0.22313383221626282\n",
      "iteration 20432: loss: 0.22313292324543\n",
      "iteration 20433: loss: 0.22313213348388672\n",
      "iteration 20434: loss: 0.2231312245130539\n",
      "iteration 20435: loss: 0.22313043475151062\n",
      "iteration 20436: loss: 0.22312958538532257\n",
      "iteration 20437: loss: 0.22312870621681213\n",
      "iteration 20438: loss: 0.22312791645526886\n",
      "iteration 20439: loss: 0.22312696278095245\n",
      "iteration 20440: loss: 0.22312617301940918\n",
      "iteration 20441: loss: 0.22312533855438232\n",
      "iteration 20442: loss: 0.22312453389167786\n",
      "iteration 20443: loss: 0.2231236696243286\n",
      "iteration 20444: loss: 0.22312279045581818\n",
      "iteration 20445: loss: 0.22312191128730774\n",
      "iteration 20446: loss: 0.22312109172344208\n",
      "iteration 20447: loss: 0.22312025725841522\n",
      "iteration 20448: loss: 0.22311940789222717\n",
      "iteration 20449: loss: 0.22311857342720032\n",
      "iteration 20450: loss: 0.2231176793575287\n",
      "iteration 20451: loss: 0.22311687469482422\n",
      "iteration 20452: loss: 0.22311604022979736\n",
      "iteration 20453: loss: 0.22311516106128693\n",
      "iteration 20454: loss: 0.22311434149742126\n",
      "iteration 20455: loss: 0.22311344742774963\n",
      "iteration 20456: loss: 0.2231125831604004\n",
      "iteration 20457: loss: 0.2231118232011795\n",
      "iteration 20458: loss: 0.22311091423034668\n",
      "iteration 20459: loss: 0.2231101095676422\n",
      "iteration 20460: loss: 0.22310921549797058\n",
      "iteration 20461: loss: 0.22310838103294373\n",
      "iteration 20462: loss: 0.22310754656791687\n",
      "iteration 20463: loss: 0.22310671210289001\n",
      "iteration 20464: loss: 0.22310583293437958\n",
      "iteration 20465: loss: 0.22310499846935272\n",
      "iteration 20466: loss: 0.22310411930084229\n",
      "iteration 20467: loss: 0.223103329539299\n",
      "iteration 20468: loss: 0.22310245037078857\n",
      "iteration 20469: loss: 0.22310161590576172\n",
      "iteration 20470: loss: 0.22310075163841248\n",
      "iteration 20471: loss: 0.22309991717338562\n",
      "iteration 20472: loss: 0.22309911251068115\n",
      "iteration 20473: loss: 0.22309818863868713\n",
      "iteration 20474: loss: 0.22309739887714386\n",
      "iteration 20475: loss: 0.22309660911560059\n",
      "iteration 20476: loss: 0.22309572994709015\n",
      "iteration 20477: loss: 0.2230948954820633\n",
      "iteration 20478: loss: 0.22309401631355286\n",
      "iteration 20479: loss: 0.22309312224388123\n",
      "iteration 20480: loss: 0.22309228777885437\n",
      "iteration 20481: loss: 0.22309145331382751\n",
      "iteration 20482: loss: 0.22309067845344543\n",
      "iteration 20483: loss: 0.2230898141860962\n",
      "iteration 20484: loss: 0.22308897972106934\n",
      "iteration 20485: loss: 0.2230881005525589\n",
      "iteration 20486: loss: 0.22308722138404846\n",
      "iteration 20487: loss: 0.2230863869190216\n",
      "iteration 20488: loss: 0.22308559715747833\n",
      "iteration 20489: loss: 0.22308476269245148\n",
      "iteration 20490: loss: 0.22308389842510223\n",
      "iteration 20491: loss: 0.22308306396007538\n",
      "iteration 20492: loss: 0.22308218479156494\n",
      "iteration 20493: loss: 0.22308139503002167\n",
      "iteration 20494: loss: 0.22308048605918884\n",
      "iteration 20495: loss: 0.2230796366930008\n",
      "iteration 20496: loss: 0.22307880222797394\n",
      "iteration 20497: loss: 0.22307796776294708\n",
      "iteration 20498: loss: 0.22307710349559784\n",
      "iteration 20499: loss: 0.22307626903057098\n",
      "iteration 20500: loss: 0.22307541966438293\n",
      "iteration 20501: loss: 0.22307462990283966\n",
      "iteration 20502: loss: 0.22307376563549042\n",
      "iteration 20503: loss: 0.22307296097278595\n",
      "iteration 20504: loss: 0.22307205200195312\n",
      "iteration 20505: loss: 0.22307130694389343\n",
      "iteration 20506: loss: 0.22307038307189941\n",
      "iteration 20507: loss: 0.22306950390338898\n",
      "iteration 20508: loss: 0.2230687141418457\n",
      "iteration 20509: loss: 0.22306787967681885\n",
      "iteration 20510: loss: 0.2230670154094696\n",
      "iteration 20511: loss: 0.22306613624095917\n",
      "iteration 20512: loss: 0.22306528687477112\n",
      "iteration 20513: loss: 0.22306451201438904\n",
      "iteration 20514: loss: 0.2230636179447174\n",
      "iteration 20515: loss: 0.22306282818317413\n",
      "iteration 20516: loss: 0.2230619490146637\n",
      "iteration 20517: loss: 0.22306112945079803\n",
      "iteration 20518: loss: 0.22306028008460999\n",
      "iteration 20519: loss: 0.22305946052074432\n",
      "iteration 20520: loss: 0.22305862605571747\n",
      "iteration 20521: loss: 0.22305774688720703\n",
      "iteration 20522: loss: 0.2230568826198578\n",
      "iteration 20523: loss: 0.22305603325366974\n",
      "iteration 20524: loss: 0.22305519878864288\n",
      "iteration 20525: loss: 0.22305433452129364\n",
      "iteration 20526: loss: 0.22305352985858917\n",
      "iteration 20527: loss: 0.2230527400970459\n",
      "iteration 20528: loss: 0.22305187582969666\n",
      "iteration 20529: loss: 0.2230510264635086\n",
      "iteration 20530: loss: 0.22305016219615936\n",
      "iteration 20531: loss: 0.2230493128299713\n",
      "iteration 20532: loss: 0.22304852306842804\n",
      "iteration 20533: loss: 0.22304768860340118\n",
      "iteration 20534: loss: 0.22304674983024597\n",
      "iteration 20535: loss: 0.22304591536521912\n",
      "iteration 20536: loss: 0.22304511070251465\n",
      "iteration 20537: loss: 0.22304432094097137\n",
      "iteration 20538: loss: 0.22304347157478333\n",
      "iteration 20539: loss: 0.22304260730743408\n",
      "iteration 20540: loss: 0.22304177284240723\n",
      "iteration 20541: loss: 0.2230408936738968\n",
      "iteration 20542: loss: 0.22304007411003113\n",
      "iteration 20543: loss: 0.22303923964500427\n",
      "iteration 20544: loss: 0.22303839027881622\n",
      "iteration 20545: loss: 0.22303752601146698\n",
      "iteration 20546: loss: 0.22303664684295654\n",
      "iteration 20547: loss: 0.22303585708141327\n",
      "iteration 20548: loss: 0.22303500771522522\n",
      "iteration 20549: loss: 0.22303423285484314\n",
      "iteration 20550: loss: 0.2230333387851715\n",
      "iteration 20551: loss: 0.22303251922130585\n",
      "iteration 20552: loss: 0.223031684756279\n",
      "iteration 20553: loss: 0.22303088009357452\n",
      "iteration 20554: loss: 0.2230299413204193\n",
      "iteration 20555: loss: 0.22302913665771484\n",
      "iteration 20556: loss: 0.223028302192688\n",
      "iteration 20557: loss: 0.22302746772766113\n",
      "iteration 20558: loss: 0.22302663326263428\n",
      "iteration 20559: loss: 0.2230258285999298\n",
      "iteration 20560: loss: 0.22302499413490295\n",
      "iteration 20561: loss: 0.2230241596698761\n",
      "iteration 20562: loss: 0.2230231761932373\n",
      "iteration 20563: loss: 0.22302241623401642\n",
      "iteration 20564: loss: 0.22302158176898956\n",
      "iteration 20565: loss: 0.2230207473039627\n",
      "iteration 20566: loss: 0.22301995754241943\n",
      "iteration 20567: loss: 0.223019078373909\n",
      "iteration 20568: loss: 0.22301819920539856\n",
      "iteration 20569: loss: 0.2230173647403717\n",
      "iteration 20570: loss: 0.22301657497882843\n",
      "iteration 20571: loss: 0.22301575541496277\n",
      "iteration 20572: loss: 0.22301490604877472\n",
      "iteration 20573: loss: 0.22301404178142548\n",
      "iteration 20574: loss: 0.22301319241523743\n",
      "iteration 20575: loss: 0.2230122983455658\n",
      "iteration 20576: loss: 0.2230115383863449\n",
      "iteration 20577: loss: 0.22301073372364044\n",
      "iteration 20578: loss: 0.2230098694562912\n",
      "iteration 20579: loss: 0.22300896048545837\n",
      "iteration 20580: loss: 0.2230081856250763\n",
      "iteration 20581: loss: 0.22300735116004944\n",
      "iteration 20582: loss: 0.22300653159618378\n",
      "iteration 20583: loss: 0.22300569713115692\n",
      "iteration 20584: loss: 0.2230047881603241\n",
      "iteration 20585: loss: 0.22300395369529724\n",
      "iteration 20586: loss: 0.22300314903259277\n",
      "iteration 20587: loss: 0.22300231456756592\n",
      "iteration 20588: loss: 0.22300150990486145\n",
      "iteration 20589: loss: 0.22300061583518982\n",
      "iteration 20590: loss: 0.22299984097480774\n",
      "iteration 20591: loss: 0.2229989767074585\n",
      "iteration 20592: loss: 0.22299809753894806\n",
      "iteration 20593: loss: 0.2229972779750824\n",
      "iteration 20594: loss: 0.22299647331237793\n",
      "iteration 20595: loss: 0.22299563884735107\n",
      "iteration 20596: loss: 0.22299472987651825\n",
      "iteration 20597: loss: 0.22299394011497498\n",
      "iteration 20598: loss: 0.22299310564994812\n",
      "iteration 20599: loss: 0.22299230098724365\n",
      "iteration 20600: loss: 0.2229914367198944\n",
      "iteration 20601: loss: 0.22299063205718994\n",
      "iteration 20602: loss: 0.22298979759216309\n",
      "iteration 20603: loss: 0.22298888862133026\n",
      "iteration 20604: loss: 0.2229880839586258\n",
      "iteration 20605: loss: 0.22298729419708252\n",
      "iteration 20606: loss: 0.22298641502857208\n",
      "iteration 20607: loss: 0.2229856252670288\n",
      "iteration 20608: loss: 0.22298476099967957\n",
      "iteration 20609: loss: 0.22298386693000793\n",
      "iteration 20610: loss: 0.22298303246498108\n",
      "iteration 20611: loss: 0.22298221290111542\n",
      "iteration 20612: loss: 0.22298140823841095\n",
      "iteration 20613: loss: 0.2229805737733841\n",
      "iteration 20614: loss: 0.22297975420951843\n",
      "iteration 20615: loss: 0.2229788601398468\n",
      "iteration 20616: loss: 0.22297808527946472\n",
      "iteration 20617: loss: 0.22297720611095428\n",
      "iteration 20618: loss: 0.222976416349411\n",
      "iteration 20619: loss: 0.22297556698322296\n",
      "iteration 20620: loss: 0.2229747474193573\n",
      "iteration 20621: loss: 0.22297389805316925\n",
      "iteration 20622: loss: 0.2229730635881424\n",
      "iteration 20623: loss: 0.22297224402427673\n",
      "iteration 20624: loss: 0.2229713648557663\n",
      "iteration 20625: loss: 0.22297053039073944\n",
      "iteration 20626: loss: 0.22296974062919617\n",
      "iteration 20627: loss: 0.22296886146068573\n",
      "iteration 20628: loss: 0.22296802699565887\n",
      "iteration 20629: loss: 0.22296719253063202\n",
      "iteration 20630: loss: 0.22296640276908875\n",
      "iteration 20631: loss: 0.2229655683040619\n",
      "iteration 20632: loss: 0.22296471893787384\n",
      "iteration 20633: loss: 0.2229638397693634\n",
      "iteration 20634: loss: 0.22296302020549774\n",
      "iteration 20635: loss: 0.22296221554279327\n",
      "iteration 20636: loss: 0.22296138107776642\n",
      "iteration 20637: loss: 0.22296056151390076\n",
      "iteration 20638: loss: 0.2229597121477127\n",
      "iteration 20639: loss: 0.22295883297920227\n",
      "iteration 20640: loss: 0.2229580134153366\n",
      "iteration 20641: loss: 0.22295717895030975\n",
      "iteration 20642: loss: 0.22295638918876648\n",
      "iteration 20643: loss: 0.22295555472373962\n",
      "iteration 20644: loss: 0.222954660654068\n",
      "iteration 20645: loss: 0.22295387089252472\n",
      "iteration 20646: loss: 0.22295300662517548\n",
      "iteration 20647: loss: 0.222952201962471\n",
      "iteration 20648: loss: 0.22295132279396057\n",
      "iteration 20649: loss: 0.22295048832893372\n",
      "iteration 20650: loss: 0.22294969856739044\n",
      "iteration 20651: loss: 0.22294887900352478\n",
      "iteration 20652: loss: 0.22294804453849792\n",
      "iteration 20653: loss: 0.22294721007347107\n",
      "iteration 20654: loss: 0.22294631600379944\n",
      "iteration 20655: loss: 0.22294549643993378\n",
      "iteration 20656: loss: 0.2229447066783905\n",
      "iteration 20657: loss: 0.22294387221336365\n",
      "iteration 20658: loss: 0.2229430228471756\n",
      "iteration 20659: loss: 0.22294218838214874\n",
      "iteration 20660: loss: 0.22294139862060547\n",
      "iteration 20661: loss: 0.22294048964977264\n",
      "iteration 20662: loss: 0.22293968498706818\n",
      "iteration 20663: loss: 0.2229388952255249\n",
      "iteration 20664: loss: 0.22293803095817566\n",
      "iteration 20665: loss: 0.2229371815919876\n",
      "iteration 20666: loss: 0.22293639183044434\n",
      "iteration 20667: loss: 0.2229355126619339\n",
      "iteration 20668: loss: 0.22293472290039062\n",
      "iteration 20669: loss: 0.22293391823768616\n",
      "iteration 20670: loss: 0.22293302416801453\n",
      "iteration 20671: loss: 0.22293226420879364\n",
      "iteration 20672: loss: 0.2229313850402832\n",
      "iteration 20673: loss: 0.22293058037757874\n",
      "iteration 20674: loss: 0.22292974591255188\n",
      "iteration 20675: loss: 0.22292891144752502\n",
      "iteration 20676: loss: 0.22292804718017578\n",
      "iteration 20677: loss: 0.2229272425174713\n",
      "iteration 20678: loss: 0.22292640805244446\n",
      "iteration 20679: loss: 0.2229255884885788\n",
      "iteration 20680: loss: 0.22292478382587433\n",
      "iteration 20681: loss: 0.2229238748550415\n",
      "iteration 20682: loss: 0.22292308509349823\n",
      "iteration 20683: loss: 0.22292228043079376\n",
      "iteration 20684: loss: 0.22292141616344452\n",
      "iteration 20685: loss: 0.22292056679725647\n",
      "iteration 20686: loss: 0.22291985154151917\n",
      "iteration 20687: loss: 0.22291889786720276\n",
      "iteration 20688: loss: 0.2229180783033371\n",
      "iteration 20689: loss: 0.22291727364063263\n",
      "iteration 20690: loss: 0.2229164093732834\n",
      "iteration 20691: loss: 0.22291560471057892\n",
      "iteration 20692: loss: 0.22291478514671326\n",
      "iteration 20693: loss: 0.2229139357805252\n",
      "iteration 20694: loss: 0.22291311621665955\n",
      "iteration 20695: loss: 0.2229122668504715\n",
      "iteration 20696: loss: 0.22291143238544464\n",
      "iteration 20697: loss: 0.22291064262390137\n",
      "iteration 20698: loss: 0.22290977835655212\n",
      "iteration 20699: loss: 0.22290894389152527\n",
      "iteration 20700: loss: 0.2229081392288208\n",
      "iteration 20701: loss: 0.22290730476379395\n",
      "iteration 20702: loss: 0.22290650010108948\n",
      "iteration 20703: loss: 0.22290568053722382\n",
      "iteration 20704: loss: 0.2229047566652298\n",
      "iteration 20705: loss: 0.22290396690368652\n",
      "iteration 20706: loss: 0.22290317714214325\n",
      "iteration 20707: loss: 0.2229023426771164\n",
      "iteration 20708: loss: 0.22290153801441193\n",
      "iteration 20709: loss: 0.2229006290435791\n",
      "iteration 20710: loss: 0.22289986908435822\n",
      "iteration 20711: loss: 0.22289898991584778\n",
      "iteration 20712: loss: 0.2228982001543045\n",
      "iteration 20713: loss: 0.22289733588695526\n",
      "iteration 20714: loss: 0.2228965312242508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 20715: loss: 0.22289571166038513\n",
      "iteration 20716: loss: 0.2228948324918747\n",
      "iteration 20717: loss: 0.22289402782917023\n",
      "iteration 20718: loss: 0.22289323806762695\n",
      "iteration 20719: loss: 0.22289244830608368\n",
      "iteration 20720: loss: 0.22289152443408966\n",
      "iteration 20721: loss: 0.222890704870224\n",
      "iteration 20722: loss: 0.22288993000984192\n",
      "iteration 20723: loss: 0.22288914024829865\n",
      "iteration 20724: loss: 0.2228882759809494\n",
      "iteration 20725: loss: 0.22288742661476135\n",
      "iteration 20726: loss: 0.2228866070508957\n",
      "iteration 20727: loss: 0.22288575768470764\n",
      "iteration 20728: loss: 0.22288493812084198\n",
      "iteration 20729: loss: 0.22288410365581512\n",
      "iteration 20730: loss: 0.2228832244873047\n",
      "iteration 20731: loss: 0.2228824645280838\n",
      "iteration 20732: loss: 0.22288170456886292\n",
      "iteration 20733: loss: 0.22288084030151367\n",
      "iteration 20734: loss: 0.22287997603416443\n",
      "iteration 20735: loss: 0.22287920117378235\n",
      "iteration 20736: loss: 0.22287830710411072\n",
      "iteration 20737: loss: 0.22287745773792267\n",
      "iteration 20738: loss: 0.222876638174057\n",
      "iteration 20739: loss: 0.22287583351135254\n",
      "iteration 20740: loss: 0.22287499904632568\n",
      "iteration 20741: loss: 0.22287419438362122\n",
      "iteration 20742: loss: 0.22287330031394958\n",
      "iteration 20743: loss: 0.22287246584892273\n",
      "iteration 20744: loss: 0.22287175059318542\n",
      "iteration 20745: loss: 0.2228708565235138\n",
      "iteration 20746: loss: 0.22287006676197052\n",
      "iteration 20747: loss: 0.22286924719810486\n",
      "iteration 20748: loss: 0.22286835312843323\n",
      "iteration 20749: loss: 0.22286748886108398\n",
      "iteration 20750: loss: 0.2228667438030243\n",
      "iteration 20751: loss: 0.22286586463451385\n",
      "iteration 20752: loss: 0.22286510467529297\n",
      "iteration 20753: loss: 0.2228642702102661\n",
      "iteration 20754: loss: 0.22286343574523926\n",
      "iteration 20755: loss: 0.22286257147789001\n",
      "iteration 20756: loss: 0.22286179661750793\n",
      "iteration 20757: loss: 0.22286102175712585\n",
      "iteration 20758: loss: 0.22286009788513184\n",
      "iteration 20759: loss: 0.22285933792591095\n",
      "iteration 20760: loss: 0.2228584587574005\n",
      "iteration 20761: loss: 0.22285771369934082\n",
      "iteration 20762: loss: 0.22285687923431396\n",
      "iteration 20763: loss: 0.22285600006580353\n",
      "iteration 20764: loss: 0.22285521030426025\n",
      "iteration 20765: loss: 0.2228543758392334\n",
      "iteration 20766: loss: 0.22285354137420654\n",
      "iteration 20767: loss: 0.22285273671150208\n",
      "iteration 20768: loss: 0.2228519469499588\n",
      "iteration 20769: loss: 0.22285108268260956\n",
      "iteration 20770: loss: 0.2228502780199051\n",
      "iteration 20771: loss: 0.22284945845603943\n",
      "iteration 20772: loss: 0.22284865379333496\n",
      "iteration 20773: loss: 0.22284778952598572\n",
      "iteration 20774: loss: 0.22284701466560364\n",
      "iteration 20775: loss: 0.2228461503982544\n",
      "iteration 20776: loss: 0.22284527122974396\n",
      "iteration 20777: loss: 0.2228444516658783\n",
      "iteration 20778: loss: 0.22284367680549622\n",
      "iteration 20779: loss: 0.22284281253814697\n",
      "iteration 20780: loss: 0.2228420227766037\n",
      "iteration 20781: loss: 0.22284121811389923\n",
      "iteration 20782: loss: 0.22284035384655\n",
      "iteration 20783: loss: 0.22283951938152313\n",
      "iteration 20784: loss: 0.22283872961997986\n",
      "iteration 20785: loss: 0.222837895154953\n",
      "iteration 20786: loss: 0.22283704578876495\n",
      "iteration 20787: loss: 0.2228362113237381\n",
      "iteration 20788: loss: 0.22283539175987244\n",
      "iteration 20789: loss: 0.22283458709716797\n",
      "iteration 20790: loss: 0.2228337526321411\n",
      "iteration 20791: loss: 0.22283291816711426\n",
      "iteration 20792: loss: 0.2228321135044098\n",
      "iteration 20793: loss: 0.22283124923706055\n",
      "iteration 20794: loss: 0.22283053398132324\n",
      "iteration 20795: loss: 0.2228296995162964\n",
      "iteration 20796: loss: 0.22282877564430237\n",
      "iteration 20797: loss: 0.2228280007839203\n",
      "iteration 20798: loss: 0.22282716631889343\n",
      "iteration 20799: loss: 0.22282639145851135\n",
      "iteration 20800: loss: 0.2228255718946457\n",
      "iteration 20801: loss: 0.22282472252845764\n",
      "iteration 20802: loss: 0.2228238880634308\n",
      "iteration 20803: loss: 0.22282305359840393\n",
      "iteration 20804: loss: 0.22282223403453827\n",
      "iteration 20805: loss: 0.2228214293718338\n",
      "iteration 20806: loss: 0.22282060980796814\n",
      "iteration 20807: loss: 0.2228197604417801\n",
      "iteration 20808: loss: 0.22281894087791443\n",
      "iteration 20809: loss: 0.22281818091869354\n",
      "iteration 20810: loss: 0.22281727194786072\n",
      "iteration 20811: loss: 0.2228165566921234\n",
      "iteration 20812: loss: 0.22281566262245178\n",
      "iteration 20813: loss: 0.22281484305858612\n",
      "iteration 20814: loss: 0.22281399369239807\n",
      "iteration 20815: loss: 0.222813218832016\n",
      "iteration 20816: loss: 0.22281241416931152\n",
      "iteration 20817: loss: 0.22281154990196228\n",
      "iteration 20818: loss: 0.2228107452392578\n",
      "iteration 20819: loss: 0.22280994057655334\n",
      "iteration 20820: loss: 0.22280915081501007\n",
      "iteration 20821: loss: 0.2228083610534668\n",
      "iteration 20822: loss: 0.22280748188495636\n",
      "iteration 20823: loss: 0.2228066474199295\n",
      "iteration 20824: loss: 0.22280585765838623\n",
      "iteration 20825: loss: 0.22280502319335938\n",
      "iteration 20826: loss: 0.22280418872833252\n",
      "iteration 20827: loss: 0.22280332446098328\n",
      "iteration 20828: loss: 0.22280259430408478\n",
      "iteration 20829: loss: 0.22280175983905792\n",
      "iteration 20830: loss: 0.2228008508682251\n",
      "iteration 20831: loss: 0.22280006110668182\n",
      "iteration 20832: loss: 0.22279930114746094\n",
      "iteration 20833: loss: 0.22279849648475647\n",
      "iteration 20834: loss: 0.2227976769208908\n",
      "iteration 20835: loss: 0.22279679775238037\n",
      "iteration 20836: loss: 0.22279596328735352\n",
      "iteration 20837: loss: 0.22279520332813263\n",
      "iteration 20838: loss: 0.22279436886310577\n",
      "iteration 20839: loss: 0.22279353439807892\n",
      "iteration 20840: loss: 0.22279271483421326\n",
      "iteration 20841: loss: 0.2227918803691864\n",
      "iteration 20842: loss: 0.22279104590415955\n",
      "iteration 20843: loss: 0.22279024124145508\n",
      "iteration 20844: loss: 0.2227894365787506\n",
      "iteration 20845: loss: 0.22278861701488495\n",
      "iteration 20846: loss: 0.2227877825498581\n",
      "iteration 20847: loss: 0.22278699278831482\n",
      "iteration 20848: loss: 0.22278618812561035\n",
      "iteration 20849: loss: 0.2227853238582611\n",
      "iteration 20850: loss: 0.22278451919555664\n",
      "iteration 20851: loss: 0.22278371453285217\n",
      "iteration 20852: loss: 0.22278288006782532\n",
      "iteration 20853: loss: 0.22278210520744324\n",
      "iteration 20854: loss: 0.2227812558412552\n",
      "iteration 20855: loss: 0.22278042137622833\n",
      "iteration 20856: loss: 0.22277963161468506\n",
      "iteration 20857: loss: 0.2227788269519806\n",
      "iteration 20858: loss: 0.22277799248695374\n",
      "iteration 20859: loss: 0.22277715802192688\n",
      "iteration 20860: loss: 0.22277633845806122\n",
      "iteration 20861: loss: 0.22277553379535675\n",
      "iteration 20862: loss: 0.2227746993303299\n",
      "iteration 20863: loss: 0.22277390956878662\n",
      "iteration 20864: loss: 0.22277310490608215\n",
      "iteration 20865: loss: 0.22277221083641052\n",
      "iteration 20866: loss: 0.22277143597602844\n",
      "iteration 20867: loss: 0.22277061641216278\n",
      "iteration 20868: loss: 0.2227698266506195\n",
      "iteration 20869: loss: 0.22276897728443146\n",
      "iteration 20870: loss: 0.22276821732521057\n",
      "iteration 20871: loss: 0.22276730835437775\n",
      "iteration 20872: loss: 0.22276656329631805\n",
      "iteration 20873: loss: 0.2227657288312912\n",
      "iteration 20874: loss: 0.22276487946510315\n",
      "iteration 20875: loss: 0.2227640599012375\n",
      "iteration 20876: loss: 0.22276325523853302\n",
      "iteration 20877: loss: 0.22276243567466736\n",
      "iteration 20878: loss: 0.2227616310119629\n",
      "iteration 20879: loss: 0.22276084125041962\n",
      "iteration 20880: loss: 0.22275999188423157\n",
      "iteration 20881: loss: 0.2227592021226883\n",
      "iteration 20882: loss: 0.22275838255882263\n",
      "iteration 20883: loss: 0.22275753319263458\n",
      "iteration 20884: loss: 0.2227567732334137\n",
      "iteration 20885: loss: 0.22275587916374207\n",
      "iteration 20886: loss: 0.22275510430335999\n",
      "iteration 20887: loss: 0.22275424003601074\n",
      "iteration 20888: loss: 0.22275345027446747\n",
      "iteration 20889: loss: 0.22275272011756897\n",
      "iteration 20890: loss: 0.22275185585021973\n",
      "iteration 20891: loss: 0.2227509766817093\n",
      "iteration 20892: loss: 0.2227502316236496\n",
      "iteration 20893: loss: 0.22274939715862274\n",
      "iteration 20894: loss: 0.22274859249591827\n",
      "iteration 20895: loss: 0.2227477729320526\n",
      "iteration 20896: loss: 0.22274699807167053\n",
      "iteration 20897: loss: 0.22274616360664368\n",
      "iteration 20898: loss: 0.22274534404277802\n",
      "iteration 20899: loss: 0.22274450957775116\n",
      "iteration 20900: loss: 0.2227436751127243\n",
      "iteration 20901: loss: 0.2227429449558258\n",
      "iteration 20902: loss: 0.22274205088615417\n",
      "iteration 20903: loss: 0.22274121642112732\n",
      "iteration 20904: loss: 0.22274041175842285\n",
      "iteration 20905: loss: 0.22273965179920197\n",
      "iteration 20906: loss: 0.22273878753185272\n",
      "iteration 20907: loss: 0.22273795306682587\n",
      "iteration 20908: loss: 0.22273719310760498\n",
      "iteration 20909: loss: 0.2227364033460617\n",
      "iteration 20910: loss: 0.22273552417755127\n",
      "iteration 20911: loss: 0.22273468971252441\n",
      "iteration 20912: loss: 0.22273389995098114\n",
      "iteration 20913: loss: 0.22273309528827667\n",
      "iteration 20914: loss: 0.22273223102092743\n",
      "iteration 20915: loss: 0.22273144125938416\n",
      "iteration 20916: loss: 0.22273066639900208\n",
      "iteration 20917: loss: 0.22272983193397522\n",
      "iteration 20918: loss: 0.22272905707359314\n",
      "iteration 20919: loss: 0.22272822260856628\n",
      "iteration 20920: loss: 0.22272738814353943\n",
      "iteration 20921: loss: 0.22272661328315735\n",
      "iteration 20922: loss: 0.22272582352161407\n",
      "iteration 20923: loss: 0.22272491455078125\n",
      "iteration 20924: loss: 0.22272412478923798\n",
      "iteration 20925: loss: 0.2227233201265335\n",
      "iteration 20926: loss: 0.22272250056266785\n",
      "iteration 20927: loss: 0.22272174060344696\n",
      "iteration 20928: loss: 0.2227209508419037\n",
      "iteration 20929: loss: 0.22272005677223206\n",
      "iteration 20930: loss: 0.22271928191184998\n",
      "iteration 20931: loss: 0.2227185070514679\n",
      "iteration 20932: loss: 0.22271764278411865\n",
      "iteration 20933: loss: 0.2227168083190918\n",
      "iteration 20934: loss: 0.22271600365638733\n",
      "iteration 20935: loss: 0.22271522879600525\n",
      "iteration 20936: loss: 0.2227143943309784\n",
      "iteration 20937: loss: 0.22271351516246796\n",
      "iteration 20938: loss: 0.22271278500556946\n",
      "iteration 20939: loss: 0.2227119505405426\n",
      "iteration 20940: loss: 0.22271117568016052\n",
      "iteration 20941: loss: 0.22271037101745605\n",
      "iteration 20942: loss: 0.2227095365524292\n",
      "iteration 20943: loss: 0.22270873188972473\n",
      "iteration 20944: loss: 0.22270791232585907\n",
      "iteration 20945: loss: 0.22270703315734863\n",
      "iteration 20946: loss: 0.22270627319812775\n",
      "iteration 20947: loss: 0.22270545363426208\n",
      "iteration 20948: loss: 0.22270464897155762\n",
      "iteration 20949: loss: 0.22270388901233673\n",
      "iteration 20950: loss: 0.2227030247449875\n",
      "iteration 20951: loss: 0.22270219027996063\n",
      "iteration 20952: loss: 0.22270138561725616\n",
      "iteration 20953: loss: 0.2227005511522293\n",
      "iteration 20954: loss: 0.22269979119300842\n",
      "iteration 20955: loss: 0.22269897162914276\n",
      "iteration 20956: loss: 0.22269821166992188\n",
      "iteration 20957: loss: 0.22269737720489502\n",
      "iteration 20958: loss: 0.22269657254219055\n",
      "iteration 20959: loss: 0.2226957529783249\n",
      "iteration 20960: loss: 0.22269494831562042\n",
      "iteration 20961: loss: 0.22269411385059357\n",
      "iteration 20962: loss: 0.2226932793855667\n",
      "iteration 20963: loss: 0.22269245982170105\n",
      "iteration 20964: loss: 0.22269165515899658\n",
      "iteration 20965: loss: 0.2226908653974533\n",
      "iteration 20966: loss: 0.22269006073474884\n",
      "iteration 20967: loss: 0.22268924117088318\n",
      "iteration 20968: loss: 0.2226884663105011\n",
      "iteration 20969: loss: 0.22268767654895782\n",
      "iteration 20970: loss: 0.2226867973804474\n",
      "iteration 20971: loss: 0.2226860076189041\n",
      "iteration 20972: loss: 0.22268517315387726\n",
      "iteration 20973: loss: 0.22268441319465637\n",
      "iteration 20974: loss: 0.2226836234331131\n",
      "iteration 20975: loss: 0.22268271446228027\n",
      "iteration 20976: loss: 0.2226819545030594\n",
      "iteration 20977: loss: 0.22268113493919373\n",
      "iteration 20978: loss: 0.22268033027648926\n",
      "iteration 20979: loss: 0.2226795256137848\n",
      "iteration 20980: loss: 0.22267870604991913\n",
      "iteration 20981: loss: 0.22267790138721466\n",
      "iteration 20982: loss: 0.222677081823349\n",
      "iteration 20983: loss: 0.2226763218641281\n",
      "iteration 20984: loss: 0.22267551720142365\n",
      "iteration 20985: loss: 0.2226746827363968\n",
      "iteration 20986: loss: 0.2226739227771759\n",
      "iteration 20987: loss: 0.22267305850982666\n",
      "iteration 20988: loss: 0.2226722538471222\n",
      "iteration 20989: loss: 0.22267143428325653\n",
      "iteration 20990: loss: 0.22267062962055206\n",
      "iteration 20991: loss: 0.2226698398590088\n",
      "iteration 20992: loss: 0.22266900539398193\n",
      "iteration 20993: loss: 0.22266820073127747\n",
      "iteration 20994: loss: 0.22266745567321777\n",
      "iteration 20995: loss: 0.22266662120819092\n",
      "iteration 20996: loss: 0.22266574203968048\n",
      "iteration 20997: loss: 0.2226649969816208\n",
      "iteration 20998: loss: 0.22266408801078796\n",
      "iteration 20999: loss: 0.22266340255737305\n",
      "iteration 21000: loss: 0.2226625680923462\n",
      "iteration 21001: loss: 0.22266176342964172\n",
      "iteration 21002: loss: 0.22266094386577606\n",
      "iteration 21003: loss: 0.2226601392030716\n",
      "iteration 21004: loss: 0.22265930473804474\n",
      "iteration 21005: loss: 0.22265854477882385\n",
      "iteration 21006: loss: 0.222657710313797\n",
      "iteration 21007: loss: 0.22265687584877014\n",
      "iteration 21008: loss: 0.22265610098838806\n",
      "iteration 21009: loss: 0.2226552963256836\n",
      "iteration 21010: loss: 0.22265449166297913\n",
      "iteration 21011: loss: 0.22265371680259705\n",
      "iteration 21012: loss: 0.2226528823375702\n",
      "iteration 21013: loss: 0.22265203297138214\n",
      "iteration 21014: loss: 0.22265124320983887\n",
      "iteration 21015: loss: 0.2226504534482956\n",
      "iteration 21016: loss: 0.22264964878559113\n",
      "iteration 21017: loss: 0.22264885902404785\n",
      "iteration 21018: loss: 0.222648024559021\n",
      "iteration 21019: loss: 0.22264719009399414\n",
      "iteration 21020: loss: 0.22264640033245087\n",
      "iteration 21021: loss: 0.22264564037322998\n",
      "iteration 21022: loss: 0.22264480590820312\n",
      "iteration 21023: loss: 0.22264394164085388\n",
      "iteration 21024: loss: 0.22264313697814941\n",
      "iteration 21025: loss: 0.22264237701892853\n",
      "iteration 21026: loss: 0.22264158725738525\n",
      "iteration 21027: loss: 0.22264084219932556\n",
      "iteration 21028: loss: 0.22263996303081512\n",
      "iteration 21029: loss: 0.22263917326927185\n",
      "iteration 21030: loss: 0.222638338804245\n",
      "iteration 21031: loss: 0.22263756394386292\n",
      "iteration 21032: loss: 0.22263674437999725\n",
      "iteration 21033: loss: 0.22263593971729279\n",
      "iteration 21034: loss: 0.22263507544994354\n",
      "iteration 21035: loss: 0.22263431549072266\n",
      "iteration 21036: loss: 0.22263355553150177\n",
      "iteration 21037: loss: 0.22263279557228088\n",
      "iteration 21038: loss: 0.22263190150260925\n",
      "iteration 21039: loss: 0.22263109683990479\n",
      "iteration 21040: loss: 0.22263029217720032\n",
      "iteration 21041: loss: 0.22262950241565704\n",
      "iteration 21042: loss: 0.22262874245643616\n",
      "iteration 21043: loss: 0.22262787818908691\n",
      "iteration 21044: loss: 0.2226271629333496\n",
      "iteration 21045: loss: 0.22262628376483917\n",
      "iteration 21046: loss: 0.22262544929981232\n",
      "iteration 21047: loss: 0.22262470424175262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 21048: loss: 0.22262385487556458\n",
      "iteration 21049: loss: 0.2226230800151825\n",
      "iteration 21050: loss: 0.22262230515480042\n",
      "iteration 21051: loss: 0.22262144088745117\n",
      "iteration 21052: loss: 0.22262068092823029\n",
      "iteration 21053: loss: 0.222619891166687\n",
      "iteration 21054: loss: 0.22261905670166016\n",
      "iteration 21055: loss: 0.2226182222366333\n",
      "iteration 21056: loss: 0.22261746227741241\n",
      "iteration 21057: loss: 0.22261664271354675\n",
      "iteration 21058: loss: 0.2226158082485199\n",
      "iteration 21059: loss: 0.22261497378349304\n",
      "iteration 21060: loss: 0.22261421382427216\n",
      "iteration 21061: loss: 0.2226133793592453\n",
      "iteration 21062: loss: 0.22261261940002441\n",
      "iteration 21063: loss: 0.22261182963848114\n",
      "iteration 21064: loss: 0.22261103987693787\n",
      "iteration 21065: loss: 0.222610205411911\n",
      "iteration 21066: loss: 0.22260940074920654\n",
      "iteration 21067: loss: 0.22260859608650208\n",
      "iteration 21068: loss: 0.2226078063249588\n",
      "iteration 21069: loss: 0.22260701656341553\n",
      "iteration 21070: loss: 0.22260625660419464\n",
      "iteration 21071: loss: 0.2226053774356842\n",
      "iteration 21072: loss: 0.22260458767414093\n",
      "iteration 21073: loss: 0.22260379791259766\n",
      "iteration 21074: loss: 0.2226029932498932\n",
      "iteration 21075: loss: 0.22260217368602753\n",
      "iteration 21076: loss: 0.22260141372680664\n",
      "iteration 21077: loss: 0.22260060906410217\n",
      "iteration 21078: loss: 0.22259974479675293\n",
      "iteration 21079: loss: 0.22259895503520966\n",
      "iteration 21080: loss: 0.22259816527366638\n",
      "iteration 21081: loss: 0.2225973904132843\n",
      "iteration 21082: loss: 0.22259655594825745\n",
      "iteration 21083: loss: 0.22259578108787537\n",
      "iteration 21084: loss: 0.22259502112865448\n",
      "iteration 21085: loss: 0.22259421646595\n",
      "iteration 21086: loss: 0.22259335219860077\n",
      "iteration 21087: loss: 0.2225925475358963\n",
      "iteration 21088: loss: 0.22259178757667542\n",
      "iteration 21089: loss: 0.22259096801280975\n",
      "iteration 21090: loss: 0.22259016335010529\n",
      "iteration 21091: loss: 0.22258934378623962\n",
      "iteration 21092: loss: 0.22258861362934113\n",
      "iteration 21093: loss: 0.22258777916431427\n",
      "iteration 21094: loss: 0.22258694469928741\n",
      "iteration 21095: loss: 0.22258619964122772\n",
      "iteration 21096: loss: 0.22258539497852325\n",
      "iteration 21097: loss: 0.2225845605134964\n",
      "iteration 21098: loss: 0.22258369624614716\n",
      "iteration 21099: loss: 0.22258293628692627\n",
      "iteration 21100: loss: 0.22258219122886658\n",
      "iteration 21101: loss: 0.2225813865661621\n",
      "iteration 21102: loss: 0.22258058190345764\n",
      "iteration 21103: loss: 0.2225797176361084\n",
      "iteration 21104: loss: 0.22257892787456512\n",
      "iteration 21105: loss: 0.22257821261882782\n",
      "iteration 21106: loss: 0.22257737815380096\n",
      "iteration 21107: loss: 0.2225765436887741\n",
      "iteration 21108: loss: 0.22257566452026367\n",
      "iteration 21109: loss: 0.22257494926452637\n",
      "iteration 21110: loss: 0.2225741595029831\n",
      "iteration 21111: loss: 0.22257336974143982\n",
      "iteration 21112: loss: 0.22257259488105774\n",
      "iteration 21113: loss: 0.22257180511951447\n",
      "iteration 21114: loss: 0.2225709706544876\n",
      "iteration 21115: loss: 0.22257013618946075\n",
      "iteration 21116: loss: 0.22256934642791748\n",
      "iteration 21117: loss: 0.2225685864686966\n",
      "iteration 21118: loss: 0.2225678265094757\n",
      "iteration 21119: loss: 0.22256699204444885\n",
      "iteration 21120: loss: 0.2225661277770996\n",
      "iteration 21121: loss: 0.22256538271903992\n",
      "iteration 21122: loss: 0.22256460785865784\n",
      "iteration 21123: loss: 0.2225637435913086\n",
      "iteration 21124: loss: 0.2225629836320877\n",
      "iteration 21125: loss: 0.22256222367286682\n",
      "iteration 21126: loss: 0.22256143391132355\n",
      "iteration 21127: loss: 0.2225605994462967\n",
      "iteration 21128: loss: 0.22255976498126984\n",
      "iteration 21129: loss: 0.22255897521972656\n",
      "iteration 21130: loss: 0.22255821526050568\n",
      "iteration 21131: loss: 0.22255739569664001\n",
      "iteration 21132: loss: 0.22255656123161316\n",
      "iteration 21133: loss: 0.2225557267665863\n",
      "iteration 21134: loss: 0.222555011510849\n",
      "iteration 21135: loss: 0.22255423665046692\n",
      "iteration 21136: loss: 0.22255340218544006\n",
      "iteration 21137: loss: 0.2225526124238968\n",
      "iteration 21138: loss: 0.22255174815654755\n",
      "iteration 21139: loss: 0.22255101799964905\n",
      "iteration 21140: loss: 0.22255024313926697\n",
      "iteration 21141: loss: 0.2225494384765625\n",
      "iteration 21142: loss: 0.22254860401153564\n",
      "iteration 21143: loss: 0.22254781424999237\n",
      "iteration 21144: loss: 0.2225470095872879\n",
      "iteration 21145: loss: 0.22254619002342224\n",
      "iteration 21146: loss: 0.22254538536071777\n",
      "iteration 21147: loss: 0.2225446254014969\n",
      "iteration 21148: loss: 0.222543865442276\n",
      "iteration 21149: loss: 0.22254304587841034\n",
      "iteration 21150: loss: 0.22254224121570587\n",
      "iteration 21151: loss: 0.2225414216518402\n",
      "iteration 21152: loss: 0.22254064679145813\n",
      "iteration 21153: loss: 0.2225397825241089\n",
      "iteration 21154: loss: 0.22253911197185516\n",
      "iteration 21155: loss: 0.22253826260566711\n",
      "iteration 21156: loss: 0.22253742814064026\n",
      "iteration 21157: loss: 0.22253665328025818\n",
      "iteration 21158: loss: 0.2225358486175537\n",
      "iteration 21159: loss: 0.22253505885601044\n",
      "iteration 21160: loss: 0.22253432869911194\n",
      "iteration 21161: loss: 0.22253353893756866\n",
      "iteration 21162: loss: 0.2225327044725418\n",
      "iteration 21163: loss: 0.22253188490867615\n",
      "iteration 21164: loss: 0.2225310504436493\n",
      "iteration 21165: loss: 0.2225302904844284\n",
      "iteration 21166: loss: 0.22252953052520752\n",
      "iteration 21167: loss: 0.22252869606018066\n",
      "iteration 21168: loss: 0.2225278913974762\n",
      "iteration 21169: loss: 0.22252707183361053\n",
      "iteration 21170: loss: 0.22252628207206726\n",
      "iteration 21171: loss: 0.22252556681632996\n",
      "iteration 21172: loss: 0.22252479195594788\n",
      "iteration 21173: loss: 0.22252392768859863\n",
      "iteration 21174: loss: 0.22252312302589417\n",
      "iteration 21175: loss: 0.2225223034620285\n",
      "iteration 21176: loss: 0.22252154350280762\n",
      "iteration 21177: loss: 0.22252073884010315\n",
      "iteration 21178: loss: 0.22251994907855988\n",
      "iteration 21179: loss: 0.2225191593170166\n",
      "iteration 21180: loss: 0.22251836955547333\n",
      "iteration 21181: loss: 0.22251753509044647\n",
      "iteration 21182: loss: 0.22251680493354797\n",
      "iteration 21183: loss: 0.2225160151720047\n",
      "iteration 21184: loss: 0.22251519560813904\n",
      "iteration 21185: loss: 0.22251443564891815\n",
      "iteration 21186: loss: 0.22251364588737488\n",
      "iteration 21187: loss: 0.22251281142234802\n",
      "iteration 21188: loss: 0.22251200675964355\n",
      "iteration 21189: loss: 0.22251124680042267\n",
      "iteration 21190: loss: 0.22251048684120178\n",
      "iteration 21191: loss: 0.22250965237617493\n",
      "iteration 21192: loss: 0.22250881791114807\n",
      "iteration 21193: loss: 0.222508043050766\n",
      "iteration 21194: loss: 0.2225072681903839\n",
      "iteration 21195: loss: 0.22250647842884064\n",
      "iteration 21196: loss: 0.22250568866729736\n",
      "iteration 21197: loss: 0.2225048840045929\n",
      "iteration 21198: loss: 0.22250410914421082\n",
      "iteration 21199: loss: 0.22250327467918396\n",
      "iteration 21200: loss: 0.22250251471996307\n",
      "iteration 21201: loss: 0.2225017100572586\n",
      "iteration 21202: loss: 0.22250092029571533\n",
      "iteration 21203: loss: 0.22250011563301086\n",
      "iteration 21204: loss: 0.222499281167984\n",
      "iteration 21205: loss: 0.22249853610992432\n",
      "iteration 21206: loss: 0.22249773144721985\n",
      "iteration 21207: loss: 0.22249695658683777\n",
      "iteration 21208: loss: 0.2224961519241333\n",
      "iteration 21209: loss: 0.2224954068660736\n",
      "iteration 21210: loss: 0.22249455749988556\n",
      "iteration 21211: loss: 0.2224937379360199\n",
      "iteration 21212: loss: 0.222492977976799\n",
      "iteration 21213: loss: 0.22249221801757812\n",
      "iteration 21214: loss: 0.2224913388490677\n",
      "iteration 21215: loss: 0.22249062359333038\n",
      "iteration 21216: loss: 0.2224898338317871\n",
      "iteration 21217: loss: 0.22248899936676025\n",
      "iteration 21218: loss: 0.22248825430870056\n",
      "iteration 21219: loss: 0.2224874198436737\n",
      "iteration 21220: loss: 0.22248665988445282\n",
      "iteration 21221: loss: 0.22248587012290955\n",
      "iteration 21222: loss: 0.2224850356578827\n",
      "iteration 21223: loss: 0.22248432040214539\n",
      "iteration 21224: loss: 0.2224835455417633\n",
      "iteration 21225: loss: 0.22248271107673645\n",
      "iteration 21226: loss: 0.2224818766117096\n",
      "iteration 21227: loss: 0.22248117625713348\n",
      "iteration 21228: loss: 0.22248032689094543\n",
      "iteration 21229: loss: 0.2224794626235962\n",
      "iteration 21230: loss: 0.22247882187366486\n",
      "iteration 21231: loss: 0.2224780023097992\n",
      "iteration 21232: loss: 0.22247719764709473\n",
      "iteration 21233: loss: 0.22247636318206787\n",
      "iteration 21234: loss: 0.2224755734205246\n",
      "iteration 21235: loss: 0.22247478365898132\n",
      "iteration 21236: loss: 0.22247397899627686\n",
      "iteration 21237: loss: 0.22247321903705597\n",
      "iteration 21238: loss: 0.2224724292755127\n",
      "iteration 21239: loss: 0.2224716693162918\n",
      "iteration 21240: loss: 0.22247084975242615\n",
      "iteration 21241: loss: 0.22247004508972168\n",
      "iteration 21242: loss: 0.2224692851305008\n",
      "iteration 21243: loss: 0.22246846556663513\n",
      "iteration 21244: loss: 0.22246770560741425\n",
      "iteration 21245: loss: 0.22246690094470978\n",
      "iteration 21246: loss: 0.2224661409854889\n",
      "iteration 21247: loss: 0.22246532142162323\n",
      "iteration 21248: loss: 0.22246451675891876\n",
      "iteration 21249: loss: 0.2224636971950531\n",
      "iteration 21250: loss: 0.2224629819393158\n",
      "iteration 21251: loss: 0.22246214747428894\n",
      "iteration 21252: loss: 0.22246137261390686\n",
      "iteration 21253: loss: 0.2224605530500412\n",
      "iteration 21254: loss: 0.2224598377943039\n",
      "iteration 21255: loss: 0.22245904803276062\n",
      "iteration 21256: loss: 0.22245824337005615\n",
      "iteration 21257: loss: 0.22245745360851288\n",
      "iteration 21258: loss: 0.222456693649292\n",
      "iteration 21259: loss: 0.22245588898658752\n",
      "iteration 21260: loss: 0.22245506942272186\n",
      "iteration 21261: loss: 0.2224542647600174\n",
      "iteration 21262: loss: 0.2224535495042801\n",
      "iteration 21263: loss: 0.22245271503925323\n",
      "iteration 21264: loss: 0.22245188057422638\n",
      "iteration 21265: loss: 0.2224510908126831\n",
      "iteration 21266: loss: 0.2224503457546234\n",
      "iteration 21267: loss: 0.22244954109191895\n",
      "iteration 21268: loss: 0.22244878113269806\n",
      "iteration 21269: loss: 0.22244802117347717\n",
      "iteration 21270: loss: 0.2224472314119339\n",
      "iteration 21271: loss: 0.22244636714458466\n",
      "iteration 21272: loss: 0.22244557738304138\n",
      "iteration 21273: loss: 0.2224448174238205\n",
      "iteration 21274: loss: 0.22244401276111603\n",
      "iteration 21275: loss: 0.22244325280189514\n",
      "iteration 21276: loss: 0.22244243323802948\n",
      "iteration 21277: loss: 0.22244170308113098\n",
      "iteration 21278: loss: 0.2224409133195877\n",
      "iteration 21279: loss: 0.22244009375572205\n",
      "iteration 21280: loss: 0.22243933379650116\n",
      "iteration 21281: loss: 0.2224385291337967\n",
      "iteration 21282: loss: 0.222437784075737\n",
      "iteration 21283: loss: 0.22243693470954895\n",
      "iteration 21284: loss: 0.22243615984916687\n",
      "iteration 21285: loss: 0.2224353551864624\n",
      "iteration 21286: loss: 0.22243455052375793\n",
      "iteration 21287: loss: 0.22243376076221466\n",
      "iteration 21288: loss: 0.22243304550647736\n",
      "iteration 21289: loss: 0.22243230044841766\n",
      "iteration 21290: loss: 0.22243145108222961\n",
      "iteration 21291: loss: 0.22243063151836395\n",
      "iteration 21292: loss: 0.22242991626262665\n",
      "iteration 21293: loss: 0.2224290370941162\n",
      "iteration 21294: loss: 0.2224283516407013\n",
      "iteration 21295: loss: 0.22242756187915802\n",
      "iteration 21296: loss: 0.22242669761180878\n",
      "iteration 21297: loss: 0.2224259376525879\n",
      "iteration 21298: loss: 0.22242513298988342\n",
      "iteration 21299: loss: 0.22242431342601776\n",
      "iteration 21300: loss: 0.22242358326911926\n",
      "iteration 21301: loss: 0.22242283821105957\n",
      "iteration 21302: loss: 0.2224220335483551\n",
      "iteration 21303: loss: 0.2224212884902954\n",
      "iteration 21304: loss: 0.22242048382759094\n",
      "iteration 21305: loss: 0.22241969406604767\n",
      "iteration 21306: loss: 0.22241893410682678\n",
      "iteration 21307: loss: 0.22241811454296112\n",
      "iteration 21308: loss: 0.22241728007793427\n",
      "iteration 21309: loss: 0.22241652011871338\n",
      "iteration 21310: loss: 0.2224157303571701\n",
      "iteration 21311: loss: 0.22241497039794922\n",
      "iteration 21312: loss: 0.22241425514221191\n",
      "iteration 21313: loss: 0.22241339087486267\n",
      "iteration 21314: loss: 0.2224125862121582\n",
      "iteration 21315: loss: 0.22241179645061493\n",
      "iteration 21316: loss: 0.22241103649139404\n",
      "iteration 21317: loss: 0.22241027653217316\n",
      "iteration 21318: loss: 0.22240953147411346\n",
      "iteration 21319: loss: 0.22240865230560303\n",
      "iteration 21320: loss: 0.22240784764289856\n",
      "iteration 21321: loss: 0.22240717709064484\n",
      "iteration 21322: loss: 0.2224062979221344\n",
      "iteration 21323: loss: 0.22240552306175232\n",
      "iteration 21324: loss: 0.22240476310253143\n",
      "iteration 21325: loss: 0.22240400314331055\n",
      "iteration 21326: loss: 0.22240321338176727\n",
      "iteration 21327: loss: 0.222402423620224\n",
      "iteration 21328: loss: 0.2224016636610031\n",
      "iteration 21329: loss: 0.22240087389945984\n",
      "iteration 21330: loss: 0.22240003943443298\n",
      "iteration 21331: loss: 0.22239930927753448\n",
      "iteration 21332: loss: 0.22239848971366882\n",
      "iteration 21333: loss: 0.22239775955677032\n",
      "iteration 21334: loss: 0.22239693999290466\n",
      "iteration 21335: loss: 0.2223961353302002\n",
      "iteration 21336: loss: 0.2223953753709793\n",
      "iteration 21337: loss: 0.22239461541175842\n",
      "iteration 21338: loss: 0.22239382565021515\n",
      "iteration 21339: loss: 0.22239306569099426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 21340: loss: 0.2223922312259674\n",
      "iteration 21341: loss: 0.22239144146442413\n",
      "iteration 21342: loss: 0.22239069640636444\n",
      "iteration 21343: loss: 0.22238990664482117\n",
      "iteration 21344: loss: 0.2223891317844391\n",
      "iteration 21345: loss: 0.222388356924057\n",
      "iteration 21346: loss: 0.22238759696483612\n",
      "iteration 21347: loss: 0.22238680720329285\n",
      "iteration 21348: loss: 0.2223859280347824\n",
      "iteration 21349: loss: 0.2223852127790451\n",
      "iteration 21350: loss: 0.22238445281982422\n",
      "iteration 21351: loss: 0.22238364815711975\n",
      "iteration 21352: loss: 0.22238287329673767\n",
      "iteration 21353: loss: 0.2223820984363556\n",
      "iteration 21354: loss: 0.2223813533782959\n",
      "iteration 21355: loss: 0.22238054871559143\n",
      "iteration 21356: loss: 0.2223796844482422\n",
      "iteration 21357: loss: 0.22237887978553772\n",
      "iteration 21358: loss: 0.22237813472747803\n",
      "iteration 21359: loss: 0.22237741947174072\n",
      "iteration 21360: loss: 0.22237658500671387\n",
      "iteration 21361: loss: 0.22237582504749298\n",
      "iteration 21362: loss: 0.2223750650882721\n",
      "iteration 21363: loss: 0.22237429022789001\n",
      "iteration 21364: loss: 0.22237344086170197\n",
      "iteration 21365: loss: 0.22237269580364227\n",
      "iteration 21366: loss: 0.222371906042099\n",
      "iteration 21367: loss: 0.2223711460828781\n",
      "iteration 21368: loss: 0.22237031161785126\n",
      "iteration 21369: loss: 0.22236958146095276\n",
      "iteration 21370: loss: 0.22236886620521545\n",
      "iteration 21371: loss: 0.2223680466413498\n",
      "iteration 21372: loss: 0.22236725687980652\n",
      "iteration 21373: loss: 0.22236645221710205\n",
      "iteration 21374: loss: 0.22236569225788116\n",
      "iteration 21375: loss: 0.22236493229866028\n",
      "iteration 21376: loss: 0.22236411273479462\n",
      "iteration 21377: loss: 0.22236330807209015\n",
      "iteration 21378: loss: 0.22236251831054688\n",
      "iteration 21379: loss: 0.2223617136478424\n",
      "iteration 21380: loss: 0.2223609983921051\n",
      "iteration 21381: loss: 0.22236022353172302\n",
      "iteration 21382: loss: 0.22235944867134094\n",
      "iteration 21383: loss: 0.22235862910747528\n",
      "iteration 21384: loss: 0.2223578691482544\n",
      "iteration 21385: loss: 0.2223571538925171\n",
      "iteration 21386: loss: 0.22235634922981262\n",
      "iteration 21387: loss: 0.22235551476478577\n",
      "iteration 21388: loss: 0.2223547250032425\n",
      "iteration 21389: loss: 0.2223539650440216\n",
      "iteration 21390: loss: 0.22235317528247833\n",
      "iteration 21391: loss: 0.22235243022441864\n",
      "iteration 21392: loss: 0.22235164046287537\n",
      "iteration 21393: loss: 0.2223508656024933\n",
      "iteration 21394: loss: 0.2223500907421112\n",
      "iteration 21395: loss: 0.2223493605852127\n",
      "iteration 21396: loss: 0.22234852612018585\n",
      "iteration 21397: loss: 0.22234773635864258\n",
      "iteration 21398: loss: 0.22234699130058289\n",
      "iteration 21399: loss: 0.22234627604484558\n",
      "iteration 21400: loss: 0.22234544157981873\n",
      "iteration 21401: loss: 0.22234463691711426\n",
      "iteration 21402: loss: 0.22234389185905457\n",
      "iteration 21403: loss: 0.2223430871963501\n",
      "iteration 21404: loss: 0.22234225273132324\n",
      "iteration 21405: loss: 0.22234158217906952\n",
      "iteration 21406: loss: 0.22234079241752625\n",
      "iteration 21407: loss: 0.22234006226062775\n",
      "iteration 21408: loss: 0.22233924269676208\n",
      "iteration 21409: loss: 0.22233846783638\n",
      "iteration 21410: loss: 0.2223377227783203\n",
      "iteration 21411: loss: 0.22233685851097107\n",
      "iteration 21412: loss: 0.22233609855175018\n",
      "iteration 21413: loss: 0.22233536839485168\n",
      "iteration 21414: loss: 0.2223345786333084\n",
      "iteration 21415: loss: 0.22233375906944275\n",
      "iteration 21416: loss: 0.22233299911022186\n",
      "iteration 21417: loss: 0.222332164645195\n",
      "iteration 21418: loss: 0.2223314344882965\n",
      "iteration 21419: loss: 0.22233068943023682\n",
      "iteration 21420: loss: 0.22232985496520996\n",
      "iteration 21421: loss: 0.22232910990715027\n",
      "iteration 21422: loss: 0.22232834994792938\n",
      "iteration 21423: loss: 0.2223275601863861\n",
      "iteration 21424: loss: 0.22232680022716522\n",
      "iteration 21425: loss: 0.22232596576213837\n",
      "iteration 21426: loss: 0.22232523560523987\n",
      "iteration 21427: loss: 0.2223244458436966\n",
      "iteration 21428: loss: 0.2223237305879593\n",
      "iteration 21429: loss: 0.22232289612293243\n",
      "iteration 21430: loss: 0.22232215106487274\n",
      "iteration 21431: loss: 0.22232134640216827\n",
      "iteration 21432: loss: 0.22232060134410858\n",
      "iteration 21433: loss: 0.2223198413848877\n",
      "iteration 21434: loss: 0.22231896221637726\n",
      "iteration 21435: loss: 0.22231824696063995\n",
      "iteration 21436: loss: 0.22231750190258026\n",
      "iteration 21437: loss: 0.2223166972398758\n",
      "iteration 21438: loss: 0.2223159372806549\n",
      "iteration 21439: loss: 0.22231519222259521\n",
      "iteration 21440: loss: 0.22231438755989075\n",
      "iteration 21441: loss: 0.22231361269950867\n",
      "iteration 21442: loss: 0.2223128378391266\n",
      "iteration 21443: loss: 0.2223120629787445\n",
      "iteration 21444: loss: 0.22231128811836243\n",
      "iteration 21445: loss: 0.22231049835681915\n",
      "iteration 21446: loss: 0.22230970859527588\n",
      "iteration 21447: loss: 0.2223089635372162\n",
      "iteration 21448: loss: 0.2223081886768341\n",
      "iteration 21449: loss: 0.2223074734210968\n",
      "iteration 21450: loss: 0.22230663895606995\n",
      "iteration 21451: loss: 0.22230589389801025\n",
      "iteration 21452: loss: 0.2223050594329834\n",
      "iteration 21453: loss: 0.2223042994737625\n",
      "iteration 21454: loss: 0.2223035991191864\n",
      "iteration 21455: loss: 0.22230276465415955\n",
      "iteration 21456: loss: 0.22230198979377747\n",
      "iteration 21457: loss: 0.22230121493339539\n",
      "iteration 21458: loss: 0.2223004549741745\n",
      "iteration 21459: loss: 0.22229965031147003\n",
      "iteration 21460: loss: 0.22229893505573273\n",
      "iteration 21461: loss: 0.22229811549186707\n",
      "iteration 21462: loss: 0.22229735553264618\n",
      "iteration 21463: loss: 0.22229652106761932\n",
      "iteration 21464: loss: 0.2222958505153656\n",
      "iteration 21465: loss: 0.22229501605033875\n",
      "iteration 21466: loss: 0.22229428589344025\n",
      "iteration 21467: loss: 0.22229352593421936\n",
      "iteration 21468: loss: 0.2222927063703537\n",
      "iteration 21469: loss: 0.22229191660881042\n",
      "iteration 21470: loss: 0.22229118645191193\n",
      "iteration 21471: loss: 0.22229042649269104\n",
      "iteration 21472: loss: 0.22228959202766418\n",
      "iteration 21473: loss: 0.2222888469696045\n",
      "iteration 21474: loss: 0.2222880870103836\n",
      "iteration 21475: loss: 0.2222873419523239\n",
      "iteration 21476: loss: 0.22228658199310303\n",
      "iteration 21477: loss: 0.22228574752807617\n",
      "iteration 21478: loss: 0.22228500247001648\n",
      "iteration 21479: loss: 0.22228428721427917\n",
      "iteration 21480: loss: 0.22228343784809113\n",
      "iteration 21481: loss: 0.22228269279003143\n",
      "iteration 21482: loss: 0.22228193283081055\n",
      "iteration 21483: loss: 0.2222810983657837\n",
      "iteration 21484: loss: 0.22228030860424042\n",
      "iteration 21485: loss: 0.2222795933485031\n",
      "iteration 21486: loss: 0.2222788780927658\n",
      "iteration 21487: loss: 0.22227807343006134\n",
      "iteration 21488: loss: 0.22227728366851807\n",
      "iteration 21489: loss: 0.22227653861045837\n",
      "iteration 21490: loss: 0.2222757339477539\n",
      "iteration 21491: loss: 0.22227497398853302\n",
      "iteration 21492: loss: 0.22227415442466736\n",
      "iteration 21493: loss: 0.22227343916893005\n",
      "iteration 21494: loss: 0.22227267920970917\n",
      "iteration 21495: loss: 0.2222718894481659\n",
      "iteration 21496: loss: 0.2222711145877838\n",
      "iteration 21497: loss: 0.22227033972740173\n",
      "iteration 21498: loss: 0.22226960957050323\n",
      "iteration 21499: loss: 0.22226881980895996\n",
      "iteration 21500: loss: 0.2222680151462555\n",
      "iteration 21501: loss: 0.22226731479167938\n",
      "iteration 21502: loss: 0.2222665548324585\n",
      "iteration 21503: loss: 0.22226572036743164\n",
      "iteration 21504: loss: 0.22226496040821075\n",
      "iteration 21505: loss: 0.22226421535015106\n",
      "iteration 21506: loss: 0.2222634255886078\n",
      "iteration 21507: loss: 0.22226262092590332\n",
      "iteration 21508: loss: 0.22226187586784363\n",
      "iteration 21509: loss: 0.22226111590862274\n",
      "iteration 21510: loss: 0.22226040065288544\n",
      "iteration 21511: loss: 0.22225956618785858\n",
      "iteration 21512: loss: 0.22225885093212128\n",
      "iteration 21513: loss: 0.2222580462694168\n",
      "iteration 21514: loss: 0.22225722670555115\n",
      "iteration 21515: loss: 0.22225646674633026\n",
      "iteration 21516: loss: 0.222255676984787\n",
      "iteration 21517: loss: 0.22225499153137207\n",
      "iteration 21518: loss: 0.2222542017698288\n",
      "iteration 21519: loss: 0.22225339710712433\n",
      "iteration 21520: loss: 0.22225265204906464\n",
      "iteration 21521: loss: 0.22225192189216614\n",
      "iteration 21522: loss: 0.22225113213062286\n",
      "iteration 21523: loss: 0.2222503125667572\n",
      "iteration 21524: loss: 0.2222495824098587\n",
      "iteration 21525: loss: 0.22224882245063782\n",
      "iteration 21526: loss: 0.22224800288677216\n",
      "iteration 21527: loss: 0.22224724292755127\n",
      "iteration 21528: loss: 0.22224655747413635\n",
      "iteration 21529: loss: 0.22224566340446472\n",
      "iteration 21530: loss: 0.22224490344524384\n",
      "iteration 21531: loss: 0.22224417328834534\n",
      "iteration 21532: loss: 0.22224345803260803\n",
      "iteration 21533: loss: 0.22224262356758118\n",
      "iteration 21534: loss: 0.22224187850952148\n",
      "iteration 21535: loss: 0.2222411185503006\n",
      "iteration 21536: loss: 0.22224032878875732\n",
      "iteration 21537: loss: 0.2222396433353424\n",
      "iteration 21538: loss: 0.22223882377147675\n",
      "iteration 21539: loss: 0.22223803400993347\n",
      "iteration 21540: loss: 0.22223730385303497\n",
      "iteration 21541: loss: 0.22223646938800812\n",
      "iteration 21542: loss: 0.2222357988357544\n",
      "iteration 21543: loss: 0.22223496437072754\n",
      "iteration 21544: loss: 0.22223420441150665\n",
      "iteration 21545: loss: 0.22223345935344696\n",
      "iteration 21546: loss: 0.2222326695919037\n",
      "iteration 21547: loss: 0.2222319096326828\n",
      "iteration 21548: loss: 0.22223114967346191\n",
      "iteration 21549: loss: 0.22223035991191864\n",
      "iteration 21550: loss: 0.22222959995269775\n",
      "iteration 21551: loss: 0.22222885489463806\n",
      "iteration 21552: loss: 0.22222800552845\n",
      "iteration 21553: loss: 0.22222737967967987\n",
      "iteration 21554: loss: 0.22222654521465302\n",
      "iteration 21555: loss: 0.22222574055194855\n",
      "iteration 21556: loss: 0.22222499549388885\n",
      "iteration 21557: loss: 0.22222428023815155\n",
      "iteration 21558: loss: 0.22222355008125305\n",
      "iteration 21559: loss: 0.2222227305173874\n",
      "iteration 21560: loss: 0.22222189605236053\n",
      "iteration 21561: loss: 0.22222116589546204\n",
      "iteration 21562: loss: 0.22222037613391876\n",
      "iteration 21563: loss: 0.22221966087818146\n",
      "iteration 21564: loss: 0.22221890091896057\n",
      "iteration 21565: loss: 0.2222181260585785\n",
      "iteration 21566: loss: 0.22221732139587402\n",
      "iteration 21567: loss: 0.22221660614013672\n",
      "iteration 21568: loss: 0.22221584618091583\n",
      "iteration 21569: loss: 0.22221508622169495\n",
      "iteration 21570: loss: 0.22221431136131287\n",
      "iteration 21571: loss: 0.2222135365009308\n",
      "iteration 21572: loss: 0.2222127616405487\n",
      "iteration 21573: loss: 0.22221200168132782\n",
      "iteration 21574: loss: 0.22221121191978455\n",
      "iteration 21575: loss: 0.22221045196056366\n",
      "iteration 21576: loss: 0.22220978140830994\n",
      "iteration 21577: loss: 0.2222089022397995\n",
      "iteration 21578: loss: 0.222208172082901\n",
      "iteration 21579: loss: 0.2222074270248413\n",
      "iteration 21580: loss: 0.22220668196678162\n",
      "iteration 21581: loss: 0.22220584750175476\n",
      "iteration 21582: loss: 0.22220513224601746\n",
      "iteration 21583: loss: 0.22220437228679657\n",
      "iteration 21584: loss: 0.22220361232757568\n",
      "iteration 21585: loss: 0.2222028225660324\n",
      "iteration 21586: loss: 0.22220201790332794\n",
      "iteration 21587: loss: 0.22220130264759064\n",
      "iteration 21588: loss: 0.22220055758953094\n",
      "iteration 21589: loss: 0.22219979763031006\n",
      "iteration 21590: loss: 0.22219903767108917\n",
      "iteration 21591: loss: 0.2221982777118683\n",
      "iteration 21592: loss: 0.222197487950325\n",
      "iteration 21593: loss: 0.2221967726945877\n",
      "iteration 21594: loss: 0.22219595313072205\n",
      "iteration 21595: loss: 0.22219519317150116\n",
      "iteration 21596: loss: 0.22219443321228027\n",
      "iteration 21597: loss: 0.22219368815422058\n",
      "iteration 21598: loss: 0.2221928834915161\n",
      "iteration 21599: loss: 0.2221921682357788\n",
      "iteration 21600: loss: 0.22219142317771912\n",
      "iteration 21601: loss: 0.22219061851501465\n",
      "iteration 21602: loss: 0.22218985855579376\n",
      "iteration 21603: loss: 0.22218914330005646\n",
      "iteration 21604: loss: 0.2221883237361908\n",
      "iteration 21605: loss: 0.2221876084804535\n",
      "iteration 21606: loss: 0.2221868336200714\n",
      "iteration 21607: loss: 0.22218601405620575\n",
      "iteration 21608: loss: 0.22218529880046844\n",
      "iteration 21609: loss: 0.22218450903892517\n",
      "iteration 21610: loss: 0.22218374907970428\n",
      "iteration 21611: loss: 0.22218306362628937\n",
      "iteration 21612: loss: 0.22218230366706848\n",
      "iteration 21613: loss: 0.22218146920204163\n",
      "iteration 21614: loss: 0.22218072414398193\n",
      "iteration 21615: loss: 0.22217997908592224\n",
      "iteration 21616: loss: 0.22217917442321777\n",
      "iteration 21617: loss: 0.2221784144639969\n",
      "iteration 21618: loss: 0.222177654504776\n",
      "iteration 21619: loss: 0.2221769392490387\n",
      "iteration 21620: loss: 0.22217611968517303\n",
      "iteration 21621: loss: 0.22217538952827454\n",
      "iteration 21622: loss: 0.22217464447021484\n",
      "iteration 21623: loss: 0.22217389941215515\n",
      "iteration 21624: loss: 0.22217312455177307\n",
      "iteration 21625: loss: 0.2221723347902298\n",
      "iteration 21626: loss: 0.2221715897321701\n",
      "iteration 21627: loss: 0.22217082977294922\n",
      "iteration 21628: loss: 0.22217006981372833\n",
      "iteration 21629: loss: 0.22216932475566864\n",
      "iteration 21630: loss: 0.22216853499412537\n",
      "iteration 21631: loss: 0.22216777503490448\n",
      "iteration 21632: loss: 0.22216704487800598\n",
      "iteration 21633: loss: 0.2221662700176239\n",
      "iteration 21634: loss: 0.22216549515724182\n",
      "iteration 21635: loss: 0.22216472029685974\n",
      "iteration 21636: loss: 0.22216399013996124\n",
      "iteration 21637: loss: 0.22216327488422394\n",
      "iteration 21638: loss: 0.22216245532035828\n",
      "iteration 21639: loss: 0.22216172516345978\n",
      "iteration 21640: loss: 0.2221609652042389\n",
      "iteration 21641: loss: 0.22216017544269562\n",
      "iteration 21642: loss: 0.22215943038463593\n",
      "iteration 21643: loss: 0.22215870022773743\n",
      "iteration 21644: loss: 0.22215792536735535\n",
      "iteration 21645: loss: 0.22215716540813446\n",
      "iteration 21646: loss: 0.2221563756465912\n",
      "iteration 21647: loss: 0.22215566039085388\n",
      "iteration 21648: loss: 0.222154900431633\n",
      "iteration 21649: loss: 0.22215406596660614\n",
      "iteration 21650: loss: 0.22215335071086884\n",
      "iteration 21651: loss: 0.22215262055397034\n",
      "iteration 21652: loss: 0.22215180099010468\n",
      "iteration 21653: loss: 0.22215111553668976\n",
      "iteration 21654: loss: 0.2221502810716629\n",
      "iteration 21655: loss: 0.2221495360136032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 21656: loss: 0.2221488207578659\n",
      "iteration 21657: loss: 0.22214803099632263\n",
      "iteration 21658: loss: 0.2221473753452301\n",
      "iteration 21659: loss: 0.22214651107788086\n",
      "iteration 21660: loss: 0.22214579582214355\n",
      "iteration 21661: loss: 0.22214503586292267\n",
      "iteration 21662: loss: 0.2221442461013794\n",
      "iteration 21663: loss: 0.2221434861421585\n",
      "iteration 21664: loss: 0.22214269638061523\n",
      "iteration 21665: loss: 0.22214190661907196\n",
      "iteration 21666: loss: 0.22214126586914062\n",
      "iteration 21667: loss: 0.22214047610759735\n",
      "iteration 21668: loss: 0.22213968634605408\n",
      "iteration 21669: loss: 0.22213897109031677\n",
      "iteration 21670: loss: 0.2221381664276123\n",
      "iteration 21671: loss: 0.22213740646839142\n",
      "iteration 21672: loss: 0.22213666141033173\n",
      "iteration 21673: loss: 0.22213593125343323\n",
      "iteration 21674: loss: 0.22213515639305115\n",
      "iteration 21675: loss: 0.22213438153266907\n",
      "iteration 21676: loss: 0.22213363647460938\n",
      "iteration 21677: loss: 0.22213292121887207\n",
      "iteration 21678: loss: 0.22213216125965118\n",
      "iteration 21679: loss: 0.2221314013004303\n",
      "iteration 21680: loss: 0.22213058173656464\n",
      "iteration 21681: loss: 0.22212989628314972\n",
      "iteration 21682: loss: 0.22212910652160645\n",
      "iteration 21683: loss: 0.22212834656238556\n",
      "iteration 21684: loss: 0.22212760150432587\n",
      "iteration 21685: loss: 0.2221268117427826\n",
      "iteration 21686: loss: 0.2221260517835617\n",
      "iteration 21687: loss: 0.2221253365278244\n",
      "iteration 21688: loss: 0.2221246063709259\n",
      "iteration 21689: loss: 0.22212381660938263\n",
      "iteration 21690: loss: 0.22212311625480652\n",
      "iteration 21691: loss: 0.22212234139442444\n",
      "iteration 21692: loss: 0.22212156653404236\n",
      "iteration 21693: loss: 0.22212079167366028\n",
      "iteration 21694: loss: 0.2221200168132782\n",
      "iteration 21695: loss: 0.2221193015575409\n",
      "iteration 21696: loss: 0.2221185266971588\n",
      "iteration 21697: loss: 0.22211773693561554\n",
      "iteration 21698: loss: 0.22211699187755585\n",
      "iteration 21699: loss: 0.22211627662181854\n",
      "iteration 21700: loss: 0.22211559116840363\n",
      "iteration 21701: loss: 0.22211475670337677\n",
      "iteration 21702: loss: 0.22211399674415588\n",
      "iteration 21703: loss: 0.2221132516860962\n",
      "iteration 21704: loss: 0.2221125066280365\n",
      "iteration 21705: loss: 0.22211173176765442\n",
      "iteration 21706: loss: 0.2221110314130783\n",
      "iteration 21707: loss: 0.22211018204689026\n",
      "iteration 21708: loss: 0.22210951149463654\n",
      "iteration 21709: loss: 0.22210876643657684\n",
      "iteration 21710: loss: 0.22210796177387238\n",
      "iteration 21711: loss: 0.22210721671581268\n",
      "iteration 21712: loss: 0.2221064567565918\n",
      "iteration 21713: loss: 0.22210566699504852\n",
      "iteration 21714: loss: 0.22210495173931122\n",
      "iteration 21715: loss: 0.22210419178009033\n",
      "iteration 21716: loss: 0.22210340201854706\n",
      "iteration 21717: loss: 0.22210268676280975\n",
      "iteration 21718: loss: 0.22210197150707245\n",
      "iteration 21719: loss: 0.22210121154785156\n",
      "iteration 21720: loss: 0.22210045158863068\n",
      "iteration 21721: loss: 0.2220996916294098\n",
      "iteration 21722: loss: 0.2220989465713501\n",
      "iteration 21723: loss: 0.2220982313156128\n",
      "iteration 21724: loss: 0.22209742665290833\n",
      "iteration 21725: loss: 0.22209663689136505\n",
      "iteration 21726: loss: 0.22209596633911133\n",
      "iteration 21727: loss: 0.22209520637989044\n",
      "iteration 21728: loss: 0.22209438681602478\n",
      "iteration 21729: loss: 0.2220936268568039\n",
      "iteration 21730: loss: 0.222092866897583\n",
      "iteration 21731: loss: 0.2220921814441681\n",
      "iteration 21732: loss: 0.22209139168262482\n",
      "iteration 21733: loss: 0.22209060192108154\n",
      "iteration 21734: loss: 0.22208988666534424\n",
      "iteration 21735: loss: 0.22208921611309052\n",
      "iteration 21736: loss: 0.22208841145038605\n",
      "iteration 21737: loss: 0.22208765149116516\n",
      "iteration 21738: loss: 0.2220868617296219\n",
      "iteration 21739: loss: 0.2220861166715622\n",
      "iteration 21740: loss: 0.2220853567123413\n",
      "iteration 21741: loss: 0.22208461165428162\n",
      "iteration 21742: loss: 0.22208388149738312\n",
      "iteration 21743: loss: 0.22208309173583984\n",
      "iteration 21744: loss: 0.22208242118358612\n",
      "iteration 21745: loss: 0.22208163142204285\n",
      "iteration 21746: loss: 0.22208082675933838\n",
      "iteration 21747: loss: 0.22208015620708466\n",
      "iteration 21748: loss: 0.22207936644554138\n",
      "iteration 21749: loss: 0.2220786064863205\n",
      "iteration 21750: loss: 0.2220778912305832\n",
      "iteration 21751: loss: 0.22207710146903992\n",
      "iteration 21752: loss: 0.22207634150981903\n",
      "iteration 21753: loss: 0.22207555174827576\n",
      "iteration 21754: loss: 0.22207483649253845\n",
      "iteration 21755: loss: 0.22207407653331757\n",
      "iteration 21756: loss: 0.22207334637641907\n",
      "iteration 21757: loss: 0.22207267582416534\n",
      "iteration 21758: loss: 0.2220718413591385\n",
      "iteration 21759: loss: 0.22207117080688477\n",
      "iteration 21760: loss: 0.2220703810453415\n",
      "iteration 21761: loss: 0.22206959128379822\n",
      "iteration 21762: loss: 0.22206886112689972\n",
      "iteration 21763: loss: 0.22206811606884003\n",
      "iteration 21764: loss: 0.22206735610961914\n",
      "iteration 21765: loss: 0.22206656634807587\n",
      "iteration 21766: loss: 0.22206588089466095\n",
      "iteration 21767: loss: 0.22206512093544006\n",
      "iteration 21768: loss: 0.2220643311738968\n",
      "iteration 21769: loss: 0.22206363081932068\n",
      "iteration 21770: loss: 0.2220628708600998\n",
      "iteration 21771: loss: 0.2220621109008789\n",
      "iteration 21772: loss: 0.22206135094165802\n",
      "iteration 21773: loss: 0.22206059098243713\n",
      "iteration 21774: loss: 0.22205987572669983\n",
      "iteration 21775: loss: 0.22205910086631775\n",
      "iteration 21776: loss: 0.22205832600593567\n",
      "iteration 21777: loss: 0.22205762565135956\n",
      "iteration 21778: loss: 0.22205686569213867\n",
      "iteration 21779: loss: 0.22205615043640137\n",
      "iteration 21780: loss: 0.2220553457736969\n",
      "iteration 21781: loss: 0.2220546305179596\n",
      "iteration 21782: loss: 0.22205384075641632\n",
      "iteration 21783: loss: 0.22205309569835663\n",
      "iteration 21784: loss: 0.22205238044261932\n",
      "iteration 21785: loss: 0.22205165028572083\n",
      "iteration 21786: loss: 0.22205078601837158\n",
      "iteration 21787: loss: 0.22205011546611786\n",
      "iteration 21788: loss: 0.22204943001270294\n",
      "iteration 21789: loss: 0.22204861044883728\n",
      "iteration 21790: loss: 0.22204788029193878\n",
      "iteration 21791: loss: 0.22204717993736267\n",
      "iteration 21792: loss: 0.2220464050769806\n",
      "iteration 21793: loss: 0.2220456600189209\n",
      "iteration 21794: loss: 0.2220449000597\n",
      "iteration 21795: loss: 0.22204411029815674\n",
      "iteration 21796: loss: 0.22204336524009705\n",
      "iteration 21797: loss: 0.22204260528087616\n",
      "iteration 21798: loss: 0.22204191982746124\n",
      "iteration 21799: loss: 0.22204113006591797\n",
      "iteration 21800: loss: 0.22204037010669708\n",
      "iteration 21801: loss: 0.22203966975212097\n",
      "iteration 21802: loss: 0.2220388650894165\n",
      "iteration 21803: loss: 0.2220381498336792\n",
      "iteration 21804: loss: 0.2220374047756195\n",
      "iteration 21805: loss: 0.222036674618721\n",
      "iteration 21806: loss: 0.22203592956066132\n",
      "iteration 21807: loss: 0.22203516960144043\n",
      "iteration 21808: loss: 0.22203445434570312\n",
      "iteration 21809: loss: 0.22203369438648224\n",
      "iteration 21810: loss: 0.22203294932842255\n",
      "iteration 21811: loss: 0.22203221917152405\n",
      "iteration 21812: loss: 0.22203144431114197\n",
      "iteration 21813: loss: 0.22203071415424347\n",
      "iteration 21814: loss: 0.2220299243927002\n",
      "iteration 21815: loss: 0.2220292091369629\n",
      "iteration 21816: loss: 0.2220284640789032\n",
      "iteration 21817: loss: 0.22202765941619873\n",
      "iteration 21818: loss: 0.222026988863945\n",
      "iteration 21819: loss: 0.22202619910240173\n",
      "iteration 21820: loss: 0.22202546894550323\n",
      "iteration 21821: loss: 0.22202475368976593\n",
      "iteration 21822: loss: 0.22202400863170624\n",
      "iteration 21823: loss: 0.22202317416667938\n",
      "iteration 21824: loss: 0.22202253341674805\n",
      "iteration 21825: loss: 0.22202177345752716\n",
      "iteration 21826: loss: 0.22202101349830627\n",
      "iteration 21827: loss: 0.22202029824256897\n",
      "iteration 21828: loss: 0.22201955318450928\n",
      "iteration 21829: loss: 0.22201868891716003\n",
      "iteration 21830: loss: 0.2220180481672287\n",
      "iteration 21831: loss: 0.2220172882080078\n",
      "iteration 21832: loss: 0.22201654314994812\n",
      "iteration 21833: loss: 0.22201581299304962\n",
      "iteration 21834: loss: 0.22201505303382874\n",
      "iteration 21835: loss: 0.22201430797576904\n",
      "iteration 21836: loss: 0.22201356291770935\n",
      "iteration 21837: loss: 0.22201280295848846\n",
      "iteration 21838: loss: 0.22201204299926758\n",
      "iteration 21839: loss: 0.2220112830400467\n",
      "iteration 21840: loss: 0.2220105677843094\n",
      "iteration 21841: loss: 0.22200985252857208\n",
      "iteration 21842: loss: 0.22200903296470642\n",
      "iteration 21843: loss: 0.22200831770896912\n",
      "iteration 21844: loss: 0.2220076322555542\n",
      "iteration 21845: loss: 0.22200682759284973\n",
      "iteration 21846: loss: 0.22200612723827362\n",
      "iteration 21847: loss: 0.22200533747673035\n",
      "iteration 21848: loss: 0.22200460731983185\n",
      "iteration 21849: loss: 0.22200384736061096\n",
      "iteration 21850: loss: 0.22200310230255127\n",
      "iteration 21851: loss: 0.22200235724449158\n",
      "iteration 21852: loss: 0.22200164198875427\n",
      "iteration 21853: loss: 0.22200092673301697\n",
      "iteration 21854: loss: 0.22200019657611847\n",
      "iteration 21855: loss: 0.22199943661689758\n",
      "iteration 21856: loss: 0.2219986617565155\n",
      "iteration 21857: loss: 0.2219979465007782\n",
      "iteration 21858: loss: 0.22199717164039612\n",
      "iteration 21859: loss: 0.22199642658233643\n",
      "iteration 21860: loss: 0.22199568152427673\n",
      "iteration 21861: loss: 0.22199496626853943\n",
      "iteration 21862: loss: 0.22199420630931854\n",
      "iteration 21863: loss: 0.22199349105358124\n",
      "iteration 21864: loss: 0.22199280560016632\n",
      "iteration 21865: loss: 0.22199206054210663\n",
      "iteration 21866: loss: 0.22199130058288574\n",
      "iteration 21867: loss: 0.2219904363155365\n",
      "iteration 21868: loss: 0.2219897210597992\n",
      "iteration 21869: loss: 0.22198903560638428\n",
      "iteration 21870: loss: 0.2219882756471634\n",
      "iteration 21871: loss: 0.2219875305891037\n",
      "iteration 21872: loss: 0.2219867706298828\n",
      "iteration 21873: loss: 0.2219860851764679\n",
      "iteration 21874: loss: 0.2219853401184082\n",
      "iteration 21875: loss: 0.22198455035686493\n",
      "iteration 21876: loss: 0.22198386490345\n",
      "iteration 21877: loss: 0.22198311984539032\n",
      "iteration 21878: loss: 0.22198235988616943\n",
      "iteration 21879: loss: 0.22198159992694855\n",
      "iteration 21880: loss: 0.22198083996772766\n",
      "iteration 21881: loss: 0.22198012471199036\n",
      "iteration 21882: loss: 0.22197940945625305\n",
      "iteration 21883: loss: 0.22197870910167694\n",
      "iteration 21884: loss: 0.2219778597354889\n",
      "iteration 21885: loss: 0.22197715938091278\n",
      "iteration 21886: loss: 0.22197648882865906\n",
      "iteration 21887: loss: 0.22197571396827698\n",
      "iteration 21888: loss: 0.2219749391078949\n",
      "iteration 21889: loss: 0.2219742238521576\n",
      "iteration 21890: loss: 0.2219734638929367\n",
      "iteration 21891: loss: 0.2219727337360382\n",
      "iteration 21892: loss: 0.22197195887565613\n",
      "iteration 21893: loss: 0.22197124361991882\n",
      "iteration 21894: loss: 0.22197046875953674\n",
      "iteration 21895: loss: 0.22196976840496063\n",
      "iteration 21896: loss: 0.22196896374225616\n",
      "iteration 21897: loss: 0.22196832299232483\n",
      "iteration 21898: loss: 0.22196750342845917\n",
      "iteration 21899: loss: 0.22196678817272186\n",
      "iteration 21900: loss: 0.22196610271930695\n",
      "iteration 21901: loss: 0.22196534276008606\n",
      "iteration 21902: loss: 0.22196459770202637\n",
      "iteration 21903: loss: 0.22196385264396667\n",
      "iteration 21904: loss: 0.22196316719055176\n",
      "iteration 21905: loss: 0.22196240723133087\n",
      "iteration 21906: loss: 0.2219616174697876\n",
      "iteration 21907: loss: 0.22196082770824432\n",
      "iteration 21908: loss: 0.2219601422548294\n",
      "iteration 21909: loss: 0.2219594419002533\n",
      "iteration 21910: loss: 0.2219587117433548\n",
      "iteration 21911: loss: 0.22195792198181152\n",
      "iteration 21912: loss: 0.2219572365283966\n",
      "iteration 21913: loss: 0.22195644676685333\n",
      "iteration 21914: loss: 0.2219557762145996\n",
      "iteration 21915: loss: 0.22195497155189514\n",
      "iteration 21916: loss: 0.22195419669151306\n",
      "iteration 21917: loss: 0.22195354104042053\n",
      "iteration 21918: loss: 0.22195276618003845\n",
      "iteration 21919: loss: 0.22195205092430115\n",
      "iteration 21920: loss: 0.22195129096508026\n",
      "iteration 21921: loss: 0.22195057570934296\n",
      "iteration 21922: loss: 0.2219497710466385\n",
      "iteration 21923: loss: 0.22194913029670715\n",
      "iteration 21924: loss: 0.22194835543632507\n",
      "iteration 21925: loss: 0.2219475954771042\n",
      "iteration 21926: loss: 0.22194688022136688\n",
      "iteration 21927: loss: 0.2219460904598236\n",
      "iteration 21928: loss: 0.2219453752040863\n",
      "iteration 21929: loss: 0.221944659948349\n",
      "iteration 21930: loss: 0.2219439297914505\n",
      "iteration 21931: loss: 0.2219431847333908\n",
      "iteration 21932: loss: 0.22194242477416992\n",
      "iteration 21933: loss: 0.22194166481494904\n",
      "iteration 21934: loss: 0.2219410240650177\n",
      "iteration 21935: loss: 0.22194020450115204\n",
      "iteration 21936: loss: 0.22193948924541473\n",
      "iteration 21937: loss: 0.22193875908851624\n",
      "iteration 21938: loss: 0.22193801403045654\n",
      "iteration 21939: loss: 0.22193726897239685\n",
      "iteration 21940: loss: 0.22193650901317596\n",
      "iteration 21941: loss: 0.22193579375743866\n",
      "iteration 21942: loss: 0.22193506360054016\n",
      "iteration 21943: loss: 0.22193436324596405\n",
      "iteration 21944: loss: 0.22193363308906555\n",
      "iteration 21945: loss: 0.22193284332752228\n",
      "iteration 21946: loss: 0.22193212807178497\n",
      "iteration 21947: loss: 0.2219313681125641\n",
      "iteration 21948: loss: 0.22193069756031036\n",
      "iteration 21949: loss: 0.22192993760108948\n",
      "iteration 21950: loss: 0.2219291627407074\n",
      "iteration 21951: loss: 0.22192847728729248\n",
      "iteration 21952: loss: 0.221927672624588\n",
      "iteration 21953: loss: 0.2219269573688507\n",
      "iteration 21954: loss: 0.2219262421131134\n",
      "iteration 21955: loss: 0.2219255268573761\n",
      "iteration 21956: loss: 0.2219247817993164\n",
      "iteration 21957: loss: 0.22192402184009552\n",
      "iteration 21958: loss: 0.22192330658435822\n",
      "iteration 21959: loss: 0.22192256152629852\n",
      "iteration 21960: loss: 0.22192180156707764\n",
      "iteration 21961: loss: 0.22192105650901794\n",
      "iteration 21962: loss: 0.22192040085792542\n",
      "iteration 21963: loss: 0.22191962599754333\n",
      "iteration 21964: loss: 0.22191894054412842\n",
      "iteration 21965: loss: 0.22191815078258514\n",
      "iteration 21966: loss: 0.22191743552684784\n",
      "iteration 21967: loss: 0.22191667556762695\n",
      "iteration 21968: loss: 0.22191593050956726\n",
      "iteration 21969: loss: 0.22191520035266876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 21970: loss: 0.22191448509693146\n",
      "iteration 21971: loss: 0.22191376984119415\n",
      "iteration 21972: loss: 0.22191300988197327\n",
      "iteration 21973: loss: 0.22191229462623596\n",
      "iteration 21974: loss: 0.22191151976585388\n",
      "iteration 21975: loss: 0.22191080451011658\n",
      "iteration 21976: loss: 0.22191008925437927\n",
      "iteration 21977: loss: 0.22190937399864197\n",
      "iteration 21978: loss: 0.22190861403942108\n",
      "iteration 21979: loss: 0.2219078540802002\n",
      "iteration 21980: loss: 0.22190718352794647\n",
      "iteration 21981: loss: 0.2219063937664032\n",
      "iteration 21982: loss: 0.22190570831298828\n",
      "iteration 21983: loss: 0.2219049483537674\n",
      "iteration 21984: loss: 0.2219042330980301\n",
      "iteration 21985: loss: 0.2219034880399704\n",
      "iteration 21986: loss: 0.22190280258655548\n",
      "iteration 21987: loss: 0.2219020426273346\n",
      "iteration 21988: loss: 0.2219012975692749\n",
      "iteration 21989: loss: 0.22190061211585999\n",
      "iteration 21990: loss: 0.2218998223543167\n",
      "iteration 21991: loss: 0.2218991219997406\n",
      "iteration 21992: loss: 0.22189836204051971\n",
      "iteration 21993: loss: 0.22189760208129883\n",
      "iteration 21994: loss: 0.2218969166278839\n",
      "iteration 21995: loss: 0.22189612686634064\n",
      "iteration 21996: loss: 0.22189541161060333\n",
      "iteration 21997: loss: 0.2218947410583496\n",
      "iteration 21998: loss: 0.22189398109912872\n",
      "iteration 21999: loss: 0.22189323604106903\n",
      "iteration 22000: loss: 0.22189252078533173\n",
      "iteration 22001: loss: 0.22189176082611084\n",
      "iteration 22002: loss: 0.22189101576805115\n",
      "iteration 22003: loss: 0.22189033031463623\n",
      "iteration 22004: loss: 0.22188958525657654\n",
      "iteration 22005: loss: 0.22188887000083923\n",
      "iteration 22006: loss: 0.22188811004161835\n",
      "iteration 22007: loss: 0.22188742458820343\n",
      "iteration 22008: loss: 0.22188663482666016\n",
      "iteration 22009: loss: 0.22188588976860046\n",
      "iteration 22010: loss: 0.22188523411750793\n",
      "iteration 22011: loss: 0.22188444435596466\n",
      "iteration 22012: loss: 0.22188374400138855\n",
      "iteration 22013: loss: 0.22188298404216766\n",
      "iteration 22014: loss: 0.22188226878643036\n",
      "iteration 22015: loss: 0.22188153862953186\n",
      "iteration 22016: loss: 0.22188079357147217\n",
      "iteration 22017: loss: 0.22188010811805725\n",
      "iteration 22018: loss: 0.22187933325767517\n",
      "iteration 22019: loss: 0.22187860310077667\n",
      "iteration 22020: loss: 0.22187790274620056\n",
      "iteration 22021: loss: 0.22187712788581848\n",
      "iteration 22022: loss: 0.22187645733356476\n",
      "iteration 22023: loss: 0.22187571227550507\n",
      "iteration 22024: loss: 0.2218749076128006\n",
      "iteration 22025: loss: 0.22187423706054688\n",
      "iteration 22026: loss: 0.22187355160713196\n",
      "iteration 22027: loss: 0.22187276184558868\n",
      "iteration 22028: loss: 0.2218720018863678\n",
      "iteration 22029: loss: 0.22187133133411407\n",
      "iteration 22030: loss: 0.2218705713748932\n",
      "iteration 22031: loss: 0.22186978161334991\n",
      "iteration 22032: loss: 0.22186915576457977\n",
      "iteration 22033: loss: 0.2218683958053589\n",
      "iteration 22034: loss: 0.221867635846138\n",
      "iteration 22035: loss: 0.2218668907880783\n",
      "iteration 22036: loss: 0.2218661606311798\n",
      "iteration 22037: loss: 0.2218654900789261\n",
      "iteration 22038: loss: 0.22186477482318878\n",
      "iteration 22039: loss: 0.2218640148639679\n",
      "iteration 22040: loss: 0.2218632996082306\n",
      "iteration 22041: loss: 0.2218625545501709\n",
      "iteration 22042: loss: 0.2218618392944336\n",
      "iteration 22043: loss: 0.22186116874217987\n",
      "iteration 22044: loss: 0.22186043858528137\n",
      "iteration 22045: loss: 0.2218596488237381\n",
      "iteration 22046: loss: 0.2218589335680008\n",
      "iteration 22047: loss: 0.2218581885099411\n",
      "iteration 22048: loss: 0.2218574583530426\n",
      "iteration 22049: loss: 0.2218567579984665\n",
      "iteration 22050: loss: 0.221856027841568\n",
      "iteration 22051: loss: 0.22185523808002472\n",
      "iteration 22052: loss: 0.2218545377254486\n",
      "iteration 22053: loss: 0.22185388207435608\n",
      "iteration 22054: loss: 0.22185306251049042\n",
      "iteration 22055: loss: 0.2218523770570755\n",
      "iteration 22056: loss: 0.2218516618013382\n",
      "iteration 22057: loss: 0.2218509167432785\n",
      "iteration 22058: loss: 0.2218502014875412\n",
      "iteration 22059: loss: 0.2218494415283203\n",
      "iteration 22060: loss: 0.2218487709760666\n",
      "iteration 22061: loss: 0.2218480110168457\n",
      "iteration 22062: loss: 0.22184734046459198\n",
      "iteration 22063: loss: 0.2218465358018875\n",
      "iteration 22064: loss: 0.2218458354473114\n",
      "iteration 22065: loss: 0.2218451052904129\n",
      "iteration 22066: loss: 0.22184434533119202\n",
      "iteration 22067: loss: 0.2218436747789383\n",
      "iteration 22068: loss: 0.2218429148197174\n",
      "iteration 22069: loss: 0.2218422144651413\n",
      "iteration 22070: loss: 0.221841499209404\n",
      "iteration 22071: loss: 0.22184070944786072\n",
      "iteration 22072: loss: 0.2218399941921234\n",
      "iteration 22073: loss: 0.22183933854103088\n",
      "iteration 22074: loss: 0.2218385636806488\n",
      "iteration 22075: loss: 0.2218378484249115\n",
      "iteration 22076: loss: 0.2218371331691742\n",
      "iteration 22077: loss: 0.22183632850646973\n",
      "iteration 22078: loss: 0.221835657954216\n",
      "iteration 22079: loss: 0.22183489799499512\n",
      "iteration 22080: loss: 0.2218342274427414\n",
      "iteration 22081: loss: 0.22183354198932648\n",
      "iteration 22082: loss: 0.22183279693126678\n",
      "iteration 22083: loss: 0.2218320071697235\n",
      "iteration 22084: loss: 0.2218312919139862\n",
      "iteration 22085: loss: 0.2218306064605713\n",
      "iteration 22086: loss: 0.2218298614025116\n",
      "iteration 22087: loss: 0.2218291312456131\n",
      "iteration 22088: loss: 0.221828430891037\n",
      "iteration 22089: loss: 0.2218276709318161\n",
      "iteration 22090: loss: 0.22182700037956238\n",
      "iteration 22091: loss: 0.2218262404203415\n",
      "iteration 22092: loss: 0.2218255251646042\n",
      "iteration 22093: loss: 0.22182483971118927\n",
      "iteration 22094: loss: 0.22182407975196838\n",
      "iteration 22095: loss: 0.22182336449623108\n",
      "iteration 22096: loss: 0.22182264924049377\n",
      "iteration 22097: loss: 0.22182190418243408\n",
      "iteration 22098: loss: 0.2218211442232132\n",
      "iteration 22099: loss: 0.22182044386863708\n",
      "iteration 22100: loss: 0.2218196839094162\n",
      "iteration 22101: loss: 0.22181899845600128\n",
      "iteration 22102: loss: 0.22181829810142517\n",
      "iteration 22103: loss: 0.22181753814220428\n",
      "iteration 22104: loss: 0.22181685268878937\n",
      "iteration 22105: loss: 0.2218160629272461\n",
      "iteration 22106: loss: 0.22181537747383118\n",
      "iteration 22107: loss: 0.22181467711925507\n",
      "iteration 22108: loss: 0.22181394696235657\n",
      "iteration 22109: loss: 0.22181320190429688\n",
      "iteration 22110: loss: 0.22181248664855957\n",
      "iteration 22111: loss: 0.22181181609630585\n",
      "iteration 22112: loss: 0.22181105613708496\n",
      "iteration 22113: loss: 0.2218102514743805\n",
      "iteration 22114: loss: 0.22180958092212677\n",
      "iteration 22115: loss: 0.22180891036987305\n",
      "iteration 22116: loss: 0.22180819511413574\n",
      "iteration 22117: loss: 0.22180745005607605\n",
      "iteration 22118: loss: 0.22180671989917755\n",
      "iteration 22119: loss: 0.22180595993995667\n",
      "iteration 22120: loss: 0.22180521488189697\n",
      "iteration 22121: loss: 0.22180454432964325\n",
      "iteration 22122: loss: 0.22180385887622833\n",
      "iteration 22123: loss: 0.22180311381816864\n",
      "iteration 22124: loss: 0.22180239856243134\n",
      "iteration 22125: loss: 0.22180166840553284\n",
      "iteration 22126: loss: 0.22180095314979553\n",
      "iteration 22127: loss: 0.22180023789405823\n",
      "iteration 22128: loss: 0.22179949283599854\n",
      "iteration 22129: loss: 0.22179873287677765\n",
      "iteration 22130: loss: 0.22179803252220154\n",
      "iteration 22131: loss: 0.2217973917722702\n",
      "iteration 22132: loss: 0.22179654240608215\n",
      "iteration 22133: loss: 0.221795916557312\n",
      "iteration 22134: loss: 0.22179517149925232\n",
      "iteration 22135: loss: 0.22179444134235382\n",
      "iteration 22136: loss: 0.22179372608661652\n",
      "iteration 22137: loss: 0.22179293632507324\n",
      "iteration 22138: loss: 0.22179222106933594\n",
      "iteration 22139: loss: 0.22179153561592102\n",
      "iteration 22140: loss: 0.22179079055786133\n",
      "iteration 22141: loss: 0.22179007530212402\n",
      "iteration 22142: loss: 0.22178931534290314\n",
      "iteration 22143: loss: 0.2217886745929718\n",
      "iteration 22144: loss: 0.2217879593372345\n",
      "iteration 22145: loss: 0.2217872142791748\n",
      "iteration 22146: loss: 0.2217864692211151\n",
      "iteration 22147: loss: 0.2217857539653778\n",
      "iteration 22148: loss: 0.2217850387096405\n",
      "iteration 22149: loss: 0.2217843234539032\n",
      "iteration 22150: loss: 0.22178363800048828\n",
      "iteration 22151: loss: 0.22178295254707336\n",
      "iteration 22152: loss: 0.2217821627855301\n",
      "iteration 22153: loss: 0.22178144752979279\n",
      "iteration 22154: loss: 0.2217807024717331\n",
      "iteration 22155: loss: 0.22178001701831818\n",
      "iteration 22156: loss: 0.22177934646606445\n",
      "iteration 22157: loss: 0.22177855670452118\n",
      "iteration 22158: loss: 0.22177782654762268\n",
      "iteration 22159: loss: 0.22177712619304657\n",
      "iteration 22160: loss: 0.22177639603614807\n",
      "iteration 22161: loss: 0.22177572548389435\n",
      "iteration 22162: loss: 0.22177496552467346\n",
      "iteration 22163: loss: 0.22177426517009735\n",
      "iteration 22164: loss: 0.22177357971668243\n",
      "iteration 22165: loss: 0.22177283465862274\n",
      "iteration 22166: loss: 0.22177210450172424\n",
      "iteration 22167: loss: 0.22177140414714813\n",
      "iteration 22168: loss: 0.22177068889141083\n",
      "iteration 22169: loss: 0.22176995873451233\n",
      "iteration 22170: loss: 0.22176924347877502\n",
      "iteration 22171: loss: 0.22176852822303772\n",
      "iteration 22172: loss: 0.22176781296730042\n",
      "iteration 22173: loss: 0.22176706790924072\n",
      "iteration 22174: loss: 0.2217663824558258\n",
      "iteration 22175: loss: 0.2217656672000885\n",
      "iteration 22176: loss: 0.2217649519443512\n",
      "iteration 22177: loss: 0.22176428139209747\n",
      "iteration 22178: loss: 0.2217634618282318\n",
      "iteration 22179: loss: 0.2217627465724945\n",
      "iteration 22180: loss: 0.22176197171211243\n",
      "iteration 22181: loss: 0.2217613160610199\n",
      "iteration 22182: loss: 0.2217606008052826\n",
      "iteration 22183: loss: 0.2217598855495453\n",
      "iteration 22184: loss: 0.22175920009613037\n",
      "iteration 22185: loss: 0.22175851464271545\n",
      "iteration 22186: loss: 0.22175779938697815\n",
      "iteration 22187: loss: 0.22175700962543488\n",
      "iteration 22188: loss: 0.22175630927085876\n",
      "iteration 22189: loss: 0.22175557911396027\n",
      "iteration 22190: loss: 0.22175486385822296\n",
      "iteration 22191: loss: 0.22175414860248566\n",
      "iteration 22192: loss: 0.22175340354442596\n",
      "iteration 22193: loss: 0.22175273299217224\n",
      "iteration 22194: loss: 0.22175201773643494\n",
      "iteration 22195: loss: 0.22175125777721405\n",
      "iteration 22196: loss: 0.22175057232379913\n",
      "iteration 22197: loss: 0.22174987196922302\n",
      "iteration 22198: loss: 0.22174915671348572\n",
      "iteration 22199: loss: 0.2217484414577484\n",
      "iteration 22200: loss: 0.2217477262020111\n",
      "iteration 22201: loss: 0.2217469960451126\n",
      "iteration 22202: loss: 0.2217462956905365\n",
      "iteration 22203: loss: 0.22174552083015442\n",
      "iteration 22204: loss: 0.22174480557441711\n",
      "iteration 22205: loss: 0.2217441350221634\n",
      "iteration 22206: loss: 0.2217433899641037\n",
      "iteration 22207: loss: 0.22174271941184998\n",
      "iteration 22208: loss: 0.2217419147491455\n",
      "iteration 22209: loss: 0.22174124419689178\n",
      "iteration 22210: loss: 0.22174055874347687\n",
      "iteration 22211: loss: 0.22173979878425598\n",
      "iteration 22212: loss: 0.22173912823200226\n",
      "iteration 22213: loss: 0.22173838317394257\n",
      "iteration 22214: loss: 0.22173766791820526\n",
      "iteration 22215: loss: 0.221736878156662\n",
      "iteration 22216: loss: 0.22173628211021423\n",
      "iteration 22217: loss: 0.22173552215099335\n",
      "iteration 22218: loss: 0.22173483669757843\n",
      "iteration 22219: loss: 0.22173412144184113\n",
      "iteration 22220: loss: 0.22173340618610382\n",
      "iteration 22221: loss: 0.2217327058315277\n",
      "iteration 22222: loss: 0.22173194587230682\n",
      "iteration 22223: loss: 0.2217313051223755\n",
      "iteration 22224: loss: 0.2217305451631546\n",
      "iteration 22225: loss: 0.2217298448085785\n",
      "iteration 22226: loss: 0.2217290848493576\n",
      "iteration 22227: loss: 0.22172841429710388\n",
      "iteration 22228: loss: 0.22172777354717255\n",
      "iteration 22229: loss: 0.22172698378562927\n",
      "iteration 22230: loss: 0.22172626852989197\n",
      "iteration 22231: loss: 0.22172550857067108\n",
      "iteration 22232: loss: 0.22172479331493378\n",
      "iteration 22233: loss: 0.22172412276268005\n",
      "iteration 22234: loss: 0.22172336280345917\n",
      "iteration 22235: loss: 0.22172267735004425\n",
      "iteration 22236: loss: 0.22172197699546814\n",
      "iteration 22237: loss: 0.22172124683856964\n",
      "iteration 22238: loss: 0.22172057628631592\n",
      "iteration 22239: loss: 0.22171983122825623\n",
      "iteration 22240: loss: 0.22171911597251892\n",
      "iteration 22241: loss: 0.22171840071678162\n",
      "iteration 22242: loss: 0.2217177450656891\n",
      "iteration 22243: loss: 0.221716970205307\n",
      "iteration 22244: loss: 0.2217162549495697\n",
      "iteration 22245: loss: 0.22171559929847717\n",
      "iteration 22246: loss: 0.2217148095369339\n",
      "iteration 22247: loss: 0.2217140942811966\n",
      "iteration 22248: loss: 0.22171342372894287\n",
      "iteration 22249: loss: 0.22171270847320557\n",
      "iteration 22250: loss: 0.22171202301979065\n",
      "iteration 22251: loss: 0.22171130776405334\n",
      "iteration 22252: loss: 0.22171053290367126\n",
      "iteration 22253: loss: 0.22170984745025635\n",
      "iteration 22254: loss: 0.22170916199684143\n",
      "iteration 22255: loss: 0.22170844674110413\n",
      "iteration 22256: loss: 0.22170773148536682\n",
      "iteration 22257: loss: 0.22170695662498474\n",
      "iteration 22258: loss: 0.2217063456773758\n",
      "iteration 22259: loss: 0.22170564532279968\n",
      "iteration 22260: loss: 0.2217048853635788\n",
      "iteration 22261: loss: 0.2217041552066803\n",
      "iteration 22262: loss: 0.221703439950943\n",
      "iteration 22263: loss: 0.22170276939868927\n",
      "iteration 22264: loss: 0.22170200943946838\n",
      "iteration 22265: loss: 0.22170133888721466\n",
      "iteration 22266: loss: 0.22170059382915497\n",
      "iteration 22267: loss: 0.22169987857341766\n",
      "iteration 22268: loss: 0.22169919312000275\n",
      "iteration 22269: loss: 0.22169849276542664\n",
      "iteration 22270: loss: 0.22169776260852814\n",
      "iteration 22271: loss: 0.22169706225395203\n",
      "iteration 22272: loss: 0.2216964066028595\n",
      "iteration 22273: loss: 0.22169561684131622\n",
      "iteration 22274: loss: 0.2216949164867401\n",
      "iteration 22275: loss: 0.2216941863298416\n",
      "iteration 22276: loss: 0.2216935157775879\n",
      "iteration 22277: loss: 0.2216927707195282\n",
      "iteration 22278: loss: 0.2216920405626297\n",
      "iteration 22279: loss: 0.22169141471385956\n",
      "iteration 22280: loss: 0.22169065475463867\n",
      "iteration 22281: loss: 0.22168993949890137\n",
      "iteration 22282: loss: 0.22168926894664764\n",
      "iteration 22283: loss: 0.22168850898742676\n",
      "iteration 22284: loss: 0.22168774902820587\n",
      "iteration 22285: loss: 0.22168707847595215\n",
      "iteration 22286: loss: 0.221686452627182\n",
      "iteration 22287: loss: 0.2216857224702835\n",
      "iteration 22288: loss: 0.22168496251106262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 22289: loss: 0.22168433666229248\n",
      "iteration 22290: loss: 0.2216835767030716\n",
      "iteration 22291: loss: 0.22168290615081787\n",
      "iteration 22292: loss: 0.22168214619159698\n",
      "iteration 22293: loss: 0.22168143093585968\n",
      "iteration 22294: loss: 0.22168073058128357\n",
      "iteration 22295: loss: 0.22168007493019104\n",
      "iteration 22296: loss: 0.22167930006980896\n",
      "iteration 22297: loss: 0.22167861461639404\n",
      "iteration 22298: loss: 0.22167794406414032\n",
      "iteration 22299: loss: 0.22167715430259705\n",
      "iteration 22300: loss: 0.22167649865150452\n",
      "iteration 22301: loss: 0.2216757833957672\n",
      "iteration 22302: loss: 0.2216751128435135\n",
      "iteration 22303: loss: 0.2216743677854538\n",
      "iteration 22304: loss: 0.22167368233203888\n",
      "iteration 22305: loss: 0.2216729372739792\n",
      "iteration 22306: loss: 0.22167225182056427\n",
      "iteration 22307: loss: 0.22167150676250458\n",
      "iteration 22308: loss: 0.22167082130908966\n",
      "iteration 22309: loss: 0.22167010605335236\n",
      "iteration 22310: loss: 0.22166943550109863\n",
      "iteration 22311: loss: 0.22166872024536133\n",
      "iteration 22312: loss: 0.22166797518730164\n",
      "iteration 22313: loss: 0.2216673195362091\n",
      "iteration 22314: loss: 0.2216666042804718\n",
      "iteration 22315: loss: 0.2216658592224121\n",
      "iteration 22316: loss: 0.2216651439666748\n",
      "iteration 22317: loss: 0.2216644287109375\n",
      "iteration 22318: loss: 0.2216637134552002\n",
      "iteration 22319: loss: 0.22166307270526886\n",
      "iteration 22320: loss: 0.22166228294372559\n",
      "iteration 22321: loss: 0.22166165709495544\n",
      "iteration 22322: loss: 0.22166094183921814\n",
      "iteration 22323: loss: 0.22166022658348083\n",
      "iteration 22324: loss: 0.2216595709323883\n",
      "iteration 22325: loss: 0.2216588258743286\n",
      "iteration 22326: loss: 0.22165808081626892\n",
      "iteration 22327: loss: 0.221657395362854\n",
      "iteration 22328: loss: 0.2216566801071167\n",
      "iteration 22329: loss: 0.22165600955486298\n",
      "iteration 22330: loss: 0.22165529429912567\n",
      "iteration 22331: loss: 0.22165457904338837\n",
      "iteration 22332: loss: 0.22165384888648987\n",
      "iteration 22333: loss: 0.22165314853191376\n",
      "iteration 22334: loss: 0.22165246307849884\n",
      "iteration 22335: loss: 0.22165174782276154\n",
      "iteration 22336: loss: 0.22165103256702423\n",
      "iteration 22337: loss: 0.22165031731128693\n",
      "iteration 22338: loss: 0.22164960205554962\n",
      "iteration 22339: loss: 0.2216489017009735\n",
      "iteration 22340: loss: 0.22164824604988098\n",
      "iteration 22341: loss: 0.2216474711894989\n",
      "iteration 22342: loss: 0.22164686024188995\n",
      "iteration 22343: loss: 0.2216460406780243\n",
      "iteration 22344: loss: 0.22164539992809296\n",
      "iteration 22345: loss: 0.22164468467235565\n",
      "iteration 22346: loss: 0.2216440737247467\n",
      "iteration 22347: loss: 0.22164328396320343\n",
      "iteration 22348: loss: 0.22164256870746613\n",
      "iteration 22349: loss: 0.2216418981552124\n",
      "iteration 22350: loss: 0.22164110839366913\n",
      "iteration 22351: loss: 0.22164049744606018\n",
      "iteration 22352: loss: 0.2216397225856781\n",
      "iteration 22353: loss: 0.22163906693458557\n",
      "iteration 22354: loss: 0.22163832187652588\n",
      "iteration 22355: loss: 0.22163763642311096\n",
      "iteration 22356: loss: 0.22163692116737366\n",
      "iteration 22357: loss: 0.22163622081279755\n",
      "iteration 22358: loss: 0.22163549065589905\n",
      "iteration 22359: loss: 0.22163479030132294\n",
      "iteration 22360: loss: 0.2216341495513916\n",
      "iteration 22361: loss: 0.2216334044933319\n",
      "iteration 22362: loss: 0.22163276374340057\n",
      "iteration 22363: loss: 0.2216320037841797\n",
      "iteration 22364: loss: 0.22163128852844238\n",
      "iteration 22365: loss: 0.22163064777851105\n",
      "iteration 22366: loss: 0.22162990272045135\n",
      "iteration 22367: loss: 0.22162917256355286\n",
      "iteration 22368: loss: 0.22162850201129913\n",
      "iteration 22369: loss: 0.22162778675556183\n",
      "iteration 22370: loss: 0.22162708640098572\n",
      "iteration 22371: loss: 0.2216263711452484\n",
      "iteration 22372: loss: 0.2216256856918335\n",
      "iteration 22373: loss: 0.2216249704360962\n",
      "iteration 22374: loss: 0.2216242551803589\n",
      "iteration 22375: loss: 0.22162356972694397\n",
      "iteration 22376: loss: 0.22162286937236786\n",
      "iteration 22377: loss: 0.22162213921546936\n",
      "iteration 22378: loss: 0.22162146866321564\n",
      "iteration 22379: loss: 0.22162075340747833\n",
      "iteration 22380: loss: 0.22162005305290222\n",
      "iteration 22381: loss: 0.2216193675994873\n",
      "iteration 22382: loss: 0.22161860764026642\n",
      "iteration 22383: loss: 0.22161796689033508\n",
      "iteration 22384: loss: 0.22161726653575897\n",
      "iteration 22385: loss: 0.22161658108234406\n",
      "iteration 22386: loss: 0.22161586582660675\n",
      "iteration 22387: loss: 0.22161519527435303\n",
      "iteration 22388: loss: 0.22161448001861572\n",
      "iteration 22389: loss: 0.22161373496055603\n",
      "iteration 22390: loss: 0.22161301970481873\n",
      "iteration 22391: loss: 0.2216123640537262\n",
      "iteration 22392: loss: 0.22161158919334412\n",
      "iteration 22393: loss: 0.22161094844341278\n",
      "iteration 22394: loss: 0.22161027789115906\n",
      "iteration 22395: loss: 0.22160950303077698\n",
      "iteration 22396: loss: 0.22160887718200684\n",
      "iteration 22397: loss: 0.2216082066297531\n",
      "iteration 22398: loss: 0.22160744667053223\n",
      "iteration 22399: loss: 0.2216067612171173\n",
      "iteration 22400: loss: 0.22160604596138\n",
      "iteration 22401: loss: 0.2216053307056427\n",
      "iteration 22402: loss: 0.2216046154499054\n",
      "iteration 22403: loss: 0.22160391509532928\n",
      "iteration 22404: loss: 0.22160327434539795\n",
      "iteration 22405: loss: 0.22160252928733826\n",
      "iteration 22406: loss: 0.22160181403160095\n",
      "iteration 22407: loss: 0.22160115838050842\n",
      "iteration 22408: loss: 0.22160044312477112\n",
      "iteration 22409: loss: 0.2215997725725174\n",
      "iteration 22410: loss: 0.22159907221794128\n",
      "iteration 22411: loss: 0.22159834206104279\n",
      "iteration 22412: loss: 0.22159764170646667\n",
      "iteration 22413: loss: 0.22159692645072937\n",
      "iteration 22414: loss: 0.22159624099731445\n",
      "iteration 22415: loss: 0.22159557044506073\n",
      "iteration 22416: loss: 0.22159485518932343\n",
      "iteration 22417: loss: 0.2215941846370697\n",
      "iteration 22418: loss: 0.22159342467784882\n",
      "iteration 22419: loss: 0.2215927541255951\n",
      "iteration 22420: loss: 0.2215920388698578\n",
      "iteration 22421: loss: 0.22159135341644287\n",
      "iteration 22422: loss: 0.22159063816070557\n",
      "iteration 22423: loss: 0.22158987820148468\n",
      "iteration 22424: loss: 0.22158925235271454\n",
      "iteration 22425: loss: 0.22158856689929962\n",
      "iteration 22426: loss: 0.2215878963470459\n",
      "iteration 22427: loss: 0.2215871810913086\n",
      "iteration 22428: loss: 0.2215864360332489\n",
      "iteration 22429: loss: 0.22158578038215637\n",
      "iteration 22430: loss: 0.22158506512641907\n",
      "iteration 22431: loss: 0.22158436477184296\n",
      "iteration 22432: loss: 0.22158364951610565\n",
      "iteration 22433: loss: 0.22158297896385193\n",
      "iteration 22434: loss: 0.22158226370811462\n",
      "iteration 22435: loss: 0.2215815782546997\n",
      "iteration 22436: loss: 0.2215808629989624\n",
      "iteration 22437: loss: 0.22158017754554749\n",
      "iteration 22438: loss: 0.22157947719097137\n",
      "iteration 22439: loss: 0.22157879173755646\n",
      "iteration 22440: loss: 0.22157804667949677\n",
      "iteration 22441: loss: 0.22157737612724304\n",
      "iteration 22442: loss: 0.22157664597034454\n",
      "iteration 22443: loss: 0.2215760201215744\n",
      "iteration 22444: loss: 0.2215752899646759\n",
      "iteration 22445: loss: 0.2215745896100998\n",
      "iteration 22446: loss: 0.2215738743543625\n",
      "iteration 22447: loss: 0.22157315909862518\n",
      "iteration 22448: loss: 0.22157251834869385\n",
      "iteration 22449: loss: 0.22157183289527893\n",
      "iteration 22450: loss: 0.22157105803489685\n",
      "iteration 22451: loss: 0.22157041728496552\n",
      "iteration 22452: loss: 0.22156968712806702\n",
      "iteration 22453: loss: 0.2215690165758133\n",
      "iteration 22454: loss: 0.221568301320076\n",
      "iteration 22455: loss: 0.22156767547130585\n",
      "iteration 22456: loss: 0.22156694531440735\n",
      "iteration 22457: loss: 0.22156620025634766\n",
      "iteration 22458: loss: 0.22156552970409393\n",
      "iteration 22459: loss: 0.2215648889541626\n",
      "iteration 22460: loss: 0.2215641438961029\n",
      "iteration 22461: loss: 0.2215634137392044\n",
      "iteration 22462: loss: 0.22156277298927307\n",
      "iteration 22463: loss: 0.22156207263469696\n",
      "iteration 22464: loss: 0.2215612828731537\n",
      "iteration 22465: loss: 0.22156068682670593\n",
      "iteration 22466: loss: 0.22155997157096863\n",
      "iteration 22467: loss: 0.2215592861175537\n",
      "iteration 22468: loss: 0.22155854105949402\n",
      "iteration 22469: loss: 0.2215578556060791\n",
      "iteration 22470: loss: 0.22155717015266418\n",
      "iteration 22471: loss: 0.22155645489692688\n",
      "iteration 22472: loss: 0.22155578434467316\n",
      "iteration 22473: loss: 0.22155511379241943\n",
      "iteration 22474: loss: 0.22155432403087616\n",
      "iteration 22475: loss: 0.2215537279844284\n",
      "iteration 22476: loss: 0.2215530127286911\n",
      "iteration 22477: loss: 0.2215522825717926\n",
      "iteration 22478: loss: 0.22155162692070007\n",
      "iteration 22479: loss: 0.22155094146728516\n",
      "iteration 22480: loss: 0.22155022621154785\n",
      "iteration 22481: loss: 0.22154951095581055\n",
      "iteration 22482: loss: 0.2215488851070404\n",
      "iteration 22483: loss: 0.2215481549501419\n",
      "iteration 22484: loss: 0.22154748439788818\n",
      "iteration 22485: loss: 0.22154676914215088\n",
      "iteration 22486: loss: 0.22154608368873596\n",
      "iteration 22487: loss: 0.22154545783996582\n",
      "iteration 22488: loss: 0.22154466807842255\n",
      "iteration 22489: loss: 0.2215440273284912\n",
      "iteration 22490: loss: 0.22154328227043152\n",
      "iteration 22491: loss: 0.22154268622398376\n",
      "iteration 22492: loss: 0.22154195606708527\n",
      "iteration 22493: loss: 0.22154124081134796\n",
      "iteration 22494: loss: 0.22154057025909424\n",
      "iteration 22495: loss: 0.22153988480567932\n",
      "iteration 22496: loss: 0.2215391844511032\n",
      "iteration 22497: loss: 0.2215384542942047\n",
      "iteration 22498: loss: 0.221537783741951\n",
      "iteration 22499: loss: 0.22153711318969727\n",
      "iteration 22500: loss: 0.22153639793395996\n",
      "iteration 22501: loss: 0.22153571248054504\n",
      "iteration 22502: loss: 0.22153501212596893\n",
      "iteration 22503: loss: 0.2215343415737152\n",
      "iteration 22504: loss: 0.22153356671333313\n",
      "iteration 22505: loss: 0.22153286635875702\n",
      "iteration 22506: loss: 0.2215321958065033\n",
      "iteration 22507: loss: 0.22153151035308838\n",
      "iteration 22508: loss: 0.22153082489967346\n",
      "iteration 22509: loss: 0.22153019905090332\n",
      "iteration 22510: loss: 0.22152945399284363\n",
      "iteration 22511: loss: 0.2215287685394287\n",
      "iteration 22512: loss: 0.22152812778949738\n",
      "iteration 22513: loss: 0.22152738273143768\n",
      "iteration 22514: loss: 0.22152665257453918\n",
      "iteration 22515: loss: 0.22152605652809143\n",
      "iteration 22516: loss: 0.22152534127235413\n",
      "iteration 22517: loss: 0.22152459621429443\n",
      "iteration 22518: loss: 0.2215239554643631\n",
      "iteration 22519: loss: 0.2215232104063034\n",
      "iteration 22520: loss: 0.22152253985404968\n",
      "iteration 22521: loss: 0.22152185440063477\n",
      "iteration 22522: loss: 0.22152113914489746\n",
      "iteration 22523: loss: 0.22152046859264374\n",
      "iteration 22524: loss: 0.22151979804039001\n",
      "iteration 22525: loss: 0.2215190827846527\n",
      "iteration 22526: loss: 0.22151844203472137\n",
      "iteration 22527: loss: 0.22151772677898407\n",
      "iteration 22528: loss: 0.22151704132556915\n",
      "iteration 22529: loss: 0.22151634097099304\n",
      "iteration 22530: loss: 0.22151565551757812\n",
      "iteration 22531: loss: 0.22151489555835724\n",
      "iteration 22532: loss: 0.2215142697095871\n",
      "iteration 22533: loss: 0.22151362895965576\n",
      "iteration 22534: loss: 0.22151288390159607\n",
      "iteration 22535: loss: 0.22151216864585876\n",
      "iteration 22536: loss: 0.22151148319244385\n",
      "iteration 22537: loss: 0.22151079773902893\n",
      "iteration 22538: loss: 0.2215101271867752\n",
      "iteration 22539: loss: 0.22150945663452148\n",
      "iteration 22540: loss: 0.22150874137878418\n",
      "iteration 22541: loss: 0.22150802612304688\n",
      "iteration 22542: loss: 0.22150738537311554\n",
      "iteration 22543: loss: 0.22150667011737823\n",
      "iteration 22544: loss: 0.22150595486164093\n",
      "iteration 22545: loss: 0.2215053141117096\n",
      "iteration 22546: loss: 0.2215045988559723\n",
      "iteration 22547: loss: 0.2215038537979126\n",
      "iteration 22548: loss: 0.22150325775146484\n",
      "iteration 22549: loss: 0.22150258719921112\n",
      "iteration 22550: loss: 0.22150182723999023\n",
      "iteration 22551: loss: 0.2215011864900589\n",
      "iteration 22552: loss: 0.2215004861354828\n",
      "iteration 22553: loss: 0.22149980068206787\n",
      "iteration 22554: loss: 0.22149908542633057\n",
      "iteration 22555: loss: 0.22149837017059326\n",
      "iteration 22556: loss: 0.22149772942066193\n",
      "iteration 22557: loss: 0.2214970588684082\n",
      "iteration 22558: loss: 0.2214963436126709\n",
      "iteration 22559: loss: 0.2214956283569336\n",
      "iteration 22560: loss: 0.22149494290351868\n",
      "iteration 22561: loss: 0.22149428725242615\n",
      "iteration 22562: loss: 0.22149355709552765\n",
      "iteration 22563: loss: 0.2214929163455963\n",
      "iteration 22564: loss: 0.22149226069450378\n",
      "iteration 22565: loss: 0.22149153053760529\n",
      "iteration 22566: loss: 0.22149083018302917\n",
      "iteration 22567: loss: 0.22149017453193665\n",
      "iteration 22568: loss: 0.22148947417736053\n",
      "iteration 22569: loss: 0.22148878872394562\n",
      "iteration 22570: loss: 0.2214880883693695\n",
      "iteration 22571: loss: 0.22148743271827698\n",
      "iteration 22572: loss: 0.22148673236370087\n",
      "iteration 22573: loss: 0.22148601710796356\n",
      "iteration 22574: loss: 0.22148533165454865\n",
      "iteration 22575: loss: 0.22148463129997253\n",
      "iteration 22576: loss: 0.22148397564888\n",
      "iteration 22577: loss: 0.22148331999778748\n",
      "iteration 22578: loss: 0.22148258984088898\n",
      "iteration 22579: loss: 0.22148196399211884\n",
      "iteration 22580: loss: 0.22148123383522034\n",
      "iteration 22581: loss: 0.2214805632829666\n",
      "iteration 22582: loss: 0.22147981822490692\n",
      "iteration 22583: loss: 0.22147917747497559\n",
      "iteration 22584: loss: 0.22147846221923828\n",
      "iteration 22585: loss: 0.22147774696350098\n",
      "iteration 22586: loss: 0.22147710621356964\n",
      "iteration 22587: loss: 0.22147643566131592\n",
      "iteration 22588: loss: 0.221475750207901\n",
      "iteration 22589: loss: 0.2214750349521637\n",
      "iteration 22590: loss: 0.22147436439990997\n",
      "iteration 22591: loss: 0.22147372364997864\n",
      "iteration 22592: loss: 0.22147293388843536\n",
      "iteration 22593: loss: 0.22147229313850403\n",
      "iteration 22594: loss: 0.22147159278392792\n",
      "iteration 22595: loss: 0.2214709222316742\n",
      "iteration 22596: loss: 0.2214702069759369\n",
      "iteration 22597: loss: 0.22146959602832794\n",
      "iteration 22598: loss: 0.22146888077259064\n",
      "iteration 22599: loss: 0.22146821022033691\n",
      "iteration 22600: loss: 0.22146746516227722\n",
      "iteration 22601: loss: 0.2214668095111847\n",
      "iteration 22602: loss: 0.22146610915660858\n",
      "iteration 22603: loss: 0.22146546840667725\n",
      "iteration 22604: loss: 0.22146478295326233\n",
      "iteration 22605: loss: 0.22146406769752502\n",
      "iteration 22606: loss: 0.2214633673429489\n",
      "iteration 22607: loss: 0.2214626967906952\n",
      "iteration 22608: loss: 0.22146201133728027\n",
      "iteration 22609: loss: 0.22146134078502655\n",
      "iteration 22610: loss: 0.22146067023277283\n",
      "iteration 22611: loss: 0.22145995497703552\n",
      "iteration 22612: loss: 0.221459299325943\n",
      "iteration 22613: loss: 0.22145859897136688\n",
      "iteration 22614: loss: 0.22145791351795197\n",
      "iteration 22615: loss: 0.22145719826221466\n",
      "iteration 22616: loss: 0.22145657241344452\n",
      "iteration 22617: loss: 0.2214558869600296\n",
      "iteration 22618: loss: 0.2214551866054535\n",
      "iteration 22619: loss: 0.22145450115203857\n",
      "iteration 22620: loss: 0.22145378589630127\n",
      "iteration 22621: loss: 0.22145311534404755\n",
      "iteration 22622: loss: 0.22145244479179382\n",
      "iteration 22623: loss: 0.22145172953605652\n",
      "iteration 22624: loss: 0.2214510440826416\n",
      "iteration 22625: loss: 0.22145035862922668\n",
      "iteration 22626: loss: 0.22144970297813416\n",
      "iteration 22627: loss: 0.22144904732704163\n",
      "iteration 22628: loss: 0.22144833207130432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 22629: loss: 0.2214476764202118\n",
      "iteration 22630: loss: 0.22144702076911926\n",
      "iteration 22631: loss: 0.22144632041454315\n",
      "iteration 22632: loss: 0.2214457094669342\n",
      "iteration 22633: loss: 0.22144491970539093\n",
      "iteration 22634: loss: 0.2214442789554596\n",
      "iteration 22635: loss: 0.2214435636997223\n",
      "iteration 22636: loss: 0.22144290804862976\n",
      "iteration 22637: loss: 0.22144226729869843\n",
      "iteration 22638: loss: 0.22144150733947754\n",
      "iteration 22639: loss: 0.2214408665895462\n",
      "iteration 22640: loss: 0.22144010663032532\n",
      "iteration 22641: loss: 0.2214394509792328\n",
      "iteration 22642: loss: 0.22143881022930145\n",
      "iteration 22643: loss: 0.22143813967704773\n",
      "iteration 22644: loss: 0.2214374542236328\n",
      "iteration 22645: loss: 0.2214367687702179\n",
      "iteration 22646: loss: 0.22143599390983582\n",
      "iteration 22647: loss: 0.22143538296222687\n",
      "iteration 22648: loss: 0.22143468260765076\n",
      "iteration 22649: loss: 0.22143404185771942\n",
      "iteration 22650: loss: 0.2214333564043045\n",
      "iteration 22651: loss: 0.22143271565437317\n",
      "iteration 22652: loss: 0.22143200039863586\n",
      "iteration 22653: loss: 0.22143132984638214\n",
      "iteration 22654: loss: 0.22143062949180603\n",
      "iteration 22655: loss: 0.2214299738407135\n",
      "iteration 22656: loss: 0.22142919898033142\n",
      "iteration 22657: loss: 0.22142858803272247\n",
      "iteration 22658: loss: 0.22142791748046875\n",
      "iteration 22659: loss: 0.22142727673053741\n",
      "iteration 22660: loss: 0.2214265614748001\n",
      "iteration 22661: loss: 0.221425861120224\n",
      "iteration 22662: loss: 0.22142517566680908\n",
      "iteration 22663: loss: 0.22142449021339417\n",
      "iteration 22664: loss: 0.22142381966114044\n",
      "iteration 22665: loss: 0.22142310440540314\n",
      "iteration 22666: loss: 0.22142238914966583\n",
      "iteration 22667: loss: 0.22142186760902405\n",
      "iteration 22668: loss: 0.22142116725444794\n",
      "iteration 22669: loss: 0.22142048180103302\n",
      "iteration 22670: loss: 0.22141976654529572\n",
      "iteration 22671: loss: 0.22141912579536438\n",
      "iteration 22672: loss: 0.2214183509349823\n",
      "iteration 22673: loss: 0.22141773998737335\n",
      "iteration 22674: loss: 0.22141702473163605\n",
      "iteration 22675: loss: 0.2214163839817047\n",
      "iteration 22676: loss: 0.221415713429451\n",
      "iteration 22677: loss: 0.22141499817371368\n",
      "iteration 22678: loss: 0.22141432762145996\n",
      "iteration 22679: loss: 0.22141361236572266\n",
      "iteration 22680: loss: 0.22141294181346893\n",
      "iteration 22681: loss: 0.22141225636005402\n",
      "iteration 22682: loss: 0.2214115560054779\n",
      "iteration 22683: loss: 0.22141090035438538\n",
      "iteration 22684: loss: 0.22141024470329285\n",
      "iteration 22685: loss: 0.22140952944755554\n",
      "iteration 22686: loss: 0.2214088886976242\n",
      "iteration 22687: loss: 0.22140824794769287\n",
      "iteration 22688: loss: 0.22140756249427795\n",
      "iteration 22689: loss: 0.22140686213970184\n",
      "iteration 22690: loss: 0.22140619158744812\n",
      "iteration 22691: loss: 0.22140547633171082\n",
      "iteration 22692: loss: 0.22140483558177948\n",
      "iteration 22693: loss: 0.22140419483184814\n",
      "iteration 22694: loss: 0.22140347957611084\n",
      "iteration 22695: loss: 0.22140279412269592\n",
      "iteration 22696: loss: 0.2214021384716034\n",
      "iteration 22697: loss: 0.22140145301818848\n",
      "iteration 22698: loss: 0.22140078246593475\n",
      "iteration 22699: loss: 0.22140011191368103\n",
      "iteration 22700: loss: 0.22139939665794373\n",
      "iteration 22701: loss: 0.2213987112045288\n",
      "iteration 22702: loss: 0.22139807045459747\n",
      "iteration 22703: loss: 0.22139739990234375\n",
      "iteration 22704: loss: 0.22139668464660645\n",
      "iteration 22705: loss: 0.22139601409435272\n",
      "iteration 22706: loss: 0.221395343542099\n",
      "iteration 22707: loss: 0.22139474749565125\n",
      "iteration 22708: loss: 0.22139406204223633\n",
      "iteration 22709: loss: 0.22139334678649902\n",
      "iteration 22710: loss: 0.22139272093772888\n",
      "iteration 22711: loss: 0.22139191627502441\n",
      "iteration 22712: loss: 0.22139129042625427\n",
      "iteration 22713: loss: 0.22139063477516174\n",
      "iteration 22714: loss: 0.22138993442058563\n",
      "iteration 22715: loss: 0.2213892638683319\n",
      "iteration 22716: loss: 0.2213885486125946\n",
      "iteration 22717: loss: 0.22138793766498566\n",
      "iteration 22718: loss: 0.22138726711273193\n",
      "iteration 22719: loss: 0.22138658165931702\n",
      "iteration 22720: loss: 0.2213859111070633\n",
      "iteration 22721: loss: 0.22138521075248718\n",
      "iteration 22722: loss: 0.22138457000255585\n",
      "iteration 22723: loss: 0.22138381004333496\n",
      "iteration 22724: loss: 0.22138313949108124\n",
      "iteration 22725: loss: 0.2213824987411499\n",
      "iteration 22726: loss: 0.22138181328773499\n",
      "iteration 22727: loss: 0.22138118743896484\n",
      "iteration 22728: loss: 0.22138050198554993\n",
      "iteration 22729: loss: 0.22137978672981262\n",
      "iteration 22730: loss: 0.2213791161775589\n",
      "iteration 22731: loss: 0.22137847542762756\n",
      "iteration 22732: loss: 0.22137780487537384\n",
      "iteration 22733: loss: 0.22137710452079773\n",
      "iteration 22734: loss: 0.22137638926506042\n",
      "iteration 22735: loss: 0.2213757336139679\n",
      "iteration 22736: loss: 0.22137507796287537\n",
      "iteration 22737: loss: 0.22137436270713806\n",
      "iteration 22738: loss: 0.22137372195720673\n",
      "iteration 22739: loss: 0.2213730812072754\n",
      "iteration 22740: loss: 0.22137244045734406\n",
      "iteration 22741: loss: 0.22137174010276794\n",
      "iteration 22742: loss: 0.22137100994586945\n",
      "iteration 22743: loss: 0.2213703840970993\n",
      "iteration 22744: loss: 0.2213696986436844\n",
      "iteration 22745: loss: 0.22136905789375305\n",
      "iteration 22746: loss: 0.22136838734149933\n",
      "iteration 22747: loss: 0.22136768698692322\n",
      "iteration 22748: loss: 0.2213670313358307\n",
      "iteration 22749: loss: 0.22136631608009338\n",
      "iteration 22750: loss: 0.22136569023132324\n",
      "iteration 22751: loss: 0.22136497497558594\n",
      "iteration 22752: loss: 0.22136430442333221\n",
      "iteration 22753: loss: 0.22136369347572327\n",
      "iteration 22754: loss: 0.22136297821998596\n",
      "iteration 22755: loss: 0.22136227786540985\n",
      "iteration 22756: loss: 0.22136163711547852\n",
      "iteration 22757: loss: 0.2213609665632248\n",
      "iteration 22758: loss: 0.2213602364063263\n",
      "iteration 22759: loss: 0.22135961055755615\n",
      "iteration 22760: loss: 0.22135892510414124\n",
      "iteration 22761: loss: 0.2213582992553711\n",
      "iteration 22762: loss: 0.2213575392961502\n",
      "iteration 22763: loss: 0.22135689854621887\n",
      "iteration 22764: loss: 0.22135622799396515\n",
      "iteration 22765: loss: 0.22135552763938904\n",
      "iteration 22766: loss: 0.2213548868894577\n",
      "iteration 22767: loss: 0.22135424613952637\n",
      "iteration 22768: loss: 0.22135357558727264\n",
      "iteration 22769: loss: 0.22135284543037415\n",
      "iteration 22770: loss: 0.2213522493839264\n",
      "iteration 22771: loss: 0.2213515341281891\n",
      "iteration 22772: loss: 0.22135087847709656\n",
      "iteration 22773: loss: 0.22135019302368164\n",
      "iteration 22774: loss: 0.22134947776794434\n",
      "iteration 22775: loss: 0.2213488072156906\n",
      "iteration 22776: loss: 0.22134819626808167\n",
      "iteration 22777: loss: 0.22134748101234436\n",
      "iteration 22778: loss: 0.22134685516357422\n",
      "iteration 22779: loss: 0.2213461846113205\n",
      "iteration 22780: loss: 0.22134549915790558\n",
      "iteration 22781: loss: 0.22134482860565186\n",
      "iteration 22782: loss: 0.22134414315223694\n",
      "iteration 22783: loss: 0.22134347259998322\n",
      "iteration 22784: loss: 0.22134284675121307\n",
      "iteration 22785: loss: 0.2213420867919922\n",
      "iteration 22786: loss: 0.22134144604206085\n",
      "iteration 22787: loss: 0.2213408201932907\n",
      "iteration 22788: loss: 0.221340149641037\n",
      "iteration 22789: loss: 0.22133946418762207\n",
      "iteration 22790: loss: 0.22133879363536835\n",
      "iteration 22791: loss: 0.221338152885437\n",
      "iteration 22792: loss: 0.2213374674320221\n",
      "iteration 22793: loss: 0.2213367521762848\n",
      "iteration 22794: loss: 0.22133608162403107\n",
      "iteration 22795: loss: 0.22133545577526093\n",
      "iteration 22796: loss: 0.2213347852230072\n",
      "iteration 22797: loss: 0.22133409976959229\n",
      "iteration 22798: loss: 0.22133342921733856\n",
      "iteration 22799: loss: 0.22133275866508484\n",
      "iteration 22800: loss: 0.2213321030139923\n",
      "iteration 22801: loss: 0.2213314324617386\n",
      "iteration 22802: loss: 0.22133073210716248\n",
      "iteration 22803: loss: 0.22133007645606995\n",
      "iteration 22804: loss: 0.22132940590381622\n",
      "iteration 22805: loss: 0.2213287353515625\n",
      "iteration 22806: loss: 0.22132806479930878\n",
      "iteration 22807: loss: 0.22132739424705505\n",
      "iteration 22808: loss: 0.2213267832994461\n",
      "iteration 22809: loss: 0.22132611274719238\n",
      "iteration 22810: loss: 0.22132542729377747\n",
      "iteration 22811: loss: 0.22132471203804016\n",
      "iteration 22812: loss: 0.2213241159915924\n",
      "iteration 22813: loss: 0.22132337093353271\n",
      "iteration 22814: loss: 0.22132273018360138\n",
      "iteration 22815: loss: 0.22132202982902527\n",
      "iteration 22816: loss: 0.22132138907909393\n",
      "iteration 22817: loss: 0.2213207483291626\n",
      "iteration 22818: loss: 0.22132007777690887\n",
      "iteration 22819: loss: 0.22131943702697754\n",
      "iteration 22820: loss: 0.22131876647472382\n",
      "iteration 22821: loss: 0.22131803631782532\n",
      "iteration 22822: loss: 0.2213173806667328\n",
      "iteration 22823: loss: 0.22131666541099548\n",
      "iteration 22824: loss: 0.22131605446338654\n",
      "iteration 22825: loss: 0.2213153839111328\n",
      "iteration 22826: loss: 0.2213147133588791\n",
      "iteration 22827: loss: 0.22131404280662537\n",
      "iteration 22828: loss: 0.22131340205669403\n",
      "iteration 22829: loss: 0.2213127315044403\n",
      "iteration 22830: loss: 0.2213120460510254\n",
      "iteration 22831: loss: 0.22131137549877167\n",
      "iteration 22832: loss: 0.22131069004535675\n",
      "iteration 22833: loss: 0.221310093998909\n",
      "iteration 22834: loss: 0.2213093340396881\n",
      "iteration 22835: loss: 0.22130873799324036\n",
      "iteration 22836: loss: 0.22130806744098663\n",
      "iteration 22837: loss: 0.2213074266910553\n",
      "iteration 22838: loss: 0.22130677103996277\n",
      "iteration 22839: loss: 0.22130604088306427\n",
      "iteration 22840: loss: 0.22130541503429413\n",
      "iteration 22841: loss: 0.2213047444820404\n",
      "iteration 22842: loss: 0.2213040292263031\n",
      "iteration 22843: loss: 0.22130338847637177\n",
      "iteration 22844: loss: 0.22130271792411804\n",
      "iteration 22845: loss: 0.22130203247070312\n",
      "iteration 22846: loss: 0.22130140662193298\n",
      "iteration 22847: loss: 0.22130069136619568\n",
      "iteration 22848: loss: 0.22130008041858673\n",
      "iteration 22849: loss: 0.22129936516284943\n",
      "iteration 22850: loss: 0.22129876911640167\n",
      "iteration 22851: loss: 0.22129806876182556\n",
      "iteration 22852: loss: 0.22129735350608826\n",
      "iteration 22853: loss: 0.22129671275615692\n",
      "iteration 22854: loss: 0.22129610180854797\n",
      "iteration 22855: loss: 0.22129540145397186\n",
      "iteration 22856: loss: 0.22129473090171814\n",
      "iteration 22857: loss: 0.2212940901517868\n",
      "iteration 22858: loss: 0.22129341959953308\n",
      "iteration 22859: loss: 0.22129273414611816\n",
      "iteration 22860: loss: 0.22129210829734802\n",
      "iteration 22861: loss: 0.22129139304161072\n",
      "iteration 22862: loss: 0.22129078209400177\n",
      "iteration 22863: loss: 0.22129006683826447\n",
      "iteration 22864: loss: 0.22128939628601074\n",
      "iteration 22865: loss: 0.221288800239563\n",
      "iteration 22866: loss: 0.22128808498382568\n",
      "iteration 22867: loss: 0.22128745913505554\n",
      "iteration 22868: loss: 0.221286803483963\n",
      "iteration 22869: loss: 0.2212861329317093\n",
      "iteration 22870: loss: 0.22128541767597198\n",
      "iteration 22871: loss: 0.22128477692604065\n",
      "iteration 22872: loss: 0.22128412127494812\n",
      "iteration 22873: loss: 0.2212834656238556\n",
      "iteration 22874: loss: 0.22128276526927948\n",
      "iteration 22875: loss: 0.22128212451934814\n",
      "iteration 22876: loss: 0.22128145396709442\n",
      "iteration 22877: loss: 0.2212807685136795\n",
      "iteration 22878: loss: 0.22128018736839294\n",
      "iteration 22879: loss: 0.22127947211265564\n",
      "iteration 22880: loss: 0.2212788313627243\n",
      "iteration 22881: loss: 0.22127819061279297\n",
      "iteration 22882: loss: 0.22127747535705566\n",
      "iteration 22883: loss: 0.22127680480480194\n",
      "iteration 22884: loss: 0.2212761640548706\n",
      "iteration 22885: loss: 0.2212754786014557\n",
      "iteration 22886: loss: 0.22127488255500793\n",
      "iteration 22887: loss: 0.22127418220043182\n",
      "iteration 22888: loss: 0.2212735414505005\n",
      "iteration 22889: loss: 0.22127285599708557\n",
      "iteration 22890: loss: 0.22127218544483185\n",
      "iteration 22891: loss: 0.22127148509025574\n",
      "iteration 22892: loss: 0.2212708443403244\n",
      "iteration 22893: loss: 0.22127020359039307\n",
      "iteration 22894: loss: 0.22126953303813934\n",
      "iteration 22895: loss: 0.221268892288208\n",
      "iteration 22896: loss: 0.22126826643943787\n",
      "iteration 22897: loss: 0.22126755118370056\n",
      "iteration 22898: loss: 0.22126689553260803\n",
      "iteration 22899: loss: 0.2212662696838379\n",
      "iteration 22900: loss: 0.2212655246257782\n",
      "iteration 22901: loss: 0.22126488387584686\n",
      "iteration 22902: loss: 0.22126424312591553\n",
      "iteration 22903: loss: 0.2212635725736618\n",
      "iteration 22904: loss: 0.22126296162605286\n",
      "iteration 22905: loss: 0.22126226127147675\n",
      "iteration 22906: loss: 0.22126159071922302\n",
      "iteration 22907: loss: 0.22126099467277527\n",
      "iteration 22908: loss: 0.22126033902168274\n",
      "iteration 22909: loss: 0.22125962376594543\n",
      "iteration 22910: loss: 0.2212589532136917\n",
      "iteration 22911: loss: 0.2212582528591156\n",
      "iteration 22912: loss: 0.22125764191150665\n",
      "iteration 22913: loss: 0.2212570160627365\n",
      "iteration 22914: loss: 0.22125637531280518\n",
      "iteration 22915: loss: 0.22125563025474548\n",
      "iteration 22916: loss: 0.22125506401062012\n",
      "iteration 22917: loss: 0.2212543934583664\n",
      "iteration 22918: loss: 0.2212536334991455\n",
      "iteration 22919: loss: 0.22125300765037537\n",
      "iteration 22920: loss: 0.22125235199928284\n",
      "iteration 22921: loss: 0.22125175595283508\n",
      "iteration 22922: loss: 0.2212510108947754\n",
      "iteration 22923: loss: 0.22125038504600525\n",
      "iteration 22924: loss: 0.2212497889995575\n",
      "iteration 22925: loss: 0.221249058842659\n",
      "iteration 22926: loss: 0.22124843299388885\n",
      "iteration 22927: loss: 0.22124779224395752\n",
      "iteration 22928: loss: 0.2212471067905426\n",
      "iteration 22929: loss: 0.2212463915348053\n",
      "iteration 22930: loss: 0.22124573588371277\n",
      "iteration 22931: loss: 0.22124509513378143\n",
      "iteration 22932: loss: 0.2212444543838501\n",
      "iteration 22933: loss: 0.22124381363391876\n",
      "iteration 22934: loss: 0.22124311327934265\n",
      "iteration 22935: loss: 0.22124247252941132\n",
      "iteration 22936: loss: 0.22124183177947998\n",
      "iteration 22937: loss: 0.22124114632606506\n",
      "iteration 22938: loss: 0.22124049067497253\n",
      "iteration 22939: loss: 0.22123980522155762\n",
      "iteration 22940: loss: 0.22123920917510986\n",
      "iteration 22941: loss: 0.22123853862285614\n",
      "iteration 22942: loss: 0.22123785316944122\n",
      "iteration 22943: loss: 0.22123725712299347\n",
      "iteration 22944: loss: 0.22123658657073975\n",
      "iteration 22945: loss: 0.22123584151268005\n",
      "iteration 22946: loss: 0.22123520076274872\n",
      "iteration 22947: loss: 0.22123456001281738\n",
      "iteration 22948: loss: 0.22123393416404724\n",
      "iteration 22949: loss: 0.2212332785129547\n",
      "iteration 22950: loss: 0.221232607960701\n",
      "iteration 22951: loss: 0.22123193740844727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 22952: loss: 0.22123125195503235\n",
      "iteration 22953: loss: 0.2212306559085846\n",
      "iteration 22954: loss: 0.22122998535633087\n",
      "iteration 22955: loss: 0.22122931480407715\n",
      "iteration 22956: loss: 0.221228688955307\n",
      "iteration 22957: loss: 0.2212279587984085\n",
      "iteration 22958: loss: 0.22122736275196075\n",
      "iteration 22959: loss: 0.22122672200202942\n",
      "iteration 22960: loss: 0.22122600674629211\n",
      "iteration 22961: loss: 0.22122538089752197\n",
      "iteration 22962: loss: 0.22122474014759064\n",
      "iteration 22963: loss: 0.2212240993976593\n",
      "iteration 22964: loss: 0.22122347354888916\n",
      "iteration 22965: loss: 0.22122278809547424\n",
      "iteration 22966: loss: 0.22122208774089813\n",
      "iteration 22967: loss: 0.2212214469909668\n",
      "iteration 22968: loss: 0.22122082114219666\n",
      "iteration 22969: loss: 0.22122009098529816\n",
      "iteration 22970: loss: 0.2212194949388504\n",
      "iteration 22971: loss: 0.2212187796831131\n",
      "iteration 22972: loss: 0.22121815383434296\n",
      "iteration 22973: loss: 0.22121748328208923\n",
      "iteration 22974: loss: 0.22121687233448029\n",
      "iteration 22975: loss: 0.22121623158454895\n",
      "iteration 22976: loss: 0.2212156057357788\n",
      "iteration 22977: loss: 0.22121486067771912\n",
      "iteration 22978: loss: 0.22121424973011017\n",
      "iteration 22979: loss: 0.22121357917785645\n",
      "iteration 22980: loss: 0.2212129384279251\n",
      "iteration 22981: loss: 0.2212122678756714\n",
      "iteration 22982: loss: 0.22121159732341766\n",
      "iteration 22983: loss: 0.22121098637580872\n",
      "iteration 22984: loss: 0.221210315823555\n",
      "iteration 22985: loss: 0.22120961546897888\n",
      "iteration 22986: loss: 0.22120897471904755\n",
      "iteration 22987: loss: 0.22120830416679382\n",
      "iteration 22988: loss: 0.2212076634168625\n",
      "iteration 22989: loss: 0.22120706737041473\n",
      "iteration 22990: loss: 0.22120638191699982\n",
      "iteration 22991: loss: 0.22120574116706848\n",
      "iteration 22992: loss: 0.22120508551597595\n",
      "iteration 22993: loss: 0.22120442986488342\n",
      "iteration 22994: loss: 0.2212037593126297\n",
      "iteration 22995: loss: 0.22120311856269836\n",
      "iteration 22996: loss: 0.22120241820812225\n",
      "iteration 22997: loss: 0.22120177745819092\n",
      "iteration 22998: loss: 0.22120115160942078\n",
      "iteration 22999: loss: 0.22120049595832825\n",
      "iteration 23000: loss: 0.22119979560375214\n",
      "iteration 23001: loss: 0.2211991548538208\n",
      "iteration 23002: loss: 0.22119851410388947\n",
      "iteration 23003: loss: 0.22119788825511932\n",
      "iteration 23004: loss: 0.2211972028017044\n",
      "iteration 23005: loss: 0.22119665145874023\n",
      "iteration 23006: loss: 0.22119590640068054\n",
      "iteration 23007: loss: 0.2211952954530716\n",
      "iteration 23008: loss: 0.22119459509849548\n",
      "iteration 23009: loss: 0.22119398415088654\n",
      "iteration 23010: loss: 0.22119326889514923\n",
      "iteration 23011: loss: 0.2211926430463791\n",
      "iteration 23012: loss: 0.22119204699993134\n",
      "iteration 23013: loss: 0.2211913764476776\n",
      "iteration 23014: loss: 0.2211906909942627\n",
      "iteration 23015: loss: 0.22119009494781494\n",
      "iteration 23016: loss: 0.2211894541978836\n",
      "iteration 23017: loss: 0.2211887389421463\n",
      "iteration 23018: loss: 0.22118806838989258\n",
      "iteration 23019: loss: 0.22118739783763885\n",
      "iteration 23020: loss: 0.22118675708770752\n",
      "iteration 23021: loss: 0.22118616104125977\n",
      "iteration 23022: loss: 0.22118549048900604\n",
      "iteration 23023: loss: 0.2211848497390747\n",
      "iteration 23024: loss: 0.2211841642856598\n",
      "iteration 23025: loss: 0.22118353843688965\n",
      "iteration 23026: loss: 0.22118286788463593\n",
      "iteration 23027: loss: 0.2211822271347046\n",
      "iteration 23028: loss: 0.22118155658245087\n",
      "iteration 23029: loss: 0.22118091583251953\n",
      "iteration 23030: loss: 0.2211802452802658\n",
      "iteration 23031: loss: 0.22117960453033447\n",
      "iteration 23032: loss: 0.22117897868156433\n",
      "iteration 23033: loss: 0.22117836773395538\n",
      "iteration 23034: loss: 0.22117766737937927\n",
      "iteration 23035: loss: 0.22117701172828674\n",
      "iteration 23036: loss: 0.22117634117603302\n",
      "iteration 23037: loss: 0.22117571532726288\n",
      "iteration 23038: loss: 0.22117504477500916\n",
      "iteration 23039: loss: 0.22117440402507782\n",
      "iteration 23040: loss: 0.2211737334728241\n",
      "iteration 23041: loss: 0.22117307782173157\n",
      "iteration 23042: loss: 0.2211724817752838\n",
      "iteration 23043: loss: 0.2211717814207077\n",
      "iteration 23044: loss: 0.22117121517658234\n",
      "iteration 23045: loss: 0.22117051482200623\n",
      "iteration 23046: loss: 0.22116990387439728\n",
      "iteration 23047: loss: 0.22116918861865997\n",
      "iteration 23048: loss: 0.22116856276988983\n",
      "iteration 23049: loss: 0.22116787731647491\n",
      "iteration 23050: loss: 0.22116725146770477\n",
      "iteration 23051: loss: 0.22116661071777344\n",
      "iteration 23052: loss: 0.22116594016551971\n",
      "iteration 23053: loss: 0.22116534411907196\n",
      "iteration 23054: loss: 0.22116461396217346\n",
      "iteration 23055: loss: 0.22116398811340332\n",
      "iteration 23056: loss: 0.22116336226463318\n",
      "iteration 23057: loss: 0.22116270661354065\n",
      "iteration 23058: loss: 0.22116205096244812\n",
      "iteration 23059: loss: 0.2211613953113556\n",
      "iteration 23060: loss: 0.22116076946258545\n",
      "iteration 23061: loss: 0.22116009891033173\n",
      "iteration 23062: loss: 0.22115948796272278\n",
      "iteration 23063: loss: 0.22115883231163025\n",
      "iteration 23064: loss: 0.22115811705589294\n",
      "iteration 23065: loss: 0.221157506108284\n",
      "iteration 23066: loss: 0.22115683555603027\n",
      "iteration 23067: loss: 0.22115616500377655\n",
      "iteration 23068: loss: 0.22115552425384521\n",
      "iteration 23069: loss: 0.22115492820739746\n",
      "iteration 23070: loss: 0.22115430235862732\n",
      "iteration 23071: loss: 0.2211536467075348\n",
      "iteration 23072: loss: 0.22115302085876465\n",
      "iteration 23073: loss: 0.22115233540534973\n",
      "iteration 23074: loss: 0.2211516797542572\n",
      "iteration 23075: loss: 0.22115102410316467\n",
      "iteration 23076: loss: 0.22115042805671692\n",
      "iteration 23077: loss: 0.22114971280097961\n",
      "iteration 23078: loss: 0.22114911675453186\n",
      "iteration 23079: loss: 0.22114844620227814\n",
      "iteration 23080: loss: 0.22114777565002441\n",
      "iteration 23081: loss: 0.22114714980125427\n",
      "iteration 23082: loss: 0.22114649415016174\n",
      "iteration 23083: loss: 0.221145898103714\n",
      "iteration 23084: loss: 0.22114518284797668\n",
      "iteration 23085: loss: 0.22114458680152893\n",
      "iteration 23086: loss: 0.22114399075508118\n",
      "iteration 23087: loss: 0.22114324569702148\n",
      "iteration 23088: loss: 0.22114264965057373\n",
      "iteration 23089: loss: 0.2211419641971588\n",
      "iteration 23090: loss: 0.22114138305187225\n",
      "iteration 23091: loss: 0.22114069759845734\n",
      "iteration 23092: loss: 0.221140056848526\n",
      "iteration 23093: loss: 0.2211393564939499\n",
      "iteration 23094: loss: 0.22113868594169617\n",
      "iteration 23095: loss: 0.22113807499408722\n",
      "iteration 23096: loss: 0.22113747894763947\n",
      "iteration 23097: loss: 0.22113673388957977\n",
      "iteration 23098: loss: 0.2211361676454544\n",
      "iteration 23099: loss: 0.22113549709320068\n",
      "iteration 23100: loss: 0.22113487124443054\n",
      "iteration 23101: loss: 0.2211342304944992\n",
      "iteration 23102: loss: 0.22113363444805145\n",
      "iteration 23103: loss: 0.22113290429115295\n",
      "iteration 23104: loss: 0.2211323231458664\n",
      "iteration 23105: loss: 0.2211316078901291\n",
      "iteration 23106: loss: 0.22113099694252014\n",
      "iteration 23107: loss: 0.22113037109375\n",
      "iteration 23108: loss: 0.22112973034381866\n",
      "iteration 23109: loss: 0.22112910449504852\n",
      "iteration 23110: loss: 0.2211284339427948\n",
      "iteration 23111: loss: 0.22112779319286346\n",
      "iteration 23112: loss: 0.22112710773944855\n",
      "iteration 23113: loss: 0.2211264818906784\n",
      "iteration 23114: loss: 0.22112584114074707\n",
      "iteration 23115: loss: 0.22112520039081573\n",
      "iteration 23116: loss: 0.2211245596408844\n",
      "iteration 23117: loss: 0.22112388908863068\n",
      "iteration 23118: loss: 0.22112326323986053\n",
      "iteration 23119: loss: 0.2211226522922516\n",
      "iteration 23120: loss: 0.22112199664115906\n",
      "iteration 23121: loss: 0.22112134099006653\n",
      "iteration 23122: loss: 0.2211206704378128\n",
      "iteration 23123: loss: 0.22112004458904266\n",
      "iteration 23124: loss: 0.22111937403678894\n",
      "iteration 23125: loss: 0.2211187332868576\n",
      "iteration 23126: loss: 0.22111813724040985\n",
      "iteration 23127: loss: 0.22111745178699493\n",
      "iteration 23128: loss: 0.2211168259382248\n",
      "iteration 23129: loss: 0.22111621499061584\n",
      "iteration 23130: loss: 0.2211155891418457\n",
      "iteration 23131: loss: 0.2211148738861084\n",
      "iteration 23132: loss: 0.22111424803733826\n",
      "iteration 23133: loss: 0.22111356258392334\n",
      "iteration 23134: loss: 0.2211129367351532\n",
      "iteration 23135: loss: 0.22111232578754425\n",
      "iteration 23136: loss: 0.22111165523529053\n",
      "iteration 23137: loss: 0.22111102938652039\n",
      "iteration 23138: loss: 0.22111034393310547\n",
      "iteration 23139: loss: 0.22110971808433533\n",
      "iteration 23140: loss: 0.2211090624332428\n",
      "iteration 23141: loss: 0.22110848128795624\n",
      "iteration 23142: loss: 0.22110781073570251\n",
      "iteration 23143: loss: 0.22110719978809357\n",
      "iteration 23144: loss: 0.22110649943351746\n",
      "iteration 23145: loss: 0.22110584378242493\n",
      "iteration 23146: loss: 0.22110529243946075\n",
      "iteration 23147: loss: 0.22110465168952942\n",
      "iteration 23148: loss: 0.22110390663146973\n",
      "iteration 23149: loss: 0.22110331058502197\n",
      "iteration 23150: loss: 0.22110264003276825\n",
      "iteration 23151: loss: 0.2211020439863205\n",
      "iteration 23152: loss: 0.22110140323638916\n",
      "iteration 23153: loss: 0.22110076248645782\n",
      "iteration 23154: loss: 0.2211001217365265\n",
      "iteration 23155: loss: 0.22109949588775635\n",
      "iteration 23156: loss: 0.22109881043434143\n",
      "iteration 23157: loss: 0.22109822928905487\n",
      "iteration 23158: loss: 0.22109749913215637\n",
      "iteration 23159: loss: 0.2210969477891922\n",
      "iteration 23160: loss: 0.2210962474346161\n",
      "iteration 23161: loss: 0.22109560668468475\n",
      "iteration 23162: loss: 0.22109496593475342\n",
      "iteration 23163: loss: 0.22109439969062805\n",
      "iteration 23164: loss: 0.22109372913837433\n",
      "iteration 23165: loss: 0.22109302878379822\n",
      "iteration 23166: loss: 0.22109246253967285\n",
      "iteration 23167: loss: 0.22109179198741913\n",
      "iteration 23168: loss: 0.22109118103981018\n",
      "iteration 23169: loss: 0.22109051048755646\n",
      "iteration 23170: loss: 0.22108983993530273\n",
      "iteration 23171: loss: 0.22108915448188782\n",
      "iteration 23172: loss: 0.22108860313892365\n",
      "iteration 23173: loss: 0.22108793258666992\n",
      "iteration 23174: loss: 0.2210872918367386\n",
      "iteration 23175: loss: 0.22108665108680725\n",
      "iteration 23176: loss: 0.2210860699415207\n",
      "iteration 23177: loss: 0.2210853546857834\n",
      "iteration 23178: loss: 0.22108475863933563\n",
      "iteration 23179: loss: 0.2210841178894043\n",
      "iteration 23180: loss: 0.22108343243598938\n",
      "iteration 23181: loss: 0.22108280658721924\n",
      "iteration 23182: loss: 0.22108225524425507\n",
      "iteration 23183: loss: 0.22108156979084015\n",
      "iteration 23184: loss: 0.22108086943626404\n",
      "iteration 23185: loss: 0.22108030319213867\n",
      "iteration 23186: loss: 0.22107966244220734\n",
      "iteration 23187: loss: 0.221079021692276\n",
      "iteration 23188: loss: 0.2210783064365387\n",
      "iteration 23189: loss: 0.22107768058776855\n",
      "iteration 23190: loss: 0.2210770547389984\n",
      "iteration 23191: loss: 0.22107644379138947\n",
      "iteration 23192: loss: 0.2210758477449417\n",
      "iteration 23193: loss: 0.22107510268688202\n",
      "iteration 23194: loss: 0.22107449173927307\n",
      "iteration 23195: loss: 0.22107386589050293\n",
      "iteration 23196: loss: 0.22107326984405518\n",
      "iteration 23197: loss: 0.22107259929180145\n",
      "iteration 23198: loss: 0.22107195854187012\n",
      "iteration 23199: loss: 0.22107133269309998\n",
      "iteration 23200: loss: 0.221070796251297\n",
      "iteration 23201: loss: 0.22107002139091492\n",
      "iteration 23202: loss: 0.22106938064098358\n",
      "iteration 23203: loss: 0.22106876969337463\n",
      "iteration 23204: loss: 0.2210681438446045\n",
      "iteration 23205: loss: 0.22106750309467316\n",
      "iteration 23206: loss: 0.22106687724590302\n",
      "iteration 23207: loss: 0.22106623649597168\n",
      "iteration 23208: loss: 0.22106564044952393\n",
      "iteration 23209: loss: 0.221064954996109\n",
      "iteration 23210: loss: 0.22106432914733887\n",
      "iteration 23211: loss: 0.22106370329856873\n",
      "iteration 23212: loss: 0.2210630178451538\n",
      "iteration 23213: loss: 0.22106245160102844\n",
      "iteration 23214: loss: 0.22106178104877472\n",
      "iteration 23215: loss: 0.22106118500232697\n",
      "iteration 23216: loss: 0.22106048464775085\n",
      "iteration 23217: loss: 0.22105984389781952\n",
      "iteration 23218: loss: 0.22105923295021057\n",
      "iteration 23219: loss: 0.22105857729911804\n",
      "iteration 23220: loss: 0.2210579365491867\n",
      "iteration 23221: loss: 0.22105737030506134\n",
      "iteration 23222: loss: 0.22105669975280762\n",
      "iteration 23223: loss: 0.2210560292005539\n",
      "iteration 23224: loss: 0.22105546295642853\n",
      "iteration 23225: loss: 0.22105474770069122\n",
      "iteration 23226: loss: 0.22105419635772705\n",
      "iteration 23227: loss: 0.22105352580547333\n",
      "iteration 23228: loss: 0.221052885055542\n",
      "iteration 23229: loss: 0.22105219960212708\n",
      "iteration 23230: loss: 0.22105157375335693\n",
      "iteration 23231: loss: 0.22105088829994202\n",
      "iteration 23232: loss: 0.22105030715465546\n",
      "iteration 23233: loss: 0.2210496962070465\n",
      "iteration 23234: loss: 0.22104902565479279\n",
      "iteration 23235: loss: 0.22104839980602264\n",
      "iteration 23236: loss: 0.2210478037595749\n",
      "iteration 23237: loss: 0.22104711830615997\n",
      "iteration 23238: loss: 0.22104649245738983\n",
      "iteration 23239: loss: 0.2210458517074585\n",
      "iteration 23240: loss: 0.22104521095752716\n",
      "iteration 23241: loss: 0.221044659614563\n",
      "iteration 23242: loss: 0.2210439145565033\n",
      "iteration 23243: loss: 0.22104330360889435\n",
      "iteration 23244: loss: 0.22104275226593018\n",
      "iteration 23245: loss: 0.22104206681251526\n",
      "iteration 23246: loss: 0.22104139626026154\n",
      "iteration 23247: loss: 0.22104080021381378\n",
      "iteration 23248: loss: 0.22104009985923767\n",
      "iteration 23249: loss: 0.2210395336151123\n",
      "iteration 23250: loss: 0.22103889286518097\n",
      "iteration 23251: loss: 0.22103822231292725\n",
      "iteration 23252: loss: 0.2210376262664795\n",
      "iteration 23253: loss: 0.22103700041770935\n",
      "iteration 23254: loss: 0.2210363894701004\n",
      "iteration 23255: loss: 0.2210356742143631\n",
      "iteration 23256: loss: 0.22103512287139893\n",
      "iteration 23257: loss: 0.2210344821214676\n",
      "iteration 23258: loss: 0.22103381156921387\n",
      "iteration 23259: loss: 0.22103314101696014\n",
      "iteration 23260: loss: 0.2210325449705124\n",
      "iteration 23261: loss: 0.22103197872638702\n",
      "iteration 23262: loss: 0.2210312783718109\n",
      "iteration 23263: loss: 0.22103066742420197\n",
      "iteration 23264: loss: 0.22103002667427063\n",
      "iteration 23265: loss: 0.2210293710231781\n",
      "iteration 23266: loss: 0.22102871537208557\n",
      "iteration 23267: loss: 0.22102811932563782\n",
      "iteration 23268: loss: 0.22102749347686768\n",
      "iteration 23269: loss: 0.22102685272693634\n",
      "iteration 23270: loss: 0.221026211977005\n",
      "iteration 23271: loss: 0.22102558612823486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 23272: loss: 0.22102496027946472\n",
      "iteration 23273: loss: 0.2210243046283722\n",
      "iteration 23274: loss: 0.22102370858192444\n",
      "iteration 23275: loss: 0.22102303802967072\n",
      "iteration 23276: loss: 0.22102241218090057\n",
      "iteration 23277: loss: 0.22102181613445282\n",
      "iteration 23278: loss: 0.2210211306810379\n",
      "iteration 23279: loss: 0.22102057933807373\n",
      "iteration 23280: loss: 0.2210199385881424\n",
      "iteration 23281: loss: 0.22101931273937225\n",
      "iteration 23282: loss: 0.22101859748363495\n",
      "iteration 23283: loss: 0.22101803123950958\n",
      "iteration 23284: loss: 0.22101740539073944\n",
      "iteration 23285: loss: 0.2210167646408081\n",
      "iteration 23286: loss: 0.22101609408855438\n",
      "iteration 23287: loss: 0.22101549804210663\n",
      "iteration 23288: loss: 0.2210148572921753\n",
      "iteration 23289: loss: 0.22101426124572754\n",
      "iteration 23290: loss: 0.22101359069347382\n",
      "iteration 23291: loss: 0.22101297974586487\n",
      "iteration 23292: loss: 0.22101238369941711\n",
      "iteration 23293: loss: 0.22101172804832458\n",
      "iteration 23294: loss: 0.22101107239723206\n",
      "iteration 23295: loss: 0.22101044654846191\n",
      "iteration 23296: loss: 0.22100980579853058\n",
      "iteration 23297: loss: 0.22100916504859924\n",
      "iteration 23298: loss: 0.22100861370563507\n",
      "iteration 23299: loss: 0.22100794315338135\n",
      "iteration 23300: loss: 0.22100730240345\n",
      "iteration 23301: loss: 0.22100666165351868\n",
      "iteration 23302: loss: 0.22100600600242615\n",
      "iteration 23303: loss: 0.2210053950548172\n",
      "iteration 23304: loss: 0.22100476920604706\n",
      "iteration 23305: loss: 0.22100412845611572\n",
      "iteration 23306: loss: 0.22100353240966797\n",
      "iteration 23307: loss: 0.22100286185741425\n",
      "iteration 23308: loss: 0.22100229561328888\n",
      "iteration 23309: loss: 0.22100159525871277\n",
      "iteration 23310: loss: 0.22100099921226501\n",
      "iteration 23311: loss: 0.22100038826465607\n",
      "iteration 23312: loss: 0.22099971771240234\n",
      "iteration 23313: loss: 0.22099915146827698\n",
      "iteration 23314: loss: 0.22099852561950684\n",
      "iteration 23315: loss: 0.22099776566028595\n",
      "iteration 23316: loss: 0.22099724411964417\n",
      "iteration 23317: loss: 0.2209966480731964\n",
      "iteration 23318: loss: 0.2209959477186203\n",
      "iteration 23319: loss: 0.22099535167217255\n",
      "iteration 23320: loss: 0.22099463641643524\n",
      "iteration 23321: loss: 0.2209939956665039\n",
      "iteration 23322: loss: 0.22099342942237854\n",
      "iteration 23323: loss: 0.220992773771286\n",
      "iteration 23324: loss: 0.22099213302135468\n",
      "iteration 23325: loss: 0.2209915667772293\n",
      "iteration 23326: loss: 0.22099092602729797\n",
      "iteration 23327: loss: 0.2209903448820114\n",
      "iteration 23328: loss: 0.2209896743297577\n",
      "iteration 23329: loss: 0.22098903357982635\n",
      "iteration 23330: loss: 0.2209884226322174\n",
      "iteration 23331: loss: 0.22098776698112488\n",
      "iteration 23332: loss: 0.2209872007369995\n",
      "iteration 23333: loss: 0.2209864854812622\n",
      "iteration 23334: loss: 0.22098593413829803\n",
      "iteration 23335: loss: 0.2209852635860443\n",
      "iteration 23336: loss: 0.22098462283611298\n",
      "iteration 23337: loss: 0.2209840714931488\n",
      "iteration 23338: loss: 0.2209833860397339\n",
      "iteration 23339: loss: 0.22098276019096375\n",
      "iteration 23340: loss: 0.2209821492433548\n",
      "iteration 23341: loss: 0.22098147869110107\n",
      "iteration 23342: loss: 0.2209809124469757\n",
      "iteration 23343: loss: 0.22098028659820557\n",
      "iteration 23344: loss: 0.22097964584827423\n",
      "iteration 23345: loss: 0.22097904980182648\n",
      "iteration 23346: loss: 0.22097840905189514\n",
      "iteration 23347: loss: 0.220977783203125\n",
      "iteration 23348: loss: 0.22097714245319366\n",
      "iteration 23349: loss: 0.22097650170326233\n",
      "iteration 23350: loss: 0.2209758460521698\n",
      "iteration 23351: loss: 0.22097523510456085\n",
      "iteration 23352: loss: 0.22097459435462952\n",
      "iteration 23353: loss: 0.22097396850585938\n",
      "iteration 23354: loss: 0.220973402261734\n",
      "iteration 23355: loss: 0.22097277641296387\n",
      "iteration 23356: loss: 0.22097213566303253\n",
      "iteration 23357: loss: 0.2209714949131012\n",
      "iteration 23358: loss: 0.22097086906433105\n",
      "iteration 23359: loss: 0.2209703028202057\n",
      "iteration 23360: loss: 0.220969557762146\n",
      "iteration 23361: loss: 0.2209690362215042\n",
      "iteration 23362: loss: 0.2209683656692505\n",
      "iteration 23363: loss: 0.22096772491931915\n",
      "iteration 23364: loss: 0.2209671288728714\n",
      "iteration 23365: loss: 0.22096650302410126\n",
      "iteration 23366: loss: 0.22096583247184753\n",
      "iteration 23367: loss: 0.2209652215242386\n",
      "iteration 23368: loss: 0.22096462547779083\n",
      "iteration 23369: loss: 0.2209639549255371\n",
      "iteration 23370: loss: 0.22096338868141174\n",
      "iteration 23371: loss: 0.2209627330303192\n",
      "iteration 23372: loss: 0.22096209228038788\n",
      "iteration 23373: loss: 0.2209615409374237\n",
      "iteration 23374: loss: 0.22096090018749237\n",
      "iteration 23375: loss: 0.22096025943756104\n",
      "iteration 23376: loss: 0.2209596186876297\n",
      "iteration 23377: loss: 0.22095899283885956\n",
      "iteration 23378: loss: 0.22095835208892822\n",
      "iteration 23379: loss: 0.22095772624015808\n",
      "iteration 23380: loss: 0.22095711529254913\n",
      "iteration 23381: loss: 0.220956489443779\n",
      "iteration 23382: loss: 0.22095584869384766\n",
      "iteration 23383: loss: 0.2209552526473999\n",
      "iteration 23384: loss: 0.22095465660095215\n",
      "iteration 23385: loss: 0.220954030752182\n",
      "iteration 23386: loss: 0.22095339000225067\n",
      "iteration 23387: loss: 0.22095279395580292\n",
      "iteration 23388: loss: 0.22095215320587158\n",
      "iteration 23389: loss: 0.22095152735710144\n",
      "iteration 23390: loss: 0.22095084190368652\n",
      "iteration 23391: loss: 0.22095024585723877\n",
      "iteration 23392: loss: 0.22094964981079102\n",
      "iteration 23393: loss: 0.2209489792585373\n",
      "iteration 23394: loss: 0.22094838321208954\n",
      "iteration 23395: loss: 0.22094778716564178\n",
      "iteration 23396: loss: 0.22094719111919403\n",
      "iteration 23397: loss: 0.22094658017158508\n",
      "iteration 23398: loss: 0.22094586491584778\n",
      "iteration 23399: loss: 0.22094523906707764\n",
      "iteration 23400: loss: 0.2209445983171463\n",
      "iteration 23401: loss: 0.22094407677650452\n",
      "iteration 23402: loss: 0.220943421125412\n",
      "iteration 23403: loss: 0.22094285488128662\n",
      "iteration 23404: loss: 0.2209421843290329\n",
      "iteration 23405: loss: 0.22094154357910156\n",
      "iteration 23406: loss: 0.2209409475326538\n",
      "iteration 23407: loss: 0.22094027698040009\n",
      "iteration 23408: loss: 0.22093968093395233\n",
      "iteration 23409: loss: 0.220939040184021\n",
      "iteration 23410: loss: 0.22093844413757324\n",
      "iteration 23411: loss: 0.22093777358531952\n",
      "iteration 23412: loss: 0.22093717753887177\n",
      "iteration 23413: loss: 0.22093656659126282\n",
      "iteration 23414: loss: 0.22093594074249268\n",
      "iteration 23415: loss: 0.2209353744983673\n",
      "iteration 23416: loss: 0.22093474864959717\n",
      "iteration 23417: loss: 0.22093406319618225\n",
      "iteration 23418: loss: 0.2209334820508957\n",
      "iteration 23419: loss: 0.22093281149864197\n",
      "iteration 23420: loss: 0.2209322154521942\n",
      "iteration 23421: loss: 0.22093161940574646\n",
      "iteration 23422: loss: 0.22093096375465393\n",
      "iteration 23423: loss: 0.22093036770820618\n",
      "iteration 23424: loss: 0.22092974185943604\n",
      "iteration 23425: loss: 0.2209291011095047\n",
      "iteration 23426: loss: 0.22092850506305695\n",
      "iteration 23427: loss: 0.2209279090166092\n",
      "iteration 23428: loss: 0.22092728316783905\n",
      "iteration 23429: loss: 0.2209266722202301\n",
      "iteration 23430: loss: 0.22092607617378235\n",
      "iteration 23431: loss: 0.22092540562152863\n",
      "iteration 23432: loss: 0.22092480957508087\n",
      "iteration 23433: loss: 0.22092418372631073\n",
      "iteration 23434: loss: 0.2209235429763794\n",
      "iteration 23435: loss: 0.22092297673225403\n",
      "iteration 23436: loss: 0.22092226147651672\n",
      "iteration 23437: loss: 0.22092171013355255\n",
      "iteration 23438: loss: 0.22092100977897644\n",
      "iteration 23439: loss: 0.22092044353485107\n",
      "iteration 23440: loss: 0.22091984748840332\n",
      "iteration 23441: loss: 0.2209191769361496\n",
      "iteration 23442: loss: 0.22091861069202423\n",
      "iteration 23443: loss: 0.2209179699420929\n",
      "iteration 23444: loss: 0.22091731429100037\n",
      "iteration 23445: loss: 0.2209167778491974\n",
      "iteration 23446: loss: 0.22091607749462128\n",
      "iteration 23447: loss: 0.2209155112504959\n",
      "iteration 23448: loss: 0.22091491520404816\n",
      "iteration 23449: loss: 0.22091428935527802\n",
      "iteration 23450: loss: 0.2209136039018631\n",
      "iteration 23451: loss: 0.22091302275657654\n",
      "iteration 23452: loss: 0.22091242671012878\n",
      "iteration 23453: loss: 0.22091178596019745\n",
      "iteration 23454: loss: 0.2209111452102661\n",
      "iteration 23455: loss: 0.22091059386730194\n",
      "iteration 23456: loss: 0.22090987861156464\n",
      "iteration 23457: loss: 0.22090932726860046\n",
      "iteration 23458: loss: 0.22090868651866913\n",
      "iteration 23459: loss: 0.22090809047222137\n",
      "iteration 23460: loss: 0.22090744972229004\n",
      "iteration 23461: loss: 0.22090685367584229\n",
      "iteration 23462: loss: 0.22090622782707214\n",
      "iteration 23463: loss: 0.2209055870771408\n",
      "iteration 23464: loss: 0.22090499103069305\n",
      "iteration 23465: loss: 0.2209043800830841\n",
      "iteration 23466: loss: 0.220903679728508\n",
      "iteration 23467: loss: 0.2209031581878662\n",
      "iteration 23468: loss: 0.22090259194374084\n",
      "iteration 23469: loss: 0.22090187668800354\n",
      "iteration 23470: loss: 0.2209012508392334\n",
      "iteration 23471: loss: 0.22090068459510803\n",
      "iteration 23472: loss: 0.22090008854866028\n",
      "iteration 23473: loss: 0.22089943289756775\n",
      "iteration 23474: loss: 0.2208988219499588\n",
      "iteration 23475: loss: 0.22089818120002747\n",
      "iteration 23476: loss: 0.2208976298570633\n",
      "iteration 23477: loss: 0.22089700400829315\n",
      "iteration 23478: loss: 0.22089633345603943\n",
      "iteration 23479: loss: 0.22089573740959167\n",
      "iteration 23480: loss: 0.22089512646198273\n",
      "iteration 23481: loss: 0.22089450061321259\n",
      "iteration 23482: loss: 0.22089388966560364\n",
      "iteration 23483: loss: 0.2208932340145111\n",
      "iteration 23484: loss: 0.22089266777038574\n",
      "iteration 23485: loss: 0.2208920419216156\n",
      "iteration 23486: loss: 0.22089147567749023\n",
      "iteration 23487: loss: 0.2208908051252365\n",
      "iteration 23488: loss: 0.22089020907878876\n",
      "iteration 23489: loss: 0.22088956832885742\n",
      "iteration 23490: loss: 0.22088897228240967\n",
      "iteration 23491: loss: 0.22088834643363953\n",
      "iteration 23492: loss: 0.2208877056837082\n",
      "iteration 23493: loss: 0.22088713943958282\n",
      "iteration 23494: loss: 0.22088651359081268\n",
      "iteration 23495: loss: 0.22088590264320374\n",
      "iteration 23496: loss: 0.22088532149791718\n",
      "iteration 23497: loss: 0.22088465094566345\n",
      "iteration 23498: loss: 0.2208840548992157\n",
      "iteration 23499: loss: 0.22088344395160675\n",
      "iteration 23500: loss: 0.22088277339935303\n",
      "iteration 23501: loss: 0.22088220715522766\n",
      "iteration 23502: loss: 0.22088155150413513\n",
      "iteration 23503: loss: 0.22088095545768738\n",
      "iteration 23504: loss: 0.2208804190158844\n",
      "iteration 23505: loss: 0.2208796739578247\n",
      "iteration 23506: loss: 0.22087916731834412\n",
      "iteration 23507: loss: 0.2208784520626068\n",
      "iteration 23508: loss: 0.22087791562080383\n",
      "iteration 23509: loss: 0.2208772599697113\n",
      "iteration 23510: loss: 0.22087673842906952\n",
      "iteration 23511: loss: 0.2208760529756546\n",
      "iteration 23512: loss: 0.22087542712688446\n",
      "iteration 23513: loss: 0.2208748310804367\n",
      "iteration 23514: loss: 0.22087422013282776\n",
      "iteration 23515: loss: 0.22087362408638\n",
      "iteration 23516: loss: 0.22087302803993225\n",
      "iteration 23517: loss: 0.22087232768535614\n",
      "iteration 23518: loss: 0.2208717316389084\n",
      "iteration 23519: loss: 0.22087112069129944\n",
      "iteration 23520: loss: 0.22087056934833527\n",
      "iteration 23521: loss: 0.22086986899375916\n",
      "iteration 23522: loss: 0.22086937725543976\n",
      "iteration 23523: loss: 0.22086873650550842\n",
      "iteration 23524: loss: 0.22086811065673828\n",
      "iteration 23525: loss: 0.22086744010448456\n",
      "iteration 23526: loss: 0.2208668291568756\n",
      "iteration 23527: loss: 0.22086624801158905\n",
      "iteration 23528: loss: 0.22086560726165771\n",
      "iteration 23529: loss: 0.22086501121520996\n",
      "iteration 23530: loss: 0.22086434066295624\n",
      "iteration 23531: loss: 0.22086374461650848\n",
      "iteration 23532: loss: 0.2208631932735443\n",
      "iteration 23533: loss: 0.22086258232593536\n",
      "iteration 23534: loss: 0.2208619862794876\n",
      "iteration 23535: loss: 0.22086136043071747\n",
      "iteration 23536: loss: 0.22086068987846375\n",
      "iteration 23537: loss: 0.22086012363433838\n",
      "iteration 23538: loss: 0.22085952758789062\n",
      "iteration 23539: loss: 0.2208588868379593\n",
      "iteration 23540: loss: 0.22085826098918915\n",
      "iteration 23541: loss: 0.22085770964622498\n",
      "iteration 23542: loss: 0.22085709869861603\n",
      "iteration 23543: loss: 0.22085650265216827\n",
      "iteration 23544: loss: 0.22085583209991455\n",
      "iteration 23545: loss: 0.2208552360534668\n",
      "iteration 23546: loss: 0.22085456550121307\n",
      "iteration 23547: loss: 0.2208539992570877\n",
      "iteration 23548: loss: 0.22085341811180115\n",
      "iteration 23549: loss: 0.22085273265838623\n",
      "iteration 23550: loss: 0.22085222601890564\n",
      "iteration 23551: loss: 0.2208515703678131\n",
      "iteration 23552: loss: 0.22085094451904297\n",
      "iteration 23553: loss: 0.2208503782749176\n",
      "iteration 23554: loss: 0.22084975242614746\n",
      "iteration 23555: loss: 0.22084912657737732\n",
      "iteration 23556: loss: 0.22084851562976837\n",
      "iteration 23557: loss: 0.22084791958332062\n",
      "iteration 23558: loss: 0.22084732353687286\n",
      "iteration 23559: loss: 0.22084668278694153\n",
      "iteration 23560: loss: 0.22084608674049377\n",
      "iteration 23561: loss: 0.22084541618824005\n",
      "iteration 23562: loss: 0.2208448350429535\n",
      "iteration 23563: loss: 0.22084419429302216\n",
      "iteration 23564: loss: 0.2208436280488968\n",
      "iteration 23565: loss: 0.22084304690361023\n",
      "iteration 23566: loss: 0.2208424061536789\n",
      "iteration 23567: loss: 0.22084183990955353\n",
      "iteration 23568: loss: 0.22084113955497742\n",
      "iteration 23569: loss: 0.22084061801433563\n",
      "iteration 23570: loss: 0.22084000706672668\n",
      "iteration 23571: loss: 0.22083938121795654\n",
      "iteration 23572: loss: 0.2208387553691864\n",
      "iteration 23573: loss: 0.22083815932273865\n",
      "iteration 23574: loss: 0.2208375483751297\n",
      "iteration 23575: loss: 0.22083690762519836\n",
      "iteration 23576: loss: 0.22083628177642822\n",
      "iteration 23577: loss: 0.22083568572998047\n",
      "iteration 23578: loss: 0.2208351194858551\n",
      "iteration 23579: loss: 0.220834419131279\n",
      "iteration 23580: loss: 0.22083386778831482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 23581: loss: 0.22083325684070587\n",
      "iteration 23582: loss: 0.22083263099193573\n",
      "iteration 23583: loss: 0.22083203494548798\n",
      "iteration 23584: loss: 0.2208314836025238\n",
      "iteration 23585: loss: 0.22083082795143127\n",
      "iteration 23586: loss: 0.22083024680614471\n",
      "iteration 23587: loss: 0.22082960605621338\n",
      "iteration 23588: loss: 0.220829039812088\n",
      "iteration 23589: loss: 0.22082838416099548\n",
      "iteration 23590: loss: 0.22082778811454773\n",
      "iteration 23591: loss: 0.22082719206809998\n",
      "iteration 23592: loss: 0.22082659602165222\n",
      "iteration 23593: loss: 0.2208259552717209\n",
      "iteration 23594: loss: 0.22082535922527313\n",
      "iteration 23595: loss: 0.22082476317882538\n",
      "iteration 23596: loss: 0.2208241969347\n",
      "iteration 23597: loss: 0.2208235263824463\n",
      "iteration 23598: loss: 0.22082296013832092\n",
      "iteration 23599: loss: 0.22082233428955078\n",
      "iteration 23600: loss: 0.22082170844078064\n",
      "iteration 23601: loss: 0.22082111239433289\n",
      "iteration 23602: loss: 0.22082054615020752\n",
      "iteration 23603: loss: 0.2208198755979538\n",
      "iteration 23604: loss: 0.22081932425498962\n",
      "iteration 23605: loss: 0.2208186686038971\n",
      "iteration 23606: loss: 0.22081811726093292\n",
      "iteration 23607: loss: 0.2208174765110016\n",
      "iteration 23608: loss: 0.22081685066223145\n",
      "iteration 23609: loss: 0.2208162546157837\n",
      "iteration 23610: loss: 0.22081562876701355\n",
      "iteration 23611: loss: 0.22081509232521057\n",
      "iteration 23612: loss: 0.22081446647644043\n",
      "iteration 23613: loss: 0.2208138257265091\n",
      "iteration 23614: loss: 0.22081322968006134\n",
      "iteration 23615: loss: 0.22081267833709717\n",
      "iteration 23616: loss: 0.22081203758716583\n",
      "iteration 23617: loss: 0.2208114117383957\n",
      "iteration 23618: loss: 0.22081080079078674\n",
      "iteration 23619: loss: 0.22081021964550018\n",
      "iteration 23620: loss: 0.22080962359905243\n",
      "iteration 23621: loss: 0.2208089828491211\n",
      "iteration 23622: loss: 0.22080841660499573\n",
      "iteration 23623: loss: 0.2208077609539032\n",
      "iteration 23624: loss: 0.22080722451210022\n",
      "iteration 23625: loss: 0.22080659866333008\n",
      "iteration 23626: loss: 0.22080595791339874\n",
      "iteration 23627: loss: 0.220805361866951\n",
      "iteration 23628: loss: 0.22080478072166443\n",
      "iteration 23629: loss: 0.22080418467521667\n",
      "iteration 23630: loss: 0.22080349922180176\n",
      "iteration 23631: loss: 0.2208029329776764\n",
      "iteration 23632: loss: 0.22080230712890625\n",
      "iteration 23633: loss: 0.22080174088478088\n",
      "iteration 23634: loss: 0.22080111503601074\n",
      "iteration 23635: loss: 0.22080059349536896\n",
      "iteration 23636: loss: 0.22079995274543762\n",
      "iteration 23637: loss: 0.22079935669898987\n",
      "iteration 23638: loss: 0.22079868614673615\n",
      "iteration 23639: loss: 0.2207980901002884\n",
      "iteration 23640: loss: 0.22079753875732422\n",
      "iteration 23641: loss: 0.22079691290855408\n",
      "iteration 23642: loss: 0.22079630196094513\n",
      "iteration 23643: loss: 0.22079572081565857\n",
      "iteration 23644: loss: 0.22079510986804962\n",
      "iteration 23645: loss: 0.2207944691181183\n",
      "iteration 23646: loss: 0.22079384326934814\n",
      "iteration 23647: loss: 0.22079329192638397\n",
      "iteration 23648: loss: 0.22079269587993622\n",
      "iteration 23649: loss: 0.22079208493232727\n",
      "iteration 23650: loss: 0.22079148888587952\n",
      "iteration 23651: loss: 0.22079090774059296\n",
      "iteration 23652: loss: 0.22079023718833923\n",
      "iteration 23653: loss: 0.22078967094421387\n",
      "iteration 23654: loss: 0.22078904509544373\n",
      "iteration 23655: loss: 0.22078844904899597\n",
      "iteration 23656: loss: 0.22078783810138702\n",
      "iteration 23657: loss: 0.22078725695610046\n",
      "iteration 23658: loss: 0.2207866609096527\n",
      "iteration 23659: loss: 0.22078606486320496\n",
      "iteration 23660: loss: 0.2207854688167572\n",
      "iteration 23661: loss: 0.22078482806682587\n",
      "iteration 23662: loss: 0.2207842618227005\n",
      "iteration 23663: loss: 0.22078363597393036\n",
      "iteration 23664: loss: 0.2207830846309662\n",
      "iteration 23665: loss: 0.22078247368335724\n",
      "iteration 23666: loss: 0.22078187763690948\n",
      "iteration 23667: loss: 0.22078123688697815\n",
      "iteration 23668: loss: 0.2207806408405304\n",
      "iteration 23669: loss: 0.22078004479408264\n",
      "iteration 23670: loss: 0.22077946364879608\n",
      "iteration 23671: loss: 0.22077885270118713\n",
      "iteration 23672: loss: 0.22077827155590057\n",
      "iteration 23673: loss: 0.22077766060829163\n",
      "iteration 23674: loss: 0.2207770049571991\n",
      "iteration 23675: loss: 0.22077640891075134\n",
      "iteration 23676: loss: 0.2207757979631424\n",
      "iteration 23677: loss: 0.22077521681785583\n",
      "iteration 23678: loss: 0.2207745760679245\n",
      "iteration 23679: loss: 0.22077400982379913\n",
      "iteration 23680: loss: 0.220773383975029\n",
      "iteration 23681: loss: 0.22077281773090363\n",
      "iteration 23682: loss: 0.22077222168445587\n",
      "iteration 23683: loss: 0.2207716405391693\n",
      "iteration 23684: loss: 0.22077102959156036\n",
      "iteration 23685: loss: 0.22077038884162903\n",
      "iteration 23686: loss: 0.22076985239982605\n",
      "iteration 23687: loss: 0.22076921164989471\n",
      "iteration 23688: loss: 0.22076861560344696\n",
      "iteration 23689: loss: 0.2207680493593216\n",
      "iteration 23690: loss: 0.22076746821403503\n",
      "iteration 23691: loss: 0.2207668125629425\n",
      "iteration 23692: loss: 0.22076626121997833\n",
      "iteration 23693: loss: 0.2207656353712082\n",
      "iteration 23694: loss: 0.22076502442359924\n",
      "iteration 23695: loss: 0.2207643985748291\n",
      "iteration 23696: loss: 0.22076383233070374\n",
      "iteration 23697: loss: 0.22076325118541718\n",
      "iteration 23698: loss: 0.22076258063316345\n",
      "iteration 23699: loss: 0.22076205909252167\n",
      "iteration 23700: loss: 0.22076138854026794\n",
      "iteration 23701: loss: 0.22076085209846497\n",
      "iteration 23702: loss: 0.2207602560520172\n",
      "iteration 23703: loss: 0.2207595854997635\n",
      "iteration 23704: loss: 0.22075898945331573\n",
      "iteration 23705: loss: 0.22075840830802917\n",
      "iteration 23706: loss: 0.2207578718662262\n",
      "iteration 23707: loss: 0.22075724601745605\n",
      "iteration 23708: loss: 0.22075657546520233\n",
      "iteration 23709: loss: 0.22075602412223816\n",
      "iteration 23710: loss: 0.2207554280757904\n",
      "iteration 23711: loss: 0.22075481712818146\n",
      "iteration 23712: loss: 0.22075419127941132\n",
      "iteration 23713: loss: 0.22075366973876953\n",
      "iteration 23714: loss: 0.2207530438899994\n",
      "iteration 23715: loss: 0.22075243294239044\n",
      "iteration 23716: loss: 0.2207518368959427\n",
      "iteration 23717: loss: 0.22075125575065613\n",
      "iteration 23718: loss: 0.2207506150007248\n",
      "iteration 23719: loss: 0.22075004875659943\n",
      "iteration 23720: loss: 0.2207494080066681\n",
      "iteration 23721: loss: 0.22074885666370392\n",
      "iteration 23722: loss: 0.22074823081493378\n",
      "iteration 23723: loss: 0.22074763476848602\n",
      "iteration 23724: loss: 0.22074703872203827\n",
      "iteration 23725: loss: 0.22074644267559052\n",
      "iteration 23726: loss: 0.22074584662914276\n",
      "iteration 23727: loss: 0.2207452803850174\n",
      "iteration 23728: loss: 0.22074469923973083\n",
      "iteration 23729: loss: 0.2207440584897995\n",
      "iteration 23730: loss: 0.22074346244335175\n",
      "iteration 23731: loss: 0.2207428216934204\n",
      "iteration 23732: loss: 0.22074227035045624\n",
      "iteration 23733: loss: 0.2207416594028473\n",
      "iteration 23734: loss: 0.22074107825756073\n",
      "iteration 23735: loss: 0.22074051201343536\n",
      "iteration 23736: loss: 0.22073988616466522\n",
      "iteration 23737: loss: 0.22073931992053986\n",
      "iteration 23738: loss: 0.2207387238740921\n",
      "iteration 23739: loss: 0.22073812782764435\n",
      "iteration 23740: loss: 0.2207375019788742\n",
      "iteration 23741: loss: 0.22073690593242645\n",
      "iteration 23742: loss: 0.2207362949848175\n",
      "iteration 23743: loss: 0.22073571383953094\n",
      "iteration 23744: loss: 0.22073516249656677\n",
      "iteration 23745: loss: 0.22073450684547424\n",
      "iteration 23746: loss: 0.22073395550251007\n",
      "iteration 23747: loss: 0.22073332965373993\n",
      "iteration 23748: loss: 0.22073276340961456\n",
      "iteration 23749: loss: 0.2207321673631668\n",
      "iteration 23750: loss: 0.22073152661323547\n",
      "iteration 23751: loss: 0.22073090076446533\n",
      "iteration 23752: loss: 0.22073045372962952\n",
      "iteration 23753: loss: 0.2207297831773758\n",
      "iteration 23754: loss: 0.22072920203208923\n",
      "iteration 23755: loss: 0.2207285463809967\n",
      "iteration 23756: loss: 0.22072796523571014\n",
      "iteration 23757: loss: 0.22072741389274597\n",
      "iteration 23758: loss: 0.22072680294513702\n",
      "iteration 23759: loss: 0.22072620689868927\n",
      "iteration 23760: loss: 0.22072556614875793\n",
      "iteration 23761: loss: 0.22072498500347137\n",
      "iteration 23762: loss: 0.22072438895702362\n",
      "iteration 23763: loss: 0.22072379291057587\n",
      "iteration 23764: loss: 0.2207232415676117\n",
      "iteration 23765: loss: 0.22072267532348633\n",
      "iteration 23766: loss: 0.2207220494747162\n",
      "iteration 23767: loss: 0.22072145342826843\n",
      "iteration 23768: loss: 0.22072085738182068\n",
      "iteration 23769: loss: 0.22072021663188934\n",
      "iteration 23770: loss: 0.22071966528892517\n",
      "iteration 23771: loss: 0.2207191288471222\n",
      "iteration 23772: loss: 0.22071847319602966\n",
      "iteration 23773: loss: 0.2207179069519043\n",
      "iteration 23774: loss: 0.22071723639965057\n",
      "iteration 23775: loss: 0.2207166850566864\n",
      "iteration 23776: loss: 0.22071611881256104\n",
      "iteration 23777: loss: 0.2207154780626297\n",
      "iteration 23778: loss: 0.22071489691734314\n",
      "iteration 23779: loss: 0.22071436047554016\n",
      "iteration 23780: loss: 0.2207137644290924\n",
      "iteration 23781: loss: 0.22071313858032227\n",
      "iteration 23782: loss: 0.22071251273155212\n",
      "iteration 23783: loss: 0.22071197628974915\n",
      "iteration 23784: loss: 0.22071130573749542\n",
      "iteration 23785: loss: 0.22071078419685364\n",
      "iteration 23786: loss: 0.22071018815040588\n",
      "iteration 23787: loss: 0.22070951759815216\n",
      "iteration 23788: loss: 0.220708966255188\n",
      "iteration 23789: loss: 0.22070840001106262\n",
      "iteration 23790: loss: 0.22070784866809845\n",
      "iteration 23791: loss: 0.2207072526216507\n",
      "iteration 23792: loss: 0.22070665657520294\n",
      "iteration 23793: loss: 0.22070595622062683\n",
      "iteration 23794: loss: 0.22070543467998505\n",
      "iteration 23795: loss: 0.2207048386335373\n",
      "iteration 23796: loss: 0.22070427238941193\n",
      "iteration 23797: loss: 0.2207036316394806\n",
      "iteration 23798: loss: 0.22070308029651642\n",
      "iteration 23799: loss: 0.22070245444774628\n",
      "iteration 23800: loss: 0.22070196270942688\n",
      "iteration 23801: loss: 0.22070129215717316\n",
      "iteration 23802: loss: 0.2207007110118866\n",
      "iteration 23803: loss: 0.22070007026195526\n",
      "iteration 23804: loss: 0.2206994742155075\n",
      "iteration 23805: loss: 0.22069890797138214\n",
      "iteration 23806: loss: 0.22069832682609558\n",
      "iteration 23807: loss: 0.22069773077964783\n",
      "iteration 23808: loss: 0.22069713473320007\n",
      "iteration 23809: loss: 0.2206965684890747\n",
      "iteration 23810: loss: 0.22069592773914337\n",
      "iteration 23811: loss: 0.2206953763961792\n",
      "iteration 23812: loss: 0.22069475054740906\n",
      "iteration 23813: loss: 0.22069421410560608\n",
      "iteration 23814: loss: 0.22069363296031952\n",
      "iteration 23815: loss: 0.22069299221038818\n",
      "iteration 23816: loss: 0.2206924706697464\n",
      "iteration 23817: loss: 0.22069180011749268\n",
      "iteration 23818: loss: 0.22069129347801208\n",
      "iteration 23819: loss: 0.22069065272808075\n",
      "iteration 23820: loss: 0.22069008648395538\n",
      "iteration 23821: loss: 0.22068944573402405\n",
      "iteration 23822: loss: 0.2206888198852539\n",
      "iteration 23823: loss: 0.22068825364112854\n",
      "iteration 23824: loss: 0.22068770229816437\n",
      "iteration 23825: loss: 0.2206871509552002\n",
      "iteration 23826: loss: 0.22068652510643005\n",
      "iteration 23827: loss: 0.2206859141588211\n",
      "iteration 23828: loss: 0.22068533301353455\n",
      "iteration 23829: loss: 0.22068481147289276\n",
      "iteration 23830: loss: 0.22068414092063904\n",
      "iteration 23831: loss: 0.22068354487419128\n",
      "iteration 23832: loss: 0.22068294882774353\n",
      "iteration 23833: loss: 0.22068241238594055\n",
      "iteration 23834: loss: 0.22068186104297638\n",
      "iteration 23835: loss: 0.22068123519420624\n",
      "iteration 23836: loss: 0.22068066895008087\n",
      "iteration 23837: loss: 0.22068004310131073\n",
      "iteration 23838: loss: 0.22067944705486298\n",
      "iteration 23839: loss: 0.2206788808107376\n",
      "iteration 23840: loss: 0.22067828476428986\n",
      "iteration 23841: loss: 0.2206776887178421\n",
      "iteration 23842: loss: 0.22067709267139435\n",
      "iteration 23843: loss: 0.2206765115261078\n",
      "iteration 23844: loss: 0.22067590057849884\n",
      "iteration 23845: loss: 0.22067537903785706\n",
      "iteration 23846: loss: 0.22067475318908691\n",
      "iteration 23847: loss: 0.22067415714263916\n",
      "iteration 23848: loss: 0.2206735610961914\n",
      "iteration 23849: loss: 0.22067300975322723\n",
      "iteration 23850: loss: 0.2206723690032959\n",
      "iteration 23851: loss: 0.22067177295684814\n",
      "iteration 23852: loss: 0.22067125141620636\n",
      "iteration 23853: loss: 0.2206706702709198\n",
      "iteration 23854: loss: 0.22067007422447205\n",
      "iteration 23855: loss: 0.2206694632768631\n",
      "iteration 23856: loss: 0.22066888213157654\n",
      "iteration 23857: loss: 0.2206682711839676\n",
      "iteration 23858: loss: 0.22066769003868103\n",
      "iteration 23859: loss: 0.22066709399223328\n",
      "iteration 23860: loss: 0.22066649794578552\n",
      "iteration 23861: loss: 0.22066588699817657\n",
      "iteration 23862: loss: 0.2206653654575348\n",
      "iteration 23863: loss: 0.22066476941108704\n",
      "iteration 23864: loss: 0.22066418826580048\n",
      "iteration 23865: loss: 0.22066357731819153\n",
      "iteration 23866: loss: 0.22066298127174377\n",
      "iteration 23867: loss: 0.2206624448299408\n",
      "iteration 23868: loss: 0.22066178917884827\n",
      "iteration 23869: loss: 0.2206612378358841\n",
      "iteration 23870: loss: 0.22066065669059753\n",
      "iteration 23871: loss: 0.22066006064414978\n",
      "iteration 23872: loss: 0.22065949440002441\n",
      "iteration 23873: loss: 0.22065886855125427\n",
      "iteration 23874: loss: 0.22065827250480652\n",
      "iteration 23875: loss: 0.22065773606300354\n",
      "iteration 23876: loss: 0.2206571102142334\n",
      "iteration 23877: loss: 0.22065654397010803\n",
      "iteration 23878: loss: 0.2206558883190155\n",
      "iteration 23879: loss: 0.2206553965806961\n",
      "iteration 23880: loss: 0.22065477073192596\n",
      "iteration 23881: loss: 0.2206542044878006\n",
      "iteration 23882: loss: 0.22065360844135284\n",
      "iteration 23883: loss: 0.2206530123949051\n",
      "iteration 23884: loss: 0.22065241634845734\n",
      "iteration 23885: loss: 0.22065186500549316\n",
      "iteration 23886: loss: 0.22065123915672302\n",
      "iteration 23887: loss: 0.22065067291259766\n",
      "iteration 23888: loss: 0.2206501066684723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 23889: loss: 0.22064948081970215\n",
      "iteration 23890: loss: 0.22064891457557678\n",
      "iteration 23891: loss: 0.2206483632326126\n",
      "iteration 23892: loss: 0.22064772248268127\n",
      "iteration 23893: loss: 0.2206471711397171\n",
      "iteration 23894: loss: 0.22064661979675293\n",
      "iteration 23895: loss: 0.22064602375030518\n",
      "iteration 23896: loss: 0.22064538300037384\n",
      "iteration 23897: loss: 0.22064490616321564\n",
      "iteration 23898: loss: 0.2206442803144455\n",
      "iteration 23899: loss: 0.22064366936683655\n",
      "iteration 23900: loss: 0.2206430435180664\n",
      "iteration 23901: loss: 0.22064249217510223\n",
      "iteration 23902: loss: 0.22064194083213806\n",
      "iteration 23903: loss: 0.2206413447856903\n",
      "iteration 23904: loss: 0.22064073383808136\n",
      "iteration 23905: loss: 0.2206401526927948\n",
      "iteration 23906: loss: 0.22063955664634705\n",
      "iteration 23907: loss: 0.22063903510570526\n",
      "iteration 23908: loss: 0.2206384688615799\n",
      "iteration 23909: loss: 0.22063779830932617\n",
      "iteration 23910: loss: 0.22063729166984558\n",
      "iteration 23911: loss: 0.22063663601875305\n",
      "iteration 23912: loss: 0.22063608467578888\n",
      "iteration 23913: loss: 0.22063545882701874\n",
      "iteration 23914: loss: 0.22063490748405457\n",
      "iteration 23915: loss: 0.2206343710422516\n",
      "iteration 23916: loss: 0.22063377499580383\n",
      "iteration 23917: loss: 0.22063323855400085\n",
      "iteration 23918: loss: 0.22063255310058594\n",
      "iteration 23919: loss: 0.22063203155994415\n",
      "iteration 23920: loss: 0.2206314504146576\n",
      "iteration 23921: loss: 0.22063091397285461\n",
      "iteration 23922: loss: 0.2206302434206009\n",
      "iteration 23923: loss: 0.22062969207763672\n",
      "iteration 23924: loss: 0.22062906622886658\n",
      "iteration 23925: loss: 0.2206285297870636\n",
      "iteration 23926: loss: 0.22062793374061584\n",
      "iteration 23927: loss: 0.22062735259532928\n",
      "iteration 23928: loss: 0.22062674164772034\n",
      "iteration 23929: loss: 0.22062623500823975\n",
      "iteration 23930: loss: 0.2206255942583084\n",
      "iteration 23931: loss: 0.22062507271766663\n",
      "iteration 23932: loss: 0.22062444686889648\n",
      "iteration 23933: loss: 0.22062385082244873\n",
      "iteration 23934: loss: 0.22062328457832336\n",
      "iteration 23935: loss: 0.2206227034330368\n",
      "iteration 23936: loss: 0.22062209248542786\n",
      "iteration 23937: loss: 0.22062154114246368\n",
      "iteration 23938: loss: 0.22062095999717712\n",
      "iteration 23939: loss: 0.22062034904956818\n",
      "iteration 23940: loss: 0.22061976790428162\n",
      "iteration 23941: loss: 0.22061920166015625\n",
      "iteration 23942: loss: 0.22061863541603088\n",
      "iteration 23943: loss: 0.2206180840730667\n",
      "iteration 23944: loss: 0.22061745822429657\n",
      "iteration 23945: loss: 0.22061686217784882\n",
      "iteration 23946: loss: 0.22061631083488464\n",
      "iteration 23947: loss: 0.2206157147884369\n",
      "iteration 23948: loss: 0.2206151783466339\n",
      "iteration 23949: loss: 0.22061452269554138\n",
      "iteration 23950: loss: 0.22061392664909363\n",
      "iteration 23951: loss: 0.22061340510845184\n",
      "iteration 23952: loss: 0.2206127941608429\n",
      "iteration 23953: loss: 0.22061225771903992\n",
      "iteration 23954: loss: 0.22061166167259216\n",
      "iteration 23955: loss: 0.2206110954284668\n",
      "iteration 23956: loss: 0.22061046957969666\n",
      "iteration 23957: loss: 0.2206099033355713\n",
      "iteration 23958: loss: 0.22060930728912354\n",
      "iteration 23959: loss: 0.22060875594615936\n",
      "iteration 23960: loss: 0.2206082046031952\n",
      "iteration 23961: loss: 0.22060760855674744\n",
      "iteration 23962: loss: 0.22060701251029968\n",
      "iteration 23963: loss: 0.22060637176036835\n",
      "iteration 23964: loss: 0.22060585021972656\n",
      "iteration 23965: loss: 0.2206052988767624\n",
      "iteration 23966: loss: 0.22060468792915344\n",
      "iteration 23967: loss: 0.22060415148735046\n",
      "iteration 23968: loss: 0.22060351073741913\n",
      "iteration 23969: loss: 0.22060294449329376\n",
      "iteration 23970: loss: 0.2206023931503296\n",
      "iteration 23971: loss: 0.22060184180736542\n",
      "iteration 23972: loss: 0.22060123085975647\n",
      "iteration 23973: loss: 0.22060072422027588\n",
      "iteration 23974: loss: 0.22060008347034454\n",
      "iteration 23975: loss: 0.22059950232505798\n",
      "iteration 23976: loss: 0.22059890627861023\n",
      "iteration 23977: loss: 0.22059834003448486\n",
      "iteration 23978: loss: 0.22059781849384308\n",
      "iteration 23979: loss: 0.22059717774391174\n",
      "iteration 23980: loss: 0.22059659659862518\n",
      "iteration 23981: loss: 0.2205960750579834\n",
      "iteration 23982: loss: 0.22059544920921326\n",
      "iteration 23983: loss: 0.2205948829650879\n",
      "iteration 23984: loss: 0.22059431672096252\n",
      "iteration 23985: loss: 0.22059373557567596\n",
      "iteration 23986: loss: 0.22059312462806702\n",
      "iteration 23987: loss: 0.22059257328510284\n",
      "iteration 23988: loss: 0.22059199213981628\n",
      "iteration 23989: loss: 0.22059142589569092\n",
      "iteration 23990: loss: 0.22059087455272675\n",
      "iteration 23991: loss: 0.2205902338027954\n",
      "iteration 23992: loss: 0.22058972716331482\n",
      "iteration 23993: loss: 0.22058908641338348\n",
      "iteration 23994: loss: 0.22058852016925812\n",
      "iteration 23995: loss: 0.22058792412281036\n",
      "iteration 23996: loss: 0.22058740258216858\n",
      "iteration 23997: loss: 0.22058677673339844\n",
      "iteration 23998: loss: 0.22058622539043427\n",
      "iteration 23999: loss: 0.2205856591463089\n",
      "iteration 24000: loss: 0.22058510780334473\n",
      "iteration 24001: loss: 0.22058454155921936\n",
      "iteration 24002: loss: 0.22058391571044922\n",
      "iteration 24003: loss: 0.22058331966400146\n",
      "iteration 24004: loss: 0.22058279812335968\n",
      "iteration 24005: loss: 0.22058221697807312\n",
      "iteration 24006: loss: 0.22058160603046417\n",
      "iteration 24007: loss: 0.2205810248851776\n",
      "iteration 24008: loss: 0.22058048844337463\n",
      "iteration 24009: loss: 0.2205798625946045\n",
      "iteration 24010: loss: 0.2205793410539627\n",
      "iteration 24011: loss: 0.22057871520519257\n",
      "iteration 24012: loss: 0.2205781638622284\n",
      "iteration 24013: loss: 0.22057759761810303\n",
      "iteration 24014: loss: 0.22057703137397766\n",
      "iteration 24015: loss: 0.2205764502286911\n",
      "iteration 24016: loss: 0.22057583928108215\n",
      "iteration 24017: loss: 0.22057533264160156\n",
      "iteration 24018: loss: 0.2205747365951538\n",
      "iteration 24019: loss: 0.22057417035102844\n",
      "iteration 24020: loss: 0.2205735743045807\n",
      "iteration 24021: loss: 0.22057294845581055\n",
      "iteration 24022: loss: 0.2205723524093628\n",
      "iteration 24023: loss: 0.2205718457698822\n",
      "iteration 24024: loss: 0.22057127952575684\n",
      "iteration 24025: loss: 0.2205706536769867\n",
      "iteration 24026: loss: 0.22057011723518372\n",
      "iteration 24027: loss: 0.22056952118873596\n",
      "iteration 24028: loss: 0.2205689698457718\n",
      "iteration 24029: loss: 0.22056838870048523\n",
      "iteration 24030: loss: 0.22056779265403748\n",
      "iteration 24031: loss: 0.22056730091571808\n",
      "iteration 24032: loss: 0.22056667506694794\n",
      "iteration 24033: loss: 0.22056607902050018\n",
      "iteration 24034: loss: 0.22056546807289124\n",
      "iteration 24035: loss: 0.22056493163108826\n",
      "iteration 24036: loss: 0.22056439518928528\n",
      "iteration 24037: loss: 0.22056376934051514\n",
      "iteration 24038: loss: 0.22056321799755096\n",
      "iteration 24039: loss: 0.2205626517534256\n",
      "iteration 24040: loss: 0.22056207060813904\n",
      "iteration 24041: loss: 0.22056147456169128\n",
      "iteration 24042: loss: 0.2205609530210495\n",
      "iteration 24043: loss: 0.22056035697460175\n",
      "iteration 24044: loss: 0.22055983543395996\n",
      "iteration 24045: loss: 0.22055920958518982\n",
      "iteration 24046: loss: 0.22055864334106445\n",
      "iteration 24047: loss: 0.2205580472946167\n",
      "iteration 24048: loss: 0.22055748105049133\n",
      "iteration 24049: loss: 0.22055697441101074\n",
      "iteration 24050: loss: 0.2205563485622406\n",
      "iteration 24051: loss: 0.2205558568239212\n",
      "iteration 24052: loss: 0.2205551564693451\n",
      "iteration 24053: loss: 0.22055462002754211\n",
      "iteration 24054: loss: 0.22055402398109436\n",
      "iteration 24055: loss: 0.22055348753929138\n",
      "iteration 24056: loss: 0.22055292129516602\n",
      "iteration 24057: loss: 0.22055239975452423\n",
      "iteration 24058: loss: 0.22055180370807648\n",
      "iteration 24059: loss: 0.22055120766162872\n",
      "iteration 24060: loss: 0.22055065631866455\n",
      "iteration 24061: loss: 0.2205500602722168\n",
      "iteration 24062: loss: 0.22054943442344666\n",
      "iteration 24063: loss: 0.22054891288280487\n",
      "iteration 24064: loss: 0.2205483466386795\n",
      "iteration 24065: loss: 0.22054775059223175\n",
      "iteration 24066: loss: 0.220547154545784\n",
      "iteration 24067: loss: 0.2205466330051422\n",
      "iteration 24068: loss: 0.22054605185985565\n",
      "iteration 24069: loss: 0.22054550051689148\n",
      "iteration 24070: loss: 0.22054484486579895\n",
      "iteration 24071: loss: 0.22054438292980194\n",
      "iteration 24072: loss: 0.22054381668567657\n",
      "iteration 24073: loss: 0.22054317593574524\n",
      "iteration 24074: loss: 0.22054262459278107\n",
      "iteration 24075: loss: 0.2205420434474945\n",
      "iteration 24076: loss: 0.22054150700569153\n",
      "iteration 24077: loss: 0.22054095566272736\n",
      "iteration 24078: loss: 0.22054032981395721\n",
      "iteration 24079: loss: 0.22053977847099304\n",
      "iteration 24080: loss: 0.2205391824245453\n",
      "iteration 24081: loss: 0.22053858637809753\n",
      "iteration 24082: loss: 0.22053804993629456\n",
      "iteration 24083: loss: 0.2205374538898468\n",
      "iteration 24084: loss: 0.22053690254688263\n",
      "iteration 24085: loss: 0.22053638100624084\n",
      "iteration 24086: loss: 0.22053582966327667\n",
      "iteration 24087: loss: 0.22053523361682892\n",
      "iteration 24088: loss: 0.22053465247154236\n",
      "iteration 24089: loss: 0.220534086227417\n",
      "iteration 24090: loss: 0.22053346037864685\n",
      "iteration 24091: loss: 0.2205328643321991\n",
      "iteration 24092: loss: 0.2205323725938797\n",
      "iteration 24093: loss: 0.22053177654743195\n",
      "iteration 24094: loss: 0.22053122520446777\n",
      "iteration 24095: loss: 0.2205306738615036\n",
      "iteration 24096: loss: 0.22053007781505585\n",
      "iteration 24097: loss: 0.22052951157093048\n",
      "iteration 24098: loss: 0.22052893042564392\n",
      "iteration 24099: loss: 0.22052839398384094\n",
      "iteration 24100: loss: 0.22052781283855438\n",
      "iteration 24101: loss: 0.2205272614955902\n",
      "iteration 24102: loss: 0.22052666544914246\n",
      "iteration 24103: loss: 0.2205260694026947\n",
      "iteration 24104: loss: 0.2205255776643753\n",
      "iteration 24105: loss: 0.22052495181560516\n",
      "iteration 24106: loss: 0.2205243557691574\n",
      "iteration 24107: loss: 0.22052383422851562\n",
      "iteration 24108: loss: 0.22052326798439026\n",
      "iteration 24109: loss: 0.2205226868391037\n",
      "iteration 24110: loss: 0.22052212059497833\n",
      "iteration 24111: loss: 0.22052159905433655\n",
      "iteration 24112: loss: 0.2205209732055664\n",
      "iteration 24113: loss: 0.22052045166492462\n",
      "iteration 24114: loss: 0.22051985561847687\n",
      "iteration 24115: loss: 0.2205192744731903\n",
      "iteration 24116: loss: 0.22051873803138733\n",
      "iteration 24117: loss: 0.22051814198493958\n",
      "iteration 24118: loss: 0.2205175906419754\n",
      "iteration 24119: loss: 0.22051700949668884\n",
      "iteration 24120: loss: 0.22051647305488586\n",
      "iteration 24121: loss: 0.2205158770084381\n",
      "iteration 24122: loss: 0.22051532566547394\n",
      "iteration 24123: loss: 0.22051472961902618\n",
      "iteration 24124: loss: 0.2205142080783844\n",
      "iteration 24125: loss: 0.22051365673542023\n",
      "iteration 24126: loss: 0.22051306068897247\n",
      "iteration 24127: loss: 0.22051246464252472\n",
      "iteration 24128: loss: 0.22051183879375458\n",
      "iteration 24129: loss: 0.22051136195659637\n",
      "iteration 24130: loss: 0.22051076591014862\n",
      "iteration 24131: loss: 0.22051024436950684\n",
      "iteration 24132: loss: 0.22050967812538147\n",
      "iteration 24133: loss: 0.22050908207893372\n",
      "iteration 24134: loss: 0.22050853073596954\n",
      "iteration 24135: loss: 0.22050797939300537\n",
      "iteration 24136: loss: 0.22050735354423523\n",
      "iteration 24137: loss: 0.22050681710243225\n",
      "iteration 24138: loss: 0.22050628066062927\n",
      "iteration 24139: loss: 0.2205057442188263\n",
      "iteration 24140: loss: 0.22050511837005615\n",
      "iteration 24141: loss: 0.2205045521259308\n",
      "iteration 24142: loss: 0.2205040007829666\n",
      "iteration 24143: loss: 0.22050340473651886\n",
      "iteration 24144: loss: 0.22050288319587708\n",
      "iteration 24145: loss: 0.22050228714942932\n",
      "iteration 24146: loss: 0.22050169110298157\n",
      "iteration 24147: loss: 0.2205011546611786\n",
      "iteration 24148: loss: 0.22050058841705322\n",
      "iteration 24149: loss: 0.22050003707408905\n",
      "iteration 24150: loss: 0.22049947082996368\n",
      "iteration 24151: loss: 0.2204989194869995\n",
      "iteration 24152: loss: 0.22049829363822937\n",
      "iteration 24153: loss: 0.220497727394104\n",
      "iteration 24154: loss: 0.22049717605113983\n",
      "iteration 24155: loss: 0.22049660980701447\n",
      "iteration 24156: loss: 0.2204960286617279\n",
      "iteration 24157: loss: 0.2204955369234085\n",
      "iteration 24158: loss: 0.22049491107463837\n",
      "iteration 24159: loss: 0.22049438953399658\n",
      "iteration 24160: loss: 0.22049382328987122\n",
      "iteration 24161: loss: 0.22049324214458466\n",
      "iteration 24162: loss: 0.2204926759004593\n",
      "iteration 24163: loss: 0.22049207985401154\n",
      "iteration 24164: loss: 0.22049157321453094\n",
      "iteration 24165: loss: 0.22049102187156677\n",
      "iteration 24166: loss: 0.22049041092395782\n",
      "iteration 24167: loss: 0.22048985958099365\n",
      "iteration 24168: loss: 0.22048930823802948\n",
      "iteration 24169: loss: 0.2204887419939041\n",
      "iteration 24170: loss: 0.22048816084861755\n",
      "iteration 24171: loss: 0.22048763930797577\n",
      "iteration 24172: loss: 0.2204870730638504\n",
      "iteration 24173: loss: 0.22048649191856384\n",
      "iteration 24174: loss: 0.22048592567443848\n",
      "iteration 24175: loss: 0.2204853743314743\n",
      "iteration 24176: loss: 0.22048477828502655\n",
      "iteration 24177: loss: 0.22048421204090118\n",
      "iteration 24178: loss: 0.22048363089561462\n",
      "iteration 24179: loss: 0.22048306465148926\n",
      "iteration 24180: loss: 0.22048251330852509\n",
      "iteration 24181: loss: 0.2204819619655609\n",
      "iteration 24182: loss: 0.22048144042491913\n",
      "iteration 24183: loss: 0.22048087418079376\n",
      "iteration 24184: loss: 0.220480278134346\n",
      "iteration 24185: loss: 0.22047972679138184\n",
      "iteration 24186: loss: 0.2204791009426117\n",
      "iteration 24187: loss: 0.2204785794019699\n",
      "iteration 24188: loss: 0.22047801315784454\n",
      "iteration 24189: loss: 0.22047749161720276\n",
      "iteration 24190: loss: 0.2204769104719162\n",
      "iteration 24191: loss: 0.22047631442546844\n",
      "iteration 24192: loss: 0.22047586739063263\n",
      "iteration 24193: loss: 0.2204752266407013\n",
      "iteration 24194: loss: 0.22047464549541473\n",
      "iteration 24195: loss: 0.22047409415245056\n",
      "iteration 24196: loss: 0.2204734981060028\n",
      "iteration 24197: loss: 0.22047297656536102\n",
      "iteration 24198: loss: 0.22047241032123566\n",
      "iteration 24199: loss: 0.2204718142747879\n",
      "iteration 24200: loss: 0.22047129273414612\n",
      "iteration 24201: loss: 0.22047074139118195\n",
      "iteration 24202: loss: 0.2204701453447342\n",
      "iteration 24203: loss: 0.2204696387052536\n",
      "iteration 24204: loss: 0.22046908736228943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 24205: loss: 0.2204684466123581\n",
      "iteration 24206: loss: 0.22046789526939392\n",
      "iteration 24207: loss: 0.22046735882759094\n",
      "iteration 24208: loss: 0.2204667627811432\n",
      "iteration 24209: loss: 0.22046628594398499\n",
      "iteration 24210: loss: 0.22046568989753723\n",
      "iteration 24211: loss: 0.22046509385108948\n",
      "iteration 24212: loss: 0.22046451270580292\n",
      "iteration 24213: loss: 0.22046396136283875\n",
      "iteration 24214: loss: 0.22046343982219696\n",
      "iteration 24215: loss: 0.2204628735780716\n",
      "iteration 24216: loss: 0.22046232223510742\n",
      "iteration 24217: loss: 0.22046175599098206\n",
      "iteration 24218: loss: 0.22046120464801788\n",
      "iteration 24219: loss: 0.2204606980085373\n",
      "iteration 24220: loss: 0.22046010196208954\n",
      "iteration 24221: loss: 0.22045950591564178\n",
      "iteration 24222: loss: 0.2204589545726776\n",
      "iteration 24223: loss: 0.22045841813087463\n",
      "iteration 24224: loss: 0.22045783698558807\n",
      "iteration 24225: loss: 0.22045724093914032\n",
      "iteration 24226: loss: 0.22045674920082092\n",
      "iteration 24227: loss: 0.2204560935497284\n",
      "iteration 24228: loss: 0.220455601811409\n",
      "iteration 24229: loss: 0.2204550802707672\n",
      "iteration 24230: loss: 0.22045449912548065\n",
      "iteration 24231: loss: 0.22045394778251648\n",
      "iteration 24232: loss: 0.22045335173606873\n",
      "iteration 24233: loss: 0.22045281529426575\n",
      "iteration 24234: loss: 0.220452219247818\n",
      "iteration 24235: loss: 0.2204517126083374\n",
      "iteration 24236: loss: 0.22045111656188965\n",
      "iteration 24237: loss: 0.22045059502124786\n",
      "iteration 24238: loss: 0.2204500138759613\n",
      "iteration 24239: loss: 0.22044941782951355\n",
      "iteration 24240: loss: 0.22044889628887177\n",
      "iteration 24241: loss: 0.2204483300447464\n",
      "iteration 24242: loss: 0.22044774889945984\n",
      "iteration 24243: loss: 0.22044721245765686\n",
      "iteration 24244: loss: 0.22044667601585388\n",
      "iteration 24245: loss: 0.22044610977172852\n",
      "iteration 24246: loss: 0.22044554352760315\n",
      "iteration 24247: loss: 0.22044499218463898\n",
      "iteration 24248: loss: 0.2204444408416748\n",
      "iteration 24249: loss: 0.22044387459754944\n",
      "iteration 24250: loss: 0.2204432487487793\n",
      "iteration 24251: loss: 0.2204427272081375\n",
      "iteration 24252: loss: 0.22044220566749573\n",
      "iteration 24253: loss: 0.22044165432453156\n",
      "iteration 24254: loss: 0.22044113278388977\n",
      "iteration 24255: loss: 0.22044050693511963\n",
      "iteration 24256: loss: 0.22043998539447784\n",
      "iteration 24257: loss: 0.2204393446445465\n",
      "iteration 24258: loss: 0.22043880820274353\n",
      "iteration 24259: loss: 0.22043828666210175\n",
      "iteration 24260: loss: 0.220437690615654\n",
      "iteration 24261: loss: 0.2204371988773346\n",
      "iteration 24262: loss: 0.22043661773204803\n",
      "iteration 24263: loss: 0.22043609619140625\n",
      "iteration 24264: loss: 0.22043552994728088\n",
      "iteration 24265: loss: 0.22043493390083313\n",
      "iteration 24266: loss: 0.22043439745903015\n",
      "iteration 24267: loss: 0.22043383121490479\n",
      "iteration 24268: loss: 0.22043326497077942\n",
      "iteration 24269: loss: 0.22043271362781525\n",
      "iteration 24270: loss: 0.22043223679065704\n",
      "iteration 24271: loss: 0.2204316407442093\n",
      "iteration 24272: loss: 0.22043104469776154\n",
      "iteration 24273: loss: 0.22043053805828094\n",
      "iteration 24274: loss: 0.220429927110672\n",
      "iteration 24275: loss: 0.2204294204711914\n",
      "iteration 24276: loss: 0.22042877972126007\n",
      "iteration 24277: loss: 0.22042831778526306\n",
      "iteration 24278: loss: 0.2204277068376541\n",
      "iteration 24279: loss: 0.22042718529701233\n",
      "iteration 24280: loss: 0.22042660415172577\n",
      "iteration 24281: loss: 0.2204260528087616\n",
      "iteration 24282: loss: 0.22042551636695862\n",
      "iteration 24283: loss: 0.22042496502399445\n",
      "iteration 24284: loss: 0.22042438387870789\n",
      "iteration 24285: loss: 0.22042381763458252\n",
      "iteration 24286: loss: 0.22042326629161835\n",
      "iteration 24287: loss: 0.22042270004749298\n",
      "iteration 24288: loss: 0.2204221934080124\n",
      "iteration 24289: loss: 0.22042159736156464\n",
      "iteration 24290: loss: 0.22042112052440643\n",
      "iteration 24291: loss: 0.22042052447795868\n",
      "iteration 24292: loss: 0.22041992843151093\n",
      "iteration 24293: loss: 0.22041940689086914\n",
      "iteration 24294: loss: 0.22041884064674377\n",
      "iteration 24295: loss: 0.2204183042049408\n",
      "iteration 24296: loss: 0.22041773796081543\n",
      "iteration 24297: loss: 0.22041717171669006\n",
      "iteration 24298: loss: 0.22041663527488708\n",
      "iteration 24299: loss: 0.22041606903076172\n",
      "iteration 24300: loss: 0.22041550278663635\n",
      "iteration 24301: loss: 0.22041495144367218\n",
      "iteration 24302: loss: 0.22041437029838562\n",
      "iteration 24303: loss: 0.22041387856006622\n",
      "iteration 24304: loss: 0.22041329741477966\n",
      "iteration 24305: loss: 0.22041277587413788\n",
      "iteration 24306: loss: 0.2204122245311737\n",
      "iteration 24307: loss: 0.22041165828704834\n",
      "iteration 24308: loss: 0.22041109204292297\n",
      "iteration 24309: loss: 0.2204105406999588\n",
      "iteration 24310: loss: 0.22040998935699463\n",
      "iteration 24311: loss: 0.22040943801403046\n",
      "iteration 24312: loss: 0.22040888667106628\n",
      "iteration 24313: loss: 0.2204083502292633\n",
      "iteration 24314: loss: 0.22040776908397675\n",
      "iteration 24315: loss: 0.22040724754333496\n",
      "iteration 24316: loss: 0.22040662169456482\n",
      "iteration 24317: loss: 0.22040610015392303\n",
      "iteration 24318: loss: 0.22040557861328125\n",
      "iteration 24319: loss: 0.22040501236915588\n",
      "iteration 24320: loss: 0.22040438652038574\n",
      "iteration 24321: loss: 0.22040386497974396\n",
      "iteration 24322: loss: 0.22040331363677979\n",
      "iteration 24323: loss: 0.2204028069972992\n",
      "iteration 24324: loss: 0.22040224075317383\n",
      "iteration 24325: loss: 0.22040171921253204\n",
      "iteration 24326: loss: 0.22040113806724548\n",
      "iteration 24327: loss: 0.2204006165266037\n",
      "iteration 24328: loss: 0.22040000557899475\n",
      "iteration 24329: loss: 0.22039945423603058\n",
      "iteration 24330: loss: 0.22039894759655\n",
      "iteration 24331: loss: 0.22039833664894104\n",
      "iteration 24332: loss: 0.22039785981178284\n",
      "iteration 24333: loss: 0.22039727866649628\n",
      "iteration 24334: loss: 0.2203967571258545\n",
      "iteration 24335: loss: 0.22039620578289032\n",
      "iteration 24336: loss: 0.22039563953876495\n",
      "iteration 24337: loss: 0.2203950434923172\n",
      "iteration 24338: loss: 0.22039449214935303\n",
      "iteration 24339: loss: 0.22039394080638885\n",
      "iteration 24340: loss: 0.22039344906806946\n",
      "iteration 24341: loss: 0.2203928679227829\n",
      "iteration 24342: loss: 0.2203923761844635\n",
      "iteration 24343: loss: 0.22039175033569336\n",
      "iteration 24344: loss: 0.2203911989927292\n",
      "iteration 24345: loss: 0.22039064764976501\n",
      "iteration 24346: loss: 0.22039011120796204\n",
      "iteration 24347: loss: 0.22038953006267548\n",
      "iteration 24348: loss: 0.2203889638185501\n",
      "iteration 24349: loss: 0.22038844227790833\n",
      "iteration 24350: loss: 0.22038793563842773\n",
      "iteration 24351: loss: 0.22038733959197998\n",
      "iteration 24352: loss: 0.2203868329524994\n",
      "iteration 24353: loss: 0.22038623690605164\n",
      "iteration 24354: loss: 0.22038570046424866\n",
      "iteration 24355: loss: 0.2203851044178009\n",
      "iteration 24356: loss: 0.2203846275806427\n",
      "iteration 24357: loss: 0.22038404643535614\n",
      "iteration 24358: loss: 0.22038349509239197\n",
      "iteration 24359: loss: 0.22038300335407257\n",
      "iteration 24360: loss: 0.2203824520111084\n",
      "iteration 24361: loss: 0.22038188576698303\n",
      "iteration 24362: loss: 0.22038128972053528\n",
      "iteration 24363: loss: 0.2203807532787323\n",
      "iteration 24364: loss: 0.22038023173809052\n",
      "iteration 24365: loss: 0.22037963569164276\n",
      "iteration 24366: loss: 0.22037914395332336\n",
      "iteration 24367: loss: 0.2203785628080368\n",
      "iteration 24368: loss: 0.22037799656391144\n",
      "iteration 24369: loss: 0.22037744522094727\n",
      "iteration 24370: loss: 0.22037693858146667\n",
      "iteration 24371: loss: 0.2203764170408249\n",
      "iteration 24372: loss: 0.22037582099437714\n",
      "iteration 24373: loss: 0.22037525475025177\n",
      "iteration 24374: loss: 0.22037474811077118\n",
      "iteration 24375: loss: 0.2203741818666458\n",
      "iteration 24376: loss: 0.22037363052368164\n",
      "iteration 24377: loss: 0.22037306427955627\n",
      "iteration 24378: loss: 0.22037258744239807\n",
      "iteration 24379: loss: 0.22037196159362793\n",
      "iteration 24380: loss: 0.22037145495414734\n",
      "iteration 24381: loss: 0.22037085890769958\n",
      "iteration 24382: loss: 0.22037038207054138\n",
      "iteration 24383: loss: 0.22036977112293243\n",
      "iteration 24384: loss: 0.22036924958229065\n",
      "iteration 24385: loss: 0.22036871314048767\n",
      "iteration 24386: loss: 0.2203681468963623\n",
      "iteration 24387: loss: 0.22036762535572052\n",
      "iteration 24388: loss: 0.22036704421043396\n",
      "iteration 24389: loss: 0.22036650776863098\n",
      "iteration 24390: loss: 0.22036591172218323\n",
      "iteration 24391: loss: 0.22036543488502502\n",
      "iteration 24392: loss: 0.22036485373973846\n",
      "iteration 24393: loss: 0.22036436200141907\n",
      "iteration 24394: loss: 0.2203638106584549\n",
      "iteration 24395: loss: 0.22036322951316833\n",
      "iteration 24396: loss: 0.2203626185655594\n",
      "iteration 24397: loss: 0.22036214172840118\n",
      "iteration 24398: loss: 0.22036151587963104\n",
      "iteration 24399: loss: 0.22036106884479523\n",
      "iteration 24400: loss: 0.22036048769950867\n",
      "iteration 24401: loss: 0.22035996615886688\n",
      "iteration 24402: loss: 0.22035939991474152\n",
      "iteration 24403: loss: 0.22035884857177734\n",
      "iteration 24404: loss: 0.2203582525253296\n",
      "iteration 24405: loss: 0.2203577756881714\n",
      "iteration 24406: loss: 0.22035720944404602\n",
      "iteration 24407: loss: 0.22035662829875946\n",
      "iteration 24408: loss: 0.22035610675811768\n",
      "iteration 24409: loss: 0.22035546600818634\n",
      "iteration 24410: loss: 0.22035500407218933\n",
      "iteration 24411: loss: 0.22035448253154755\n",
      "iteration 24412: loss: 0.22035391628742218\n",
      "iteration 24413: loss: 0.2203533947467804\n",
      "iteration 24414: loss: 0.22035284340381622\n",
      "iteration 24415: loss: 0.22035232186317444\n",
      "iteration 24416: loss: 0.22035174071788788\n",
      "iteration 24417: loss: 0.2203511893749237\n",
      "iteration 24418: loss: 0.22035066783428192\n",
      "iteration 24419: loss: 0.22035011649131775\n",
      "iteration 24420: loss: 0.22034959495067596\n",
      "iteration 24421: loss: 0.2203489989042282\n",
      "iteration 24422: loss: 0.22034844756126404\n",
      "iteration 24423: loss: 0.22034791111946106\n",
      "iteration 24424: loss: 0.2203473597764969\n",
      "iteration 24425: loss: 0.2203468382358551\n",
      "iteration 24426: loss: 0.22034628689289093\n",
      "iteration 24427: loss: 0.22034578025341034\n",
      "iteration 24428: loss: 0.2203451693058014\n",
      "iteration 24429: loss: 0.2203446626663208\n",
      "iteration 24430: loss: 0.22034409642219543\n",
      "iteration 24431: loss: 0.22034363448619843\n",
      "iteration 24432: loss: 0.22034306824207306\n",
      "iteration 24433: loss: 0.2203425168991089\n",
      "iteration 24434: loss: 0.22034192085266113\n",
      "iteration 24435: loss: 0.22034135460853577\n",
      "iteration 24436: loss: 0.22034087777137756\n",
      "iteration 24437: loss: 0.220340296626091\n",
      "iteration 24438: loss: 0.2203398048877716\n",
      "iteration 24439: loss: 0.22033925354480743\n",
      "iteration 24440: loss: 0.22033867239952087\n",
      "iteration 24441: loss: 0.2203381508588791\n",
      "iteration 24442: loss: 0.22033759951591492\n",
      "iteration 24443: loss: 0.22033703327178955\n",
      "iteration 24444: loss: 0.22033651173114777\n",
      "iteration 24445: loss: 0.2203359603881836\n",
      "iteration 24446: loss: 0.22033539414405823\n",
      "iteration 24447: loss: 0.22033484280109406\n",
      "iteration 24448: loss: 0.22033433616161346\n",
      "iteration 24449: loss: 0.2203337848186493\n",
      "iteration 24450: loss: 0.22033321857452393\n",
      "iteration 24451: loss: 0.22033271193504333\n",
      "iteration 24452: loss: 0.22033219039440155\n",
      "iteration 24453: loss: 0.22033162415027618\n",
      "iteration 24454: loss: 0.2203311026096344\n",
      "iteration 24455: loss: 0.22033052146434784\n",
      "iteration 24456: loss: 0.22033004462718964\n",
      "iteration 24457: loss: 0.22032944858074188\n",
      "iteration 24458: loss: 0.2203289270401001\n",
      "iteration 24459: loss: 0.22032836079597473\n",
      "iteration 24460: loss: 0.22032777965068817\n",
      "iteration 24461: loss: 0.22032728791236877\n",
      "iteration 24462: loss: 0.2203267365694046\n",
      "iteration 24463: loss: 0.220326229929924\n",
      "iteration 24464: loss: 0.22032567858695984\n",
      "iteration 24465: loss: 0.22032508254051208\n",
      "iteration 24466: loss: 0.2203245609998703\n",
      "iteration 24467: loss: 0.22032400965690613\n",
      "iteration 24468: loss: 0.22032344341278076\n",
      "iteration 24469: loss: 0.22032292187213898\n",
      "iteration 24470: loss: 0.2203224003314972\n",
      "iteration 24471: loss: 0.2203218638896942\n",
      "iteration 24472: loss: 0.22032132744789124\n",
      "iteration 24473: loss: 0.22032082080841064\n",
      "iteration 24474: loss: 0.22032025456428528\n",
      "iteration 24475: loss: 0.22031967341899872\n",
      "iteration 24476: loss: 0.22031918168067932\n",
      "iteration 24477: loss: 0.22031860053539276\n",
      "iteration 24478: loss: 0.2203180491924286\n",
      "iteration 24479: loss: 0.22031760215759277\n",
      "iteration 24480: loss: 0.22031700611114502\n",
      "iteration 24481: loss: 0.22031643986701965\n",
      "iteration 24482: loss: 0.22031593322753906\n",
      "iteration 24483: loss: 0.2203153818845749\n",
      "iteration 24484: loss: 0.22031483054161072\n",
      "iteration 24485: loss: 0.22031426429748535\n",
      "iteration 24486: loss: 0.22031371295452118\n",
      "iteration 24487: loss: 0.2203131914138794\n",
      "iteration 24488: loss: 0.2203126698732376\n",
      "iteration 24489: loss: 0.22031214833259583\n",
      "iteration 24490: loss: 0.22031159698963165\n",
      "iteration 24491: loss: 0.22031107544898987\n",
      "iteration 24492: loss: 0.2203104943037033\n",
      "iteration 24493: loss: 0.22030997276306152\n",
      "iteration 24494: loss: 0.22030940651893616\n",
      "iteration 24495: loss: 0.22030889987945557\n",
      "iteration 24496: loss: 0.2203083038330078\n",
      "iteration 24497: loss: 0.22030779719352722\n",
      "iteration 24498: loss: 0.22030723094940186\n",
      "iteration 24499: loss: 0.22030667960643768\n",
      "iteration 24500: loss: 0.22030623257160187\n",
      "iteration 24501: loss: 0.2203056812286377\n",
      "iteration 24502: loss: 0.22030512988567352\n",
      "iteration 24503: loss: 0.22030457854270935\n",
      "iteration 24504: loss: 0.22030401229858398\n",
      "iteration 24505: loss: 0.22030353546142578\n",
      "iteration 24506: loss: 0.22030296921730042\n",
      "iteration 24507: loss: 0.22030241787433624\n",
      "iteration 24508: loss: 0.22030189633369446\n",
      "iteration 24509: loss: 0.2203013002872467\n",
      "iteration 24510: loss: 0.2203008234500885\n",
      "iteration 24511: loss: 0.22030024230480194\n",
      "iteration 24512: loss: 0.22029972076416016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 24513: loss: 0.22029919922351837\n",
      "iteration 24514: loss: 0.2202986180782318\n",
      "iteration 24515: loss: 0.22029809653759003\n",
      "iteration 24516: loss: 0.22029760479927063\n",
      "iteration 24517: loss: 0.22029705345630646\n",
      "iteration 24518: loss: 0.2202964723110199\n",
      "iteration 24519: loss: 0.2202959507703781\n",
      "iteration 24520: loss: 0.22029538452625275\n",
      "iteration 24521: loss: 0.22029487788677216\n",
      "iteration 24522: loss: 0.22029432654380798\n",
      "iteration 24523: loss: 0.2202938050031662\n",
      "iteration 24524: loss: 0.22029328346252441\n",
      "iteration 24525: loss: 0.22029276192188263\n",
      "iteration 24526: loss: 0.22029216587543488\n",
      "iteration 24527: loss: 0.22029168903827667\n",
      "iteration 24528: loss: 0.2202911078929901\n",
      "iteration 24529: loss: 0.22029061615467072\n",
      "iteration 24530: loss: 0.22029006481170654\n",
      "iteration 24531: loss: 0.22028955817222595\n",
      "iteration 24532: loss: 0.2202889621257782\n",
      "iteration 24533: loss: 0.22028842568397522\n",
      "iteration 24534: loss: 0.22028791904449463\n",
      "iteration 24535: loss: 0.22028735280036926\n",
      "iteration 24536: loss: 0.22028681635856628\n",
      "iteration 24537: loss: 0.2202862948179245\n",
      "iteration 24538: loss: 0.22028572857379913\n",
      "iteration 24539: loss: 0.22028520703315735\n",
      "iteration 24540: loss: 0.22028474509716034\n",
      "iteration 24541: loss: 0.2202841341495514\n",
      "iteration 24542: loss: 0.2202835977077484\n",
      "iteration 24543: loss: 0.22028307616710663\n",
      "iteration 24544: loss: 0.22028255462646484\n",
      "iteration 24545: loss: 0.22028204798698425\n",
      "iteration 24546: loss: 0.2202814519405365\n",
      "iteration 24547: loss: 0.22028091549873352\n",
      "iteration 24548: loss: 0.22028040885925293\n",
      "iteration 24549: loss: 0.22027984261512756\n",
      "iteration 24550: loss: 0.22027933597564697\n",
      "iteration 24551: loss: 0.2202787697315216\n",
      "iteration 24552: loss: 0.22027826309204102\n",
      "iteration 24553: loss: 0.22027774155139923\n",
      "iteration 24554: loss: 0.22027719020843506\n",
      "iteration 24555: loss: 0.22027668356895447\n",
      "iteration 24556: loss: 0.2202760875225067\n",
      "iteration 24557: loss: 0.2202756106853485\n",
      "iteration 24558: loss: 0.22027499973773956\n",
      "iteration 24559: loss: 0.22027449309825897\n",
      "iteration 24560: loss: 0.2202739715576172\n",
      "iteration 24561: loss: 0.22027337551116943\n",
      "iteration 24562: loss: 0.22027292847633362\n",
      "iteration 24563: loss: 0.22027230262756348\n",
      "iteration 24564: loss: 0.22027187049388885\n",
      "iteration 24565: loss: 0.2202712744474411\n",
      "iteration 24566: loss: 0.22027075290679932\n",
      "iteration 24567: loss: 0.22027018666267395\n",
      "iteration 24568: loss: 0.22026972472667694\n",
      "iteration 24569: loss: 0.220269113779068\n",
      "iteration 24570: loss: 0.22026865184307098\n",
      "iteration 24571: loss: 0.22026808559894562\n",
      "iteration 24572: loss: 0.22026753425598145\n",
      "iteration 24573: loss: 0.22026701271533966\n",
      "iteration 24574: loss: 0.22026649117469788\n",
      "iteration 24575: loss: 0.2202659398317337\n",
      "iteration 24576: loss: 0.22026538848876953\n",
      "iteration 24577: loss: 0.22026486694812775\n",
      "iteration 24578: loss: 0.2202642858028412\n",
      "iteration 24579: loss: 0.2202637940645218\n",
      "iteration 24580: loss: 0.2202632874250412\n",
      "iteration 24581: loss: 0.22026272118091583\n",
      "iteration 24582: loss: 0.22026221454143524\n",
      "iteration 24583: loss: 0.22026169300079346\n",
      "iteration 24584: loss: 0.22026114165782928\n",
      "iteration 24585: loss: 0.22026057541370392\n",
      "iteration 24586: loss: 0.22026006877422333\n",
      "iteration 24587: loss: 0.22025959193706512\n",
      "iteration 24588: loss: 0.22025898098945618\n",
      "iteration 24589: loss: 0.22025851905345917\n",
      "iteration 24590: loss: 0.2202579528093338\n",
      "iteration 24591: loss: 0.22025740146636963\n",
      "iteration 24592: loss: 0.22025689482688904\n",
      "iteration 24593: loss: 0.22025637328624725\n",
      "iteration 24594: loss: 0.22025588154792786\n",
      "iteration 24595: loss: 0.2202553004026413\n",
      "iteration 24596: loss: 0.22025474905967712\n",
      "iteration 24597: loss: 0.22025421261787415\n",
      "iteration 24598: loss: 0.22025367617607117\n",
      "iteration 24599: loss: 0.22025318443775177\n",
      "iteration 24600: loss: 0.22025270760059357\n",
      "iteration 24601: loss: 0.22025206685066223\n",
      "iteration 24602: loss: 0.22025159001350403\n",
      "iteration 24603: loss: 0.22025105357170105\n",
      "iteration 24604: loss: 0.22025051712989807\n",
      "iteration 24605: loss: 0.2202499359846115\n",
      "iteration 24606: loss: 0.2202494591474533\n",
      "iteration 24607: loss: 0.22024889290332794\n",
      "iteration 24608: loss: 0.22024837136268616\n",
      "iteration 24609: loss: 0.22024783492088318\n",
      "iteration 24610: loss: 0.2202472984790802\n",
      "iteration 24611: loss: 0.22024674713611603\n",
      "iteration 24612: loss: 0.22024627029895782\n",
      "iteration 24613: loss: 0.22024567425251007\n",
      "iteration 24614: loss: 0.22024516761302948\n",
      "iteration 24615: loss: 0.22024467587471008\n",
      "iteration 24616: loss: 0.2202441692352295\n",
      "iteration 24617: loss: 0.22024360299110413\n",
      "iteration 24618: loss: 0.22024306654930115\n",
      "iteration 24619: loss: 0.22024258971214294\n",
      "iteration 24620: loss: 0.22024202346801758\n",
      "iteration 24621: loss: 0.2202414572238922\n",
      "iteration 24622: loss: 0.22024095058441162\n",
      "iteration 24623: loss: 0.22024039924144745\n",
      "iteration 24624: loss: 0.22023992240428925\n",
      "iteration 24625: loss: 0.22023935616016388\n",
      "iteration 24626: loss: 0.22023887932300568\n",
      "iteration 24627: loss: 0.2202383279800415\n",
      "iteration 24628: loss: 0.22023773193359375\n",
      "iteration 24629: loss: 0.22023722529411316\n",
      "iteration 24630: loss: 0.22023668885231018\n",
      "iteration 24631: loss: 0.2202361822128296\n",
      "iteration 24632: loss: 0.22023558616638184\n",
      "iteration 24633: loss: 0.22023510932922363\n",
      "iteration 24634: loss: 0.22023455798625946\n",
      "iteration 24635: loss: 0.22023406624794006\n",
      "iteration 24636: loss: 0.22023358941078186\n",
      "iteration 24637: loss: 0.2202329933643341\n",
      "iteration 24638: loss: 0.22023244202136993\n",
      "iteration 24639: loss: 0.22023196518421173\n",
      "iteration 24640: loss: 0.22023138403892517\n",
      "iteration 24641: loss: 0.2202308177947998\n",
      "iteration 24642: loss: 0.2202303111553192\n",
      "iteration 24643: loss: 0.2202298641204834\n",
      "iteration 24644: loss: 0.22022929787635803\n",
      "iteration 24645: loss: 0.22022876143455505\n",
      "iteration 24646: loss: 0.22022822499275208\n",
      "iteration 24647: loss: 0.22022776305675507\n",
      "iteration 24648: loss: 0.2202271670103073\n",
      "iteration 24649: loss: 0.22022664546966553\n",
      "iteration 24650: loss: 0.22022609412670135\n",
      "iteration 24651: loss: 0.22022560238838196\n",
      "iteration 24652: loss: 0.22022505104541779\n",
      "iteration 24653: loss: 0.2202245444059372\n",
      "iteration 24654: loss: 0.22022394835948944\n",
      "iteration 24655: loss: 0.22022350132465363\n",
      "iteration 24656: loss: 0.22022294998168945\n",
      "iteration 24657: loss: 0.22022239863872528\n",
      "iteration 24658: loss: 0.22022192180156708\n",
      "iteration 24659: loss: 0.22022132575511932\n",
      "iteration 24660: loss: 0.22022080421447754\n",
      "iteration 24661: loss: 0.22022037208080292\n",
      "iteration 24662: loss: 0.22021982073783875\n",
      "iteration 24663: loss: 0.22021929919719696\n",
      "iteration 24664: loss: 0.2202187031507492\n",
      "iteration 24665: loss: 0.220218226313591\n",
      "iteration 24666: loss: 0.22021766006946564\n",
      "iteration 24667: loss: 0.22021715342998505\n",
      "iteration 24668: loss: 0.22021658718585968\n",
      "iteration 24669: loss: 0.22021612524986267\n",
      "iteration 24670: loss: 0.2202156037092209\n",
      "iteration 24671: loss: 0.22021503746509552\n",
      "iteration 24672: loss: 0.22021451592445374\n",
      "iteration 24673: loss: 0.22021403908729553\n",
      "iteration 24674: loss: 0.22021344304084778\n",
      "iteration 24675: loss: 0.2202129364013672\n",
      "iteration 24676: loss: 0.22021234035491943\n",
      "iteration 24677: loss: 0.22021189332008362\n",
      "iteration 24678: loss: 0.22021134197711945\n",
      "iteration 24679: loss: 0.22021083533763885\n",
      "iteration 24680: loss: 0.2202102690935135\n",
      "iteration 24681: loss: 0.2202097475528717\n",
      "iteration 24682: loss: 0.2202092707157135\n",
      "iteration 24683: loss: 0.22020873427391052\n",
      "iteration 24684: loss: 0.22020821273326874\n",
      "iteration 24685: loss: 0.22020761668682098\n",
      "iteration 24686: loss: 0.2202071249485016\n",
      "iteration 24687: loss: 0.2202065885066986\n",
      "iteration 24688: loss: 0.2202061116695404\n",
      "iteration 24689: loss: 0.22020554542541504\n",
      "iteration 24690: loss: 0.22020506858825684\n",
      "iteration 24691: loss: 0.22020454704761505\n",
      "iteration 24692: loss: 0.2202039659023285\n",
      "iteration 24693: loss: 0.2202034741640091\n",
      "iteration 24694: loss: 0.2202029675245285\n",
      "iteration 24695: loss: 0.22020240128040314\n",
      "iteration 24696: loss: 0.22020193934440613\n",
      "iteration 24697: loss: 0.22020137310028076\n",
      "iteration 24698: loss: 0.22020092606544495\n",
      "iteration 24699: loss: 0.2202003300189972\n",
      "iteration 24700: loss: 0.2201997935771942\n",
      "iteration 24701: loss: 0.22019925713539124\n",
      "iteration 24702: loss: 0.22019870579242706\n",
      "iteration 24703: loss: 0.22019824385643005\n",
      "iteration 24704: loss: 0.22019770741462708\n",
      "iteration 24705: loss: 0.2201971560716629\n",
      "iteration 24706: loss: 0.22019660472869873\n",
      "iteration 24707: loss: 0.22019609808921814\n",
      "iteration 24708: loss: 0.22019557654857635\n",
      "iteration 24709: loss: 0.22019505500793457\n",
      "iteration 24710: loss: 0.2201944887638092\n",
      "iteration 24711: loss: 0.220194011926651\n",
      "iteration 24712: loss: 0.22019357979297638\n",
      "iteration 24713: loss: 0.2201930582523346\n",
      "iteration 24714: loss: 0.22019247710704803\n",
      "iteration 24715: loss: 0.22019198536872864\n",
      "iteration 24716: loss: 0.22019143402576447\n",
      "iteration 24717: loss: 0.2201908528804779\n",
      "iteration 24718: loss: 0.22019028663635254\n",
      "iteration 24719: loss: 0.22018980979919434\n",
      "iteration 24720: loss: 0.22018936276435852\n",
      "iteration 24721: loss: 0.22018876671791077\n",
      "iteration 24722: loss: 0.22018828988075256\n",
      "iteration 24723: loss: 0.22018775343894958\n",
      "iteration 24724: loss: 0.2201872318983078\n",
      "iteration 24725: loss: 0.22018666565418243\n",
      "iteration 24726: loss: 0.22018615901470184\n",
      "iteration 24727: loss: 0.22018563747406006\n",
      "iteration 24728: loss: 0.22018511593341827\n",
      "iteration 24729: loss: 0.2201845943927765\n",
      "iteration 24730: loss: 0.2201841175556183\n",
      "iteration 24731: loss: 0.22018353641033173\n",
      "iteration 24732: loss: 0.22018304467201233\n",
      "iteration 24733: loss: 0.22018253803253174\n",
      "iteration 24734: loss: 0.22018194198608398\n",
      "iteration 24735: loss: 0.22018149495124817\n",
      "iteration 24736: loss: 0.2201809585094452\n",
      "iteration 24737: loss: 0.2201804667711258\n",
      "iteration 24738: loss: 0.22017991542816162\n",
      "iteration 24739: loss: 0.22017939388751984\n",
      "iteration 24740: loss: 0.22017884254455566\n",
      "iteration 24741: loss: 0.2201782912015915\n",
      "iteration 24742: loss: 0.2201778143644333\n",
      "iteration 24743: loss: 0.2201773226261139\n",
      "iteration 24744: loss: 0.22017677128314972\n",
      "iteration 24745: loss: 0.22017626464366913\n",
      "iteration 24746: loss: 0.22017574310302734\n",
      "iteration 24747: loss: 0.22017522156238556\n",
      "iteration 24748: loss: 0.22017474472522736\n",
      "iteration 24749: loss: 0.2201741635799408\n",
      "iteration 24750: loss: 0.2201736718416214\n",
      "iteration 24751: loss: 0.22017309069633484\n",
      "iteration 24752: loss: 0.22017255425453186\n",
      "iteration 24753: loss: 0.22017207741737366\n",
      "iteration 24754: loss: 0.22017152607440948\n",
      "iteration 24755: loss: 0.2201710194349289\n",
      "iteration 24756: loss: 0.2201705425977707\n",
      "iteration 24757: loss: 0.2201700508594513\n",
      "iteration 24758: loss: 0.22016939520835876\n",
      "iteration 24759: loss: 0.22016899287700653\n",
      "iteration 24760: loss: 0.22016839683055878\n",
      "iteration 24761: loss: 0.22016794979572296\n",
      "iteration 24762: loss: 0.2201673686504364\n",
      "iteration 24763: loss: 0.2201668918132782\n",
      "iteration 24764: loss: 0.2201663702726364\n",
      "iteration 24765: loss: 0.2201658934354782\n",
      "iteration 24766: loss: 0.22016532719135284\n",
      "iteration 24767: loss: 0.22016477584838867\n",
      "iteration 24768: loss: 0.2201642543077469\n",
      "iteration 24769: loss: 0.22016379237174988\n",
      "iteration 24770: loss: 0.2201632559299469\n",
      "iteration 24771: loss: 0.2201627492904663\n",
      "iteration 24772: loss: 0.22016215324401855\n",
      "iteration 24773: loss: 0.22016164660453796\n",
      "iteration 24774: loss: 0.22016115486621857\n",
      "iteration 24775: loss: 0.22016063332557678\n",
      "iteration 24776: loss: 0.2201600819826126\n",
      "iteration 24777: loss: 0.22015957534313202\n",
      "iteration 24778: loss: 0.22015909850597382\n",
      "iteration 24779: loss: 0.22015857696533203\n",
      "iteration 24780: loss: 0.22015810012817383\n",
      "iteration 24781: loss: 0.22015754878520966\n",
      "iteration 24782: loss: 0.22015702724456787\n",
      "iteration 24783: loss: 0.22015652060508728\n",
      "iteration 24784: loss: 0.2201559990644455\n",
      "iteration 24785: loss: 0.22015538811683655\n",
      "iteration 24786: loss: 0.22015495598316193\n",
      "iteration 24787: loss: 0.22015443444252014\n",
      "iteration 24788: loss: 0.22015392780303955\n",
      "iteration 24789: loss: 0.22015342116355896\n",
      "iteration 24790: loss: 0.2201528549194336\n",
      "iteration 24791: loss: 0.220152348279953\n",
      "iteration 24792: loss: 0.22015182673931122\n",
      "iteration 24793: loss: 0.22015127539634705\n",
      "iteration 24794: loss: 0.22015079855918884\n",
      "iteration 24795: loss: 0.22015027701854706\n",
      "iteration 24796: loss: 0.2201497107744217\n",
      "iteration 24797: loss: 0.22014924883842468\n",
      "iteration 24798: loss: 0.22014875710010529\n",
      "iteration 24799: loss: 0.22014816105365753\n",
      "iteration 24800: loss: 0.22014769911766052\n",
      "iteration 24801: loss: 0.22014717757701874\n",
      "iteration 24802: loss: 0.22014662623405457\n",
      "iteration 24803: loss: 0.22014613449573517\n",
      "iteration 24804: loss: 0.22014561295509338\n",
      "iteration 24805: loss: 0.2201451063156128\n",
      "iteration 24806: loss: 0.2201445996761322\n",
      "iteration 24807: loss: 0.22014403343200684\n",
      "iteration 24808: loss: 0.22014352679252625\n",
      "iteration 24809: loss: 0.22014300525188446\n",
      "iteration 24810: loss: 0.22014248371124268\n",
      "iteration 24811: loss: 0.22014197707176208\n",
      "iteration 24812: loss: 0.2201414853334427\n",
      "iteration 24813: loss: 0.2201409637928009\n",
      "iteration 24814: loss: 0.2201404571533203\n",
      "iteration 24815: loss: 0.22013993561267853\n",
      "iteration 24816: loss: 0.22013941407203674\n",
      "iteration 24817: loss: 0.22013890743255615\n",
      "iteration 24818: loss: 0.22013840079307556\n",
      "iteration 24819: loss: 0.22013776004314423\n",
      "iteration 24820: loss: 0.2201373130083084\n",
      "iteration 24821: loss: 0.22013679146766663\n",
      "iteration 24822: loss: 0.220136359333992\n",
      "iteration 24823: loss: 0.22013580799102783\n",
      "iteration 24824: loss: 0.22013528645038605\n",
      "iteration 24825: loss: 0.22013473510742188\n",
      "iteration 24826: loss: 0.22013428807258606\n",
      "iteration 24827: loss: 0.2201337367296219\n",
      "iteration 24828: loss: 0.2201332151889801\n",
      "iteration 24829: loss: 0.22013266384601593\n",
      "iteration 24830: loss: 0.22013215720653534\n",
      "iteration 24831: loss: 0.22013168036937714\n",
      "iteration 24832: loss: 0.22013112902641296\n",
      "iteration 24833: loss: 0.22013063728809357\n",
      "iteration 24834: loss: 0.22013017535209656\n",
      "iteration 24835: loss: 0.22012963891029358\n",
      "iteration 24836: loss: 0.22012917697429657\n",
      "iteration 24837: loss: 0.22012853622436523\n",
      "iteration 24838: loss: 0.22012805938720703\n",
      "iteration 24839: loss: 0.22012753784656525\n",
      "iteration 24840: loss: 0.22012706100940704\n",
      "iteration 24841: loss: 0.22012650966644287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 24842: loss: 0.22012600302696228\n",
      "iteration 24843: loss: 0.2201254814863205\n",
      "iteration 24844: loss: 0.2201249897480011\n",
      "iteration 24845: loss: 0.22012445330619812\n",
      "iteration 24846: loss: 0.22012397646903992\n",
      "iteration 24847: loss: 0.22012341022491455\n",
      "iteration 24848: loss: 0.22012290358543396\n",
      "iteration 24849: loss: 0.22012241184711456\n",
      "iteration 24850: loss: 0.22012190520763397\n",
      "iteration 24851: loss: 0.2201213389635086\n",
      "iteration 24852: loss: 0.22012090682983398\n",
      "iteration 24853: loss: 0.22012034058570862\n",
      "iteration 24854: loss: 0.22011975944042206\n",
      "iteration 24855: loss: 0.22011923789978027\n",
      "iteration 24856: loss: 0.22011883556842804\n",
      "iteration 24857: loss: 0.22011831402778625\n",
      "iteration 24858: loss: 0.22011777758598328\n",
      "iteration 24859: loss: 0.2201172113418579\n",
      "iteration 24860: loss: 0.2201167345046997\n",
      "iteration 24861: loss: 0.22011618316173553\n",
      "iteration 24862: loss: 0.22011573612689972\n",
      "iteration 24863: loss: 0.22011522948741913\n",
      "iteration 24864: loss: 0.22011463344097137\n",
      "iteration 24865: loss: 0.22011420130729675\n",
      "iteration 24866: loss: 0.22011372447013855\n",
      "iteration 24867: loss: 0.22011315822601318\n",
      "iteration 24868: loss: 0.2201126515865326\n",
      "iteration 24869: loss: 0.2201121747493744\n",
      "iteration 24870: loss: 0.2201116383075714\n",
      "iteration 24871: loss: 0.22011104226112366\n",
      "iteration 24872: loss: 0.22011058032512665\n",
      "iteration 24873: loss: 0.22011010348796844\n",
      "iteration 24874: loss: 0.22010958194732666\n",
      "iteration 24875: loss: 0.2201090306043625\n",
      "iteration 24876: loss: 0.22010855376720428\n",
      "iteration 24877: loss: 0.2201080024242401\n",
      "iteration 24878: loss: 0.22010751068592072\n",
      "iteration 24879: loss: 0.22010698914527893\n",
      "iteration 24880: loss: 0.22010651230812073\n",
      "iteration 24881: loss: 0.22010597586631775\n",
      "iteration 24882: loss: 0.22010549902915955\n",
      "iteration 24883: loss: 0.22010496258735657\n",
      "iteration 24884: loss: 0.2201044261455536\n",
      "iteration 24885: loss: 0.22010394930839539\n",
      "iteration 24886: loss: 0.2201034128665924\n",
      "iteration 24887: loss: 0.22010287642478943\n",
      "iteration 24888: loss: 0.22010238468647003\n",
      "iteration 24889: loss: 0.22010192275047302\n",
      "iteration 24890: loss: 0.22010138630867004\n",
      "iteration 24891: loss: 0.22010087966918945\n",
      "iteration 24892: loss: 0.22010035812854767\n",
      "iteration 24893: loss: 0.22009988129138947\n",
      "iteration 24894: loss: 0.2200993001461029\n",
      "iteration 24895: loss: 0.2200988233089447\n",
      "iteration 24896: loss: 0.2200983762741089\n",
      "iteration 24897: loss: 0.22009778022766113\n",
      "iteration 24898: loss: 0.22009730339050293\n",
      "iteration 24899: loss: 0.22009682655334473\n",
      "iteration 24900: loss: 0.22009626030921936\n",
      "iteration 24901: loss: 0.22009578347206116\n",
      "iteration 24902: loss: 0.22009530663490295\n",
      "iteration 24903: loss: 0.2200947254896164\n",
      "iteration 24904: loss: 0.2200942039489746\n",
      "iteration 24905: loss: 0.2200937718153\n",
      "iteration 24906: loss: 0.22009322047233582\n",
      "iteration 24907: loss: 0.2200927436351776\n",
      "iteration 24908: loss: 0.22009220719337463\n",
      "iteration 24909: loss: 0.22009165585041046\n",
      "iteration 24910: loss: 0.22009114921092987\n",
      "iteration 24911: loss: 0.22009065747261047\n",
      "iteration 24912: loss: 0.22009018063545227\n",
      "iteration 24913: loss: 0.2200896292924881\n",
      "iteration 24914: loss: 0.2200891226530075\n",
      "iteration 24915: loss: 0.22008869051933289\n",
      "iteration 24916: loss: 0.2200881540775299\n",
      "iteration 24917: loss: 0.22008761763572693\n",
      "iteration 24918: loss: 0.22008708119392395\n",
      "iteration 24919: loss: 0.22008660435676575\n",
      "iteration 24920: loss: 0.22008612751960754\n",
      "iteration 24921: loss: 0.22008557617664337\n",
      "iteration 24922: loss: 0.2200850546360016\n",
      "iteration 24923: loss: 0.2200845181941986\n",
      "iteration 24924: loss: 0.2200840413570404\n",
      "iteration 24925: loss: 0.2200835645198822\n",
      "iteration 24926: loss: 0.22008304297924042\n",
      "iteration 24927: loss: 0.22008256614208221\n",
      "iteration 24928: loss: 0.22008197009563446\n",
      "iteration 24929: loss: 0.22008149325847626\n",
      "iteration 24930: loss: 0.22008100152015686\n",
      "iteration 24931: loss: 0.2200804203748703\n",
      "iteration 24932: loss: 0.2200799435377121\n",
      "iteration 24933: loss: 0.2200794667005539\n",
      "iteration 24934: loss: 0.2200789749622345\n",
      "iteration 24935: loss: 0.2200784683227539\n",
      "iteration 24936: loss: 0.22007796168327332\n",
      "iteration 24937: loss: 0.22007742524147034\n",
      "iteration 24938: loss: 0.22007691860198975\n",
      "iteration 24939: loss: 0.22007639706134796\n",
      "iteration 24940: loss: 0.22007592022418976\n",
      "iteration 24941: loss: 0.22007545828819275\n",
      "iteration 24942: loss: 0.22007489204406738\n",
      "iteration 24943: loss: 0.22007441520690918\n",
      "iteration 24944: loss: 0.2200739085674286\n",
      "iteration 24945: loss: 0.2200734168291092\n",
      "iteration 24946: loss: 0.2200728952884674\n",
      "iteration 24947: loss: 0.22007235884666443\n",
      "iteration 24948: loss: 0.22007188200950623\n",
      "iteration 24949: loss: 0.22007136046886444\n",
      "iteration 24950: loss: 0.22007086873054504\n",
      "iteration 24951: loss: 0.22007033228874207\n",
      "iteration 24952: loss: 0.22006985545158386\n",
      "iteration 24953: loss: 0.22006931900978088\n",
      "iteration 24954: loss: 0.22006885707378387\n",
      "iteration 24955: loss: 0.2200682908296585\n",
      "iteration 24956: loss: 0.22006778419017792\n",
      "iteration 24957: loss: 0.22006729245185852\n",
      "iteration 24958: loss: 0.2200668603181839\n",
      "iteration 24959: loss: 0.22006630897521973\n",
      "iteration 24960: loss: 0.22006578743457794\n",
      "iteration 24961: loss: 0.22006526589393616\n",
      "iteration 24962: loss: 0.22006472945213318\n",
      "iteration 24963: loss: 0.22006425261497498\n",
      "iteration 24964: loss: 0.2200637310743332\n",
      "iteration 24965: loss: 0.220063254237175\n",
      "iteration 24966: loss: 0.22006270289421082\n",
      "iteration 24967: loss: 0.2200622260570526\n",
      "iteration 24968: loss: 0.2200617790222168\n",
      "iteration 24969: loss: 0.2200612723827362\n",
      "iteration 24970: loss: 0.22006075084209442\n",
      "iteration 24971: loss: 0.22006022930145264\n",
      "iteration 24972: loss: 0.22005975246429443\n",
      "iteration 24973: loss: 0.22005918622016907\n",
      "iteration 24974: loss: 0.22005872428417206\n",
      "iteration 24975: loss: 0.22005820274353027\n",
      "iteration 24976: loss: 0.22005769610404968\n",
      "iteration 24977: loss: 0.22005724906921387\n",
      "iteration 24978: loss: 0.22005662322044373\n",
      "iteration 24979: loss: 0.2200562059879303\n",
      "iteration 24980: loss: 0.2200557291507721\n",
      "iteration 24981: loss: 0.22005514800548553\n",
      "iteration 24982: loss: 0.22005467116832733\n",
      "iteration 24983: loss: 0.22005414962768555\n",
      "iteration 24984: loss: 0.22005364298820496\n",
      "iteration 24985: loss: 0.22005310654640198\n",
      "iteration 24986: loss: 0.22005267441272736\n",
      "iteration 24987: loss: 0.2200520932674408\n",
      "iteration 24988: loss: 0.22005164623260498\n",
      "iteration 24989: loss: 0.2200511395931244\n",
      "iteration 24990: loss: 0.2200506627559662\n",
      "iteration 24991: loss: 0.2200501412153244\n",
      "iteration 24992: loss: 0.22004958987236023\n",
      "iteration 24993: loss: 0.22004914283752441\n",
      "iteration 24994: loss: 0.22004862129688263\n",
      "iteration 24995: loss: 0.22004806995391846\n",
      "iteration 24996: loss: 0.22004762291908264\n",
      "iteration 24997: loss: 0.22004707157611847\n",
      "iteration 24998: loss: 0.22004655003547668\n",
      "iteration 24999: loss: 0.22004611790180206\n",
      "iteration 25000: loss: 0.22004561126232147\n",
      "iteration 25001: loss: 0.22004513442516327\n",
      "iteration 25002: loss: 0.2200445830821991\n",
      "iteration 25003: loss: 0.2200440913438797\n",
      "iteration 25004: loss: 0.2200435847043991\n",
      "iteration 25005: loss: 0.22004306316375732\n",
      "iteration 25006: loss: 0.22004254162311554\n",
      "iteration 25007: loss: 0.22004207968711853\n",
      "iteration 25008: loss: 0.22004160284996033\n",
      "iteration 25009: loss: 0.22004108130931854\n",
      "iteration 25010: loss: 0.22004060447216034\n",
      "iteration 25011: loss: 0.22004008293151855\n",
      "iteration 25012: loss: 0.22003953158855438\n",
      "iteration 25013: loss: 0.22003908455371857\n",
      "iteration 25014: loss: 0.22003856301307678\n",
      "iteration 25015: loss: 0.2200380563735962\n",
      "iteration 25016: loss: 0.2200375348329544\n",
      "iteration 25017: loss: 0.22003702819347382\n",
      "iteration 25018: loss: 0.2200365513563156\n",
      "iteration 25019: loss: 0.2200360745191574\n",
      "iteration 25020: loss: 0.22003552317619324\n",
      "iteration 25021: loss: 0.22003503143787384\n",
      "iteration 25022: loss: 0.22003450989723206\n",
      "iteration 25023: loss: 0.22003400325775146\n",
      "iteration 25024: loss: 0.22003352642059326\n",
      "iteration 25025: loss: 0.22003309428691864\n",
      "iteration 25026: loss: 0.22003252804279327\n",
      "iteration 25027: loss: 0.22003202140331268\n",
      "iteration 25028: loss: 0.2200315296649933\n",
      "iteration 25029: loss: 0.2200310230255127\n",
      "iteration 25030: loss: 0.2200305461883545\n",
      "iteration 25031: loss: 0.22002997994422913\n",
      "iteration 25032: loss: 0.22002951800823212\n",
      "iteration 25033: loss: 0.22002902626991272\n",
      "iteration 25034: loss: 0.22002851963043213\n",
      "iteration 25035: loss: 0.22002804279327393\n",
      "iteration 25036: loss: 0.22002752125263214\n",
      "iteration 25037: loss: 0.22002705931663513\n",
      "iteration 25038: loss: 0.22002652287483215\n",
      "iteration 25039: loss: 0.22002598643302917\n",
      "iteration 25040: loss: 0.22002549469470978\n",
      "iteration 25041: loss: 0.22002503275871277\n",
      "iteration 25042: loss: 0.22002454102039337\n",
      "iteration 25043: loss: 0.22002406418323517\n",
      "iteration 25044: loss: 0.22002354264259338\n",
      "iteration 25045: loss: 0.22002306580543518\n",
      "iteration 25046: loss: 0.2200225293636322\n",
      "iteration 25047: loss: 0.22002200782299042\n",
      "iteration 25048: loss: 0.2200215607881546\n",
      "iteration 25049: loss: 0.22002097964286804\n",
      "iteration 25050: loss: 0.2200205773115158\n",
      "iteration 25051: loss: 0.22002001106739044\n",
      "iteration 25052: loss: 0.22001948952674866\n",
      "iteration 25053: loss: 0.22001905739307404\n",
      "iteration 25054: loss: 0.22001846134662628\n",
      "iteration 25055: loss: 0.22001798450946808\n",
      "iteration 25056: loss: 0.22001750767230988\n",
      "iteration 25057: loss: 0.2200169861316681\n",
      "iteration 25058: loss: 0.2200165092945099\n",
      "iteration 25059: loss: 0.22001604735851288\n",
      "iteration 25060: loss: 0.2200154960155487\n",
      "iteration 25061: loss: 0.2200150191783905\n",
      "iteration 25062: loss: 0.2200145274400711\n",
      "iteration 25063: loss: 0.22001400589942932\n",
      "iteration 25064: loss: 0.22001352906227112\n",
      "iteration 25065: loss: 0.22001302242279053\n",
      "iteration 25066: loss: 0.22001251578330994\n",
      "iteration 25067: loss: 0.22001203894615173\n",
      "iteration 25068: loss: 0.22001159191131592\n",
      "iteration 25069: loss: 0.22001102566719055\n",
      "iteration 25070: loss: 0.22001048922538757\n",
      "iteration 25071: loss: 0.22000999748706818\n",
      "iteration 25072: loss: 0.22000953555107117\n",
      "iteration 25073: loss: 0.2200089991092682\n",
      "iteration 25074: loss: 0.22000852227210999\n",
      "iteration 25075: loss: 0.22000804543495178\n",
      "iteration 25076: loss: 0.2200075089931488\n",
      "iteration 25077: loss: 0.22000709176063538\n",
      "iteration 25078: loss: 0.2200065553188324\n",
      "iteration 25079: loss: 0.22000601887702942\n",
      "iteration 25080: loss: 0.22000551223754883\n",
      "iteration 25081: loss: 0.22000503540039062\n",
      "iteration 25082: loss: 0.2200045883655548\n",
      "iteration 25083: loss: 0.22000403702259064\n",
      "iteration 25084: loss: 0.22000356018543243\n",
      "iteration 25085: loss: 0.22000308334827423\n",
      "iteration 25086: loss: 0.22000253200531006\n",
      "iteration 25087: loss: 0.22000205516815186\n",
      "iteration 25088: loss: 0.22000153362751007\n",
      "iteration 25089: loss: 0.22000102698802948\n",
      "iteration 25090: loss: 0.22000059485435486\n",
      "iteration 25091: loss: 0.22000007331371307\n",
      "iteration 25092: loss: 0.21999962627887726\n",
      "iteration 25093: loss: 0.21999911963939667\n",
      "iteration 25094: loss: 0.21999859809875488\n",
      "iteration 25095: loss: 0.21999812126159668\n",
      "iteration 25096: loss: 0.2199975997209549\n",
      "iteration 25097: loss: 0.2199970930814743\n",
      "iteration 25098: loss: 0.2199966162443161\n",
      "iteration 25099: loss: 0.21999609470367432\n",
      "iteration 25100: loss: 0.21999554336071014\n",
      "iteration 25101: loss: 0.21999509632587433\n",
      "iteration 25102: loss: 0.21999461948871613\n",
      "iteration 25103: loss: 0.21999414265155792\n",
      "iteration 25104: loss: 0.21999359130859375\n",
      "iteration 25105: loss: 0.21999315917491913\n",
      "iteration 25106: loss: 0.21999260783195496\n",
      "iteration 25107: loss: 0.21999207139015198\n",
      "iteration 25108: loss: 0.21999159455299377\n",
      "iteration 25109: loss: 0.21999116241931915\n",
      "iteration 25110: loss: 0.21999068558216095\n",
      "iteration 25111: loss: 0.2199901044368744\n",
      "iteration 25112: loss: 0.21998968720436096\n",
      "iteration 25113: loss: 0.21998915076255798\n",
      "iteration 25114: loss: 0.2199886292219162\n",
      "iteration 25115: loss: 0.219988152384758\n",
      "iteration 25116: loss: 0.2199876606464386\n",
      "iteration 25117: loss: 0.21998722851276398\n",
      "iteration 25118: loss: 0.21998675167560577\n",
      "iteration 25119: loss: 0.21998615562915802\n",
      "iteration 25120: loss: 0.219985693693161\n",
      "iteration 25121: loss: 0.2199852168560028\n",
      "iteration 25122: loss: 0.21998468041419983\n",
      "iteration 25123: loss: 0.21998417377471924\n",
      "iteration 25124: loss: 0.21998369693756104\n",
      "iteration 25125: loss: 0.21998322010040283\n",
      "iteration 25126: loss: 0.21998274326324463\n",
      "iteration 25127: loss: 0.21998223662376404\n",
      "iteration 25128: loss: 0.21998174488544464\n",
      "iteration 25129: loss: 0.21998122334480286\n",
      "iteration 25130: loss: 0.21998076140880585\n",
      "iteration 25131: loss: 0.21998028457164764\n",
      "iteration 25132: loss: 0.21997976303100586\n",
      "iteration 25133: loss: 0.21997933089733124\n",
      "iteration 25134: loss: 0.21997876465320587\n",
      "iteration 25135: loss: 0.21997830271720886\n",
      "iteration 25136: loss: 0.21997778117656708\n",
      "iteration 25137: loss: 0.21997733414173126\n",
      "iteration 25138: loss: 0.2199767529964447\n",
      "iteration 25139: loss: 0.2199762761592865\n",
      "iteration 25140: loss: 0.2199757993221283\n",
      "iteration 25141: loss: 0.2199753224849701\n",
      "iteration 25142: loss: 0.2199748456478119\n",
      "iteration 25143: loss: 0.2199743241071701\n",
      "iteration 25144: loss: 0.2199738472700119\n",
      "iteration 25145: loss: 0.2199733555316925\n",
      "iteration 25146: loss: 0.21997284889221191\n",
      "iteration 25147: loss: 0.2199723720550537\n",
      "iteration 25148: loss: 0.2199718952178955\n",
      "iteration 25149: loss: 0.2199714183807373\n",
      "iteration 25150: loss: 0.21997085213661194\n",
      "iteration 25151: loss: 0.21997042000293732\n",
      "iteration 25152: loss: 0.21996989846229553\n",
      "iteration 25153: loss: 0.21996936202049255\n",
      "iteration 25154: loss: 0.21996887028217316\n",
      "iteration 25155: loss: 0.21996846795082092\n",
      "iteration 25156: loss: 0.21996788680553436\n",
      "iteration 25157: loss: 0.21996740996837616\n",
      "iteration 25158: loss: 0.21996688842773438\n",
      "iteration 25159: loss: 0.21996650099754333\n",
      "iteration 25160: loss: 0.21996596455574036\n",
      "iteration 25161: loss: 0.21996548771858215\n",
      "iteration 25162: loss: 0.21996493637561798\n",
      "iteration 25163: loss: 0.21996447443962097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 25164: loss: 0.21996399760246277\n",
      "iteration 25165: loss: 0.2199634611606598\n",
      "iteration 25166: loss: 0.2199629545211792\n",
      "iteration 25167: loss: 0.219962477684021\n",
      "iteration 25168: loss: 0.2199620008468628\n",
      "iteration 25169: loss: 0.2199615240097046\n",
      "iteration 25170: loss: 0.219961017370224\n",
      "iteration 25171: loss: 0.2199605405330658\n",
      "iteration 25172: loss: 0.21996009349822998\n",
      "iteration 25173: loss: 0.21995952725410461\n",
      "iteration 25174: loss: 0.21995902061462402\n",
      "iteration 25175: loss: 0.2199585884809494\n",
      "iteration 25176: loss: 0.2199581116437912\n",
      "iteration 25177: loss: 0.2199576199054718\n",
      "iteration 25178: loss: 0.2199571132659912\n",
      "iteration 25179: loss: 0.21995656192302704\n",
      "iteration 25180: loss: 0.2199561595916748\n",
      "iteration 25181: loss: 0.21995563805103302\n",
      "iteration 25182: loss: 0.21995516121387482\n",
      "iteration 25183: loss: 0.21995460987091064\n",
      "iteration 25184: loss: 0.21995416283607483\n",
      "iteration 25185: loss: 0.21995365619659424\n",
      "iteration 25186: loss: 0.21995320916175842\n",
      "iteration 25187: loss: 0.21995270252227783\n",
      "iteration 25188: loss: 0.21995219588279724\n",
      "iteration 25189: loss: 0.21995174884796143\n",
      "iteration 25190: loss: 0.21995119750499725\n",
      "iteration 25191: loss: 0.21995070576667786\n",
      "iteration 25192: loss: 0.21995022892951965\n",
      "iteration 25193: loss: 0.21994981169700623\n",
      "iteration 25194: loss: 0.21994921565055847\n",
      "iteration 25195: loss: 0.21994876861572266\n",
      "iteration 25196: loss: 0.21994826197624207\n",
      "iteration 25197: loss: 0.21994778513908386\n",
      "iteration 25198: loss: 0.21994724869728088\n",
      "iteration 25199: loss: 0.21994681656360626\n",
      "iteration 25200: loss: 0.21994630992412567\n",
      "iteration 25201: loss: 0.21994586288928986\n",
      "iteration 25202: loss: 0.21994540095329285\n",
      "iteration 25203: loss: 0.21994486451148987\n",
      "iteration 25204: loss: 0.21994435787200928\n",
      "iteration 25205: loss: 0.21994395554065704\n",
      "iteration 25206: loss: 0.21994343400001526\n",
      "iteration 25207: loss: 0.2199428826570511\n",
      "iteration 25208: loss: 0.21994242072105408\n",
      "iteration 25209: loss: 0.21994194388389587\n",
      "iteration 25210: loss: 0.2199414074420929\n",
      "iteration 25211: loss: 0.2199409455060959\n",
      "iteration 25212: loss: 0.2199404239654541\n",
      "iteration 25213: loss: 0.2199399769306183\n",
      "iteration 25214: loss: 0.21993950009346008\n",
      "iteration 25215: loss: 0.2199389636516571\n",
      "iteration 25216: loss: 0.2199384868144989\n",
      "iteration 25217: loss: 0.2199379950761795\n",
      "iteration 25218: loss: 0.21993756294250488\n",
      "iteration 25219: loss: 0.2199370414018631\n",
      "iteration 25220: loss: 0.2199365645647049\n",
      "iteration 25221: loss: 0.2199360877275467\n",
      "iteration 25222: loss: 0.21993553638458252\n",
      "iteration 25223: loss: 0.2199350893497467\n",
      "iteration 25224: loss: 0.21993455290794373\n",
      "iteration 25225: loss: 0.21993406116962433\n",
      "iteration 25226: loss: 0.2199336588382721\n",
      "iteration 25227: loss: 0.21993312239646912\n",
      "iteration 25228: loss: 0.2199326753616333\n",
      "iteration 25229: loss: 0.21993215382099152\n",
      "iteration 25230: loss: 0.21993164718151093\n",
      "iteration 25231: loss: 0.21993115544319153\n",
      "iteration 25232: loss: 0.21993067860603333\n",
      "iteration 25233: loss: 0.21993020176887512\n",
      "iteration 25234: loss: 0.2199297398328781\n",
      "iteration 25235: loss: 0.21992921829223633\n",
      "iteration 25236: loss: 0.2199287861585617\n",
      "iteration 25237: loss: 0.21992823481559753\n",
      "iteration 25238: loss: 0.21992775797843933\n",
      "iteration 25239: loss: 0.21992728114128113\n",
      "iteration 25240: loss: 0.2199268341064453\n",
      "iteration 25241: loss: 0.21992631256580353\n",
      "iteration 25242: loss: 0.21992579102516174\n",
      "iteration 25243: loss: 0.21992537379264832\n",
      "iteration 25244: loss: 0.21992488205432892\n",
      "iteration 25245: loss: 0.21992436051368713\n",
      "iteration 25246: loss: 0.21992385387420654\n",
      "iteration 25247: loss: 0.21992340683937073\n",
      "iteration 25248: loss: 0.21992293000221252\n",
      "iteration 25249: loss: 0.21992242336273193\n",
      "iteration 25250: loss: 0.21992191672325134\n",
      "iteration 25251: loss: 0.21992142498493195\n",
      "iteration 25252: loss: 0.21992099285125732\n",
      "iteration 25253: loss: 0.21992047131061554\n",
      "iteration 25254: loss: 0.21991996467113495\n",
      "iteration 25255: loss: 0.21991948783397675\n",
      "iteration 25256: loss: 0.21991905570030212\n",
      "iteration 25257: loss: 0.21991851925849915\n",
      "iteration 25258: loss: 0.21991808712482452\n",
      "iteration 25259: loss: 0.21991756558418274\n",
      "iteration 25260: loss: 0.21991710364818573\n",
      "iteration 25261: loss: 0.21991662681102753\n",
      "iteration 25262: loss: 0.21991613507270813\n",
      "iteration 25263: loss: 0.21991562843322754\n",
      "iteration 25264: loss: 0.21991510689258575\n",
      "iteration 25265: loss: 0.21991467475891113\n",
      "iteration 25266: loss: 0.21991419792175293\n",
      "iteration 25267: loss: 0.21991367638111115\n",
      "iteration 25268: loss: 0.21991316974163055\n",
      "iteration 25269: loss: 0.21991273760795593\n",
      "iteration 25270: loss: 0.21991226077079773\n",
      "iteration 25271: loss: 0.21991176903247833\n",
      "iteration 25272: loss: 0.21991126239299774\n",
      "iteration 25273: loss: 0.21991078555583954\n",
      "iteration 25274: loss: 0.21991026401519775\n",
      "iteration 25275: loss: 0.21990978717803955\n",
      "iteration 25276: loss: 0.21990934014320374\n",
      "iteration 25277: loss: 0.21990887820720673\n",
      "iteration 25278: loss: 0.21990832686424255\n",
      "iteration 25279: loss: 0.21990785002708435\n",
      "iteration 25280: loss: 0.21990737318992615\n",
      "iteration 25281: loss: 0.21990689635276794\n",
      "iteration 25282: loss: 0.21990641951560974\n",
      "iteration 25283: loss: 0.21990594267845154\n",
      "iteration 25284: loss: 0.21990546584129333\n",
      "iteration 25285: loss: 0.21990494430065155\n",
      "iteration 25286: loss: 0.21990449726581573\n",
      "iteration 25287: loss: 0.21990403532981873\n",
      "iteration 25288: loss: 0.21990354359149933\n",
      "iteration 25289: loss: 0.21990303695201874\n",
      "iteration 25290: loss: 0.21990256011486053\n",
      "iteration 25291: loss: 0.21990208327770233\n",
      "iteration 25292: loss: 0.21990159153938293\n",
      "iteration 25293: loss: 0.21990111470222473\n",
      "iteration 25294: loss: 0.21990063786506653\n",
      "iteration 25295: loss: 0.21990008652210236\n",
      "iteration 25296: loss: 0.21989965438842773\n",
      "iteration 25297: loss: 0.21989913284778595\n",
      "iteration 25298: loss: 0.21989873051643372\n",
      "iteration 25299: loss: 0.21989817917346954\n",
      "iteration 25300: loss: 0.21989770233631134\n",
      "iteration 25301: loss: 0.21989727020263672\n",
      "iteration 25302: loss: 0.21989679336547852\n",
      "iteration 25303: loss: 0.21989627182483673\n",
      "iteration 25304: loss: 0.21989579498767853\n",
      "iteration 25305: loss: 0.21989528834819794\n",
      "iteration 25306: loss: 0.21989473700523376\n",
      "iteration 25307: loss: 0.21989433467388153\n",
      "iteration 25308: loss: 0.21989388763904572\n",
      "iteration 25309: loss: 0.21989338099956512\n",
      "iteration 25310: loss: 0.21989288926124573\n",
      "iteration 25311: loss: 0.21989242732524872\n",
      "iteration 25312: loss: 0.21989193558692932\n",
      "iteration 25313: loss: 0.21989139914512634\n",
      "iteration 25314: loss: 0.2198909968137741\n",
      "iteration 25315: loss: 0.2198905497789383\n",
      "iteration 25316: loss: 0.21988999843597412\n",
      "iteration 25317: loss: 0.21988952159881592\n",
      "iteration 25318: loss: 0.21988904476165771\n",
      "iteration 25319: loss: 0.2198885977268219\n",
      "iteration 25320: loss: 0.21988806128501892\n",
      "iteration 25321: loss: 0.21988756954669952\n",
      "iteration 25322: loss: 0.21988710761070251\n",
      "iteration 25323: loss: 0.2198866307735443\n",
      "iteration 25324: loss: 0.2198861837387085\n",
      "iteration 25325: loss: 0.2198857069015503\n",
      "iteration 25326: loss: 0.2198851853609085\n",
      "iteration 25327: loss: 0.2198847085237503\n",
      "iteration 25328: loss: 0.2198842465877533\n",
      "iteration 25329: loss: 0.2198837548494339\n",
      "iteration 25330: loss: 0.2198832482099533\n",
      "iteration 25331: loss: 0.2198828011751175\n",
      "iteration 25332: loss: 0.2198822945356369\n",
      "iteration 25333: loss: 0.2198818475008011\n",
      "iteration 25334: loss: 0.2198813408613205\n",
      "iteration 25335: loss: 0.2198808193206787\n",
      "iteration 25336: loss: 0.21988043189048767\n",
      "iteration 25337: loss: 0.2198799103498459\n",
      "iteration 25338: loss: 0.2198794186115265\n",
      "iteration 25339: loss: 0.21987898647785187\n",
      "iteration 25340: loss: 0.2198784053325653\n",
      "iteration 25341: loss: 0.21987800300121307\n",
      "iteration 25342: loss: 0.21987751126289368\n",
      "iteration 25343: loss: 0.21987703442573547\n",
      "iteration 25344: loss: 0.21987652778625488\n",
      "iteration 25345: loss: 0.21987612545490265\n",
      "iteration 25346: loss: 0.21987561881542206\n",
      "iteration 25347: loss: 0.21987509727478027\n",
      "iteration 25348: loss: 0.21987459063529968\n",
      "iteration 25349: loss: 0.21987414360046387\n",
      "iteration 25350: loss: 0.21987363696098328\n",
      "iteration 25351: loss: 0.21987321972846985\n",
      "iteration 25352: loss: 0.21987271308898926\n",
      "iteration 25353: loss: 0.21987226605415344\n",
      "iteration 25354: loss: 0.21987178921699524\n",
      "iteration 25355: loss: 0.21987128257751465\n",
      "iteration 25356: loss: 0.21987077593803406\n",
      "iteration 25357: loss: 0.21987035870552063\n",
      "iteration 25358: loss: 0.21986980736255646\n",
      "iteration 25359: loss: 0.21986941993236542\n",
      "iteration 25360: loss: 0.21986892819404602\n",
      "iteration 25361: loss: 0.21986842155456543\n",
      "iteration 25362: loss: 0.21986791491508484\n",
      "iteration 25363: loss: 0.2198675125837326\n",
      "iteration 25364: loss: 0.21986696124076843\n",
      "iteration 25365: loss: 0.21986651420593262\n",
      "iteration 25366: loss: 0.21986600756645203\n",
      "iteration 25367: loss: 0.2198655605316162\n",
      "iteration 25368: loss: 0.2198651134967804\n",
      "iteration 25369: loss: 0.21986457705497742\n",
      "iteration 25370: loss: 0.2198641300201416\n",
      "iteration 25371: loss: 0.2198636829853058\n",
      "iteration 25372: loss: 0.2198631763458252\n",
      "iteration 25373: loss: 0.2198626548051834\n",
      "iteration 25374: loss: 0.21986214816570282\n",
      "iteration 25375: loss: 0.219861701130867\n",
      "iteration 25376: loss: 0.21986123919487\n",
      "iteration 25377: loss: 0.2198607176542282\n",
      "iteration 25378: loss: 0.2198602855205536\n",
      "iteration 25379: loss: 0.21985980868339539\n",
      "iteration 25380: loss: 0.2198592722415924\n",
      "iteration 25381: loss: 0.21985888481140137\n",
      "iteration 25382: loss: 0.21985836327075958\n",
      "iteration 25383: loss: 0.21985790133476257\n",
      "iteration 25384: loss: 0.2198573797941208\n",
      "iteration 25385: loss: 0.21985694766044617\n",
      "iteration 25386: loss: 0.21985645592212677\n",
      "iteration 25387: loss: 0.21985599398612976\n",
      "iteration 25388: loss: 0.21985551714897156\n",
      "iteration 25389: loss: 0.21985499560832977\n",
      "iteration 25390: loss: 0.21985450387001038\n",
      "iteration 25391: loss: 0.21985408663749695\n",
      "iteration 25392: loss: 0.21985360980033875\n",
      "iteration 25393: loss: 0.21985316276550293\n",
      "iteration 25394: loss: 0.21985264122486115\n",
      "iteration 25395: loss: 0.2198522537946701\n",
      "iteration 25396: loss: 0.21985173225402832\n",
      "iteration 25397: loss: 0.21985125541687012\n",
      "iteration 25398: loss: 0.2198508083820343\n",
      "iteration 25399: loss: 0.2198503017425537\n",
      "iteration 25400: loss: 0.21984975039958954\n",
      "iteration 25401: loss: 0.2198493480682373\n",
      "iteration 25402: loss: 0.21984882652759552\n",
      "iteration 25403: loss: 0.2198484241962433\n",
      "iteration 25404: loss: 0.2198478728532791\n",
      "iteration 25405: loss: 0.21984747052192688\n",
      "iteration 25406: loss: 0.2198469191789627\n",
      "iteration 25407: loss: 0.21984648704528809\n",
      "iteration 25408: loss: 0.2198459804058075\n",
      "iteration 25409: loss: 0.21984553337097168\n",
      "iteration 25410: loss: 0.2198450267314911\n",
      "iteration 25411: loss: 0.21984457969665527\n",
      "iteration 25412: loss: 0.21984413266181946\n",
      "iteration 25413: loss: 0.21984362602233887\n",
      "iteration 25414: loss: 0.21984311938285828\n",
      "iteration 25415: loss: 0.21984264254570007\n",
      "iteration 25416: loss: 0.21984222531318665\n",
      "iteration 25417: loss: 0.21984171867370605\n",
      "iteration 25418: loss: 0.21984124183654785\n",
      "iteration 25419: loss: 0.21984079480171204\n",
      "iteration 25420: loss: 0.21984031796455383\n",
      "iteration 25421: loss: 0.21983984112739563\n",
      "iteration 25422: loss: 0.21983933448791504\n",
      "iteration 25423: loss: 0.21983888745307922\n",
      "iteration 25424: loss: 0.21983841061592102\n",
      "iteration 25425: loss: 0.21983793377876282\n",
      "iteration 25426: loss: 0.2198375165462494\n",
      "iteration 25427: loss: 0.2198369801044464\n",
      "iteration 25428: loss: 0.2198365479707718\n",
      "iteration 25429: loss: 0.2198360413312912\n",
      "iteration 25430: loss: 0.219835564494133\n",
      "iteration 25431: loss: 0.2198350727558136\n",
      "iteration 25432: loss: 0.21983464062213898\n",
      "iteration 25433: loss: 0.2198340892791748\n",
      "iteration 25434: loss: 0.21983365714550018\n",
      "iteration 25435: loss: 0.21983318030834198\n",
      "iteration 25436: loss: 0.21983268857002258\n",
      "iteration 25437: loss: 0.21983227133750916\n",
      "iteration 25438: loss: 0.21983174979686737\n",
      "iteration 25439: loss: 0.21983131766319275\n",
      "iteration 25440: loss: 0.21983087062835693\n",
      "iteration 25441: loss: 0.21983042359352112\n",
      "iteration 25442: loss: 0.21982988715171814\n",
      "iteration 25443: loss: 0.21982944011688232\n",
      "iteration 25444: loss: 0.21982893347740173\n",
      "iteration 25445: loss: 0.2198285162448883\n",
      "iteration 25446: loss: 0.21982797980308533\n",
      "iteration 25447: loss: 0.2198275327682495\n",
      "iteration 25448: loss: 0.2198271006345749\n",
      "iteration 25449: loss: 0.2198266237974167\n",
      "iteration 25450: loss: 0.21982617676258087\n",
      "iteration 25451: loss: 0.2198256552219391\n",
      "iteration 25452: loss: 0.21982517838478088\n",
      "iteration 25453: loss: 0.21982470154762268\n",
      "iteration 25454: loss: 0.21982422471046448\n",
      "iteration 25455: loss: 0.21982374787330627\n",
      "iteration 25456: loss: 0.21982324123382568\n",
      "iteration 25457: loss: 0.21982283890247345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 25458: loss: 0.21982233226299286\n",
      "iteration 25459: loss: 0.21982185542583466\n",
      "iteration 25460: loss: 0.21982137858867645\n",
      "iteration 25461: loss: 0.21982090175151825\n",
      "iteration 25462: loss: 0.21982042491436005\n",
      "iteration 25463: loss: 0.21981997787952423\n",
      "iteration 25464: loss: 0.21981942653656006\n",
      "iteration 25465: loss: 0.21981899440288544\n",
      "iteration 25466: loss: 0.2198186218738556\n",
      "iteration 25467: loss: 0.21981807053089142\n",
      "iteration 25468: loss: 0.2198176085948944\n",
      "iteration 25469: loss: 0.21981708705425262\n",
      "iteration 25470: loss: 0.219816654920578\n",
      "iteration 25471: loss: 0.2198161631822586\n",
      "iteration 25472: loss: 0.21981573104858398\n",
      "iteration 25473: loss: 0.21981525421142578\n",
      "iteration 25474: loss: 0.21981482207775116\n",
      "iteration 25475: loss: 0.21981434524059296\n",
      "iteration 25476: loss: 0.21981385350227356\n",
      "iteration 25477: loss: 0.21981337666511536\n",
      "iteration 25478: loss: 0.21981294453144073\n",
      "iteration 25479: loss: 0.21981248259544373\n",
      "iteration 25480: loss: 0.21981194615364075\n",
      "iteration 25481: loss: 0.21981146931648254\n",
      "iteration 25482: loss: 0.21981100738048553\n",
      "iteration 25483: loss: 0.21981053054332733\n",
      "iteration 25484: loss: 0.21981015801429749\n",
      "iteration 25485: loss: 0.2198096215724945\n",
      "iteration 25486: loss: 0.2198091298341751\n",
      "iteration 25487: loss: 0.2198086529970169\n",
      "iteration 25488: loss: 0.2198081910610199\n",
      "iteration 25489: loss: 0.2198077142238617\n",
      "iteration 25490: loss: 0.21980726718902588\n",
      "iteration 25491: loss: 0.21980683505535126\n",
      "iteration 25492: loss: 0.21980628371238708\n",
      "iteration 25493: loss: 0.21980591118335724\n",
      "iteration 25494: loss: 0.21980538964271545\n",
      "iteration 25495: loss: 0.21980491280555725\n",
      "iteration 25496: loss: 0.21980445086956024\n",
      "iteration 25497: loss: 0.21980400383472443\n",
      "iteration 25498: loss: 0.21980348229408264\n",
      "iteration 25499: loss: 0.2198030650615692\n",
      "iteration 25500: loss: 0.21980254352092743\n",
      "iteration 25501: loss: 0.2198021113872528\n",
      "iteration 25502: loss: 0.2198016345500946\n",
      "iteration 25503: loss: 0.21980121731758118\n",
      "iteration 25504: loss: 0.21980074048042297\n",
      "iteration 25505: loss: 0.21980020403862\n",
      "iteration 25506: loss: 0.21979975700378418\n",
      "iteration 25507: loss: 0.2197992354631424\n",
      "iteration 25508: loss: 0.21979884803295135\n",
      "iteration 25509: loss: 0.21979837119579315\n",
      "iteration 25510: loss: 0.21979789435863495\n",
      "iteration 25511: loss: 0.21979740262031555\n",
      "iteration 25512: loss: 0.21979694068431854\n",
      "iteration 25513: loss: 0.21979649364948273\n",
      "iteration 25514: loss: 0.21979601681232452\n",
      "iteration 25515: loss: 0.21979553997516632\n",
      "iteration 25516: loss: 0.2197951376438141\n",
      "iteration 25517: loss: 0.2197946012020111\n",
      "iteration 25518: loss: 0.2197941243648529\n",
      "iteration 25519: loss: 0.21979370713233948\n",
      "iteration 25520: loss: 0.21979323029518127\n",
      "iteration 25521: loss: 0.2197926938533783\n",
      "iteration 25522: loss: 0.21979224681854248\n",
      "iteration 25523: loss: 0.21979179978370667\n",
      "iteration 25524: loss: 0.21979136765003204\n",
      "iteration 25525: loss: 0.21979084610939026\n",
      "iteration 25526: loss: 0.21979038417339325\n",
      "iteration 25527: loss: 0.21978995203971863\n",
      "iteration 25528: loss: 0.21978946030139923\n",
      "iteration 25529: loss: 0.21978895366191864\n",
      "iteration 25530: loss: 0.21978850662708282\n",
      "iteration 25531: loss: 0.21978804469108582\n",
      "iteration 25532: loss: 0.21978768706321716\n",
      "iteration 25533: loss: 0.21978716552257538\n",
      "iteration 25534: loss: 0.2197866439819336\n",
      "iteration 25535: loss: 0.2197861671447754\n",
      "iteration 25536: loss: 0.21978576481342316\n",
      "iteration 25537: loss: 0.21978521347045898\n",
      "iteration 25538: loss: 0.21978478133678436\n",
      "iteration 25539: loss: 0.21978434920310974\n",
      "iteration 25540: loss: 0.21978382766246796\n",
      "iteration 25541: loss: 0.2197834551334381\n",
      "iteration 25542: loss: 0.21978291869163513\n",
      "iteration 25543: loss: 0.2197825014591217\n",
      "iteration 25544: loss: 0.2197819948196411\n",
      "iteration 25545: loss: 0.2197815477848053\n",
      "iteration 25546: loss: 0.2197810858488083\n",
      "iteration 25547: loss: 0.2197805941104889\n",
      "iteration 25548: loss: 0.2197801172733307\n",
      "iteration 25549: loss: 0.21977970004081726\n",
      "iteration 25550: loss: 0.21977925300598145\n",
      "iteration 25551: loss: 0.21977877616882324\n",
      "iteration 25552: loss: 0.21977829933166504\n",
      "iteration 25553: loss: 0.21977785229682922\n",
      "iteration 25554: loss: 0.21977734565734863\n",
      "iteration 25555: loss: 0.21977689862251282\n",
      "iteration 25556: loss: 0.21977636218070984\n",
      "iteration 25557: loss: 0.2197759598493576\n",
      "iteration 25558: loss: 0.2197755128145218\n",
      "iteration 25559: loss: 0.21977505087852478\n",
      "iteration 25560: loss: 0.21977455914020538\n",
      "iteration 25561: loss: 0.21977408230304718\n",
      "iteration 25562: loss: 0.2197735756635666\n",
      "iteration 25563: loss: 0.21977314352989197\n",
      "iteration 25564: loss: 0.21977269649505615\n",
      "iteration 25565: loss: 0.21977224946022034\n",
      "iteration 25566: loss: 0.21977174282073975\n",
      "iteration 25567: loss: 0.2197713404893875\n",
      "iteration 25568: loss: 0.21977083384990692\n",
      "iteration 25569: loss: 0.21977035701274872\n",
      "iteration 25570: loss: 0.2197699099779129\n",
      "iteration 25571: loss: 0.2197694331407547\n",
      "iteration 25572: loss: 0.2197689712047577\n",
      "iteration 25573: loss: 0.21976852416992188\n",
      "iteration 25574: loss: 0.21976801753044128\n",
      "iteration 25575: loss: 0.21976757049560547\n",
      "iteration 25576: loss: 0.21976712346076965\n",
      "iteration 25577: loss: 0.21976664662361145\n",
      "iteration 25578: loss: 0.21976622939109802\n",
      "iteration 25579: loss: 0.21976570785045624\n",
      "iteration 25580: loss: 0.219765305519104\n",
      "iteration 25581: loss: 0.2197647988796234\n",
      "iteration 25582: loss: 0.21976438164710999\n",
      "iteration 25583: loss: 0.21976390480995178\n",
      "iteration 25584: loss: 0.2197633981704712\n",
      "iteration 25585: loss: 0.21976296603679657\n",
      "iteration 25586: loss: 0.21976251900196075\n",
      "iteration 25587: loss: 0.21976204216480255\n",
      "iteration 25588: loss: 0.21976153552532196\n",
      "iteration 25589: loss: 0.21976108849048615\n",
      "iteration 25590: loss: 0.21976065635681152\n",
      "iteration 25591: loss: 0.21976014971733093\n",
      "iteration 25592: loss: 0.21975970268249512\n",
      "iteration 25593: loss: 0.2197592705488205\n",
      "iteration 25594: loss: 0.2197587937116623\n",
      "iteration 25595: loss: 0.2197583019733429\n",
      "iteration 25596: loss: 0.21975788474082947\n",
      "iteration 25597: loss: 0.21975736320018768\n",
      "iteration 25598: loss: 0.21975693106651306\n",
      "iteration 25599: loss: 0.21975645422935486\n",
      "iteration 25600: loss: 0.21975597739219666\n",
      "iteration 25601: loss: 0.21975557506084442\n",
      "iteration 25602: loss: 0.21975509822368622\n",
      "iteration 25603: loss: 0.21975462138652802\n",
      "iteration 25604: loss: 0.21975412964820862\n",
      "iteration 25605: loss: 0.219753697514534\n",
      "iteration 25606: loss: 0.2197532206773758\n",
      "iteration 25607: loss: 0.21975275874137878\n",
      "iteration 25608: loss: 0.21975235641002655\n",
      "iteration 25609: loss: 0.21975180506706238\n",
      "iteration 25610: loss: 0.21975140273571014\n",
      "iteration 25611: loss: 0.21975092589855194\n",
      "iteration 25612: loss: 0.21975047886371613\n",
      "iteration 25613: loss: 0.2197500467300415\n",
      "iteration 25614: loss: 0.2197495400905609\n",
      "iteration 25615: loss: 0.2197490930557251\n",
      "iteration 25616: loss: 0.2197486162185669\n",
      "iteration 25617: loss: 0.2197481095790863\n",
      "iteration 25618: loss: 0.21974769234657288\n",
      "iteration 25619: loss: 0.21974723041057587\n",
      "iteration 25620: loss: 0.21974675357341766\n",
      "iteration 25621: loss: 0.21974627673625946\n",
      "iteration 25622: loss: 0.21974587440490723\n",
      "iteration 25623: loss: 0.2197454273700714\n",
      "iteration 25624: loss: 0.2197449505329132\n",
      "iteration 25625: loss: 0.2197445183992386\n",
      "iteration 25626: loss: 0.21974404156208038\n",
      "iteration 25627: loss: 0.2197435200214386\n",
      "iteration 25628: loss: 0.2197430431842804\n",
      "iteration 25629: loss: 0.21974262595176697\n",
      "iteration 25630: loss: 0.21974214911460876\n",
      "iteration 25631: loss: 0.21974173188209534\n",
      "iteration 25632: loss: 0.21974126994609833\n",
      "iteration 25633: loss: 0.21974077820777893\n",
      "iteration 25634: loss: 0.21974030137062073\n",
      "iteration 25635: loss: 0.2197398841381073\n",
      "iteration 25636: loss: 0.2197394073009491\n",
      "iteration 25637: loss: 0.21973899006843567\n",
      "iteration 25638: loss: 0.21973848342895508\n",
      "iteration 25639: loss: 0.21973803639411926\n",
      "iteration 25640: loss: 0.21973755955696106\n",
      "iteration 25641: loss: 0.21973714232444763\n",
      "iteration 25642: loss: 0.21973666548728943\n",
      "iteration 25643: loss: 0.21973618865013123\n",
      "iteration 25644: loss: 0.2197357416152954\n",
      "iteration 25645: loss: 0.21973523497581482\n",
      "iteration 25646: loss: 0.2197348177433014\n",
      "iteration 25647: loss: 0.21973438560962677\n",
      "iteration 25648: loss: 0.21973390877246857\n",
      "iteration 25649: loss: 0.21973343193531036\n",
      "iteration 25650: loss: 0.21973299980163574\n",
      "iteration 25651: loss: 0.21973249316215515\n",
      "iteration 25652: loss: 0.21973209083080292\n",
      "iteration 25653: loss: 0.21973161399364471\n",
      "iteration 25654: loss: 0.2197311371564865\n",
      "iteration 25655: loss: 0.21973073482513428\n",
      "iteration 25656: loss: 0.2197302132844925\n",
      "iteration 25657: loss: 0.21972975134849548\n",
      "iteration 25658: loss: 0.21972933411598206\n",
      "iteration 25659: loss: 0.21972879767417908\n",
      "iteration 25660: loss: 0.21972842514514923\n",
      "iteration 25661: loss: 0.21972794830799103\n",
      "iteration 25662: loss: 0.21972747147083282\n",
      "iteration 25663: loss: 0.2197270691394806\n",
      "iteration 25664: loss: 0.2197265625\n",
      "iteration 25665: loss: 0.2197260558605194\n",
      "iteration 25666: loss: 0.21972563862800598\n",
      "iteration 25667: loss: 0.21972520649433136\n",
      "iteration 25668: loss: 0.21972472965717316\n",
      "iteration 25669: loss: 0.21972426772117615\n",
      "iteration 25670: loss: 0.21972379088401794\n",
      "iteration 25671: loss: 0.21972334384918213\n",
      "iteration 25672: loss: 0.2197229415178299\n",
      "iteration 25673: loss: 0.2197224646806717\n",
      "iteration 25674: loss: 0.21972200274467468\n",
      "iteration 25675: loss: 0.21972152590751648\n",
      "iteration 25676: loss: 0.21972103416919708\n",
      "iteration 25677: loss: 0.21972057223320007\n",
      "iteration 25678: loss: 0.21972012519836426\n",
      "iteration 25679: loss: 0.21971969306468964\n",
      "iteration 25680: loss: 0.21971926093101501\n",
      "iteration 25681: loss: 0.2197187840938568\n",
      "iteration 25682: loss: 0.219718337059021\n",
      "iteration 25683: loss: 0.2197178304195404\n",
      "iteration 25684: loss: 0.21971741318702698\n",
      "iteration 25685: loss: 0.21971693634986877\n",
      "iteration 25686: loss: 0.21971647441387177\n",
      "iteration 25687: loss: 0.21971610188484192\n",
      "iteration 25688: loss: 0.2197156399488449\n",
      "iteration 25689: loss: 0.21971507370471954\n",
      "iteration 25690: loss: 0.2197146862745285\n",
      "iteration 25691: loss: 0.2197142094373703\n",
      "iteration 25692: loss: 0.21971383690834045\n",
      "iteration 25693: loss: 0.21971330046653748\n",
      "iteration 25694: loss: 0.21971280872821808\n",
      "iteration 25695: loss: 0.21971240639686584\n",
      "iteration 25696: loss: 0.21971197426319122\n",
      "iteration 25697: loss: 0.2197115123271942\n",
      "iteration 25698: loss: 0.21971102058887482\n",
      "iteration 25699: loss: 0.2197105884552002\n",
      "iteration 25700: loss: 0.2197100669145584\n",
      "iteration 25701: loss: 0.21970967948436737\n",
      "iteration 25702: loss: 0.21970918774604797\n",
      "iteration 25703: loss: 0.21970877051353455\n",
      "iteration 25704: loss: 0.21970832347869873\n",
      "iteration 25705: loss: 0.21970787644386292\n",
      "iteration 25706: loss: 0.21970736980438232\n",
      "iteration 25707: loss: 0.2197069674730301\n",
      "iteration 25708: loss: 0.2197064608335495\n",
      "iteration 25709: loss: 0.21970601379871368\n",
      "iteration 25710: loss: 0.21970553696155548\n",
      "iteration 25711: loss: 0.21970510482788086\n",
      "iteration 25712: loss: 0.21970462799072266\n",
      "iteration 25713: loss: 0.21970415115356445\n",
      "iteration 25714: loss: 0.2197037637233734\n",
      "iteration 25715: loss: 0.2197032868862152\n",
      "iteration 25716: loss: 0.2197028398513794\n",
      "iteration 25717: loss: 0.2197023630142212\n",
      "iteration 25718: loss: 0.21970191597938538\n",
      "iteration 25719: loss: 0.21970143914222717\n",
      "iteration 25720: loss: 0.21970100700855255\n",
      "iteration 25721: loss: 0.21970054507255554\n",
      "iteration 25722: loss: 0.21970012784004211\n",
      "iteration 25723: loss: 0.21969962120056152\n",
      "iteration 25724: loss: 0.21969914436340332\n",
      "iteration 25725: loss: 0.21969875693321228\n",
      "iteration 25726: loss: 0.21969828009605408\n",
      "iteration 25727: loss: 0.21969783306121826\n",
      "iteration 25728: loss: 0.21969740092754364\n",
      "iteration 25729: loss: 0.21969695389270782\n",
      "iteration 25730: loss: 0.21969644725322723\n",
      "iteration 25731: loss: 0.2196960747241974\n",
      "iteration 25732: loss: 0.2196955680847168\n",
      "iteration 25733: loss: 0.21969513595104218\n",
      "iteration 25734: loss: 0.21969464421272278\n",
      "iteration 25735: loss: 0.21969422698020935\n",
      "iteration 25736: loss: 0.21969373524188995\n",
      "iteration 25737: loss: 0.2196933478116989\n",
      "iteration 25738: loss: 0.21969279646873474\n",
      "iteration 25739: loss: 0.2196923941373825\n",
      "iteration 25740: loss: 0.2196919471025467\n",
      "iteration 25741: loss: 0.21969148516654968\n",
      "iteration 25742: loss: 0.21969103813171387\n",
      "iteration 25743: loss: 0.21969060599803925\n",
      "iteration 25744: loss: 0.21969017386436462\n",
      "iteration 25745: loss: 0.21968963742256165\n",
      "iteration 25746: loss: 0.21968920528888702\n",
      "iteration 25747: loss: 0.21968874335289001\n",
      "iteration 25748: loss: 0.2196882963180542\n",
      "iteration 25749: loss: 0.21968784928321838\n",
      "iteration 25750: loss: 0.21968741714954376\n",
      "iteration 25751: loss: 0.21968694031238556\n",
      "iteration 25752: loss: 0.21968647837638855\n",
      "iteration 25753: loss: 0.21968607604503632\n",
      "iteration 25754: loss: 0.21968558430671692\n",
      "iteration 25755: loss: 0.2196851521730423\n",
      "iteration 25756: loss: 0.2196846455335617\n",
      "iteration 25757: loss: 0.21968427300453186\n",
      "iteration 25758: loss: 0.21968379616737366\n",
      "iteration 25759: loss: 0.21968333423137665\n",
      "iteration 25760: loss: 0.21968293190002441\n",
      "iteration 25761: loss: 0.2196824848651886\n",
      "iteration 25762: loss: 0.21968194842338562\n",
      "iteration 25763: loss: 0.2196815013885498\n",
      "iteration 25764: loss: 0.21968106925487518\n",
      "iteration 25765: loss: 0.21968066692352295\n",
      "iteration 25766: loss: 0.21968016028404236\n",
      "iteration 25767: loss: 0.21967974305152893\n",
      "iteration 25768: loss: 0.21967923641204834\n",
      "iteration 25769: loss: 0.2196788340806961\n",
      "iteration 25770: loss: 0.21967840194702148\n",
      "iteration 25771: loss: 0.21967792510986328\n",
      "iteration 25772: loss: 0.21967744827270508\n",
      "iteration 25773: loss: 0.21967704594135284\n",
      "iteration 25774: loss: 0.21967661380767822\n",
      "iteration 25775: loss: 0.21967609226703644\n",
      "iteration 25776: loss: 0.21967573463916779\n",
      "iteration 25777: loss: 0.2196752279996872\n",
      "iteration 25778: loss: 0.21967478096485138\n",
      "iteration 25779: loss: 0.21967434883117676\n",
      "iteration 25780: loss: 0.21967387199401855\n",
      "iteration 25781: loss: 0.21967339515686035\n",
      "iteration 25782: loss: 0.2196730077266693\n",
      "iteration 25783: loss: 0.21967248618602753\n",
      "iteration 25784: loss: 0.2196720540523529\n",
      "iteration 25785: loss: 0.21967165172100067\n",
      "iteration 25786: loss: 0.21967120468616486\n",
      "iteration 25787: loss: 0.21967072784900665\n",
      "iteration 25788: loss: 0.21967025101184845\n",
      "iteration 25789: loss: 0.21966978907585144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 25790: loss: 0.219669371843338\n",
      "iteration 25791: loss: 0.2196689397096634\n",
      "iteration 25792: loss: 0.2196684181690216\n",
      "iteration 25793: loss: 0.21966807544231415\n",
      "iteration 25794: loss: 0.2196674793958664\n",
      "iteration 25795: loss: 0.21966712176799774\n",
      "iteration 25796: loss: 0.21966668963432312\n",
      "iteration 25797: loss: 0.21966619789600372\n",
      "iteration 25798: loss: 0.2196657657623291\n",
      "iteration 25799: loss: 0.21966536343097687\n",
      "iteration 25800: loss: 0.21966485679149628\n",
      "iteration 25801: loss: 0.21966440975666046\n",
      "iteration 25802: loss: 0.21966394782066345\n",
      "iteration 25803: loss: 0.21966353058815002\n",
      "iteration 25804: loss: 0.21966302394866943\n",
      "iteration 25805: loss: 0.21966269612312317\n",
      "iteration 25806: loss: 0.219662144780159\n",
      "iteration 25807: loss: 0.2196616679430008\n",
      "iteration 25808: loss: 0.21966128051280975\n",
      "iteration 25809: loss: 0.21966084837913513\n",
      "iteration 25810: loss: 0.21966040134429932\n",
      "iteration 25811: loss: 0.2196599543094635\n",
      "iteration 25812: loss: 0.2196594774723053\n",
      "iteration 25813: loss: 0.2196590006351471\n",
      "iteration 25814: loss: 0.21965861320495605\n",
      "iteration 25815: loss: 0.21965818107128143\n",
      "iteration 25816: loss: 0.21965765953063965\n",
      "iteration 25817: loss: 0.21965721249580383\n",
      "iteration 25818: loss: 0.2196568250656128\n",
      "iteration 25819: loss: 0.219656303524971\n",
      "iteration 25820: loss: 0.21965591609477997\n",
      "iteration 25821: loss: 0.21965542435646057\n",
      "iteration 25822: loss: 0.21965503692626953\n",
      "iteration 25823: loss: 0.2196546047925949\n",
      "iteration 25824: loss: 0.2196541130542755\n",
      "iteration 25825: loss: 0.2196536362171173\n",
      "iteration 25826: loss: 0.21965321898460388\n",
      "iteration 25827: loss: 0.21965274214744568\n",
      "iteration 25828: loss: 0.21965236961841583\n",
      "iteration 25829: loss: 0.21965189278125763\n",
      "iteration 25830: loss: 0.21965137124061584\n",
      "iteration 25831: loss: 0.2196509838104248\n",
      "iteration 25832: loss: 0.21965058147907257\n",
      "iteration 25833: loss: 0.21965007483959198\n",
      "iteration 25834: loss: 0.21964962780475616\n",
      "iteration 25835: loss: 0.21964924037456512\n",
      "iteration 25836: loss: 0.21964876353740692\n",
      "iteration 25837: loss: 0.2196483314037323\n",
      "iteration 25838: loss: 0.21964788436889648\n",
      "iteration 25839: loss: 0.2196473777294159\n",
      "iteration 25840: loss: 0.21964697539806366\n",
      "iteration 25841: loss: 0.21964648365974426\n",
      "iteration 25842: loss: 0.21964606642723083\n",
      "iteration 25843: loss: 0.21964561939239502\n",
      "iteration 25844: loss: 0.2196451872587204\n",
      "iteration 25845: loss: 0.2196447104215622\n",
      "iteration 25846: loss: 0.21964426338672638\n",
      "iteration 25847: loss: 0.21964383125305176\n",
      "iteration 25848: loss: 0.21964339911937714\n",
      "iteration 25849: loss: 0.21964287757873535\n",
      "iteration 25850: loss: 0.2196424901485443\n",
      "iteration 25851: loss: 0.2196420431137085\n",
      "iteration 25852: loss: 0.21964159607887268\n",
      "iteration 25853: loss: 0.21964113414287567\n",
      "iteration 25854: loss: 0.21964070200920105\n",
      "iteration 25855: loss: 0.21964025497436523\n",
      "iteration 25856: loss: 0.21963977813720703\n",
      "iteration 25857: loss: 0.2196393758058548\n",
      "iteration 25858: loss: 0.2196388691663742\n",
      "iteration 25859: loss: 0.21963849663734436\n",
      "iteration 25860: loss: 0.21963807940483093\n",
      "iteration 25861: loss: 0.21963755786418915\n",
      "iteration 25862: loss: 0.21963712573051453\n",
      "iteration 25863: loss: 0.2196367233991623\n",
      "iteration 25864: loss: 0.21963635087013245\n",
      "iteration 25865: loss: 0.21963579952716827\n",
      "iteration 25866: loss: 0.21963533759117126\n",
      "iteration 25867: loss: 0.21963496506214142\n",
      "iteration 25868: loss: 0.21963444352149963\n",
      "iteration 25869: loss: 0.2196340560913086\n",
      "iteration 25870: loss: 0.21963362395763397\n",
      "iteration 25871: loss: 0.21963317692279816\n",
      "iteration 25872: loss: 0.21963267028331757\n",
      "iteration 25873: loss: 0.21963226795196533\n",
      "iteration 25874: loss: 0.2196318358182907\n",
      "iteration 25875: loss: 0.2196313589811325\n",
      "iteration 25876: loss: 0.2196309119462967\n",
      "iteration 25877: loss: 0.21963045001029968\n",
      "iteration 25878: loss: 0.21963000297546387\n",
      "iteration 25879: loss: 0.21962957084178925\n",
      "iteration 25880: loss: 0.2196292132139206\n",
      "iteration 25881: loss: 0.2196287214756012\n",
      "iteration 25882: loss: 0.21962830424308777\n",
      "iteration 25883: loss: 0.21962782740592957\n",
      "iteration 25884: loss: 0.21962742507457733\n",
      "iteration 25885: loss: 0.21962693333625793\n",
      "iteration 25886: loss: 0.21962647140026093\n",
      "iteration 25887: loss: 0.2196260243654251\n",
      "iteration 25888: loss: 0.2196255624294281\n",
      "iteration 25889: loss: 0.21962514519691467\n",
      "iteration 25890: loss: 0.21962475776672363\n",
      "iteration 25891: loss: 0.21962423622608185\n",
      "iteration 25892: loss: 0.21962377429008484\n",
      "iteration 25893: loss: 0.219623401761055\n",
      "iteration 25894: loss: 0.21962296962738037\n",
      "iteration 25895: loss: 0.21962249279022217\n",
      "iteration 25896: loss: 0.21962210536003113\n",
      "iteration 25897: loss: 0.21962158381938934\n",
      "iteration 25898: loss: 0.2196211814880371\n",
      "iteration 25899: loss: 0.2196207493543625\n",
      "iteration 25900: loss: 0.21962031722068787\n",
      "iteration 25901: loss: 0.21961984038352966\n",
      "iteration 25902: loss: 0.21961946785449982\n",
      "iteration 25903: loss: 0.21961894631385803\n",
      "iteration 25904: loss: 0.2196185290813446\n",
      "iteration 25905: loss: 0.2196180373430252\n",
      "iteration 25906: loss: 0.21961764991283417\n",
      "iteration 25907: loss: 0.21961721777915955\n",
      "iteration 25908: loss: 0.21961677074432373\n",
      "iteration 25909: loss: 0.2196163386106491\n",
      "iteration 25910: loss: 0.2196158617734909\n",
      "iteration 25911: loss: 0.2196154147386551\n",
      "iteration 25912: loss: 0.21961501240730286\n",
      "iteration 25913: loss: 0.21961455047130585\n",
      "iteration 25914: loss: 0.21961411833763123\n",
      "iteration 25915: loss: 0.219613716006279\n",
      "iteration 25916: loss: 0.21961316466331482\n",
      "iteration 25917: loss: 0.21961279213428497\n",
      "iteration 25918: loss: 0.21961236000061035\n",
      "iteration 25919: loss: 0.21961191296577454\n",
      "iteration 25920: loss: 0.21961145102977753\n",
      "iteration 25921: loss: 0.2196110188961029\n",
      "iteration 25922: loss: 0.21961060166358948\n",
      "iteration 25923: loss: 0.21961013972759247\n",
      "iteration 25924: loss: 0.21960970759391785\n",
      "iteration 25925: loss: 0.21960921585559845\n",
      "iteration 25926: loss: 0.21960873901844025\n",
      "iteration 25927: loss: 0.219608336687088\n",
      "iteration 25928: loss: 0.2196079045534134\n",
      "iteration 25929: loss: 0.21960744261741638\n",
      "iteration 25930: loss: 0.21960707008838654\n",
      "iteration 25931: loss: 0.21960659325122833\n",
      "iteration 25932: loss: 0.2196061611175537\n",
      "iteration 25933: loss: 0.2196056842803955\n",
      "iteration 25934: loss: 0.21960528194904327\n",
      "iteration 25935: loss: 0.21960482001304626\n",
      "iteration 25936: loss: 0.21960437297821045\n",
      "iteration 25937: loss: 0.21960397064685822\n",
      "iteration 25938: loss: 0.21960346400737762\n",
      "iteration 25939: loss: 0.2196030616760254\n",
      "iteration 25940: loss: 0.21960265934467316\n",
      "iteration 25941: loss: 0.21960222721099854\n",
      "iteration 25942: loss: 0.2196017950773239\n",
      "iteration 25943: loss: 0.2196013480424881\n",
      "iteration 25944: loss: 0.2196008265018463\n",
      "iteration 25945: loss: 0.21960040926933289\n",
      "iteration 25946: loss: 0.21960000693798065\n",
      "iteration 25947: loss: 0.21959953010082245\n",
      "iteration 25948: loss: 0.21959905326366425\n",
      "iteration 25949: loss: 0.219598650932312\n",
      "iteration 25950: loss: 0.21959824860095978\n",
      "iteration 25951: loss: 0.21959777176380157\n",
      "iteration 25952: loss: 0.21959736943244934\n",
      "iteration 25953: loss: 0.21959686279296875\n",
      "iteration 25954: loss: 0.21959646046161652\n",
      "iteration 25955: loss: 0.2195959985256195\n",
      "iteration 25956: loss: 0.21959558129310608\n",
      "iteration 25957: loss: 0.21959522366523743\n",
      "iteration 25958: loss: 0.21959471702575684\n",
      "iteration 25959: loss: 0.21959428489208221\n",
      "iteration 25960: loss: 0.2195938378572464\n",
      "iteration 25961: loss: 0.2195933759212494\n",
      "iteration 25962: loss: 0.21959297358989716\n",
      "iteration 25963: loss: 0.21959252655506134\n",
      "iteration 25964: loss: 0.21959206461906433\n",
      "iteration 25965: loss: 0.21959161758422852\n",
      "iteration 25966: loss: 0.21959123015403748\n",
      "iteration 25967: loss: 0.21959073841571808\n",
      "iteration 25968: loss: 0.21959033608436584\n",
      "iteration 25969: loss: 0.21958990395069122\n",
      "iteration 25970: loss: 0.21958942711353302\n",
      "iteration 25971: loss: 0.219588965177536\n",
      "iteration 25972: loss: 0.21958860754966736\n",
      "iteration 25973: loss: 0.21958820521831512\n",
      "iteration 25974: loss: 0.21958771347999573\n",
      "iteration 25975: loss: 0.2195873260498047\n",
      "iteration 25976: loss: 0.2195868045091629\n",
      "iteration 25977: loss: 0.21958640217781067\n",
      "iteration 25978: loss: 0.21958594024181366\n",
      "iteration 25979: loss: 0.21958553791046143\n",
      "iteration 25980: loss: 0.2195850908756256\n",
      "iteration 25981: loss: 0.21958470344543457\n",
      "iteration 25982: loss: 0.2195841521024704\n",
      "iteration 25983: loss: 0.21958377957344055\n",
      "iteration 25984: loss: 0.21958331763744354\n",
      "iteration 25985: loss: 0.21958284080028534\n",
      "iteration 25986: loss: 0.2195824384689331\n",
      "iteration 25987: loss: 0.21958200633525848\n",
      "iteration 25988: loss: 0.21958155930042267\n",
      "iteration 25989: loss: 0.21958115696907043\n",
      "iteration 25990: loss: 0.219580739736557\n",
      "iteration 25991: loss: 0.2195802628993988\n",
      "iteration 25992: loss: 0.219579815864563\n",
      "iteration 25993: loss: 0.21957936882972717\n",
      "iteration 25994: loss: 0.21957898139953613\n",
      "iteration 25995: loss: 0.21957853436470032\n",
      "iteration 25996: loss: 0.2195780724287033\n",
      "iteration 25997: loss: 0.2195776402950287\n",
      "iteration 25998: loss: 0.21957726776599884\n",
      "iteration 25999: loss: 0.21957676112651825\n",
      "iteration 26000: loss: 0.21957631409168243\n",
      "iteration 26001: loss: 0.21957585215568542\n",
      "iteration 26002: loss: 0.219575434923172\n",
      "iteration 26003: loss: 0.21957501769065857\n",
      "iteration 26004: loss: 0.21957461535930634\n",
      "iteration 26005: loss: 0.21957413852214813\n",
      "iteration 26006: loss: 0.21957378089427948\n",
      "iteration 26007: loss: 0.21957330405712128\n",
      "iteration 26008: loss: 0.21957282721996307\n",
      "iteration 26009: loss: 0.21957238018512726\n",
      "iteration 26010: loss: 0.2195720225572586\n",
      "iteration 26011: loss: 0.2195715457201004\n",
      "iteration 26012: loss: 0.21957111358642578\n",
      "iteration 26013: loss: 0.21957063674926758\n",
      "iteration 26014: loss: 0.21957027912139893\n",
      "iteration 26015: loss: 0.21956980228424072\n",
      "iteration 26016: loss: 0.2195693701505661\n",
      "iteration 26017: loss: 0.21956893801689148\n",
      "iteration 26018: loss: 0.21956852078437805\n",
      "iteration 26019: loss: 0.21956805884838104\n",
      "iteration 26020: loss: 0.21956761181354523\n",
      "iteration 26021: loss: 0.21956725418567657\n",
      "iteration 26022: loss: 0.21956677734851837\n",
      "iteration 26023: loss: 0.21956637501716614\n",
      "iteration 26024: loss: 0.21956586837768555\n",
      "iteration 26025: loss: 0.2195654660463333\n",
      "iteration 26026: loss: 0.21956495940685272\n",
      "iteration 26027: loss: 0.21956464648246765\n",
      "iteration 26028: loss: 0.21956415474414825\n",
      "iteration 26029: loss: 0.21956369280815125\n",
      "iteration 26030: loss: 0.21956327557563782\n",
      "iteration 26031: loss: 0.2195628583431244\n",
      "iteration 26032: loss: 0.2195623815059662\n",
      "iteration 26033: loss: 0.21956193447113037\n",
      "iteration 26034: loss: 0.21956154704093933\n",
      "iteration 26035: loss: 0.21956110000610352\n",
      "iteration 26036: loss: 0.2195606678724289\n",
      "iteration 26037: loss: 0.21956022083759308\n",
      "iteration 26038: loss: 0.21955975890159607\n",
      "iteration 26039: loss: 0.21955935657024384\n",
      "iteration 26040: loss: 0.21955890953540802\n",
      "iteration 26041: loss: 0.2195585072040558\n",
      "iteration 26042: loss: 0.21955803036689758\n",
      "iteration 26043: loss: 0.21955764293670654\n",
      "iteration 26044: loss: 0.21955719590187073\n",
      "iteration 26045: loss: 0.2195567786693573\n",
      "iteration 26046: loss: 0.21955633163452148\n",
      "iteration 26047: loss: 0.21955589950084686\n",
      "iteration 26048: loss: 0.21955546736717224\n",
      "iteration 26049: loss: 0.2195550948381424\n",
      "iteration 26050: loss: 0.2195546180009842\n",
      "iteration 26051: loss: 0.219554141163826\n",
      "iteration 26052: loss: 0.21955375373363495\n",
      "iteration 26053: loss: 0.21955330669879913\n",
      "iteration 26054: loss: 0.21955284476280212\n",
      "iteration 26055: loss: 0.2195524424314499\n",
      "iteration 26056: loss: 0.21955199539661407\n",
      "iteration 26057: loss: 0.21955153346061707\n",
      "iteration 26058: loss: 0.21955116093158722\n",
      "iteration 26059: loss: 0.2195506989955902\n",
      "iteration 26060: loss: 0.21955028176307678\n",
      "iteration 26061: loss: 0.21954986453056335\n",
      "iteration 26062: loss: 0.2195495069026947\n",
      "iteration 26063: loss: 0.2195490151643753\n",
      "iteration 26064: loss: 0.21954850852489471\n",
      "iteration 26065: loss: 0.21954813599586487\n",
      "iteration 26066: loss: 0.21954767405986786\n",
      "iteration 26067: loss: 0.21954724192619324\n",
      "iteration 26068: loss: 0.219546839594841\n",
      "iteration 26069: loss: 0.2195463478565216\n",
      "iteration 26070: loss: 0.21954596042633057\n",
      "iteration 26071: loss: 0.21954552829265594\n",
      "iteration 26072: loss: 0.21954508125782013\n",
      "iteration 26073: loss: 0.2195446938276291\n",
      "iteration 26074: loss: 0.21954424679279327\n",
      "iteration 26075: loss: 0.21954378485679626\n",
      "iteration 26076: loss: 0.21954338252544403\n",
      "iteration 26077: loss: 0.21954293549060822\n",
      "iteration 26078: loss: 0.2195425033569336\n",
      "iteration 26079: loss: 0.21954211592674255\n",
      "iteration 26080: loss: 0.21954166889190674\n",
      "iteration 26081: loss: 0.21954122185707092\n",
      "iteration 26082: loss: 0.2195408046245575\n",
      "iteration 26083: loss: 0.2195403277873993\n",
      "iteration 26084: loss: 0.21953992545604706\n",
      "iteration 26085: loss: 0.21953944861888885\n",
      "iteration 26086: loss: 0.219539076089859\n",
      "iteration 26087: loss: 0.219538614153862\n",
      "iteration 26088: loss: 0.21953824162483215\n",
      "iteration 26089: loss: 0.21953773498535156\n",
      "iteration 26090: loss: 0.21953730285167694\n",
      "iteration 26091: loss: 0.21953687071800232\n",
      "iteration 26092: loss: 0.2195364534854889\n",
      "iteration 26093: loss: 0.21953606605529785\n",
      "iteration 26094: loss: 0.21953563392162323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 26095: loss: 0.21953515708446503\n",
      "iteration 26096: loss: 0.21953478455543518\n",
      "iteration 26097: loss: 0.21953430771827698\n",
      "iteration 26098: loss: 0.21953392028808594\n",
      "iteration 26099: loss: 0.21953347325325012\n",
      "iteration 26100: loss: 0.2195330411195755\n",
      "iteration 26101: loss: 0.21953260898590088\n",
      "iteration 26102: loss: 0.21953216195106506\n",
      "iteration 26103: loss: 0.21953174471855164\n",
      "iteration 26104: loss: 0.21953129768371582\n",
      "iteration 26105: loss: 0.21953085064888\n",
      "iteration 26106: loss: 0.21953043341636658\n",
      "iteration 26107: loss: 0.21953001618385315\n",
      "iteration 26108: loss: 0.21952959895133972\n",
      "iteration 26109: loss: 0.2195291519165039\n",
      "iteration 26110: loss: 0.2195286750793457\n",
      "iteration 26111: loss: 0.21952834725379944\n",
      "iteration 26112: loss: 0.21952787041664124\n",
      "iteration 26113: loss: 0.2195274829864502\n",
      "iteration 26114: loss: 0.219527006149292\n",
      "iteration 26115: loss: 0.21952661871910095\n",
      "iteration 26116: loss: 0.21952617168426514\n",
      "iteration 26117: loss: 0.21952572464942932\n",
      "iteration 26118: loss: 0.2195252627134323\n",
      "iteration 26119: loss: 0.21952486038208008\n",
      "iteration 26120: loss: 0.21952442824840546\n",
      "iteration 26121: loss: 0.2195240557193756\n",
      "iteration 26122: loss: 0.21952351927757263\n",
      "iteration 26123: loss: 0.21952319145202637\n",
      "iteration 26124: loss: 0.21952271461486816\n",
      "iteration 26125: loss: 0.21952232718467712\n",
      "iteration 26126: loss: 0.2195218801498413\n",
      "iteration 26127: loss: 0.2195214480161667\n",
      "iteration 26128: loss: 0.21952104568481445\n",
      "iteration 26129: loss: 0.21952052414417267\n",
      "iteration 26130: loss: 0.21952013671398163\n",
      "iteration 26131: loss: 0.21951976418495178\n",
      "iteration 26132: loss: 0.2195192575454712\n",
      "iteration 26133: loss: 0.21951887011528015\n",
      "iteration 26134: loss: 0.21951839327812195\n",
      "iteration 26135: loss: 0.2195180356502533\n",
      "iteration 26136: loss: 0.21951761841773987\n",
      "iteration 26137: loss: 0.21951715648174286\n",
      "iteration 26138: loss: 0.21951675415039062\n",
      "iteration 26139: loss: 0.219516322016716\n",
      "iteration 26140: loss: 0.21951588988304138\n",
      "iteration 26141: loss: 0.21951547265052795\n",
      "iteration 26142: loss: 0.21951504051685333\n",
      "iteration 26143: loss: 0.21951457858085632\n",
      "iteration 26144: loss: 0.2195141762495041\n",
      "iteration 26145: loss: 0.21951372921466827\n",
      "iteration 26146: loss: 0.21951326727867126\n",
      "iteration 26147: loss: 0.21951286494731903\n",
      "iteration 26148: loss: 0.21951249241828918\n",
      "iteration 26149: loss: 0.21951201558113098\n",
      "iteration 26150: loss: 0.21951167285442352\n",
      "iteration 26151: loss: 0.21951115131378174\n",
      "iteration 26152: loss: 0.21951079368591309\n",
      "iteration 26153: loss: 0.21951031684875488\n",
      "iteration 26154: loss: 0.21950991451740265\n",
      "iteration 26155: loss: 0.21950948238372803\n",
      "iteration 26156: loss: 0.2195090353488922\n",
      "iteration 26157: loss: 0.21950864791870117\n",
      "iteration 26158: loss: 0.21950821578502655\n",
      "iteration 26159: loss: 0.21950776875019073\n",
      "iteration 26160: loss: 0.2195073366165161\n",
      "iteration 26161: loss: 0.2195068895816803\n",
      "iteration 26162: loss: 0.21950650215148926\n",
      "iteration 26163: loss: 0.21950607001781464\n",
      "iteration 26164: loss: 0.21950563788414001\n",
      "iteration 26165: loss: 0.21950526535511017\n",
      "iteration 26166: loss: 0.21950480341911316\n",
      "iteration 26167: loss: 0.21950440108776093\n",
      "iteration 26168: loss: 0.21950392425060272\n",
      "iteration 26169: loss: 0.2195034921169281\n",
      "iteration 26170: loss: 0.21950311958789825\n",
      "iteration 26171: loss: 0.21950265765190125\n",
      "iteration 26172: loss: 0.21950221061706543\n",
      "iteration 26173: loss: 0.21950188279151917\n",
      "iteration 26174: loss: 0.21950140595436096\n",
      "iteration 26175: loss: 0.21950094401836395\n",
      "iteration 26176: loss: 0.21950049698352814\n",
      "iteration 26177: loss: 0.21950006484985352\n",
      "iteration 26178: loss: 0.21949967741966248\n",
      "iteration 26179: loss: 0.21949926018714905\n",
      "iteration 26180: loss: 0.219498872756958\n",
      "iteration 26181: loss: 0.2194984406232834\n",
      "iteration 26182: loss: 0.21949796378612518\n",
      "iteration 26183: loss: 0.21949759125709534\n",
      "iteration 26184: loss: 0.21949711441993713\n",
      "iteration 26185: loss: 0.2194967269897461\n",
      "iteration 26186: loss: 0.21949627995491028\n",
      "iteration 26187: loss: 0.21949589252471924\n",
      "iteration 26188: loss: 0.21949546039104462\n",
      "iteration 26189: loss: 0.21949502825737\n",
      "iteration 26190: loss: 0.21949461102485657\n",
      "iteration 26191: loss: 0.21949417889118195\n",
      "iteration 26192: loss: 0.21949371695518494\n",
      "iteration 26193: loss: 0.2194932997226715\n",
      "iteration 26194: loss: 0.2194928675889969\n",
      "iteration 26195: loss: 0.21949250996112823\n",
      "iteration 26196: loss: 0.21949204802513123\n",
      "iteration 26197: loss: 0.2194916307926178\n",
      "iteration 26198: loss: 0.21949119865894318\n",
      "iteration 26199: loss: 0.21949079632759094\n",
      "iteration 26200: loss: 0.21949033439159393\n",
      "iteration 26201: loss: 0.21948988735675812\n",
      "iteration 26202: loss: 0.21948948502540588\n",
      "iteration 26203: loss: 0.21948906779289246\n",
      "iteration 26204: loss: 0.21948866546154022\n",
      "iteration 26205: loss: 0.2194882333278656\n",
      "iteration 26206: loss: 0.21948781609535217\n",
      "iteration 26207: loss: 0.21948735415935516\n",
      "iteration 26208: loss: 0.21948695182800293\n",
      "iteration 26209: loss: 0.2194865196943283\n",
      "iteration 26210: loss: 0.21948611736297607\n",
      "iteration 26211: loss: 0.21948572993278503\n",
      "iteration 26212: loss: 0.21948525309562683\n",
      "iteration 26213: loss: 0.21948489546775818\n",
      "iteration 26214: loss: 0.21948440372943878\n",
      "iteration 26215: loss: 0.21948400139808655\n",
      "iteration 26216: loss: 0.21948358416557312\n",
      "iteration 26217: loss: 0.2194831371307373\n",
      "iteration 26218: loss: 0.21948270499706268\n",
      "iteration 26219: loss: 0.21948230266571045\n",
      "iteration 26220: loss: 0.21948185563087463\n",
      "iteration 26221: loss: 0.21948149800300598\n",
      "iteration 26222: loss: 0.21948108077049255\n",
      "iteration 26223: loss: 0.21948055922985077\n",
      "iteration 26224: loss: 0.21948018670082092\n",
      "iteration 26225: loss: 0.21947979927062988\n",
      "iteration 26226: loss: 0.21947932243347168\n",
      "iteration 26227: loss: 0.21947893500328064\n",
      "iteration 26228: loss: 0.21947848796844482\n",
      "iteration 26229: loss: 0.21947813034057617\n",
      "iteration 26230: loss: 0.21947768330574036\n",
      "iteration 26231: loss: 0.21947722136974335\n",
      "iteration 26232: loss: 0.2194768637418747\n",
      "iteration 26233: loss: 0.2194763869047165\n",
      "iteration 26234: loss: 0.21947598457336426\n",
      "iteration 26235: loss: 0.21947555243968964\n",
      "iteration 26236: loss: 0.21947507560253143\n",
      "iteration 26237: loss: 0.21947471797466278\n",
      "iteration 26238: loss: 0.21947427093982697\n",
      "iteration 26239: loss: 0.21947388350963593\n",
      "iteration 26240: loss: 0.21947340667247772\n",
      "iteration 26241: loss: 0.21947303414344788\n",
      "iteration 26242: loss: 0.21947264671325684\n",
      "iteration 26243: loss: 0.21947219967842102\n",
      "iteration 26244: loss: 0.219471737742424\n",
      "iteration 26245: loss: 0.21947136521339417\n",
      "iteration 26246: loss: 0.21947094798088074\n",
      "iteration 26247: loss: 0.21947047114372253\n",
      "iteration 26248: loss: 0.21947011351585388\n",
      "iteration 26249: loss: 0.21946969628334045\n",
      "iteration 26250: loss: 0.21946923434734344\n",
      "iteration 26251: loss: 0.2194688767194748\n",
      "iteration 26252: loss: 0.21946832537651062\n",
      "iteration 26253: loss: 0.2194679081439972\n",
      "iteration 26254: loss: 0.21946759521961212\n",
      "iteration 26255: loss: 0.2194671332836151\n",
      "iteration 26256: loss: 0.21946673095226288\n",
      "iteration 26257: loss: 0.21946632862091064\n",
      "iteration 26258: loss: 0.21946585178375244\n",
      "iteration 26259: loss: 0.2194654494524002\n",
      "iteration 26260: loss: 0.21946504712104797\n",
      "iteration 26261: loss: 0.21946462988853455\n",
      "iteration 26262: loss: 0.2194642275571823\n",
      "iteration 26263: loss: 0.2194637507200241\n",
      "iteration 26264: loss: 0.2194633185863495\n",
      "iteration 26265: loss: 0.21946291625499725\n",
      "iteration 26266: loss: 0.21946248412132263\n",
      "iteration 26267: loss: 0.2194620668888092\n",
      "iteration 26268: loss: 0.21946172416210175\n",
      "iteration 26269: loss: 0.21946129202842712\n",
      "iteration 26270: loss: 0.2194608449935913\n",
      "iteration 26271: loss: 0.2194603979587555\n",
      "iteration 26272: loss: 0.21946001052856445\n",
      "iteration 26273: loss: 0.21945957839488983\n",
      "iteration 26274: loss: 0.21945913136005402\n",
      "iteration 26275: loss: 0.21945874392986298\n",
      "iteration 26276: loss: 0.21945826709270477\n",
      "iteration 26277: loss: 0.2194579392671585\n",
      "iteration 26278: loss: 0.2194574773311615\n",
      "iteration 26279: loss: 0.21945703029632568\n",
      "iteration 26280: loss: 0.21945667266845703\n",
      "iteration 26281: loss: 0.2194562405347824\n",
      "iteration 26282: loss: 0.2194558084011078\n",
      "iteration 26283: loss: 0.21945536136627197\n",
      "iteration 26284: loss: 0.21945491433143616\n",
      "iteration 26285: loss: 0.2194546014070511\n",
      "iteration 26286: loss: 0.21945413947105408\n",
      "iteration 26287: loss: 0.21945366263389587\n",
      "iteration 26288: loss: 0.2194533348083496\n",
      "iteration 26289: loss: 0.219452902674675\n",
      "iteration 26290: loss: 0.21945247054100037\n",
      "iteration 26291: loss: 0.21945209801197052\n",
      "iteration 26292: loss: 0.21945154666900635\n",
      "iteration 26293: loss: 0.21945123374462128\n",
      "iteration 26294: loss: 0.21945075690746307\n",
      "iteration 26295: loss: 0.21945035457611084\n",
      "iteration 26296: loss: 0.2194499522447586\n",
      "iteration 26297: loss: 0.21944956481456757\n",
      "iteration 26298: loss: 0.21944916248321533\n",
      "iteration 26299: loss: 0.2194487303495407\n",
      "iteration 26300: loss: 0.2194482833147049\n",
      "iteration 26301: loss: 0.21944782137870789\n",
      "iteration 26302: loss: 0.219447523355484\n",
      "iteration 26303: loss: 0.2194470912218094\n",
      "iteration 26304: loss: 0.21944665908813477\n",
      "iteration 26305: loss: 0.21944621205329895\n",
      "iteration 26306: loss: 0.21944575011730194\n",
      "iteration 26307: loss: 0.2194453775882721\n",
      "iteration 26308: loss: 0.21944494545459747\n",
      "iteration 26309: loss: 0.21944455802440643\n",
      "iteration 26310: loss: 0.21944411098957062\n",
      "iteration 26311: loss: 0.21944372355937958\n",
      "iteration 26312: loss: 0.21944327652454376\n",
      "iteration 26313: loss: 0.21944287419319153\n",
      "iteration 26314: loss: 0.2194424420595169\n",
      "iteration 26315: loss: 0.21944205462932587\n",
      "iteration 26316: loss: 0.21944160759449005\n",
      "iteration 26317: loss: 0.2194412648677826\n",
      "iteration 26318: loss: 0.2194407433271408\n",
      "iteration 26319: loss: 0.21944038569927216\n",
      "iteration 26320: loss: 0.21943995356559753\n",
      "iteration 26321: loss: 0.21943950653076172\n",
      "iteration 26322: loss: 0.21943914890289307\n",
      "iteration 26323: loss: 0.21943870186805725\n",
      "iteration 26324: loss: 0.2194383144378662\n",
      "iteration 26325: loss: 0.2194378674030304\n",
      "iteration 26326: loss: 0.21943743526935577\n",
      "iteration 26327: loss: 0.21943707764148712\n",
      "iteration 26328: loss: 0.2194366455078125\n",
      "iteration 26329: loss: 0.21943621337413788\n",
      "iteration 26330: loss: 0.21943585574626923\n",
      "iteration 26331: loss: 0.21943537890911102\n",
      "iteration 26332: loss: 0.2194349765777588\n",
      "iteration 26333: loss: 0.21943454444408417\n",
      "iteration 26334: loss: 0.21943414211273193\n",
      "iteration 26335: loss: 0.2194337546825409\n",
      "iteration 26336: loss: 0.2194332778453827\n",
      "iteration 26337: loss: 0.21943283081054688\n",
      "iteration 26338: loss: 0.21943244338035583\n",
      "iteration 26339: loss: 0.219432070851326\n",
      "iteration 26340: loss: 0.21943163871765137\n",
      "iteration 26341: loss: 0.21943123638629913\n",
      "iteration 26342: loss: 0.2194308042526245\n",
      "iteration 26343: loss: 0.2194303572177887\n",
      "iteration 26344: loss: 0.21942996978759766\n",
      "iteration 26345: loss: 0.21942956745624542\n",
      "iteration 26346: loss: 0.2194291651248932\n",
      "iteration 26347: loss: 0.21942873299121857\n",
      "iteration 26348: loss: 0.21942827105522156\n",
      "iteration 26349: loss: 0.2194279432296753\n",
      "iteration 26350: loss: 0.21942749619483948\n",
      "iteration 26351: loss: 0.21942707896232605\n",
      "iteration 26352: loss: 0.21942667663097382\n",
      "iteration 26353: loss: 0.2194262444972992\n",
      "iteration 26354: loss: 0.21942582726478577\n",
      "iteration 26355: loss: 0.21942546963691711\n",
      "iteration 26356: loss: 0.2194250077009201\n",
      "iteration 26357: loss: 0.21942457556724548\n",
      "iteration 26358: loss: 0.21942420303821564\n",
      "iteration 26359: loss: 0.21942377090454102\n",
      "iteration 26360: loss: 0.21942336857318878\n",
      "iteration 26361: loss: 0.21942290663719177\n",
      "iteration 26362: loss: 0.21942253410816193\n",
      "iteration 26363: loss: 0.2194221317768097\n",
      "iteration 26364: loss: 0.21942166984081268\n",
      "iteration 26365: loss: 0.21942131221294403\n",
      "iteration 26366: loss: 0.21942082047462463\n",
      "iteration 26367: loss: 0.21942047774791718\n",
      "iteration 26368: loss: 0.21942003071308136\n",
      "iteration 26369: loss: 0.21941959857940674\n",
      "iteration 26370: loss: 0.2194191962480545\n",
      "iteration 26371: loss: 0.21941885352134705\n",
      "iteration 26372: loss: 0.21941837668418884\n",
      "iteration 26373: loss: 0.21941795945167542\n",
      "iteration 26374: loss: 0.21941757202148438\n",
      "iteration 26375: loss: 0.21941718459129333\n",
      "iteration 26376: loss: 0.2194167673587799\n",
      "iteration 26377: loss: 0.2194163054227829\n",
      "iteration 26378: loss: 0.21941587328910828\n",
      "iteration 26379: loss: 0.21941545605659485\n",
      "iteration 26380: loss: 0.2194150984287262\n",
      "iteration 26381: loss: 0.21941471099853516\n",
      "iteration 26382: loss: 0.21941423416137695\n",
      "iteration 26383: loss: 0.2194138467311859\n",
      "iteration 26384: loss: 0.21941344439983368\n",
      "iteration 26385: loss: 0.21941304206848145\n",
      "iteration 26386: loss: 0.2194126397371292\n",
      "iteration 26387: loss: 0.21941213309764862\n",
      "iteration 26388: loss: 0.21941177546977997\n",
      "iteration 26389: loss: 0.21941137313842773\n",
      "iteration 26390: loss: 0.2194109708070755\n",
      "iteration 26391: loss: 0.21941056847572327\n",
      "iteration 26392: loss: 0.21941009163856506\n",
      "iteration 26393: loss: 0.21940970420837402\n",
      "iteration 26394: loss: 0.2194092720746994\n",
      "iteration 26395: loss: 0.21940889954566956\n",
      "iteration 26396: loss: 0.21940846741199493\n",
      "iteration 26397: loss: 0.21940812468528748\n",
      "iteration 26398: loss: 0.21940763294696808\n",
      "iteration 26399: loss: 0.21940724551677704\n",
      "iteration 26400: loss: 0.2194068729877472\n",
      "iteration 26401: loss: 0.21940644085407257\n",
      "iteration 26402: loss: 0.21940597891807556\n",
      "iteration 26403: loss: 0.2194056212902069\n",
      "iteration 26404: loss: 0.2194051742553711\n",
      "iteration 26405: loss: 0.21940481662750244\n",
      "iteration 26406: loss: 0.2194044142961502\n",
      "iteration 26407: loss: 0.21940398216247559\n",
      "iteration 26408: loss: 0.21940353512763977\n",
      "iteration 26409: loss: 0.21940311789512634\n",
      "iteration 26410: loss: 0.2194027602672577\n",
      "iteration 26411: loss: 0.21940235793590546\n",
      "iteration 26412: loss: 0.21940191090106964\n",
      "iteration 26413: loss: 0.2194015085697174\n",
      "iteration 26414: loss: 0.21940107643604279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 26415: loss: 0.21940067410469055\n",
      "iteration 26416: loss: 0.2194003164768219\n",
      "iteration 26417: loss: 0.21939988434314728\n",
      "iteration 26418: loss: 0.21939945220947266\n",
      "iteration 26419: loss: 0.21939906477928162\n",
      "iteration 26420: loss: 0.21939857304096222\n",
      "iteration 26421: loss: 0.21939821541309357\n",
      "iteration 26422: loss: 0.21939781308174133\n",
      "iteration 26423: loss: 0.21939745545387268\n",
      "iteration 26424: loss: 0.21939699351787567\n",
      "iteration 26425: loss: 0.21939662098884583\n",
      "iteration 26426: loss: 0.21939611434936523\n",
      "iteration 26427: loss: 0.2193957269191742\n",
      "iteration 26428: loss: 0.21939530968666077\n",
      "iteration 26429: loss: 0.2193949669599533\n",
      "iteration 26430: loss: 0.2193944901227951\n",
      "iteration 26431: loss: 0.21939416229724884\n",
      "iteration 26432: loss: 0.21939370036125183\n",
      "iteration 26433: loss: 0.2193932831287384\n",
      "iteration 26434: loss: 0.21939289569854736\n",
      "iteration 26435: loss: 0.21939246356487274\n",
      "iteration 26436: loss: 0.21939203143119812\n",
      "iteration 26437: loss: 0.21939165890216827\n",
      "iteration 26438: loss: 0.21939119696617126\n",
      "iteration 26439: loss: 0.219390869140625\n",
      "iteration 26440: loss: 0.21939048171043396\n",
      "iteration 26441: loss: 0.21939003467559814\n",
      "iteration 26442: loss: 0.2193896472454071\n",
      "iteration 26443: loss: 0.21938927471637726\n",
      "iteration 26444: loss: 0.21938879787921906\n",
      "iteration 26445: loss: 0.21938839554786682\n",
      "iteration 26446: loss: 0.21938800811767578\n",
      "iteration 26447: loss: 0.21938757598400116\n",
      "iteration 26448: loss: 0.21938717365264893\n",
      "iteration 26449: loss: 0.21938678622245789\n",
      "iteration 26450: loss: 0.21938638389110565\n",
      "iteration 26451: loss: 0.2193860113620758\n",
      "iteration 26452: loss: 0.21938550472259521\n",
      "iteration 26453: loss: 0.21938514709472656\n",
      "iteration 26454: loss: 0.21938475966453552\n",
      "iteration 26455: loss: 0.2193843126296997\n",
      "iteration 26456: loss: 0.21938391029834747\n",
      "iteration 26457: loss: 0.21938356757164001\n",
      "iteration 26458: loss: 0.21938307583332062\n",
      "iteration 26459: loss: 0.2193826138973236\n",
      "iteration 26460: loss: 0.21938225626945496\n",
      "iteration 26461: loss: 0.21938185393810272\n",
      "iteration 26462: loss: 0.21938148140907288\n",
      "iteration 26463: loss: 0.21938109397888184\n",
      "iteration 26464: loss: 0.21938061714172363\n",
      "iteration 26465: loss: 0.2193802297115326\n",
      "iteration 26466: loss: 0.21937978267669678\n",
      "iteration 26467: loss: 0.21937942504882812\n",
      "iteration 26468: loss: 0.2193790227174759\n",
      "iteration 26469: loss: 0.21937863528728485\n",
      "iteration 26470: loss: 0.21937820315361023\n",
      "iteration 26471: loss: 0.219377800822258\n",
      "iteration 26472: loss: 0.21937736868858337\n",
      "iteration 26473: loss: 0.21937699615955353\n",
      "iteration 26474: loss: 0.2193765640258789\n",
      "iteration 26475: loss: 0.21937616169452667\n",
      "iteration 26476: loss: 0.21937572956085205\n",
      "iteration 26477: loss: 0.2193753719329834\n",
      "iteration 26478: loss: 0.21937498450279236\n",
      "iteration 26479: loss: 0.21937453746795654\n",
      "iteration 26480: loss: 0.2193741351366043\n",
      "iteration 26481: loss: 0.2193737030029297\n",
      "iteration 26482: loss: 0.21937330067157745\n",
      "iteration 26483: loss: 0.21937286853790283\n",
      "iteration 26484: loss: 0.21937251091003418\n",
      "iteration 26485: loss: 0.21937207877635956\n",
      "iteration 26486: loss: 0.2193717062473297\n",
      "iteration 26487: loss: 0.2193712443113327\n",
      "iteration 26488: loss: 0.21937081217765808\n",
      "iteration 26489: loss: 0.21937045454978943\n",
      "iteration 26490: loss: 0.21937008202075958\n",
      "iteration 26491: loss: 0.21936964988708496\n",
      "iteration 26492: loss: 0.21936921775341034\n",
      "iteration 26493: loss: 0.2193688601255417\n",
      "iteration 26494: loss: 0.21936841309070587\n",
      "iteration 26495: loss: 0.2193680703639984\n",
      "iteration 26496: loss: 0.21936766803264618\n",
      "iteration 26497: loss: 0.21936723589897156\n",
      "iteration 26498: loss: 0.21936678886413574\n",
      "iteration 26499: loss: 0.2193664014339447\n",
      "iteration 26500: loss: 0.21936604380607605\n",
      "iteration 26501: loss: 0.21936559677124023\n",
      "iteration 26502: loss: 0.219365194439888\n",
      "iteration 26503: loss: 0.21936479210853577\n",
      "iteration 26504: loss: 0.21936443448066711\n",
      "iteration 26505: loss: 0.2193640023469925\n",
      "iteration 26506: loss: 0.21936354041099548\n",
      "iteration 26507: loss: 0.21936318278312683\n",
      "iteration 26508: loss: 0.2193627804517746\n",
      "iteration 26509: loss: 0.21936234831809998\n",
      "iteration 26510: loss: 0.21936193108558655\n",
      "iteration 26511: loss: 0.2193615436553955\n",
      "iteration 26512: loss: 0.21936115622520447\n",
      "iteration 26513: loss: 0.21936078369617462\n",
      "iteration 26514: loss: 0.2193603813648224\n",
      "iteration 26515: loss: 0.21935991942882538\n",
      "iteration 26516: loss: 0.21935951709747314\n",
      "iteration 26517: loss: 0.2193591147661209\n",
      "iteration 26518: loss: 0.21935875713825226\n",
      "iteration 26519: loss: 0.21935835480690002\n",
      "iteration 26520: loss: 0.21935789287090302\n",
      "iteration 26521: loss: 0.21935752034187317\n",
      "iteration 26522: loss: 0.21935708820819855\n",
      "iteration 26523: loss: 0.2193567007780075\n",
      "iteration 26524: loss: 0.21935632824897766\n",
      "iteration 26525: loss: 0.21935589611530304\n",
      "iteration 26526: loss: 0.2193554937839508\n",
      "iteration 26527: loss: 0.21935510635375977\n",
      "iteration 26528: loss: 0.21935467422008514\n",
      "iteration 26529: loss: 0.2193542718887329\n",
      "iteration 26530: loss: 0.2193538248538971\n",
      "iteration 26531: loss: 0.21935346722602844\n",
      "iteration 26532: loss: 0.21935304999351501\n",
      "iteration 26533: loss: 0.21935264766216278\n",
      "iteration 26534: loss: 0.21935227513313293\n",
      "iteration 26535: loss: 0.2193518579006195\n",
      "iteration 26536: loss: 0.21935145556926727\n",
      "iteration 26537: loss: 0.21935105323791504\n",
      "iteration 26538: loss: 0.21935062110424042\n",
      "iteration 26539: loss: 0.21935021877288818\n",
      "iteration 26540: loss: 0.21934986114501953\n",
      "iteration 26541: loss: 0.2193494737148285\n",
      "iteration 26542: loss: 0.21934905648231506\n",
      "iteration 26543: loss: 0.21934862434864044\n",
      "iteration 26544: loss: 0.2193482220172882\n",
      "iteration 26545: loss: 0.21934787929058075\n",
      "iteration 26546: loss: 0.21934743225574493\n",
      "iteration 26547: loss: 0.2193470448255539\n",
      "iteration 26548: loss: 0.21934667229652405\n",
      "iteration 26549: loss: 0.21934619545936584\n",
      "iteration 26550: loss: 0.2193458527326584\n",
      "iteration 26551: loss: 0.21934540569782257\n",
      "iteration 26552: loss: 0.21934500336647034\n",
      "iteration 26553: loss: 0.21934464573860168\n",
      "iteration 26554: loss: 0.2193441390991211\n",
      "iteration 26555: loss: 0.21934381127357483\n",
      "iteration 26556: loss: 0.2193433940410614\n",
      "iteration 26557: loss: 0.21934303641319275\n",
      "iteration 26558: loss: 0.21934258937835693\n",
      "iteration 26559: loss: 0.2193422019481659\n",
      "iteration 26560: loss: 0.21934178471565247\n",
      "iteration 26561: loss: 0.21934139728546143\n",
      "iteration 26562: loss: 0.2193409949541092\n",
      "iteration 26563: loss: 0.21934060752391815\n",
      "iteration 26564: loss: 0.21934020519256592\n",
      "iteration 26565: loss: 0.2193397730588913\n",
      "iteration 26566: loss: 0.21933937072753906\n",
      "iteration 26567: loss: 0.21933896839618683\n",
      "iteration 26568: loss: 0.2193385362625122\n",
      "iteration 26569: loss: 0.21933817863464355\n",
      "iteration 26570: loss: 0.2193378210067749\n",
      "iteration 26571: loss: 0.2193373441696167\n",
      "iteration 26572: loss: 0.21933698654174805\n",
      "iteration 26573: loss: 0.219336599111557\n",
      "iteration 26574: loss: 0.21933618187904358\n",
      "iteration 26575: loss: 0.21933582425117493\n",
      "iteration 26576: loss: 0.2193353921175003\n",
      "iteration 26577: loss: 0.21933495998382568\n",
      "iteration 26578: loss: 0.21933457255363464\n",
      "iteration 26579: loss: 0.2193341702222824\n",
      "iteration 26580: loss: 0.2193337380886078\n",
      "iteration 26581: loss: 0.21933338046073914\n",
      "iteration 26582: loss: 0.2193330079317093\n",
      "iteration 26583: loss: 0.21933260560035706\n",
      "iteration 26584: loss: 0.21933221817016602\n",
      "iteration 26585: loss: 0.2193317711353302\n",
      "iteration 26586: loss: 0.21933138370513916\n",
      "iteration 26587: loss: 0.21933093667030334\n",
      "iteration 26588: loss: 0.2193305939435959\n",
      "iteration 26589: loss: 0.21933016180992126\n",
      "iteration 26590: loss: 0.2193298041820526\n",
      "iteration 26591: loss: 0.21932938694953918\n",
      "iteration 26592: loss: 0.21932896971702576\n",
      "iteration 26593: loss: 0.21932855248451233\n",
      "iteration 26594: loss: 0.2193281650543213\n",
      "iteration 26595: loss: 0.21932776272296906\n",
      "iteration 26596: loss: 0.21932736039161682\n",
      "iteration 26597: loss: 0.21932701766490936\n",
      "iteration 26598: loss: 0.21932658553123474\n",
      "iteration 26599: loss: 0.2193261682987213\n",
      "iteration 26600: loss: 0.21932578086853027\n",
      "iteration 26601: loss: 0.21932539343833923\n",
      "iteration 26602: loss: 0.2193249762058258\n",
      "iteration 26603: loss: 0.21932458877563477\n",
      "iteration 26604: loss: 0.2193242311477661\n",
      "iteration 26605: loss: 0.2193237841129303\n",
      "iteration 26606: loss: 0.21932335197925568\n",
      "iteration 26607: loss: 0.21932291984558105\n",
      "iteration 26608: loss: 0.2193225920200348\n",
      "iteration 26609: loss: 0.21932215988636017\n",
      "iteration 26610: loss: 0.21932177245616913\n",
      "iteration 26611: loss: 0.2193213403224945\n",
      "iteration 26612: loss: 0.21932096779346466\n",
      "iteration 26613: loss: 0.219320610165596\n",
      "iteration 26614: loss: 0.2193201780319214\n",
      "iteration 26615: loss: 0.21931979060173035\n",
      "iteration 26616: loss: 0.21931946277618408\n",
      "iteration 26617: loss: 0.21931903064250946\n",
      "iteration 26618: loss: 0.21931859850883484\n",
      "iteration 26619: loss: 0.21931815147399902\n",
      "iteration 26620: loss: 0.21931776404380798\n",
      "iteration 26621: loss: 0.21931740641593933\n",
      "iteration 26622: loss: 0.21931704878807068\n",
      "iteration 26623: loss: 0.21931660175323486\n",
      "iteration 26624: loss: 0.21931619942188263\n",
      "iteration 26625: loss: 0.219315767288208\n",
      "iteration 26626: loss: 0.21931537985801697\n",
      "iteration 26627: loss: 0.21931500732898712\n",
      "iteration 26628: loss: 0.21931461989879608\n",
      "iteration 26629: loss: 0.21931424736976624\n",
      "iteration 26630: loss: 0.21931378543376923\n",
      "iteration 26631: loss: 0.21931342780590057\n",
      "iteration 26632: loss: 0.21931305527687073\n",
      "iteration 26633: loss: 0.2193126678466797\n",
      "iteration 26634: loss: 0.21931222081184387\n",
      "iteration 26635: loss: 0.21931183338165283\n",
      "iteration 26636: loss: 0.2193114459514618\n",
      "iteration 26637: loss: 0.21931104362010956\n",
      "iteration 26638: loss: 0.2193106710910797\n",
      "iteration 26639: loss: 0.21931025385856628\n",
      "iteration 26640: loss: 0.21930985152721405\n",
      "iteration 26641: loss: 0.2193094789981842\n",
      "iteration 26642: loss: 0.21930904686450958\n",
      "iteration 26643: loss: 0.21930861473083496\n",
      "iteration 26644: loss: 0.2193082869052887\n",
      "iteration 26645: loss: 0.21930786967277527\n",
      "iteration 26646: loss: 0.21930746734142303\n",
      "iteration 26647: loss: 0.21930702030658722\n",
      "iteration 26648: loss: 0.21930663287639618\n",
      "iteration 26649: loss: 0.21930630505084991\n",
      "iteration 26650: loss: 0.2193058431148529\n",
      "iteration 26651: loss: 0.21930547058582306\n",
      "iteration 26652: loss: 0.21930503845214844\n",
      "iteration 26653: loss: 0.21930471062660217\n",
      "iteration 26654: loss: 0.21930427849292755\n",
      "iteration 26655: loss: 0.2193038910627365\n",
      "iteration 26656: loss: 0.2193034589290619\n",
      "iteration 26657: loss: 0.21930313110351562\n",
      "iteration 26658: loss: 0.2193027287721634\n",
      "iteration 26659: loss: 0.21930232644081116\n",
      "iteration 26660: loss: 0.21930186450481415\n",
      "iteration 26661: loss: 0.2193015068769455\n",
      "iteration 26662: loss: 0.21930113434791565\n",
      "iteration 26663: loss: 0.219300776720047\n",
      "iteration 26664: loss: 0.21930031478405\n",
      "iteration 26665: loss: 0.21929994225502014\n",
      "iteration 26666: loss: 0.21929951012134552\n",
      "iteration 26667: loss: 0.21929912269115448\n",
      "iteration 26668: loss: 0.21929875016212463\n",
      "iteration 26669: loss: 0.21929839253425598\n",
      "iteration 26670: loss: 0.21929796040058136\n",
      "iteration 26671: loss: 0.21929755806922913\n",
      "iteration 26672: loss: 0.2192971408367157\n",
      "iteration 26673: loss: 0.21929673850536346\n",
      "iteration 26674: loss: 0.21929636597633362\n",
      "iteration 26675: loss: 0.21929597854614258\n",
      "iteration 26676: loss: 0.21929562091827393\n",
      "iteration 26677: loss: 0.2192952185869217\n",
      "iteration 26678: loss: 0.21929478645324707\n",
      "iteration 26679: loss: 0.2192944586277008\n",
      "iteration 26680: loss: 0.21929395198822021\n",
      "iteration 26681: loss: 0.21929362416267395\n",
      "iteration 26682: loss: 0.2192932665348053\n",
      "iteration 26683: loss: 0.21929284930229187\n",
      "iteration 26684: loss: 0.21929240226745605\n",
      "iteration 26685: loss: 0.21929208934307098\n",
      "iteration 26686: loss: 0.21929164230823517\n",
      "iteration 26687: loss: 0.21929125487804413\n",
      "iteration 26688: loss: 0.2192908525466919\n",
      "iteration 26689: loss: 0.21929045021533966\n",
      "iteration 26690: loss: 0.219290092587471\n",
      "iteration 26691: loss: 0.219289630651474\n",
      "iteration 26692: loss: 0.21928925812244415\n",
      "iteration 26693: loss: 0.2192889153957367\n",
      "iteration 26694: loss: 0.21928849816322327\n",
      "iteration 26695: loss: 0.21928811073303223\n",
      "iteration 26696: loss: 0.2192876636981964\n",
      "iteration 26697: loss: 0.21928727626800537\n",
      "iteration 26698: loss: 0.2192869484424591\n",
      "iteration 26699: loss: 0.21928653120994568\n",
      "iteration 26700: loss: 0.21928617358207703\n",
      "iteration 26701: loss: 0.2192857265472412\n",
      "iteration 26702: loss: 0.21928539872169495\n",
      "iteration 26703: loss: 0.21928496658802032\n",
      "iteration 26704: loss: 0.21928457915782928\n",
      "iteration 26705: loss: 0.21928414702415466\n",
      "iteration 26706: loss: 0.21928377449512482\n",
      "iteration 26707: loss: 0.21928343176841736\n",
      "iteration 26708: loss: 0.21928295493125916\n",
      "iteration 26709: loss: 0.2192825973033905\n",
      "iteration 26710: loss: 0.21928218007087708\n",
      "iteration 26711: loss: 0.21928183734416962\n",
      "iteration 26712: loss: 0.2192813903093338\n",
      "iteration 26713: loss: 0.21928100287914276\n",
      "iteration 26714: loss: 0.21928063035011292\n",
      "iteration 26715: loss: 0.2192802131175995\n",
      "iteration 26716: loss: 0.21927984058856964\n",
      "iteration 26717: loss: 0.2192794531583786\n",
      "iteration 26718: loss: 0.21927905082702637\n",
      "iteration 26719: loss: 0.21927866339683533\n",
      "iteration 26720: loss: 0.21927829086780548\n",
      "iteration 26721: loss: 0.21927788853645325\n",
      "iteration 26722: loss: 0.21927745640277863\n",
      "iteration 26723: loss: 0.21927714347839355\n",
      "iteration 26724: loss: 0.21927674114704132\n",
      "iteration 26725: loss: 0.21927635371685028\n",
      "iteration 26726: loss: 0.21927586197853088\n",
      "iteration 26727: loss: 0.2192755490541458\n",
      "iteration 26728: loss: 0.21927519142627716\n",
      "iteration 26729: loss: 0.21927475929260254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 26730: loss: 0.2192743718624115\n",
      "iteration 26731: loss: 0.21927395462989807\n",
      "iteration 26732: loss: 0.21927356719970703\n",
      "iteration 26733: loss: 0.219273179769516\n",
      "iteration 26734: loss: 0.21927280724048615\n",
      "iteration 26735: loss: 0.2192724496126175\n",
      "iteration 26736: loss: 0.21927201747894287\n",
      "iteration 26737: loss: 0.21927163004875183\n",
      "iteration 26738: loss: 0.2192712128162384\n",
      "iteration 26739: loss: 0.21927082538604736\n",
      "iteration 26740: loss: 0.21927042305469513\n",
      "iteration 26741: loss: 0.21927008032798767\n",
      "iteration 26742: loss: 0.21926960349082947\n",
      "iteration 26743: loss: 0.21926923096179962\n",
      "iteration 26744: loss: 0.21926887333393097\n",
      "iteration 26745: loss: 0.21926847100257874\n",
      "iteration 26746: loss: 0.21926812827587128\n",
      "iteration 26747: loss: 0.21926765143871307\n",
      "iteration 26748: loss: 0.21926727890968323\n",
      "iteration 26749: loss: 0.21926696598529816\n",
      "iteration 26750: loss: 0.21926653385162354\n",
      "iteration 26751: loss: 0.2192661464214325\n",
      "iteration 26752: loss: 0.21926574409008026\n",
      "iteration 26753: loss: 0.21926529705524445\n",
      "iteration 26754: loss: 0.21926498413085938\n",
      "iteration 26755: loss: 0.21926459670066833\n",
      "iteration 26756: loss: 0.2192642241716385\n",
      "iteration 26757: loss: 0.21926379203796387\n",
      "iteration 26758: loss: 0.2192634791135788\n",
      "iteration 26759: loss: 0.2192629873752594\n",
      "iteration 26760: loss: 0.21926259994506836\n",
      "iteration 26761: loss: 0.2192622423171997\n",
      "iteration 26762: loss: 0.21926183998584747\n",
      "iteration 26763: loss: 0.21926145255565643\n",
      "iteration 26764: loss: 0.21926112473011017\n",
      "iteration 26765: loss: 0.21926064789295197\n",
      "iteration 26766: loss: 0.21926026046276093\n",
      "iteration 26767: loss: 0.2192598581314087\n",
      "iteration 26768: loss: 0.21925950050354004\n",
      "iteration 26769: loss: 0.219259113073349\n",
      "iteration 26770: loss: 0.21925875544548035\n",
      "iteration 26771: loss: 0.21925833821296692\n",
      "iteration 26772: loss: 0.21925798058509827\n",
      "iteration 26773: loss: 0.21925759315490723\n",
      "iteration 26774: loss: 0.219257190823555\n",
      "iteration 26775: loss: 0.21925678849220276\n",
      "iteration 26776: loss: 0.21925640106201172\n",
      "iteration 26777: loss: 0.21925607323646545\n",
      "iteration 26778: loss: 0.21925565600395203\n",
      "iteration 26779: loss: 0.2192552387714386\n",
      "iteration 26780: loss: 0.21925485134124756\n",
      "iteration 26781: loss: 0.21925446391105652\n",
      "iteration 26782: loss: 0.2192540168762207\n",
      "iteration 26783: loss: 0.21925370395183563\n",
      "iteration 26784: loss: 0.21925325691699982\n",
      "iteration 26785: loss: 0.2192528247833252\n",
      "iteration 26786: loss: 0.21925249695777893\n",
      "iteration 26787: loss: 0.21925215423107147\n",
      "iteration 26788: loss: 0.21925175189971924\n",
      "iteration 26789: loss: 0.2192513644695282\n",
      "iteration 26790: loss: 0.21925100684165955\n",
      "iteration 26791: loss: 0.21925053000450134\n",
      "iteration 26792: loss: 0.21925011277198792\n",
      "iteration 26793: loss: 0.21924972534179688\n",
      "iteration 26794: loss: 0.2192493975162506\n",
      "iteration 26795: loss: 0.21924898028373718\n",
      "iteration 26796: loss: 0.21924860775470734\n",
      "iteration 26797: loss: 0.2192482203245163\n",
      "iteration 26798: loss: 0.21924786269664764\n",
      "iteration 26799: loss: 0.21924743056297302\n",
      "iteration 26800: loss: 0.2192470282316208\n",
      "iteration 26801: loss: 0.21924667060375214\n",
      "iteration 26802: loss: 0.2192462682723999\n",
      "iteration 26803: loss: 0.21924588084220886\n",
      "iteration 26804: loss: 0.2192455232143402\n",
      "iteration 26805: loss: 0.21924512088298798\n",
      "iteration 26806: loss: 0.21924476325511932\n",
      "iteration 26807: loss: 0.2192443311214447\n",
      "iteration 26808: loss: 0.21924392879009247\n",
      "iteration 26809: loss: 0.21924355626106262\n",
      "iteration 26810: loss: 0.21924321353435516\n",
      "iteration 26811: loss: 0.21924284100532532\n",
      "iteration 26812: loss: 0.2192424088716507\n",
      "iteration 26813: loss: 0.21924206614494324\n",
      "iteration 26814: loss: 0.2192416489124298\n",
      "iteration 26815: loss: 0.21924129128456116\n",
      "iteration 26816: loss: 0.21924085915088654\n",
      "iteration 26817: loss: 0.21924051642417908\n",
      "iteration 26818: loss: 0.21924014389514923\n",
      "iteration 26819: loss: 0.2192397564649582\n",
      "iteration 26820: loss: 0.21923938393592834\n",
      "iteration 26821: loss: 0.21923890709877014\n",
      "iteration 26822: loss: 0.21923859417438507\n",
      "iteration 26823: loss: 0.21923813223838806\n",
      "iteration 26824: loss: 0.2192377746105194\n",
      "iteration 26825: loss: 0.21923741698265076\n",
      "iteration 26826: loss: 0.2192370444536209\n",
      "iteration 26827: loss: 0.21923664212226868\n",
      "iteration 26828: loss: 0.21923625469207764\n",
      "iteration 26829: loss: 0.21923582255840302\n",
      "iteration 26830: loss: 0.21923542022705078\n",
      "iteration 26831: loss: 0.21923509240150452\n",
      "iteration 26832: loss: 0.21923470497131348\n",
      "iteration 26833: loss: 0.21923431754112244\n",
      "iteration 26834: loss: 0.2192339450120926\n",
      "iteration 26835: loss: 0.21923354268074036\n",
      "iteration 26836: loss: 0.21923315525054932\n",
      "iteration 26837: loss: 0.21923275291919708\n",
      "iteration 26838: loss: 0.21923239529132843\n",
      "iteration 26839: loss: 0.2192319929599762\n",
      "iteration 26840: loss: 0.21923160552978516\n",
      "iteration 26841: loss: 0.2192312479019165\n",
      "iteration 26842: loss: 0.21923084557056427\n",
      "iteration 26843: loss: 0.21923045814037323\n",
      "iteration 26844: loss: 0.21923008561134338\n",
      "iteration 26845: loss: 0.21922972798347473\n",
      "iteration 26846: loss: 0.2192292958498001\n",
      "iteration 26847: loss: 0.21922890841960907\n",
      "iteration 26848: loss: 0.21922855079174042\n",
      "iteration 26849: loss: 0.21922814846038818\n",
      "iteration 26850: loss: 0.21922774612903595\n",
      "iteration 26851: loss: 0.2192273586988449\n",
      "iteration 26852: loss: 0.21922698616981506\n",
      "iteration 26853: loss: 0.21922659873962402\n",
      "iteration 26854: loss: 0.2192261964082718\n",
      "iteration 26855: loss: 0.21922583878040314\n",
      "iteration 26856: loss: 0.21922540664672852\n",
      "iteration 26857: loss: 0.21922507882118225\n",
      "iteration 26858: loss: 0.21922466158866882\n",
      "iteration 26859: loss: 0.21922430396080017\n",
      "iteration 26860: loss: 0.21922393143177032\n",
      "iteration 26861: loss: 0.21922346949577332\n",
      "iteration 26862: loss: 0.21922317147254944\n",
      "iteration 26863: loss: 0.2192227840423584\n",
      "iteration 26864: loss: 0.21922239661216736\n",
      "iteration 26865: loss: 0.2192220240831375\n",
      "iteration 26866: loss: 0.2192215919494629\n",
      "iteration 26867: loss: 0.21922120451927185\n",
      "iteration 26868: loss: 0.21922080218791962\n",
      "iteration 26869: loss: 0.21922048926353455\n",
      "iteration 26870: loss: 0.21922001242637634\n",
      "iteration 26871: loss: 0.21921972930431366\n",
      "iteration 26872: loss: 0.21921929717063904\n",
      "iteration 26873: loss: 0.21921896934509277\n",
      "iteration 26874: loss: 0.21921852231025696\n",
      "iteration 26875: loss: 0.2192181646823883\n",
      "iteration 26876: loss: 0.21921777725219727\n",
      "iteration 26877: loss: 0.21921733021736145\n",
      "iteration 26878: loss: 0.219216987490654\n",
      "iteration 26879: loss: 0.21921658515930176\n",
      "iteration 26880: loss: 0.2192162573337555\n",
      "iteration 26881: loss: 0.21921586990356445\n",
      "iteration 26882: loss: 0.21921546757221222\n",
      "iteration 26883: loss: 0.21921508014202118\n",
      "iteration 26884: loss: 0.21921470761299133\n",
      "iteration 26885: loss: 0.21921436488628387\n",
      "iteration 26886: loss: 0.21921391785144806\n",
      "iteration 26887: loss: 0.2192135602235794\n",
      "iteration 26888: loss: 0.21921315789222717\n",
      "iteration 26889: loss: 0.21921281516551971\n",
      "iteration 26890: loss: 0.21921245753765106\n",
      "iteration 26891: loss: 0.21921201050281525\n",
      "iteration 26892: loss: 0.2192116230726242\n",
      "iteration 26893: loss: 0.21921126544475555\n",
      "iteration 26894: loss: 0.2192109078168869\n",
      "iteration 26895: loss: 0.21921050548553467\n",
      "iteration 26896: loss: 0.21921010315418243\n",
      "iteration 26897: loss: 0.21920974552631378\n",
      "iteration 26898: loss: 0.21920938789844513\n",
      "iteration 26899: loss: 0.2192089855670929\n",
      "iteration 26900: loss: 0.21920856833457947\n",
      "iteration 26901: loss: 0.21920819580554962\n",
      "iteration 26902: loss: 0.21920783817768097\n",
      "iteration 26903: loss: 0.21920745074748993\n",
      "iteration 26904: loss: 0.2192070484161377\n",
      "iteration 26905: loss: 0.21920673549175262\n",
      "iteration 26906: loss: 0.219206303358078\n",
      "iteration 26907: loss: 0.21920590102672577\n",
      "iteration 26908: loss: 0.21920554339885712\n",
      "iteration 26909: loss: 0.21920518577098846\n",
      "iteration 26910: loss: 0.21920481324195862\n",
      "iteration 26911: loss: 0.219204381108284\n",
      "iteration 26912: loss: 0.21920399367809296\n",
      "iteration 26913: loss: 0.2192036658525467\n",
      "iteration 26914: loss: 0.21920323371887207\n",
      "iteration 26915: loss: 0.21920287609100342\n",
      "iteration 26916: loss: 0.21920248866081238\n",
      "iteration 26917: loss: 0.21920211613178253\n",
      "iteration 26918: loss: 0.2192016839981079\n",
      "iteration 26919: loss: 0.21920135617256165\n",
      "iteration 26920: loss: 0.21920093894004822\n",
      "iteration 26921: loss: 0.21920056641101837\n",
      "iteration 26922: loss: 0.2192002236843109\n",
      "iteration 26923: loss: 0.21919980645179749\n",
      "iteration 26924: loss: 0.21919941902160645\n",
      "iteration 26925: loss: 0.2191990613937378\n",
      "iteration 26926: loss: 0.21919870376586914\n",
      "iteration 26927: loss: 0.2191983163356781\n",
      "iteration 26928: loss: 0.21919794380664825\n",
      "iteration 26929: loss: 0.21919751167297363\n",
      "iteration 26930: loss: 0.21919718384742737\n",
      "iteration 26931: loss: 0.21919676661491394\n",
      "iteration 26932: loss: 0.2191963642835617\n",
      "iteration 26933: loss: 0.21919600665569305\n",
      "iteration 26934: loss: 0.2191956341266632\n",
      "iteration 26935: loss: 0.21919521689414978\n",
      "iteration 26936: loss: 0.21919488906860352\n",
      "iteration 26937: loss: 0.21919453144073486\n",
      "iteration 26938: loss: 0.21919409930706024\n",
      "iteration 26939: loss: 0.2191937416791916\n",
      "iteration 26940: loss: 0.21919336915016174\n",
      "iteration 26941: loss: 0.21919302642345428\n",
      "iteration 26942: loss: 0.21919262409210205\n",
      "iteration 26943: loss: 0.21919217705726624\n",
      "iteration 26944: loss: 0.21919183433055878\n",
      "iteration 26945: loss: 0.21919146180152893\n",
      "iteration 26946: loss: 0.21919110417366028\n",
      "iteration 26947: loss: 0.21919068694114685\n",
      "iteration 26948: loss: 0.21919026970863342\n",
      "iteration 26949: loss: 0.21918992698192596\n",
      "iteration 26950: loss: 0.21918955445289612\n",
      "iteration 26951: loss: 0.2191891372203827\n",
      "iteration 26952: loss: 0.2191888391971588\n",
      "iteration 26953: loss: 0.21918845176696777\n",
      "iteration 26954: loss: 0.21918804943561554\n",
      "iteration 26955: loss: 0.21918770670890808\n",
      "iteration 26956: loss: 0.21918730437755585\n",
      "iteration 26957: loss: 0.2191869020462036\n",
      "iteration 26958: loss: 0.21918654441833496\n",
      "iteration 26959: loss: 0.2191861867904663\n",
      "iteration 26960: loss: 0.2191857546567917\n",
      "iteration 26961: loss: 0.21918539702892303\n",
      "iteration 26962: loss: 0.2191850244998932\n",
      "iteration 26963: loss: 0.21918463706970215\n",
      "iteration 26964: loss: 0.21918423473834991\n",
      "iteration 26965: loss: 0.21918389201164246\n",
      "iteration 26966: loss: 0.21918348968029022\n",
      "iteration 26967: loss: 0.21918316185474396\n",
      "iteration 26968: loss: 0.21918272972106934\n",
      "iteration 26969: loss: 0.2191823273897171\n",
      "iteration 26970: loss: 0.21918193995952606\n",
      "iteration 26971: loss: 0.21918153762817383\n",
      "iteration 26972: loss: 0.21918125450611115\n",
      "iteration 26973: loss: 0.2191808670759201\n",
      "iteration 26974: loss: 0.21918043494224548\n",
      "iteration 26975: loss: 0.21918006241321564\n",
      "iteration 26976: loss: 0.21917974948883057\n",
      "iteration 26977: loss: 0.21917931735515594\n",
      "iteration 26978: loss: 0.21917898952960968\n",
      "iteration 26979: loss: 0.21917860209941864\n",
      "iteration 26980: loss: 0.21917824447155\n",
      "iteration 26981: loss: 0.21917781233787537\n",
      "iteration 26982: loss: 0.2191774845123291\n",
      "iteration 26983: loss: 0.21917705237865448\n",
      "iteration 26984: loss: 0.21917672455310822\n",
      "iteration 26985: loss: 0.2191762924194336\n",
      "iteration 26986: loss: 0.21917596459388733\n",
      "iteration 26987: loss: 0.2191755771636963\n",
      "iteration 26988: loss: 0.21917518973350525\n",
      "iteration 26989: loss: 0.2191748172044754\n",
      "iteration 26990: loss: 0.21917442977428436\n",
      "iteration 26991: loss: 0.21917405724525452\n",
      "iteration 26992: loss: 0.21917369961738586\n",
      "iteration 26993: loss: 0.2191733419895172\n",
      "iteration 26994: loss: 0.21917295455932617\n",
      "iteration 26995: loss: 0.21917256712913513\n",
      "iteration 26996: loss: 0.21917219460010529\n",
      "iteration 26997: loss: 0.21917176246643066\n",
      "iteration 26998: loss: 0.21917133033275604\n",
      "iteration 26999: loss: 0.21917112171649933\n",
      "iteration 27000: loss: 0.2191707193851471\n",
      "iteration 27001: loss: 0.21917028725147247\n",
      "iteration 27002: loss: 0.21916992962360382\n",
      "iteration 27003: loss: 0.21916957199573517\n",
      "iteration 27004: loss: 0.21916918456554413\n",
      "iteration 27005: loss: 0.21916882693767548\n",
      "iteration 27006: loss: 0.21916845440864563\n",
      "iteration 27007: loss: 0.2191680371761322\n",
      "iteration 27008: loss: 0.21916767954826355\n",
      "iteration 27009: loss: 0.2191673070192337\n",
      "iteration 27010: loss: 0.21916694939136505\n",
      "iteration 27011: loss: 0.21916648745536804\n",
      "iteration 27012: loss: 0.2191661298274994\n",
      "iteration 27013: loss: 0.21916580200195312\n",
      "iteration 27014: loss: 0.21916544437408447\n",
      "iteration 27015: loss: 0.21916504204273224\n",
      "iteration 27016: loss: 0.2191646546125412\n",
      "iteration 27017: loss: 0.21916422247886658\n",
      "iteration 27018: loss: 0.2191639393568039\n",
      "iteration 27019: loss: 0.21916353702545166\n",
      "iteration 27020: loss: 0.21916313469409943\n",
      "iteration 27021: loss: 0.21916279196739197\n",
      "iteration 27022: loss: 0.21916238963603973\n",
      "iteration 27023: loss: 0.21916203200817108\n",
      "iteration 27024: loss: 0.21916165947914124\n",
      "iteration 27025: loss: 0.2191612720489502\n",
      "iteration 27026: loss: 0.21916094422340393\n",
      "iteration 27027: loss: 0.21916048228740692\n",
      "iteration 27028: loss: 0.21916015446186066\n",
      "iteration 27029: loss: 0.21915976703166962\n",
      "iteration 27030: loss: 0.21915939450263977\n",
      "iteration 27031: loss: 0.2191590815782547\n",
      "iteration 27032: loss: 0.21915864944458008\n",
      "iteration 27033: loss: 0.21915829181671143\n",
      "iteration 27034: loss: 0.21915793418884277\n",
      "iteration 27035: loss: 0.21915750205516815\n",
      "iteration 27036: loss: 0.2191571742296219\n",
      "iteration 27037: loss: 0.21915678679943085\n",
      "iteration 27038: loss: 0.21915647387504578\n",
      "iteration 27039: loss: 0.21915605664253235\n",
      "iteration 27040: loss: 0.21915563941001892\n",
      "iteration 27041: loss: 0.21915526688098907\n",
      "iteration 27042: loss: 0.2191549837589264\n",
      "iteration 27043: loss: 0.21915462613105774\n",
      "iteration 27044: loss: 0.21915411949157715\n",
      "iteration 27045: loss: 0.21915383636951447\n",
      "iteration 27046: loss: 0.21915343403816223\n",
      "iteration 27047: loss: 0.21915307641029358\n",
      "iteration 27048: loss: 0.21915265917778015\n",
      "iteration 27049: loss: 0.2191523015499115\n",
      "iteration 27050: loss: 0.21915192902088165\n",
      "iteration 27051: loss: 0.21915152668952942\n",
      "iteration 27052: loss: 0.21915118396282196\n",
      "iteration 27053: loss: 0.2191508263349533\n",
      "iteration 27054: loss: 0.21915045380592346\n",
      "iteration 27055: loss: 0.21915006637573242\n",
      "iteration 27056: loss: 0.21914967894554138\n",
      "iteration 27057: loss: 0.21914932131767273\n",
      "iteration 27058: loss: 0.21914896368980408\n",
      "iteration 27059: loss: 0.21914859116077423\n",
      "iteration 27060: loss: 0.2191481590270996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 27061: loss: 0.21914777159690857\n",
      "iteration 27062: loss: 0.21914739906787872\n",
      "iteration 27063: loss: 0.21914708614349365\n",
      "iteration 27064: loss: 0.21914665400981903\n",
      "iteration 27065: loss: 0.21914640069007874\n",
      "iteration 27066: loss: 0.2191459685564041\n",
      "iteration 27067: loss: 0.2191455364227295\n",
      "iteration 27068: loss: 0.21914517879486084\n",
      "iteration 27069: loss: 0.21914485096931458\n",
      "iteration 27070: loss: 0.21914449334144592\n",
      "iteration 27071: loss: 0.2191440761089325\n",
      "iteration 27072: loss: 0.21914377808570862\n",
      "iteration 27073: loss: 0.2191433608531952\n",
      "iteration 27074: loss: 0.21914298832416534\n",
      "iteration 27075: loss: 0.2191425859928131\n",
      "iteration 27076: loss: 0.21914222836494446\n",
      "iteration 27077: loss: 0.21914181113243103\n",
      "iteration 27078: loss: 0.21914152801036835\n",
      "iteration 27079: loss: 0.21914121508598328\n",
      "iteration 27080: loss: 0.21914076805114746\n",
      "iteration 27081: loss: 0.21914036571979523\n",
      "iteration 27082: loss: 0.21914002299308777\n",
      "iteration 27083: loss: 0.21913962066173553\n",
      "iteration 27084: loss: 0.2191392481327057\n",
      "iteration 27085: loss: 0.21913890540599823\n",
      "iteration 27086: loss: 0.21913853287696838\n",
      "iteration 27087: loss: 0.21913814544677734\n",
      "iteration 27088: loss: 0.21913783252239227\n",
      "iteration 27089: loss: 0.21913740038871765\n",
      "iteration 27090: loss: 0.219137042760849\n",
      "iteration 27091: loss: 0.21913664042949677\n",
      "iteration 27092: loss: 0.2191362828016281\n",
      "iteration 27093: loss: 0.21913592517375946\n",
      "iteration 27094: loss: 0.21913555264472961\n",
      "iteration 27095: loss: 0.21913523972034454\n",
      "iteration 27096: loss: 0.21913477778434753\n",
      "iteration 27097: loss: 0.21913442015647888\n",
      "iteration 27098: loss: 0.21913409233093262\n",
      "iteration 27099: loss: 0.219133660197258\n",
      "iteration 27100: loss: 0.21913333237171173\n",
      "iteration 27101: loss: 0.2191329300403595\n",
      "iteration 27102: loss: 0.21913257241249084\n",
      "iteration 27103: loss: 0.2191322296857834\n",
      "iteration 27104: loss: 0.21913187205791473\n",
      "iteration 27105: loss: 0.2191314697265625\n",
      "iteration 27106: loss: 0.21913108229637146\n",
      "iteration 27107: loss: 0.2191307097673416\n",
      "iteration 27108: loss: 0.21913035213947296\n",
      "iteration 27109: loss: 0.2191299945116043\n",
      "iteration 27110: loss: 0.21912963688373566\n",
      "iteration 27111: loss: 0.21912920475006104\n",
      "iteration 27112: loss: 0.21912889182567596\n",
      "iteration 27113: loss: 0.21912851929664612\n",
      "iteration 27114: loss: 0.21912813186645508\n",
      "iteration 27115: loss: 0.21912774443626404\n",
      "iteration 27116: loss: 0.21912741661071777\n",
      "iteration 27117: loss: 0.21912702918052673\n",
      "iteration 27118: loss: 0.2191266566514969\n",
      "iteration 27119: loss: 0.21912631392478943\n",
      "iteration 27120: loss: 0.21912594139575958\n",
      "iteration 27121: loss: 0.21912555396556854\n",
      "iteration 27122: loss: 0.2191251963376999\n",
      "iteration 27123: loss: 0.21912483870983124\n",
      "iteration 27124: loss: 0.21912439167499542\n",
      "iteration 27125: loss: 0.21912403404712677\n",
      "iteration 27126: loss: 0.2191237509250641\n",
      "iteration 27127: loss: 0.21912336349487305\n",
      "iteration 27128: loss: 0.21912293136119843\n",
      "iteration 27129: loss: 0.21912261843681335\n",
      "iteration 27130: loss: 0.2191222459077835\n",
      "iteration 27131: loss: 0.21912184357643127\n",
      "iteration 27132: loss: 0.21912148594856262\n",
      "iteration 27133: loss: 0.21912112832069397\n",
      "iteration 27134: loss: 0.21912077069282532\n",
      "iteration 27135: loss: 0.21912041306495667\n",
      "iteration 27136: loss: 0.21911998093128204\n",
      "iteration 27137: loss: 0.2191196233034134\n",
      "iteration 27138: loss: 0.21911923587322235\n",
      "iteration 27139: loss: 0.21911892294883728\n",
      "iteration 27140: loss: 0.21911855041980743\n",
      "iteration 27141: loss: 0.21911819279193878\n",
      "iteration 27142: loss: 0.21911779046058655\n",
      "iteration 27143: loss: 0.2191174477338791\n",
      "iteration 27144: loss: 0.21911711990833282\n",
      "iteration 27145: loss: 0.21911664307117462\n",
      "iteration 27146: loss: 0.21911628544330597\n",
      "iteration 27147: loss: 0.2191159725189209\n",
      "iteration 27148: loss: 0.21911561489105225\n",
      "iteration 27149: loss: 0.2191152572631836\n",
      "iteration 27150: loss: 0.21911486983299255\n",
      "iteration 27151: loss: 0.21911445260047913\n",
      "iteration 27152: loss: 0.21911410987377167\n",
      "iteration 27153: loss: 0.2191137820482254\n",
      "iteration 27154: loss: 0.21911334991455078\n",
      "iteration 27155: loss: 0.21911299228668213\n",
      "iteration 27156: loss: 0.21911266446113586\n",
      "iteration 27157: loss: 0.21911227703094482\n",
      "iteration 27158: loss: 0.21911188960075378\n",
      "iteration 27159: loss: 0.21911156177520752\n",
      "iteration 27160: loss: 0.21911123394966125\n",
      "iteration 27161: loss: 0.21911081671714783\n",
      "iteration 27162: loss: 0.21911048889160156\n",
      "iteration 27163: loss: 0.21911010146141052\n",
      "iteration 27164: loss: 0.21910974383354187\n",
      "iteration 27165: loss: 0.21910934150218964\n",
      "iteration 27166: loss: 0.21910901367664337\n",
      "iteration 27167: loss: 0.21910862624645233\n",
      "iteration 27168: loss: 0.21910829842090607\n",
      "iteration 27169: loss: 0.21910786628723145\n",
      "iteration 27170: loss: 0.2191075086593628\n",
      "iteration 27171: loss: 0.21910710632801056\n",
      "iteration 27172: loss: 0.2191067487001419\n",
      "iteration 27173: loss: 0.21910640597343445\n",
      "iteration 27174: loss: 0.21910600364208221\n",
      "iteration 27175: loss: 0.21910564601421356\n",
      "iteration 27176: loss: 0.2191052883863449\n",
      "iteration 27177: loss: 0.21910493075847626\n",
      "iteration 27178: loss: 0.2191045731306076\n",
      "iteration 27179: loss: 0.21910420060157776\n",
      "iteration 27180: loss: 0.2191038429737091\n",
      "iteration 27181: loss: 0.21910345554351807\n",
      "iteration 27182: loss: 0.21910309791564941\n",
      "iteration 27183: loss: 0.21910271048545837\n",
      "iteration 27184: loss: 0.21910235285758972\n",
      "iteration 27185: loss: 0.21910199522972107\n",
      "iteration 27186: loss: 0.2191016674041748\n",
      "iteration 27187: loss: 0.21910130977630615\n",
      "iteration 27188: loss: 0.21910087764263153\n",
      "iteration 27189: loss: 0.21910056471824646\n",
      "iteration 27190: loss: 0.21910014748573303\n",
      "iteration 27191: loss: 0.21909980475902557\n",
      "iteration 27192: loss: 0.21909943222999573\n",
      "iteration 27193: loss: 0.21909907460212708\n",
      "iteration 27194: loss: 0.219098761677742\n",
      "iteration 27195: loss: 0.21909832954406738\n",
      "iteration 27196: loss: 0.21909797191619873\n",
      "iteration 27197: loss: 0.21909765899181366\n",
      "iteration 27198: loss: 0.21909725666046143\n",
      "iteration 27199: loss: 0.21909689903259277\n",
      "iteration 27200: loss: 0.2190965861082077\n",
      "iteration 27201: loss: 0.21909615397453308\n",
      "iteration 27202: loss: 0.21909579634666443\n",
      "iteration 27203: loss: 0.21909546852111816\n",
      "iteration 27204: loss: 0.2190951108932495\n",
      "iteration 27205: loss: 0.2190946638584137\n",
      "iteration 27206: loss: 0.21909432113170624\n",
      "iteration 27207: loss: 0.21909403800964355\n",
      "iteration 27208: loss: 0.21909360587596893\n",
      "iteration 27209: loss: 0.2190932035446167\n",
      "iteration 27210: loss: 0.21909284591674805\n",
      "iteration 27211: loss: 0.21909253299236298\n",
      "iteration 27212: loss: 0.21909217536449432\n",
      "iteration 27213: loss: 0.21909180283546448\n",
      "iteration 27214: loss: 0.21909141540527344\n",
      "iteration 27215: loss: 0.2190910279750824\n",
      "iteration 27216: loss: 0.21909070014953613\n",
      "iteration 27217: loss: 0.21909034252166748\n",
      "iteration 27218: loss: 0.2190900295972824\n",
      "iteration 27219: loss: 0.2190895974636078\n",
      "iteration 27220: loss: 0.21908922493457794\n",
      "iteration 27221: loss: 0.2190888673067093\n",
      "iteration 27222: loss: 0.21908850967884064\n",
      "iteration 27223: loss: 0.2190881073474884\n",
      "iteration 27224: loss: 0.21908779442310333\n",
      "iteration 27225: loss: 0.2190874069929123\n",
      "iteration 27226: loss: 0.21908703446388245\n",
      "iteration 27227: loss: 0.2190866470336914\n",
      "iteration 27228: loss: 0.21908628940582275\n",
      "iteration 27229: loss: 0.21908597648143768\n",
      "iteration 27230: loss: 0.21908560395240784\n",
      "iteration 27231: loss: 0.2190852165222168\n",
      "iteration 27232: loss: 0.21908485889434814\n",
      "iteration 27233: loss: 0.2190845012664795\n",
      "iteration 27234: loss: 0.21908411383628845\n",
      "iteration 27235: loss: 0.21908381581306458\n",
      "iteration 27236: loss: 0.21908345818519592\n",
      "iteration 27237: loss: 0.2190830409526825\n",
      "iteration 27238: loss: 0.21908263862133026\n",
      "iteration 27239: loss: 0.2190822809934616\n",
      "iteration 27240: loss: 0.21908195316791534\n",
      "iteration 27241: loss: 0.21908161044120789\n",
      "iteration 27242: loss: 0.21908120810985565\n",
      "iteration 27243: loss: 0.2190808802843094\n",
      "iteration 27244: loss: 0.21908052265644073\n",
      "iteration 27245: loss: 0.21908016502857208\n",
      "iteration 27246: loss: 0.219079852104187\n",
      "iteration 27247: loss: 0.2190794050693512\n",
      "iteration 27248: loss: 0.21907904744148254\n",
      "iteration 27249: loss: 0.21907870471477509\n",
      "iteration 27250: loss: 0.21907833218574524\n",
      "iteration 27251: loss: 0.2190779745578766\n",
      "iteration 27252: loss: 0.21907766163349152\n",
      "iteration 27253: loss: 0.2190771996974945\n",
      "iteration 27254: loss: 0.21907691657543182\n",
      "iteration 27255: loss: 0.2190765142440796\n",
      "iteration 27256: loss: 0.21907620131969452\n",
      "iteration 27257: loss: 0.21907579898834229\n",
      "iteration 27258: loss: 0.21907547116279602\n",
      "iteration 27259: loss: 0.21907508373260498\n",
      "iteration 27260: loss: 0.21907468140125275\n",
      "iteration 27261: loss: 0.2190743386745453\n",
      "iteration 27262: loss: 0.21907396614551544\n",
      "iteration 27263: loss: 0.2190735787153244\n",
      "iteration 27264: loss: 0.21907325088977814\n",
      "iteration 27265: loss: 0.21907289326190948\n",
      "iteration 27266: loss: 0.21907255053520203\n",
      "iteration 27267: loss: 0.21907217800617218\n",
      "iteration 27268: loss: 0.21907182037830353\n",
      "iteration 27269: loss: 0.2190714329481125\n",
      "iteration 27270: loss: 0.21907110512256622\n",
      "iteration 27271: loss: 0.21907071769237518\n",
      "iteration 27272: loss: 0.21907038986682892\n",
      "iteration 27273: loss: 0.21907003223896027\n",
      "iteration 27274: loss: 0.2190696746110916\n",
      "iteration 27275: loss: 0.21906927227973938\n",
      "iteration 27276: loss: 0.21906885504722595\n",
      "iteration 27277: loss: 0.21906861662864685\n",
      "iteration 27278: loss: 0.21906819939613342\n",
      "iteration 27279: loss: 0.21906785666942596\n",
      "iteration 27280: loss: 0.2190674990415573\n",
      "iteration 27281: loss: 0.2190670669078827\n",
      "iteration 27282: loss: 0.21906673908233643\n",
      "iteration 27283: loss: 0.21906642615795135\n",
      "iteration 27284: loss: 0.2190660685300827\n",
      "iteration 27285: loss: 0.21906569600105286\n",
      "iteration 27286: loss: 0.21906530857086182\n",
      "iteration 27287: loss: 0.21906498074531555\n",
      "iteration 27288: loss: 0.2190645933151245\n",
      "iteration 27289: loss: 0.21906423568725586\n",
      "iteration 27290: loss: 0.21906384825706482\n",
      "iteration 27291: loss: 0.21906352043151855\n",
      "iteration 27292: loss: 0.21906320750713348\n",
      "iteration 27293: loss: 0.21906280517578125\n",
      "iteration 27294: loss: 0.2190624177455902\n",
      "iteration 27295: loss: 0.21906211972236633\n",
      "iteration 27296: loss: 0.2190616875886917\n",
      "iteration 27297: loss: 0.21906137466430664\n",
      "iteration 27298: loss: 0.2190609723329544\n",
      "iteration 27299: loss: 0.21906068921089172\n",
      "iteration 27300: loss: 0.2190602719783783\n",
      "iteration 27301: loss: 0.21905989944934845\n",
      "iteration 27302: loss: 0.219059556722641\n",
      "iteration 27303: loss: 0.21905915439128876\n",
      "iteration 27304: loss: 0.2190588414669037\n",
      "iteration 27305: loss: 0.21905843913555145\n",
      "iteration 27306: loss: 0.21905815601348877\n",
      "iteration 27307: loss: 0.21905775368213654\n",
      "iteration 27308: loss: 0.21905741095542908\n",
      "iteration 27309: loss: 0.21905705332756042\n",
      "iteration 27310: loss: 0.2190566509962082\n",
      "iteration 27311: loss: 0.21905629336833954\n",
      "iteration 27312: loss: 0.2190559357404709\n",
      "iteration 27313: loss: 0.2190556526184082\n",
      "iteration 27314: loss: 0.21905525028705597\n",
      "iteration 27315: loss: 0.2190549075603485\n",
      "iteration 27316: loss: 0.21905454993247986\n",
      "iteration 27317: loss: 0.21905414760112762\n",
      "iteration 27318: loss: 0.21905383467674255\n",
      "iteration 27319: loss: 0.2190534770488739\n",
      "iteration 27320: loss: 0.21905310451984406\n",
      "iteration 27321: loss: 0.21905271708965302\n",
      "iteration 27322: loss: 0.21905238926410675\n",
      "iteration 27323: loss: 0.2190520316362381\n",
      "iteration 27324: loss: 0.21905162930488586\n",
      "iteration 27325: loss: 0.2190512865781784\n",
      "iteration 27326: loss: 0.21905097365379333\n",
      "iteration 27327: loss: 0.2190505713224411\n",
      "iteration 27328: loss: 0.21905024349689484\n",
      "iteration 27329: loss: 0.2190498411655426\n",
      "iteration 27330: loss: 0.21904954314231873\n",
      "iteration 27331: loss: 0.2190491259098053\n",
      "iteration 27332: loss: 0.21904882788658142\n",
      "iteration 27333: loss: 0.219048410654068\n",
      "iteration 27334: loss: 0.21904806792736053\n",
      "iteration 27335: loss: 0.21904771029949188\n",
      "iteration 27336: loss: 0.21904735267162323\n",
      "iteration 27337: loss: 0.21904698014259338\n",
      "iteration 27338: loss: 0.21904662251472473\n",
      "iteration 27339: loss: 0.21904632449150085\n",
      "iteration 27340: loss: 0.21904592216014862\n",
      "iteration 27341: loss: 0.21904556453227997\n",
      "iteration 27342: loss: 0.2190452367067337\n",
      "iteration 27343: loss: 0.21904487907886505\n",
      "iteration 27344: loss: 0.2190445214509964\n",
      "iteration 27345: loss: 0.21904413402080536\n",
      "iteration 27346: loss: 0.2190437614917755\n",
      "iteration 27347: loss: 0.21904341876506805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 27348: loss: 0.2190430462360382\n",
      "iteration 27349: loss: 0.21904270350933075\n",
      "iteration 27350: loss: 0.2190423458814621\n",
      "iteration 27351: loss: 0.21904203295707703\n",
      "iteration 27352: loss: 0.21904166042804718\n",
      "iteration 27353: loss: 0.21904122829437256\n",
      "iteration 27354: loss: 0.2190409153699875\n",
      "iteration 27355: loss: 0.21904058754444122\n",
      "iteration 27356: loss: 0.21904022991657257\n",
      "iteration 27357: loss: 0.2190398871898651\n",
      "iteration 27358: loss: 0.21903951466083527\n",
      "iteration 27359: loss: 0.2190391570329666\n",
      "iteration 27360: loss: 0.21903879940509796\n",
      "iteration 27361: loss: 0.2190384864807129\n",
      "iteration 27362: loss: 0.21903808414936066\n",
      "iteration 27363: loss: 0.2190377414226532\n",
      "iteration 27364: loss: 0.21903733909130096\n",
      "iteration 27365: loss: 0.2190370112657547\n",
      "iteration 27366: loss: 0.21903657913208008\n",
      "iteration 27367: loss: 0.219036266207695\n",
      "iteration 27368: loss: 0.21903595328330994\n",
      "iteration 27369: loss: 0.2190355360507965\n",
      "iteration 27370: loss: 0.21903519332408905\n",
      "iteration 27371: loss: 0.21903488039970398\n",
      "iteration 27372: loss: 0.21903447806835175\n",
      "iteration 27373: loss: 0.21903415024280548\n",
      "iteration 27374: loss: 0.21903379261493683\n",
      "iteration 27375: loss: 0.21903344988822937\n",
      "iteration 27376: loss: 0.21903304755687714\n",
      "iteration 27377: loss: 0.21903279423713684\n",
      "iteration 27378: loss: 0.2190323770046234\n",
      "iteration 27379: loss: 0.21903201937675476\n",
      "iteration 27380: loss: 0.21903160214424133\n",
      "iteration 27381: loss: 0.21903128921985626\n",
      "iteration 27382: loss: 0.2190309315919876\n",
      "iteration 27383: loss: 0.21903061866760254\n",
      "iteration 27384: loss: 0.2190302163362503\n",
      "iteration 27385: loss: 0.21902985870838165\n",
      "iteration 27386: loss: 0.21902954578399658\n",
      "iteration 27387: loss: 0.2190292328596115\n",
      "iteration 27388: loss: 0.2190288007259369\n",
      "iteration 27389: loss: 0.219028502702713\n",
      "iteration 27390: loss: 0.21902814507484436\n",
      "iteration 27391: loss: 0.2190278023481369\n",
      "iteration 27392: loss: 0.21902740001678467\n",
      "iteration 27393: loss: 0.21902704238891602\n",
      "iteration 27394: loss: 0.21902664005756378\n",
      "iteration 27395: loss: 0.2190263271331787\n",
      "iteration 27396: loss: 0.21902599930763245\n",
      "iteration 27397: loss: 0.21902558207511902\n",
      "iteration 27398: loss: 0.21902528405189514\n",
      "iteration 27399: loss: 0.2190249264240265\n",
      "iteration 27400: loss: 0.21902456879615784\n",
      "iteration 27401: loss: 0.21902422606945038\n",
      "iteration 27402: loss: 0.21902385354042053\n",
      "iteration 27403: loss: 0.21902351081371307\n",
      "iteration 27404: loss: 0.21902313828468323\n",
      "iteration 27405: loss: 0.2190227508544922\n",
      "iteration 27406: loss: 0.21902242302894592\n",
      "iteration 27407: loss: 0.21902208030223846\n",
      "iteration 27408: loss: 0.21902170777320862\n",
      "iteration 27409: loss: 0.21902136504650116\n",
      "iteration 27410: loss: 0.2190210521221161\n",
      "iteration 27411: loss: 0.21902069449424744\n",
      "iteration 27412: loss: 0.2190203219652176\n",
      "iteration 27413: loss: 0.21901997923851013\n",
      "iteration 27414: loss: 0.2190195769071579\n",
      "iteration 27415: loss: 0.21901926398277283\n",
      "iteration 27416: loss: 0.2190188616514206\n",
      "iteration 27417: loss: 0.2190186083316803\n",
      "iteration 27418: loss: 0.2190181314945221\n",
      "iteration 27419: loss: 0.21901783347129822\n",
      "iteration 27420: loss: 0.21901746094226837\n",
      "iteration 27421: loss: 0.2190171182155609\n",
      "iteration 27422: loss: 0.21901676058769226\n",
      "iteration 27423: loss: 0.21901647746562958\n",
      "iteration 27424: loss: 0.21901611983776093\n",
      "iteration 27425: loss: 0.2190156877040863\n",
      "iteration 27426: loss: 0.21901535987854004\n",
      "iteration 27427: loss: 0.21901503205299377\n",
      "iteration 27428: loss: 0.21901464462280273\n",
      "iteration 27429: loss: 0.21901431679725647\n",
      "iteration 27430: loss: 0.219013974070549\n",
      "iteration 27431: loss: 0.21901360154151917\n",
      "iteration 27432: loss: 0.2190132439136505\n",
      "iteration 27433: loss: 0.21901288628578186\n",
      "iteration 27434: loss: 0.21901258826255798\n",
      "iteration 27435: loss: 0.21901211142539978\n",
      "iteration 27436: loss: 0.21901187300682068\n",
      "iteration 27437: loss: 0.21901150047779083\n",
      "iteration 27438: loss: 0.21901114284992218\n",
      "iteration 27439: loss: 0.21901080012321472\n",
      "iteration 27440: loss: 0.2190103977918625\n",
      "iteration 27441: loss: 0.21901008486747742\n",
      "iteration 27442: loss: 0.21900975704193115\n",
      "iteration 27443: loss: 0.21900935471057892\n",
      "iteration 27444: loss: 0.21900899708271027\n",
      "iteration 27445: loss: 0.2190086543560028\n",
      "iteration 27446: loss: 0.21900832653045654\n",
      "iteration 27447: loss: 0.2190079241991043\n",
      "iteration 27448: loss: 0.21900764107704163\n",
      "iteration 27449: loss: 0.21900728344917297\n",
      "iteration 27450: loss: 0.21900689601898193\n",
      "iteration 27451: loss: 0.21900658309459686\n",
      "iteration 27452: loss: 0.21900615096092224\n",
      "iteration 27453: loss: 0.21900582313537598\n",
      "iteration 27454: loss: 0.2190054953098297\n",
      "iteration 27455: loss: 0.21900518238544464\n",
      "iteration 27456: loss: 0.21900475025177002\n",
      "iteration 27457: loss: 0.21900442242622375\n",
      "iteration 27458: loss: 0.2190040796995163\n",
      "iteration 27459: loss: 0.21900372207164764\n",
      "iteration 27460: loss: 0.21900339424610138\n",
      "iteration 27461: loss: 0.2190030813217163\n",
      "iteration 27462: loss: 0.2190026491880417\n",
      "iteration 27463: loss: 0.219002366065979\n",
      "iteration 27464: loss: 0.21900193393230438\n",
      "iteration 27465: loss: 0.2190016806125641\n",
      "iteration 27466: loss: 0.21900132298469543\n",
      "iteration 27467: loss: 0.2190009355545044\n",
      "iteration 27468: loss: 0.21900050342082977\n",
      "iteration 27469: loss: 0.2190001904964447\n",
      "iteration 27470: loss: 0.21899989247322083\n",
      "iteration 27471: loss: 0.21899957954883575\n",
      "iteration 27472: loss: 0.21899917721748352\n",
      "iteration 27473: loss: 0.21899883449077606\n",
      "iteration 27474: loss: 0.21899846196174622\n",
      "iteration 27475: loss: 0.21899810433387756\n",
      "iteration 27476: loss: 0.2189977616071701\n",
      "iteration 27477: loss: 0.21899743378162384\n",
      "iteration 27478: loss: 0.2189970463514328\n",
      "iteration 27479: loss: 0.21899676322937012\n",
      "iteration 27480: loss: 0.21899640560150146\n",
      "iteration 27481: loss: 0.2189960479736328\n",
      "iteration 27482: loss: 0.21899572014808655\n",
      "iteration 27483: loss: 0.2189953327178955\n",
      "iteration 27484: loss: 0.21899501979351044\n",
      "iteration 27485: loss: 0.21899466216564178\n",
      "iteration 27486: loss: 0.21899423003196716\n",
      "iteration 27487: loss: 0.2189939320087433\n",
      "iteration 27488: loss: 0.21899358928203583\n",
      "iteration 27489: loss: 0.21899323165416718\n",
      "iteration 27490: loss: 0.21899285912513733\n",
      "iteration 27491: loss: 0.21899256110191345\n",
      "iteration 27492: loss: 0.21899215877056122\n",
      "iteration 27493: loss: 0.21899184584617615\n",
      "iteration 27494: loss: 0.21899151802062988\n",
      "iteration 27495: loss: 0.21899116039276123\n",
      "iteration 27496: loss: 0.21899080276489258\n",
      "iteration 27497: loss: 0.2189904898405075\n",
      "iteration 27498: loss: 0.21899008750915527\n",
      "iteration 27499: loss: 0.21898970007896423\n",
      "iteration 27500: loss: 0.2189893275499344\n",
      "iteration 27501: loss: 0.2189890444278717\n",
      "iteration 27502: loss: 0.21898870170116425\n",
      "iteration 27503: loss: 0.2189883291721344\n",
      "iteration 27504: loss: 0.21898801624774933\n",
      "iteration 27505: loss: 0.21898765861988068\n",
      "iteration 27506: loss: 0.21898731589317322\n",
      "iteration 27507: loss: 0.21898695826530457\n",
      "iteration 27508: loss: 0.2189866304397583\n",
      "iteration 27509: loss: 0.21898622810840607\n",
      "iteration 27510: loss: 0.2189858853816986\n",
      "iteration 27511: loss: 0.21898555755615234\n",
      "iteration 27512: loss: 0.2189851999282837\n",
      "iteration 27513: loss: 0.21898487210273743\n",
      "iteration 27514: loss: 0.21898451447486877\n",
      "iteration 27515: loss: 0.21898409724235535\n",
      "iteration 27516: loss: 0.21898381412029266\n",
      "iteration 27517: loss: 0.21898344159126282\n",
      "iteration 27518: loss: 0.21898309886455536\n",
      "iteration 27519: loss: 0.2189827412366867\n",
      "iteration 27520: loss: 0.21898241341114044\n",
      "iteration 27521: loss: 0.2189820557832718\n",
      "iteration 27522: loss: 0.21898169815540314\n",
      "iteration 27523: loss: 0.21898134052753448\n",
      "iteration 27524: loss: 0.21898102760314941\n",
      "iteration 27525: loss: 0.21898066997528076\n",
      "iteration 27526: loss: 0.2189803570508957\n",
      "iteration 27527: loss: 0.21897999942302704\n",
      "iteration 27528: loss: 0.2189796417951584\n",
      "iteration 27529: loss: 0.21897932887077332\n",
      "iteration 27530: loss: 0.21897897124290466\n",
      "iteration 27531: loss: 0.21897856891155243\n",
      "iteration 27532: loss: 0.21897825598716736\n",
      "iteration 27533: loss: 0.21897795796394348\n",
      "iteration 27534: loss: 0.21897749602794647\n",
      "iteration 27535: loss: 0.2189771682024002\n",
      "iteration 27536: loss: 0.21897682547569275\n",
      "iteration 27537: loss: 0.21897649765014648\n",
      "iteration 27538: loss: 0.21897614002227783\n",
      "iteration 27539: loss: 0.2189757525920868\n",
      "iteration 27540: loss: 0.2189754992723465\n",
      "iteration 27541: loss: 0.21897514164447784\n",
      "iteration 27542: loss: 0.2189747840166092\n",
      "iteration 27543: loss: 0.21897439658641815\n",
      "iteration 27544: loss: 0.2189740687608719\n",
      "iteration 27545: loss: 0.2189737856388092\n",
      "iteration 27546: loss: 0.21897342801094055\n",
      "iteration 27547: loss: 0.21897301077842712\n",
      "iteration 27548: loss: 0.21897268295288086\n",
      "iteration 27549: loss: 0.21897228062152863\n",
      "iteration 27550: loss: 0.21897199749946594\n",
      "iteration 27551: loss: 0.2189716398715973\n",
      "iteration 27552: loss: 0.21897129714488983\n",
      "iteration 27553: loss: 0.21897096931934357\n",
      "iteration 27554: loss: 0.2189706265926361\n",
      "iteration 27555: loss: 0.21897022426128387\n",
      "iteration 27556: loss: 0.2189699411392212\n",
      "iteration 27557: loss: 0.21896955370903015\n",
      "iteration 27558: loss: 0.21896927058696747\n",
      "iteration 27559: loss: 0.21896891295909882\n",
      "iteration 27560: loss: 0.21896851062774658\n",
      "iteration 27561: loss: 0.2189681977033615\n",
      "iteration 27562: loss: 0.21896786987781525\n",
      "iteration 27563: loss: 0.21896755695343018\n",
      "iteration 27564: loss: 0.21896719932556152\n",
      "iteration 27565: loss: 0.2189667969942093\n",
      "iteration 27566: loss: 0.21896645426750183\n",
      "iteration 27567: loss: 0.21896612644195557\n",
      "iteration 27568: loss: 0.21896576881408691\n",
      "iteration 27569: loss: 0.21896541118621826\n",
      "iteration 27570: loss: 0.2189650535583496\n",
      "iteration 27571: loss: 0.21896474063396454\n",
      "iteration 27572: loss: 0.21896441280841827\n",
      "iteration 27573: loss: 0.21896401047706604\n",
      "iteration 27574: loss: 0.21896366775035858\n",
      "iteration 27575: loss: 0.21896333992481232\n",
      "iteration 27576: loss: 0.21896302700042725\n",
      "iteration 27577: loss: 0.2189626395702362\n",
      "iteration 27578: loss: 0.21896228194236755\n",
      "iteration 27579: loss: 0.21896199882030487\n",
      "iteration 27580: loss: 0.21896162629127502\n",
      "iteration 27581: loss: 0.21896126866340637\n",
      "iteration 27582: loss: 0.2189609557390213\n",
      "iteration 27583: loss: 0.21896056830883026\n",
      "iteration 27584: loss: 0.2189602106809616\n",
      "iteration 27585: loss: 0.21895985305309296\n",
      "iteration 27586: loss: 0.2189594954252243\n",
      "iteration 27587: loss: 0.21895916759967804\n",
      "iteration 27588: loss: 0.21895888447761536\n",
      "iteration 27589: loss: 0.2189585417509079\n",
      "iteration 27590: loss: 0.21895810961723328\n",
      "iteration 27591: loss: 0.2189578115940094\n",
      "iteration 27592: loss: 0.21895742416381836\n",
      "iteration 27593: loss: 0.21895714104175568\n",
      "iteration 27594: loss: 0.218956857919693\n",
      "iteration 27595: loss: 0.21895642578601837\n",
      "iteration 27596: loss: 0.21895615756511688\n",
      "iteration 27597: loss: 0.21895572543144226\n",
      "iteration 27598: loss: 0.21895544230937958\n",
      "iteration 27599: loss: 0.21895506978034973\n",
      "iteration 27600: loss: 0.21895472705364227\n",
      "iteration 27601: loss: 0.21895435452461243\n",
      "iteration 27602: loss: 0.21895404160022736\n",
      "iteration 27603: loss: 0.21895365417003632\n",
      "iteration 27604: loss: 0.21895337104797363\n",
      "iteration 27605: loss: 0.2189529836177826\n",
      "iteration 27606: loss: 0.21895268559455872\n",
      "iteration 27607: loss: 0.21895237267017365\n",
      "iteration 27608: loss: 0.2189519703388214\n",
      "iteration 27609: loss: 0.21895165741443634\n",
      "iteration 27610: loss: 0.21895134449005127\n",
      "iteration 27611: loss: 0.218951016664505\n",
      "iteration 27612: loss: 0.218950554728508\n",
      "iteration 27613: loss: 0.21895024180412292\n",
      "iteration 27614: loss: 0.21894988417625427\n",
      "iteration 27615: loss: 0.21894952654838562\n",
      "iteration 27616: loss: 0.21894927322864532\n",
      "iteration 27617: loss: 0.2189488410949707\n",
      "iteration 27618: loss: 0.21894855797290802\n",
      "iteration 27619: loss: 0.21894820034503937\n",
      "iteration 27620: loss: 0.21894793212413788\n",
      "iteration 27621: loss: 0.21894757449626923\n",
      "iteration 27622: loss: 0.2189471423625946\n",
      "iteration 27623: loss: 0.21894684433937073\n",
      "iteration 27624: loss: 0.21894648671150208\n",
      "iteration 27625: loss: 0.21894609928131104\n",
      "iteration 27626: loss: 0.21894581615924835\n",
      "iteration 27627: loss: 0.21894538402557373\n",
      "iteration 27628: loss: 0.21894511580467224\n",
      "iteration 27629: loss: 0.2189447432756424\n",
      "iteration 27630: loss: 0.21894440054893494\n",
      "iteration 27631: loss: 0.21894410252571106\n",
      "iteration 27632: loss: 0.21894368529319763\n",
      "iteration 27633: loss: 0.21894340217113495\n",
      "iteration 27634: loss: 0.2189430296421051\n",
      "iteration 27635: loss: 0.218942791223526\n",
      "iteration 27636: loss: 0.21894235908985138\n",
      "iteration 27637: loss: 0.21894200146198273\n",
      "iteration 27638: loss: 0.21894173324108124\n",
      "iteration 27639: loss: 0.21894137561321259\n",
      "iteration 27640: loss: 0.21894100308418274\n",
      "iteration 27641: loss: 0.21894066035747528\n",
      "iteration 27642: loss: 0.21894028782844543\n",
      "iteration 27643: loss: 0.21894001960754395\n",
      "iteration 27644: loss: 0.21893958747386932\n",
      "iteration 27645: loss: 0.21893933415412903\n",
      "iteration 27646: loss: 0.21893902122974396\n",
      "iteration 27647: loss: 0.2189386636018753\n",
      "iteration 27648: loss: 0.21893826127052307\n",
      "iteration 27649: loss: 0.21893790364265442\n",
      "iteration 27650: loss: 0.21893760561943054\n",
      "iteration 27651: loss: 0.2189372330904007\n",
      "iteration 27652: loss: 0.21893692016601562\n",
      "iteration 27653: loss: 0.21893656253814697\n",
      "iteration 27654: loss: 0.21893620491027832\n",
      "iteration 27655: loss: 0.21893581748008728\n",
      "iteration 27656: loss: 0.21893556416034698\n",
      "iteration 27657: loss: 0.21893517673015594\n",
      "iteration 27658: loss: 0.21893484890460968\n",
      "iteration 27659: loss: 0.2189345359802246\n",
      "iteration 27660: loss: 0.21893420815467834\n",
      "iteration 27661: loss: 0.2189338654279709\n",
      "iteration 27662: loss: 0.21893349289894104\n",
      "iteration 27663: loss: 0.21893315017223358\n",
      "iteration 27664: loss: 0.21893282234668732\n",
      "iteration 27665: loss: 0.21893247961997986\n",
      "iteration 27666: loss: 0.2189321219921112\n",
      "iteration 27667: loss: 0.21893183887004852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 27668: loss: 0.21893148124217987\n",
      "iteration 27669: loss: 0.21893112361431122\n",
      "iteration 27670: loss: 0.21893075108528137\n",
      "iteration 27671: loss: 0.21893039345741272\n",
      "iteration 27672: loss: 0.21893012523651123\n",
      "iteration 27673: loss: 0.2189297378063202\n",
      "iteration 27674: loss: 0.21892938017845154\n",
      "iteration 27675: loss: 0.21892905235290527\n",
      "iteration 27676: loss: 0.2189287692308426\n",
      "iteration 27677: loss: 0.21892838180065155\n",
      "iteration 27678: loss: 0.21892805397510529\n",
      "iteration 27679: loss: 0.21892771124839783\n",
      "iteration 27680: loss: 0.21892738342285156\n",
      "iteration 27681: loss: 0.2189270257949829\n",
      "iteration 27682: loss: 0.21892674267292023\n",
      "iteration 27683: loss: 0.21892638504505157\n",
      "iteration 27684: loss: 0.21892598271369934\n",
      "iteration 27685: loss: 0.2189256250858307\n",
      "iteration 27686: loss: 0.2189253866672516\n",
      "iteration 27687: loss: 0.21892495453357697\n",
      "iteration 27688: loss: 0.21892467141151428\n",
      "iteration 27689: loss: 0.21892428398132324\n",
      "iteration 27690: loss: 0.21892400085926056\n",
      "iteration 27691: loss: 0.2189236432313919\n",
      "iteration 27692: loss: 0.21892328560352325\n",
      "iteration 27693: loss: 0.21892300248146057\n",
      "iteration 27694: loss: 0.21892264485359192\n",
      "iteration 27695: loss: 0.21892228722572327\n",
      "iteration 27696: loss: 0.2189219444990158\n",
      "iteration 27697: loss: 0.21892161667346954\n",
      "iteration 27698: loss: 0.21892130374908447\n",
      "iteration 27699: loss: 0.21892091631889343\n",
      "iteration 27700: loss: 0.21892058849334717\n",
      "iteration 27701: loss: 0.2189202606678009\n",
      "iteration 27702: loss: 0.21891990303993225\n",
      "iteration 27703: loss: 0.2189195454120636\n",
      "iteration 27704: loss: 0.21891920268535614\n",
      "iteration 27705: loss: 0.21891888976097107\n",
      "iteration 27706: loss: 0.2189185619354248\n",
      "iteration 27707: loss: 0.21891824901103973\n",
      "iteration 27708: loss: 0.2189178764820099\n",
      "iteration 27709: loss: 0.21891753375530243\n",
      "iteration 27710: loss: 0.21891722083091736\n",
      "iteration 27711: loss: 0.2189168632030487\n",
      "iteration 27712: loss: 0.21891656517982483\n",
      "iteration 27713: loss: 0.21891622245311737\n",
      "iteration 27714: loss: 0.21891586482524872\n",
      "iteration 27715: loss: 0.21891553699970245\n",
      "iteration 27716: loss: 0.2189151793718338\n",
      "iteration 27717: loss: 0.21891489624977112\n",
      "iteration 27718: loss: 0.21891450881958008\n",
      "iteration 27719: loss: 0.21891412138938904\n",
      "iteration 27720: loss: 0.21891383826732635\n",
      "iteration 27721: loss: 0.2189134657382965\n",
      "iteration 27722: loss: 0.21891312301158905\n",
      "iteration 27723: loss: 0.21891279518604279\n",
      "iteration 27724: loss: 0.21891248226165771\n",
      "iteration 27725: loss: 0.21891212463378906\n",
      "iteration 27726: loss: 0.2189117968082428\n",
      "iteration 27727: loss: 0.21891145408153534\n",
      "iteration 27728: loss: 0.21891112625598907\n",
      "iteration 27729: loss: 0.21891078352928162\n",
      "iteration 27730: loss: 0.21891050040721893\n",
      "iteration 27731: loss: 0.2189101278781891\n",
      "iteration 27732: loss: 0.21890981495380402\n",
      "iteration 27733: loss: 0.2189093828201294\n",
      "iteration 27734: loss: 0.2189090996980667\n",
      "iteration 27735: loss: 0.21890874207019806\n",
      "iteration 27736: loss: 0.218908429145813\n",
      "iteration 27737: loss: 0.21890802681446075\n",
      "iteration 27738: loss: 0.21890771389007568\n",
      "iteration 27739: loss: 0.21890738606452942\n",
      "iteration 27740: loss: 0.21890707314014435\n",
      "iteration 27741: loss: 0.21890676021575928\n",
      "iteration 27742: loss: 0.218906432390213\n",
      "iteration 27743: loss: 0.21890611946582794\n",
      "iteration 27744: loss: 0.21890583634376526\n",
      "iteration 27745: loss: 0.21890532970428467\n",
      "iteration 27746: loss: 0.2189050167798996\n",
      "iteration 27747: loss: 0.21890470385551453\n",
      "iteration 27748: loss: 0.21890440583229065\n",
      "iteration 27749: loss: 0.2189040184020996\n",
      "iteration 27750: loss: 0.21890373528003693\n",
      "iteration 27751: loss: 0.21890339255332947\n",
      "iteration 27752: loss: 0.21890302002429962\n",
      "iteration 27753: loss: 0.21890270709991455\n",
      "iteration 27754: loss: 0.2189023196697235\n",
      "iteration 27755: loss: 0.21890199184417725\n",
      "iteration 27756: loss: 0.2189016342163086\n",
      "iteration 27757: loss: 0.21890132129192352\n",
      "iteration 27758: loss: 0.21890100836753845\n",
      "iteration 27759: loss: 0.21890071034431458\n",
      "iteration 27760: loss: 0.21890029311180115\n",
      "iteration 27761: loss: 0.21890000998973846\n",
      "iteration 27762: loss: 0.2188996821641922\n",
      "iteration 27763: loss: 0.21889933943748474\n",
      "iteration 27764: loss: 0.2188989669084549\n",
      "iteration 27765: loss: 0.21889862418174744\n",
      "iteration 27766: loss: 0.21889838576316833\n",
      "iteration 27767: loss: 0.2188979685306549\n",
      "iteration 27768: loss: 0.2188977301120758\n",
      "iteration 27769: loss: 0.21889731287956238\n",
      "iteration 27770: loss: 0.2188969850540161\n",
      "iteration 27771: loss: 0.21889670193195343\n",
      "iteration 27772: loss: 0.2188962996006012\n",
      "iteration 27773: loss: 0.21889600157737732\n",
      "iteration 27774: loss: 0.21889567375183105\n",
      "iteration 27775: loss: 0.2188953161239624\n",
      "iteration 27776: loss: 0.21889503300189972\n",
      "iteration 27777: loss: 0.21889467537403107\n",
      "iteration 27778: loss: 0.2188943326473236\n",
      "iteration 27779: loss: 0.21889391541481018\n",
      "iteration 27780: loss: 0.2188936173915863\n",
      "iteration 27781: loss: 0.21889333426952362\n",
      "iteration 27782: loss: 0.21889302134513855\n",
      "iteration 27783: loss: 0.2188926637172699\n",
      "iteration 27784: loss: 0.21889230608940125\n",
      "iteration 27785: loss: 0.2188919484615326\n",
      "iteration 27786: loss: 0.2188916653394699\n",
      "iteration 27787: loss: 0.21889129281044006\n",
      "iteration 27788: loss: 0.218890979886055\n",
      "iteration 27789: loss: 0.21889066696166992\n",
      "iteration 27790: loss: 0.2188902646303177\n",
      "iteration 27791: loss: 0.21888995170593262\n",
      "iteration 27792: loss: 0.21888959407806396\n",
      "iteration 27793: loss: 0.2188892811536789\n",
      "iteration 27794: loss: 0.21888896822929382\n",
      "iteration 27795: loss: 0.21888861060142517\n",
      "iteration 27796: loss: 0.21888825297355652\n",
      "iteration 27797: loss: 0.21888795495033264\n",
      "iteration 27798: loss: 0.21888761222362518\n",
      "iteration 27799: loss: 0.2188872992992401\n",
      "iteration 27800: loss: 0.21888692677021027\n",
      "iteration 27801: loss: 0.2188866138458252\n",
      "iteration 27802: loss: 0.21888628602027893\n",
      "iteration 27803: loss: 0.21888592839241028\n",
      "iteration 27804: loss: 0.2188856303691864\n",
      "iteration 27805: loss: 0.21888533234596252\n",
      "iteration 27806: loss: 0.21888494491577148\n",
      "iteration 27807: loss: 0.2188846319913864\n",
      "iteration 27808: loss: 0.21888431906700134\n",
      "iteration 27809: loss: 0.2188839167356491\n",
      "iteration 27810: loss: 0.21888363361358643\n",
      "iteration 27811: loss: 0.21888324618339539\n",
      "iteration 27812: loss: 0.2188829481601715\n",
      "iteration 27813: loss: 0.21888260543346405\n",
      "iteration 27814: loss: 0.2188822478055954\n",
      "iteration 27815: loss: 0.21888193488121033\n",
      "iteration 27816: loss: 0.21888157725334167\n",
      "iteration 27817: loss: 0.218881294131279\n",
      "iteration 27818: loss: 0.21888093650341034\n",
      "iteration 27819: loss: 0.21888062357902527\n",
      "iteration 27820: loss: 0.2188803255558014\n",
      "iteration 27821: loss: 0.21887996792793274\n",
      "iteration 27822: loss: 0.2188795804977417\n",
      "iteration 27823: loss: 0.21887926757335663\n",
      "iteration 27824: loss: 0.21887898445129395\n",
      "iteration 27825: loss: 0.2188785970211029\n",
      "iteration 27826: loss: 0.21887826919555664\n",
      "iteration 27827: loss: 0.218877911567688\n",
      "iteration 27828: loss: 0.2188776284456253\n",
      "iteration 27829: loss: 0.21887728571891785\n",
      "iteration 27830: loss: 0.2188769280910492\n",
      "iteration 27831: loss: 0.2188766747713089\n",
      "iteration 27832: loss: 0.21887639164924622\n",
      "iteration 27833: loss: 0.2188759744167328\n",
      "iteration 27834: loss: 0.2188756912946701\n",
      "iteration 27835: loss: 0.21887525916099548\n",
      "iteration 27836: loss: 0.21887493133544922\n",
      "iteration 27837: loss: 0.21887466311454773\n",
      "iteration 27838: loss: 0.21887436509132385\n",
      "iteration 27839: loss: 0.2188739776611328\n",
      "iteration 27840: loss: 0.21887362003326416\n",
      "iteration 27841: loss: 0.2188733071088791\n",
      "iteration 27842: loss: 0.2188730239868164\n",
      "iteration 27843: loss: 0.21887263655662537\n",
      "iteration 27844: loss: 0.2188723087310791\n",
      "iteration 27845: loss: 0.21887198090553284\n",
      "iteration 27846: loss: 0.21887163817882538\n",
      "iteration 27847: loss: 0.2188713252544403\n",
      "iteration 27848: loss: 0.21887095272541046\n",
      "iteration 27849: loss: 0.2188706398010254\n",
      "iteration 27850: loss: 0.21887032687664032\n",
      "iteration 27851: loss: 0.21887001395225525\n",
      "iteration 27852: loss: 0.2188696414232254\n",
      "iteration 27853: loss: 0.21886932849884033\n",
      "iteration 27854: loss: 0.21886906027793884\n",
      "iteration 27855: loss: 0.2188686579465866\n",
      "iteration 27856: loss: 0.21886837482452393\n",
      "iteration 27857: loss: 0.2188679724931717\n",
      "iteration 27858: loss: 0.218867689371109\n",
      "iteration 27859: loss: 0.21886734664440155\n",
      "iteration 27860: loss: 0.21886701881885529\n",
      "iteration 27861: loss: 0.21886667609214783\n",
      "iteration 27862: loss: 0.21886634826660156\n",
      "iteration 27863: loss: 0.21886606514453888\n",
      "iteration 27864: loss: 0.21886570751667023\n",
      "iteration 27865: loss: 0.21886534988880157\n",
      "iteration 27866: loss: 0.21886500716209412\n",
      "iteration 27867: loss: 0.21886472404003143\n",
      "iteration 27868: loss: 0.2188643217086792\n",
      "iteration 27869: loss: 0.21886400878429413\n",
      "iteration 27870: loss: 0.21886372566223145\n",
      "iteration 27871: loss: 0.21886341273784637\n",
      "iteration 27872: loss: 0.2188630849123001\n",
      "iteration 27873: loss: 0.21886268258094788\n",
      "iteration 27874: loss: 0.21886233985424042\n",
      "iteration 27875: loss: 0.21886205673217773\n",
      "iteration 27876: loss: 0.21886172890663147\n",
      "iteration 27877: loss: 0.2188614308834076\n",
      "iteration 27878: loss: 0.21886107325553894\n",
      "iteration 27879: loss: 0.21886074542999268\n",
      "iteration 27880: loss: 0.21886038780212402\n",
      "iteration 27881: loss: 0.21886005997657776\n",
      "iteration 27882: loss: 0.2188597470521927\n",
      "iteration 27883: loss: 0.21885938942432404\n",
      "iteration 27884: loss: 0.21885912120342255\n",
      "iteration 27885: loss: 0.2188587635755539\n",
      "iteration 27886: loss: 0.21885840594768524\n",
      "iteration 27887: loss: 0.21885812282562256\n",
      "iteration 27888: loss: 0.2188577950000763\n",
      "iteration 27889: loss: 0.21885748207569122\n",
      "iteration 27890: loss: 0.21885709464550018\n",
      "iteration 27891: loss: 0.2188568115234375\n",
      "iteration 27892: loss: 0.21885642409324646\n",
      "iteration 27893: loss: 0.21885614097118378\n",
      "iteration 27894: loss: 0.2188558280467987\n",
      "iteration 27895: loss: 0.21885547041893005\n",
      "iteration 27896: loss: 0.21885518729686737\n",
      "iteration 27897: loss: 0.21885478496551514\n",
      "iteration 27898: loss: 0.21885451674461365\n",
      "iteration 27899: loss: 0.21885418891906738\n",
      "iteration 27900: loss: 0.21885386109352112\n",
      "iteration 27901: loss: 0.21885351836681366\n",
      "iteration 27902: loss: 0.21885311603546143\n",
      "iteration 27903: loss: 0.21885283291339874\n",
      "iteration 27904: loss: 0.21885249018669128\n",
      "iteration 27905: loss: 0.2188522070646286\n",
      "iteration 27906: loss: 0.21885180473327637\n",
      "iteration 27907: loss: 0.2188514918088913\n",
      "iteration 27908: loss: 0.21885116398334503\n",
      "iteration 27909: loss: 0.21885082125663757\n",
      "iteration 27910: loss: 0.21885056793689728\n",
      "iteration 27911: loss: 0.21885022521018982\n",
      "iteration 27912: loss: 0.21884986758232117\n",
      "iteration 27913: loss: 0.2188495695590973\n",
      "iteration 27914: loss: 0.21884921193122864\n",
      "iteration 27915: loss: 0.21884889900684357\n",
      "iteration 27916: loss: 0.2188485562801361\n",
      "iteration 27917: loss: 0.21884825825691223\n",
      "iteration 27918: loss: 0.21884790062904358\n",
      "iteration 27919: loss: 0.21884755790233612\n",
      "iteration 27920: loss: 0.21884727478027344\n",
      "iteration 27921: loss: 0.21884691715240479\n",
      "iteration 27922: loss: 0.21884660422801971\n",
      "iteration 27923: loss: 0.21884624660015106\n",
      "iteration 27924: loss: 0.218845933675766\n",
      "iteration 27925: loss: 0.2188456505537033\n",
      "iteration 27926: loss: 0.21884532272815704\n",
      "iteration 27927: loss: 0.2188449651002884\n",
      "iteration 27928: loss: 0.21884457767009735\n",
      "iteration 27929: loss: 0.21884429454803467\n",
      "iteration 27930: loss: 0.2188439816236496\n",
      "iteration 27931: loss: 0.21884365379810333\n",
      "iteration 27932: loss: 0.21884331107139587\n",
      "iteration 27933: loss: 0.21884293854236603\n",
      "iteration 27934: loss: 0.21884270012378693\n",
      "iteration 27935: loss: 0.2188423126935959\n",
      "iteration 27936: loss: 0.2188420295715332\n",
      "iteration 27937: loss: 0.21884170174598694\n",
      "iteration 27938: loss: 0.21884135901927948\n",
      "iteration 27939: loss: 0.21884098649024963\n",
      "iteration 27940: loss: 0.21884074807167053\n",
      "iteration 27941: loss: 0.21884039044380188\n",
      "iteration 27942: loss: 0.21884000301361084\n",
      "iteration 27943: loss: 0.21883973479270935\n",
      "iteration 27944: loss: 0.21883945167064667\n",
      "iteration 27945: loss: 0.21883909404277802\n",
      "iteration 27946: loss: 0.21883876621723175\n",
      "iteration 27947: loss: 0.2188384234905243\n",
      "iteration 27948: loss: 0.21883806586265564\n",
      "iteration 27949: loss: 0.21883778274059296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 27950: loss: 0.21883749961853027\n",
      "iteration 27951: loss: 0.21883709728717804\n",
      "iteration 27952: loss: 0.2188367396593094\n",
      "iteration 27953: loss: 0.2188364714384079\n",
      "iteration 27954: loss: 0.21883614361286163\n",
      "iteration 27955: loss: 0.21883583068847656\n",
      "iteration 27956: loss: 0.21883544325828552\n",
      "iteration 27957: loss: 0.21883516013622284\n",
      "iteration 27958: loss: 0.21883483231067657\n",
      "iteration 27959: loss: 0.21883447468280792\n",
      "iteration 27960: loss: 0.21883416175842285\n",
      "iteration 27961: loss: 0.21883389353752136\n",
      "iteration 27962: loss: 0.2188335359096527\n",
      "iteration 27963: loss: 0.21883320808410645\n",
      "iteration 27964: loss: 0.21883288025856018\n",
      "iteration 27965: loss: 0.21883253753185272\n",
      "iteration 27966: loss: 0.21883220970630646\n",
      "iteration 27967: loss: 0.218831866979599\n",
      "iteration 27968: loss: 0.21883158385753632\n",
      "iteration 27969: loss: 0.21883121132850647\n",
      "iteration 27970: loss: 0.2188309133052826\n",
      "iteration 27971: loss: 0.21883061528205872\n",
      "iteration 27972: loss: 0.21883030235767365\n",
      "iteration 27973: loss: 0.2188299149274826\n",
      "iteration 27974: loss: 0.21882963180541992\n",
      "iteration 27975: loss: 0.21882930397987366\n",
      "iteration 27976: loss: 0.218828946352005\n",
      "iteration 27977: loss: 0.21882860362529755\n",
      "iteration 27978: loss: 0.21882832050323486\n",
      "iteration 27979: loss: 0.2188280075788498\n",
      "iteration 27980: loss: 0.21882767975330353\n",
      "iteration 27981: loss: 0.21882733702659607\n",
      "iteration 27982: loss: 0.2188270092010498\n",
      "iteration 27983: loss: 0.21882669627666473\n",
      "iteration 27984: loss: 0.21882641315460205\n",
      "iteration 27985: loss: 0.21882610023021698\n",
      "iteration 27986: loss: 0.21882569789886475\n",
      "iteration 27987: loss: 0.21882538497447968\n",
      "iteration 27988: loss: 0.2188250571489334\n",
      "iteration 27989: loss: 0.21882471442222595\n",
      "iteration 27990: loss: 0.21882443130016327\n",
      "iteration 27991: loss: 0.21882407367229462\n",
      "iteration 27992: loss: 0.21882376074790955\n",
      "iteration 27993: loss: 0.21882347762584686\n",
      "iteration 27994: loss: 0.2188231199979782\n",
      "iteration 27995: loss: 0.21882283687591553\n",
      "iteration 27996: loss: 0.21882250905036926\n",
      "iteration 27997: loss: 0.2188221514225006\n",
      "iteration 27998: loss: 0.21882180869579315\n",
      "iteration 27999: loss: 0.21882155537605286\n",
      "iteration 28000: loss: 0.2188212126493454\n",
      "iteration 28001: loss: 0.21882085502147675\n",
      "iteration 28002: loss: 0.21882054209709167\n",
      "iteration 28003: loss: 0.2188202440738678\n",
      "iteration 28004: loss: 0.21881988644599915\n",
      "iteration 28005: loss: 0.2188195437192917\n",
      "iteration 28006: loss: 0.21881921589374542\n",
      "iteration 28007: loss: 0.21881890296936035\n",
      "iteration 28008: loss: 0.21881861984729767\n",
      "iteration 28009: loss: 0.21881823241710663\n",
      "iteration 28010: loss: 0.21881791949272156\n",
      "iteration 28011: loss: 0.2188175916671753\n",
      "iteration 28012: loss: 0.21881727874279022\n",
      "iteration 28013: loss: 0.21881696581840515\n",
      "iteration 28014: loss: 0.21881668269634247\n",
      "iteration 28015: loss: 0.21881639957427979\n",
      "iteration 28016: loss: 0.21881595253944397\n",
      "iteration 28017: loss: 0.21881568431854248\n",
      "iteration 28018: loss: 0.2188154011964798\n",
      "iteration 28019: loss: 0.21881504356861115\n",
      "iteration 28020: loss: 0.2188147008419037\n",
      "iteration 28021: loss: 0.218814417719841\n",
      "iteration 28022: loss: 0.21881404519081116\n",
      "iteration 28023: loss: 0.21881377696990967\n",
      "iteration 28024: loss: 0.21881341934204102\n",
      "iteration 28025: loss: 0.21881306171417236\n",
      "iteration 28026: loss: 0.21881279349327087\n",
      "iteration 28027: loss: 0.2188125103712082\n",
      "iteration 28028: loss: 0.21881210803985596\n",
      "iteration 28029: loss: 0.2188117802143097\n",
      "iteration 28030: loss: 0.2188115119934082\n",
      "iteration 28031: loss: 0.21881119906902313\n",
      "iteration 28032: loss: 0.2188108265399933\n",
      "iteration 28033: loss: 0.21881051361560822\n",
      "iteration 28034: loss: 0.21881020069122314\n",
      "iteration 28035: loss: 0.21880987286567688\n",
      "iteration 28036: loss: 0.21880951523780823\n",
      "iteration 28037: loss: 0.21880921721458435\n",
      "iteration 28038: loss: 0.21880896389484406\n",
      "iteration 28039: loss: 0.2188086062669754\n",
      "iteration 28040: loss: 0.21880821883678436\n",
      "iteration 28041: loss: 0.2188079059123993\n",
      "iteration 28042: loss: 0.218807652592659\n",
      "iteration 28043: loss: 0.21880730986595154\n",
      "iteration 28044: loss: 0.21880701184272766\n",
      "iteration 28045: loss: 0.2188066691160202\n",
      "iteration 28046: loss: 0.21880629658699036\n",
      "iteration 28047: loss: 0.21880602836608887\n",
      "iteration 28048: loss: 0.21880574524402618\n",
      "iteration 28049: loss: 0.21880535781383514\n",
      "iteration 28050: loss: 0.21880507469177246\n",
      "iteration 28051: loss: 0.21880468726158142\n",
      "iteration 28052: loss: 0.21880435943603516\n",
      "iteration 28053: loss: 0.21880407631397247\n",
      "iteration 28054: loss: 0.2188037633895874\n",
      "iteration 28055: loss: 0.21880343556404114\n",
      "iteration 28056: loss: 0.21880312263965607\n",
      "iteration 28057: loss: 0.2188027799129486\n",
      "iteration 28058: loss: 0.21880245208740234\n",
      "iteration 28059: loss: 0.21880212426185608\n",
      "iteration 28060: loss: 0.218801811337471\n",
      "iteration 28061: loss: 0.21880152821540833\n",
      "iteration 28062: loss: 0.21880117058753967\n",
      "iteration 28063: loss: 0.21880082786083221\n",
      "iteration 28064: loss: 0.21880054473876953\n",
      "iteration 28065: loss: 0.21880018711090088\n",
      "iteration 28066: loss: 0.2187999039888382\n",
      "iteration 28067: loss: 0.21879956126213074\n",
      "iteration 28068: loss: 0.21879926323890686\n",
      "iteration 28069: loss: 0.2187989503145218\n",
      "iteration 28070: loss: 0.21879863739013672\n",
      "iteration 28071: loss: 0.21879835426807404\n",
      "iteration 28072: loss: 0.21879801154136658\n",
      "iteration 28073: loss: 0.2187977135181427\n",
      "iteration 28074: loss: 0.21879732608795166\n",
      "iteration 28075: loss: 0.21879705786705017\n",
      "iteration 28076: loss: 0.21879668533802032\n",
      "iteration 28077: loss: 0.21879637241363525\n",
      "iteration 28078: loss: 0.21879605948925018\n",
      "iteration 28079: loss: 0.2187957763671875\n",
      "iteration 28080: loss: 0.21879538893699646\n",
      "iteration 28081: loss: 0.21879515051841736\n",
      "iteration 28082: loss: 0.2187948226928711\n",
      "iteration 28083: loss: 0.21879442036151886\n",
      "iteration 28084: loss: 0.21879413723945618\n",
      "iteration 28085: loss: 0.2187938243150711\n",
      "iteration 28086: loss: 0.21879346668720245\n",
      "iteration 28087: loss: 0.21879319846630096\n",
      "iteration 28088: loss: 0.2187928706407547\n",
      "iteration 28089: loss: 0.2187926024198532\n",
      "iteration 28090: loss: 0.21879220008850098\n",
      "iteration 28091: loss: 0.2187918871641159\n",
      "iteration 28092: loss: 0.21879157423973083\n",
      "iteration 28093: loss: 0.21879127621650696\n",
      "iteration 28094: loss: 0.2187909185886383\n",
      "iteration 28095: loss: 0.21879065036773682\n",
      "iteration 28096: loss: 0.21879032254219055\n",
      "iteration 28097: loss: 0.2187899798154831\n",
      "iteration 28098: loss: 0.21878965198993683\n",
      "iteration 28099: loss: 0.21878938376903534\n",
      "iteration 28100: loss: 0.2187890261411667\n",
      "iteration 28101: loss: 0.218788743019104\n",
      "iteration 28102: loss: 0.21878838539123535\n",
      "iteration 28103: loss: 0.21878811717033386\n",
      "iteration 28104: loss: 0.21878774464130402\n",
      "iteration 28105: loss: 0.21878738701343536\n",
      "iteration 28106: loss: 0.2187870442867279\n",
      "iteration 28107: loss: 0.21878676116466522\n",
      "iteration 28108: loss: 0.21878643333911896\n",
      "iteration 28109: loss: 0.21878615021705627\n",
      "iteration 28110: loss: 0.21878579258918762\n",
      "iteration 28111: loss: 0.21878549456596375\n",
      "iteration 28112: loss: 0.21878519654273987\n",
      "iteration 28113: loss: 0.2187848538160324\n",
      "iteration 28114: loss: 0.2187846153974533\n",
      "iteration 28115: loss: 0.21878421306610107\n",
      "iteration 28116: loss: 0.21878397464752197\n",
      "iteration 28117: loss: 0.2187836468219757\n",
      "iteration 28118: loss: 0.21878328919410706\n",
      "iteration 28119: loss: 0.2187829315662384\n",
      "iteration 28120: loss: 0.21878263354301453\n",
      "iteration 28121: loss: 0.21878230571746826\n",
      "iteration 28122: loss: 0.21878202259540558\n",
      "iteration 28123: loss: 0.21878167986869812\n",
      "iteration 28124: loss: 0.21878139674663544\n",
      "iteration 28125: loss: 0.21878106892108917\n",
      "iteration 28126: loss: 0.2187807559967041\n",
      "iteration 28127: loss: 0.21878039836883545\n",
      "iteration 28128: loss: 0.21878013014793396\n",
      "iteration 28129: loss: 0.2187798023223877\n",
      "iteration 28130: loss: 0.21877944469451904\n",
      "iteration 28131: loss: 0.21877920627593994\n",
      "iteration 28132: loss: 0.21877887845039368\n",
      "iteration 28133: loss: 0.2187785655260086\n",
      "iteration 28134: loss: 0.21877820789813995\n",
      "iteration 28135: loss: 0.2187778651714325\n",
      "iteration 28136: loss: 0.21877756714820862\n",
      "iteration 28137: loss: 0.21877725422382355\n",
      "iteration 28138: loss: 0.21877701580524445\n",
      "iteration 28139: loss: 0.2187766283750534\n",
      "iteration 28140: loss: 0.21877630054950714\n",
      "iteration 28141: loss: 0.21877598762512207\n",
      "iteration 28142: loss: 0.2187756597995758\n",
      "iteration 28143: loss: 0.21877534687519073\n",
      "iteration 28144: loss: 0.21877503395080566\n",
      "iteration 28145: loss: 0.2187747210264206\n",
      "iteration 28146: loss: 0.21877439320087433\n",
      "iteration 28147: loss: 0.21877415478229523\n",
      "iteration 28148: loss: 0.218773752450943\n",
      "iteration 28149: loss: 0.21877343952655792\n",
      "iteration 28150: loss: 0.21877315640449524\n",
      "iteration 28151: loss: 0.21877285838127136\n",
      "iteration 28152: loss: 0.21877245604991913\n",
      "iteration 28153: loss: 0.21877220273017883\n",
      "iteration 28154: loss: 0.2187718152999878\n",
      "iteration 28155: loss: 0.21877160668373108\n",
      "iteration 28156: loss: 0.21877117455005646\n",
      "iteration 28157: loss: 0.21877089142799377\n",
      "iteration 28158: loss: 0.21877062320709229\n",
      "iteration 28159: loss: 0.21877026557922363\n",
      "iteration 28160: loss: 0.21877002716064453\n",
      "iteration 28161: loss: 0.2187696248292923\n",
      "iteration 28162: loss: 0.21876934170722961\n",
      "iteration 28163: loss: 0.21876904368400574\n",
      "iteration 28164: loss: 0.2187686711549759\n",
      "iteration 28165: loss: 0.2187684029340744\n",
      "iteration 28166: loss: 0.21876807510852814\n",
      "iteration 28167: loss: 0.21876773238182068\n",
      "iteration 28168: loss: 0.21876747906208038\n",
      "iteration 28169: loss: 0.21876712143421173\n",
      "iteration 28170: loss: 0.21876680850982666\n",
      "iteration 28171: loss: 0.2187664955854416\n",
      "iteration 28172: loss: 0.21876616775989532\n",
      "iteration 28173: loss: 0.21876585483551025\n",
      "iteration 28174: loss: 0.2187654972076416\n",
      "iteration 28175: loss: 0.2187652289867401\n",
      "iteration 28176: loss: 0.21876485645771027\n",
      "iteration 28177: loss: 0.21876458823680878\n",
      "iteration 28178: loss: 0.21876433491706848\n",
      "iteration 28179: loss: 0.21876394748687744\n",
      "iteration 28180: loss: 0.21876370906829834\n",
      "iteration 28181: loss: 0.2187633067369461\n",
      "iteration 28182: loss: 0.21876299381256104\n",
      "iteration 28183: loss: 0.21876272559165955\n",
      "iteration 28184: loss: 0.21876239776611328\n",
      "iteration 28185: loss: 0.21876206994056702\n",
      "iteration 28186: loss: 0.21876177191734314\n",
      "iteration 28187: loss: 0.2187614142894745\n",
      "iteration 28188: loss: 0.2187611609697342\n",
      "iteration 28189: loss: 0.21876077353954315\n",
      "iteration 28190: loss: 0.21876049041748047\n",
      "iteration 28191: loss: 0.21876020729541779\n",
      "iteration 28192: loss: 0.21875986456871033\n",
      "iteration 28193: loss: 0.21875956654548645\n",
      "iteration 28194: loss: 0.218759223818779\n",
      "iteration 28195: loss: 0.21875901520252228\n",
      "iteration 28196: loss: 0.21875858306884766\n",
      "iteration 28197: loss: 0.21875834465026855\n",
      "iteration 28198: loss: 0.21875795722007751\n",
      "iteration 28199: loss: 0.21875767409801483\n",
      "iteration 28200: loss: 0.21875730156898499\n",
      "iteration 28201: loss: 0.21875706315040588\n",
      "iteration 28202: loss: 0.218756765127182\n",
      "iteration 28203: loss: 0.21875639259815216\n",
      "iteration 28204: loss: 0.2187560796737671\n",
      "iteration 28205: loss: 0.21875576674938202\n",
      "iteration 28206: loss: 0.21875545382499695\n",
      "iteration 28207: loss: 0.2187550961971283\n",
      "iteration 28208: loss: 0.218754842877388\n",
      "iteration 28209: loss: 0.21875455975532532\n",
      "iteration 28210: loss: 0.21875420212745667\n",
      "iteration 28211: loss: 0.2187538892030716\n",
      "iteration 28212: loss: 0.2187536060810089\n",
      "iteration 28213: loss: 0.21875324845314026\n",
      "iteration 28214: loss: 0.21875295042991638\n",
      "iteration 28215: loss: 0.2187526524066925\n",
      "iteration 28216: loss: 0.21875229477882385\n",
      "iteration 28217: loss: 0.21875199675559998\n",
      "iteration 28218: loss: 0.21875163912773132\n",
      "iteration 28219: loss: 0.2187514752149582\n",
      "iteration 28220: loss: 0.21875107288360596\n",
      "iteration 28221: loss: 0.2187507152557373\n",
      "iteration 28222: loss: 0.21875043213367462\n",
      "iteration 28223: loss: 0.21875008940696716\n",
      "iteration 28224: loss: 0.21874980628490448\n",
      "iteration 28225: loss: 0.21874947845935822\n",
      "iteration 28226: loss: 0.21874921023845673\n",
      "iteration 28227: loss: 0.21874892711639404\n",
      "iteration 28228: loss: 0.2187485694885254\n",
      "iteration 28229: loss: 0.21874825656414032\n",
      "iteration 28230: loss: 0.21874792873859406\n",
      "iteration 28231: loss: 0.21874761581420898\n",
      "iteration 28232: loss: 0.2187473326921463\n",
      "iteration 28233: loss: 0.21874704957008362\n",
      "iteration 28234: loss: 0.21874670684337616\n",
      "iteration 28235: loss: 0.2187463939189911\n",
      "iteration 28236: loss: 0.21874603629112244\n",
      "iteration 28237: loss: 0.21874575316905975\n",
      "iteration 28238: loss: 0.2187453806400299\n",
      "iteration 28239: loss: 0.2187451422214508\n",
      "iteration 28240: loss: 0.21874479949474335\n",
      "iteration 28241: loss: 0.21874448657035828\n",
      "iteration 28242: loss: 0.218744158744812\n",
      "iteration 28243: loss: 0.21874384582042694\n",
      "iteration 28244: loss: 0.21874353289604187\n",
      "iteration 28245: loss: 0.21874327957630157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 28246: loss: 0.21874293684959412\n",
      "iteration 28247: loss: 0.21874256432056427\n",
      "iteration 28248: loss: 0.2187422513961792\n",
      "iteration 28249: loss: 0.2187420129776001\n",
      "iteration 28250: loss: 0.21874168515205383\n",
      "iteration 28251: loss: 0.21874141693115234\n",
      "iteration 28252: loss: 0.2187410593032837\n",
      "iteration 28253: loss: 0.21874074637889862\n",
      "iteration 28254: loss: 0.21874038875102997\n",
      "iteration 28255: loss: 0.2187400758266449\n",
      "iteration 28256: loss: 0.2187398225069046\n",
      "iteration 28257: loss: 0.21873946487903595\n",
      "iteration 28258: loss: 0.21873919665813446\n",
      "iteration 28259: loss: 0.2187388837337494\n",
      "iteration 28260: loss: 0.2187385857105255\n",
      "iteration 28261: loss: 0.21873827278614044\n",
      "iteration 28262: loss: 0.2187378853559494\n",
      "iteration 28263: loss: 0.2187376320362091\n",
      "iteration 28264: loss: 0.21873733401298523\n",
      "iteration 28265: loss: 0.21873700618743896\n",
      "iteration 28266: loss: 0.21873673796653748\n",
      "iteration 28267: loss: 0.21873636543750763\n",
      "iteration 28268: loss: 0.21873608231544495\n",
      "iteration 28269: loss: 0.21873576939105988\n",
      "iteration 28270: loss: 0.2187354564666748\n",
      "iteration 28271: loss: 0.21873512864112854\n",
      "iteration 28272: loss: 0.21873483061790466\n",
      "iteration 28273: loss: 0.218734472990036\n",
      "iteration 28274: loss: 0.21873421967029572\n",
      "iteration 28275: loss: 0.21873390674591064\n",
      "iteration 28276: loss: 0.21873359382152557\n",
      "iteration 28277: loss: 0.21873323619365692\n",
      "iteration 28278: loss: 0.21873295307159424\n",
      "iteration 28279: loss: 0.21873264014720917\n",
      "iteration 28280: loss: 0.21873238682746887\n",
      "iteration 28281: loss: 0.2187320441007614\n",
      "iteration 28282: loss: 0.21873171627521515\n",
      "iteration 28283: loss: 0.21873144805431366\n",
      "iteration 28284: loss: 0.21873116493225098\n",
      "iteration 28285: loss: 0.21873077750205994\n",
      "iteration 28286: loss: 0.21873044967651367\n",
      "iteration 28287: loss: 0.2187301218509674\n",
      "iteration 28288: loss: 0.21872982382774353\n",
      "iteration 28289: loss: 0.21872957050800323\n",
      "iteration 28290: loss: 0.21872925758361816\n",
      "iteration 28291: loss: 0.21872897446155548\n",
      "iteration 28292: loss: 0.2187286615371704\n",
      "iteration 28293: loss: 0.21872827410697937\n",
      "iteration 28294: loss: 0.2187279760837555\n",
      "iteration 28295: loss: 0.218727707862854\n",
      "iteration 28296: loss: 0.21872743964195251\n",
      "iteration 28297: loss: 0.21872706711292267\n",
      "iteration 28298: loss: 0.2187267541885376\n",
      "iteration 28299: loss: 0.21872644126415253\n",
      "iteration 28300: loss: 0.21872612833976746\n",
      "iteration 28301: loss: 0.2187258005142212\n",
      "iteration 28302: loss: 0.21872559189796448\n",
      "iteration 28303: loss: 0.21872524917125702\n",
      "iteration 28304: loss: 0.21872489154338837\n",
      "iteration 28305: loss: 0.21872463822364807\n",
      "iteration 28306: loss: 0.21872428059577942\n",
      "iteration 28307: loss: 0.21872396767139435\n",
      "iteration 28308: loss: 0.21872368454933167\n",
      "iteration 28309: loss: 0.2187233418226242\n",
      "iteration 28310: loss: 0.21872305870056152\n",
      "iteration 28311: loss: 0.21872273087501526\n",
      "iteration 28312: loss: 0.2187223881483078\n",
      "iteration 28313: loss: 0.21872210502624512\n",
      "iteration 28314: loss: 0.21872182190418243\n",
      "iteration 28315: loss: 0.21872150897979736\n",
      "iteration 28316: loss: 0.2187211811542511\n",
      "iteration 28317: loss: 0.21872088313102722\n",
      "iteration 28318: loss: 0.21872058510780334\n",
      "iteration 28319: loss: 0.2187202423810959\n",
      "iteration 28320: loss: 0.2187199592590332\n",
      "iteration 28321: loss: 0.2187197208404541\n",
      "iteration 28322: loss: 0.21871928870677948\n",
      "iteration 28323: loss: 0.21871905028820038\n",
      "iteration 28324: loss: 0.2187187224626541\n",
      "iteration 28325: loss: 0.21871840953826904\n",
      "iteration 28326: loss: 0.2187180519104004\n",
      "iteration 28327: loss: 0.2187178134918213\n",
      "iteration 28328: loss: 0.21871748566627502\n",
      "iteration 28329: loss: 0.21871714293956757\n",
      "iteration 28330: loss: 0.21871688961982727\n",
      "iteration 28331: loss: 0.2187165766954422\n",
      "iteration 28332: loss: 0.2187163084745407\n",
      "iteration 28333: loss: 0.21871590614318848\n",
      "iteration 28334: loss: 0.2187156230211258\n",
      "iteration 28335: loss: 0.2187153398990631\n",
      "iteration 28336: loss: 0.21871504187583923\n",
      "iteration 28337: loss: 0.21871474385261536\n",
      "iteration 28338: loss: 0.2187144011259079\n",
      "iteration 28339: loss: 0.21871408820152283\n",
      "iteration 28340: loss: 0.21871379017829895\n",
      "iteration 28341: loss: 0.21871349215507507\n",
      "iteration 28342: loss: 0.2187131941318512\n",
      "iteration 28343: loss: 0.21871285140514374\n",
      "iteration 28344: loss: 0.21871261298656464\n",
      "iteration 28345: loss: 0.21871225535869598\n",
      "iteration 28346: loss: 0.2187119424343109\n",
      "iteration 28347: loss: 0.21871161460876465\n",
      "iteration 28348: loss: 0.21871134638786316\n",
      "iteration 28349: loss: 0.2187110185623169\n",
      "iteration 28350: loss: 0.21871069073677063\n",
      "iteration 28351: loss: 0.21871042251586914\n",
      "iteration 28352: loss: 0.2187100350856781\n",
      "iteration 28353: loss: 0.2187097817659378\n",
      "iteration 28354: loss: 0.21870946884155273\n",
      "iteration 28355: loss: 0.21870918571949005\n",
      "iteration 28356: loss: 0.21870891749858856\n",
      "iteration 28357: loss: 0.2187085896730423\n",
      "iteration 28358: loss: 0.21870824694633484\n",
      "iteration 28359: loss: 0.21870791912078857\n",
      "iteration 28360: loss: 0.2187076359987259\n",
      "iteration 28361: loss: 0.21870729327201843\n",
      "iteration 28362: loss: 0.21870703995227814\n",
      "iteration 28363: loss: 0.2187066525220871\n",
      "iteration 28364: loss: 0.21870636940002441\n",
      "iteration 28365: loss: 0.2187061756849289\n",
      "iteration 28366: loss: 0.21870577335357666\n",
      "iteration 28367: loss: 0.21870549023151398\n",
      "iteration 28368: loss: 0.21870514750480652\n",
      "iteration 28369: loss: 0.21870484948158264\n",
      "iteration 28370: loss: 0.21870450675487518\n",
      "iteration 28371: loss: 0.21870429813861847\n",
      "iteration 28372: loss: 0.21870402991771698\n",
      "iteration 28373: loss: 0.21870365738868713\n",
      "iteration 28374: loss: 0.21870335936546326\n",
      "iteration 28375: loss: 0.21870307624340057\n",
      "iteration 28376: loss: 0.2187027484178543\n",
      "iteration 28377: loss: 0.21870239078998566\n",
      "iteration 28378: loss: 0.21870210766792297\n",
      "iteration 28379: loss: 0.2187018096446991\n",
      "iteration 28380: loss: 0.2187015265226364\n",
      "iteration 28381: loss: 0.21870119869709015\n",
      "iteration 28382: loss: 0.2187008559703827\n",
      "iteration 28383: loss: 0.2187006026506424\n",
      "iteration 28384: loss: 0.21870028972625732\n",
      "iteration 28385: loss: 0.21870000660419464\n",
      "iteration 28386: loss: 0.21869969367980957\n",
      "iteration 28387: loss: 0.21869930624961853\n",
      "iteration 28388: loss: 0.21869906783103943\n",
      "iteration 28389: loss: 0.21869878470897675\n",
      "iteration 28390: loss: 0.21869845688343048\n",
      "iteration 28391: loss: 0.21869811415672302\n",
      "iteration 28392: loss: 0.21869783103466034\n",
      "iteration 28393: loss: 0.21869751811027527\n",
      "iteration 28394: loss: 0.21869723498821259\n",
      "iteration 28395: loss: 0.21869690716266632\n",
      "iteration 28396: loss: 0.21869659423828125\n",
      "iteration 28397: loss: 0.21869635581970215\n",
      "iteration 28398: loss: 0.21869602799415588\n",
      "iteration 28399: loss: 0.2186957150697708\n",
      "iteration 28400: loss: 0.21869540214538574\n",
      "iteration 28401: loss: 0.21869511902332306\n",
      "iteration 28402: loss: 0.218694806098938\n",
      "iteration 28403: loss: 0.2186945378780365\n",
      "iteration 28404: loss: 0.21869416534900665\n",
      "iteration 28405: loss: 0.21869388222694397\n",
      "iteration 28406: loss: 0.21869361400604248\n",
      "iteration 28407: loss: 0.21869322657585144\n",
      "iteration 28408: loss: 0.21869297325611115\n",
      "iteration 28409: loss: 0.21869266033172607\n",
      "iteration 28410: loss: 0.2186923325061798\n",
      "iteration 28411: loss: 0.21869206428527832\n",
      "iteration 28412: loss: 0.21869179606437683\n",
      "iteration 28413: loss: 0.2186913937330246\n",
      "iteration 28414: loss: 0.21869111061096191\n",
      "iteration 28415: loss: 0.21869082748889923\n",
      "iteration 28416: loss: 0.21869054436683655\n",
      "iteration 28417: loss: 0.2186901867389679\n",
      "iteration 28418: loss: 0.21868988871574402\n",
      "iteration 28419: loss: 0.21868959069252014\n",
      "iteration 28420: loss: 0.21868927776813507\n",
      "iteration 28421: loss: 0.21868903934955597\n",
      "iteration 28422: loss: 0.21868877112865448\n",
      "iteration 28423: loss: 0.21868833899497986\n",
      "iteration 28424: loss: 0.21868804097175598\n",
      "iteration 28425: loss: 0.2186877280473709\n",
      "iteration 28426: loss: 0.21868745982646942\n",
      "iteration 28427: loss: 0.21868722140789032\n",
      "iteration 28428: loss: 0.21868689358234406\n",
      "iteration 28429: loss: 0.21868658065795898\n",
      "iteration 28430: loss: 0.2186862677335739\n",
      "iteration 28431: loss: 0.21868595480918884\n",
      "iteration 28432: loss: 0.21868570148944855\n",
      "iteration 28433: loss: 0.2186853438615799\n",
      "iteration 28434: loss: 0.2186850756406784\n",
      "iteration 28435: loss: 0.21868476271629333\n",
      "iteration 28436: loss: 0.21868443489074707\n",
      "iteration 28437: loss: 0.21868416666984558\n",
      "iteration 28438: loss: 0.21868380904197693\n",
      "iteration 28439: loss: 0.21868351101875305\n",
      "iteration 28440: loss: 0.21868321299552917\n",
      "iteration 28441: loss: 0.2186829149723053\n",
      "iteration 28442: loss: 0.21868260204792023\n",
      "iteration 28443: loss: 0.21868231892585754\n",
      "iteration 28444: loss: 0.21868197619915009\n",
      "iteration 28445: loss: 0.21868176758289337\n",
      "iteration 28446: loss: 0.21868140995502472\n",
      "iteration 28447: loss: 0.21868112683296204\n",
      "iteration 28448: loss: 0.21868081390857697\n",
      "iteration 28449: loss: 0.21868057548999786\n",
      "iteration 28450: loss: 0.21868018805980682\n",
      "iteration 28451: loss: 0.21867993474006653\n",
      "iteration 28452: loss: 0.21867959201335907\n",
      "iteration 28453: loss: 0.2186793088912964\n",
      "iteration 28454: loss: 0.21867895126342773\n",
      "iteration 28455: loss: 0.21867868304252625\n",
      "iteration 28456: loss: 0.21867835521697998\n",
      "iteration 28457: loss: 0.2186780720949173\n",
      "iteration 28458: loss: 0.21867775917053223\n",
      "iteration 28459: loss: 0.21867752075195312\n",
      "iteration 28460: loss: 0.21867720782756805\n",
      "iteration 28461: loss: 0.2186768800020218\n",
      "iteration 28462: loss: 0.21867653727531433\n",
      "iteration 28463: loss: 0.21867628395557404\n",
      "iteration 28464: loss: 0.21867597103118896\n",
      "iteration 28465: loss: 0.2186756134033203\n",
      "iteration 28466: loss: 0.2186753749847412\n",
      "iteration 28467: loss: 0.21867504715919495\n",
      "iteration 28468: loss: 0.21867480874061584\n",
      "iteration 28469: loss: 0.2186744511127472\n",
      "iteration 28470: loss: 0.2186741828918457\n",
      "iteration 28471: loss: 0.21867379546165466\n",
      "iteration 28472: loss: 0.21867355704307556\n",
      "iteration 28473: loss: 0.2186732292175293\n",
      "iteration 28474: loss: 0.21867291629314423\n",
      "iteration 28475: loss: 0.21867266297340393\n",
      "iteration 28476: loss: 0.21867232024669647\n",
      "iteration 28477: loss: 0.2186720073223114\n",
      "iteration 28478: loss: 0.21867170929908752\n",
      "iteration 28479: loss: 0.21867141127586365\n",
      "iteration 28480: loss: 0.21867112815380096\n",
      "iteration 28481: loss: 0.2186707705259323\n",
      "iteration 28482: loss: 0.21867048740386963\n",
      "iteration 28483: loss: 0.21867020428180695\n",
      "iteration 28484: loss: 0.21866992115974426\n",
      "iteration 28485: loss: 0.2186695784330368\n",
      "iteration 28486: loss: 0.2186693400144577\n",
      "iteration 28487: loss: 0.21866901218891144\n",
      "iteration 28488: loss: 0.21866877377033234\n",
      "iteration 28489: loss: 0.2186683863401413\n",
      "iteration 28490: loss: 0.218668133020401\n",
      "iteration 28491: loss: 0.21866783499717712\n",
      "iteration 28492: loss: 0.21866753697395325\n",
      "iteration 28493: loss: 0.2186671942472458\n",
      "iteration 28494: loss: 0.21866688132286072\n",
      "iteration 28495: loss: 0.21866659820079803\n",
      "iteration 28496: loss: 0.21866635978221893\n",
      "iteration 28497: loss: 0.21866604685783386\n",
      "iteration 28498: loss: 0.2186656892299652\n",
      "iteration 28499: loss: 0.21866543591022491\n",
      "iteration 28500: loss: 0.21866507828235626\n",
      "iteration 28501: loss: 0.21866479516029358\n",
      "iteration 28502: loss: 0.2186644971370697\n",
      "iteration 28503: loss: 0.2186642587184906\n",
      "iteration 28504: loss: 0.21866388618946075\n",
      "iteration 28505: loss: 0.21866357326507568\n",
      "iteration 28506: loss: 0.218663290143013\n",
      "iteration 28507: loss: 0.2186630219221115\n",
      "iteration 28508: loss: 0.21866269409656525\n",
      "iteration 28509: loss: 0.21866238117218018\n",
      "iteration 28510: loss: 0.2186620980501175\n",
      "iteration 28511: loss: 0.21866178512573242\n",
      "iteration 28512: loss: 0.21866150200366974\n",
      "iteration 28513: loss: 0.21866121888160706\n",
      "iteration 28514: loss: 0.21866092085838318\n",
      "iteration 28515: loss: 0.21866059303283691\n",
      "iteration 28516: loss: 0.21866028010845184\n",
      "iteration 28517: loss: 0.21865999698638916\n",
      "iteration 28518: loss: 0.2186596393585205\n",
      "iteration 28519: loss: 0.2186594009399414\n",
      "iteration 28520: loss: 0.21865908801555634\n",
      "iteration 28521: loss: 0.21865883469581604\n",
      "iteration 28522: loss: 0.21865853667259216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 28523: loss: 0.2186582088470459\n",
      "iteration 28524: loss: 0.21865792572498322\n",
      "iteration 28525: loss: 0.21865765750408173\n",
      "iteration 28526: loss: 0.21865728497505188\n",
      "iteration 28527: loss: 0.218656986951828\n",
      "iteration 28528: loss: 0.2186567336320877\n",
      "iteration 28529: loss: 0.21865639090538025\n",
      "iteration 28530: loss: 0.21865610778331757\n",
      "iteration 28531: loss: 0.2186557948589325\n",
      "iteration 28532: loss: 0.2186555415391922\n",
      "iteration 28533: loss: 0.21865522861480713\n",
      "iteration 28534: loss: 0.21865494549274445\n",
      "iteration 28535: loss: 0.21865463256835938\n",
      "iteration 28536: loss: 0.21865427494049072\n",
      "iteration 28537: loss: 0.21865400671958923\n",
      "iteration 28538: loss: 0.21865370869636536\n",
      "iteration 28539: loss: 0.21865341067314148\n",
      "iteration 28540: loss: 0.21865305304527283\n",
      "iteration 28541: loss: 0.21865279972553253\n",
      "iteration 28542: loss: 0.21865248680114746\n",
      "iteration 28543: loss: 0.21865221858024597\n",
      "iteration 28544: loss: 0.21865198016166687\n",
      "iteration 28545: loss: 0.2186516523361206\n",
      "iteration 28546: loss: 0.21865133941173553\n",
      "iteration 28547: loss: 0.21865102648735046\n",
      "iteration 28548: loss: 0.2186507284641266\n",
      "iteration 28549: loss: 0.21865041553974152\n",
      "iteration 28550: loss: 0.21865013241767883\n",
      "iteration 28551: loss: 0.21864983439445496\n",
      "iteration 28552: loss: 0.21864958107471466\n",
      "iteration 28553: loss: 0.218649223446846\n",
      "iteration 28554: loss: 0.21864895522594452\n",
      "iteration 28555: loss: 0.21864864230155945\n",
      "iteration 28556: loss: 0.21864834427833557\n",
      "iteration 28557: loss: 0.2186480313539505\n",
      "iteration 28558: loss: 0.21864771842956543\n",
      "iteration 28559: loss: 0.21864745020866394\n",
      "iteration 28560: loss: 0.21864715218544006\n",
      "iteration 28561: loss: 0.21864691376686096\n",
      "iteration 28562: loss: 0.2186465561389923\n",
      "iteration 28563: loss: 0.21864624321460724\n",
      "iteration 28564: loss: 0.21864596009254456\n",
      "iteration 28565: loss: 0.21864564716815948\n",
      "iteration 28566: loss: 0.21864533424377441\n",
      "iteration 28567: loss: 0.21864505112171173\n",
      "iteration 28568: loss: 0.21864478290081024\n",
      "iteration 28569: loss: 0.21864457428455353\n",
      "iteration 28570: loss: 0.2186441719532013\n",
      "iteration 28571: loss: 0.2186439037322998\n",
      "iteration 28572: loss: 0.21864351630210876\n",
      "iteration 28573: loss: 0.21864330768585205\n",
      "iteration 28574: loss: 0.2186429798603058\n",
      "iteration 28575: loss: 0.2186427116394043\n",
      "iteration 28576: loss: 0.21864239871501923\n",
      "iteration 28577: loss: 0.21864211559295654\n",
      "iteration 28578: loss: 0.21864180266857147\n",
      "iteration 28579: loss: 0.2186414748430252\n",
      "iteration 28580: loss: 0.21864119172096252\n",
      "iteration 28581: loss: 0.21864084899425507\n",
      "iteration 28582: loss: 0.21864068508148193\n",
      "iteration 28583: loss: 0.2186402827501297\n",
      "iteration 28584: loss: 0.21863999962806702\n",
      "iteration 28585: loss: 0.21863970160484314\n",
      "iteration 28586: loss: 0.21863944828510284\n",
      "iteration 28587: loss: 0.21863916516304016\n",
      "iteration 28588: loss: 0.2186388224363327\n",
      "iteration 28589: loss: 0.21863850951194763\n",
      "iteration 28590: loss: 0.21863822638988495\n",
      "iteration 28591: loss: 0.21863797307014465\n",
      "iteration 28592: loss: 0.21863766014575958\n",
      "iteration 28593: loss: 0.2186373472213745\n",
      "iteration 28594: loss: 0.21863701939582825\n",
      "iteration 28595: loss: 0.21863670647144318\n",
      "iteration 28596: loss: 0.21863646805286407\n",
      "iteration 28597: loss: 0.21863612532615662\n",
      "iteration 28598: loss: 0.21863587200641632\n",
      "iteration 28599: loss: 0.21863555908203125\n",
      "iteration 28600: loss: 0.21863524615764618\n",
      "iteration 28601: loss: 0.2186349630355835\n",
      "iteration 28602: loss: 0.2186346799135208\n",
      "iteration 28603: loss: 0.21863439679145813\n",
      "iteration 28604: loss: 0.21863409876823425\n",
      "iteration 28605: loss: 0.21863381564617157\n",
      "iteration 28606: loss: 0.2186335027217865\n",
      "iteration 28607: loss: 0.21863317489624023\n",
      "iteration 28608: loss: 0.21863293647766113\n",
      "iteration 28609: loss: 0.21863260865211487\n",
      "iteration 28610: loss: 0.2186322957277298\n",
      "iteration 28611: loss: 0.21863201260566711\n",
      "iteration 28612: loss: 0.21863169968128204\n",
      "iteration 28613: loss: 0.21863143146038055\n",
      "iteration 28614: loss: 0.2186311036348343\n",
      "iteration 28615: loss: 0.21863076090812683\n",
      "iteration 28616: loss: 0.21863055229187012\n",
      "iteration 28617: loss: 0.21863023936748505\n",
      "iteration 28618: loss: 0.21862995624542236\n",
      "iteration 28619: loss: 0.2186295986175537\n",
      "iteration 28620: loss: 0.21862931549549103\n",
      "iteration 28621: loss: 0.2186291217803955\n",
      "iteration 28622: loss: 0.21862871944904327\n",
      "iteration 28623: loss: 0.21862845122814178\n",
      "iteration 28624: loss: 0.2186281681060791\n",
      "iteration 28625: loss: 0.21862785518169403\n",
      "iteration 28626: loss: 0.21862752735614777\n",
      "iteration 28627: loss: 0.2186272144317627\n",
      "iteration 28628: loss: 0.21862700581550598\n",
      "iteration 28629: loss: 0.2186266928911209\n",
      "iteration 28630: loss: 0.21862640976905823\n",
      "iteration 28631: loss: 0.21862617135047913\n",
      "iteration 28632: loss: 0.2186257541179657\n",
      "iteration 28633: loss: 0.2186255007982254\n",
      "iteration 28634: loss: 0.2186252772808075\n",
      "iteration 28635: loss: 0.21862497925758362\n",
      "iteration 28636: loss: 0.21862463653087616\n",
      "iteration 28637: loss: 0.2186243236064911\n",
      "iteration 28638: loss: 0.2186240702867508\n",
      "iteration 28639: loss: 0.2186238020658493\n",
      "iteration 28640: loss: 0.21862348914146423\n",
      "iteration 28641: loss: 0.21862323582172394\n",
      "iteration 28642: loss: 0.21862289309501648\n",
      "iteration 28643: loss: 0.21862253546714783\n",
      "iteration 28644: loss: 0.21862225234508514\n",
      "iteration 28645: loss: 0.21862204372882843\n",
      "iteration 28646: loss: 0.2186216562986374\n",
      "iteration 28647: loss: 0.2186213731765747\n",
      "iteration 28648: loss: 0.21862109005451202\n",
      "iteration 28649: loss: 0.21862077713012695\n",
      "iteration 28650: loss: 0.21862049400806427\n",
      "iteration 28651: loss: 0.21862022578716278\n",
      "iteration 28652: loss: 0.21861997246742249\n",
      "iteration 28653: loss: 0.21861965954303741\n",
      "iteration 28654: loss: 0.21861927211284637\n",
      "iteration 28655: loss: 0.21861903369426727\n",
      "iteration 28656: loss: 0.2186187505722046\n",
      "iteration 28657: loss: 0.2186184823513031\n",
      "iteration 28658: loss: 0.21861818432807922\n",
      "iteration 28659: loss: 0.21861787140369415\n",
      "iteration 28660: loss: 0.21861755847930908\n",
      "iteration 28661: loss: 0.21861732006072998\n",
      "iteration 28662: loss: 0.2186170071363449\n",
      "iteration 28663: loss: 0.21861669421195984\n",
      "iteration 28664: loss: 0.21861636638641357\n",
      "iteration 28665: loss: 0.21861609816551208\n",
      "iteration 28666: loss: 0.2186158150434494\n",
      "iteration 28667: loss: 0.21861550211906433\n",
      "iteration 28668: loss: 0.21861520409584045\n",
      "iteration 28669: loss: 0.21861496567726135\n",
      "iteration 28670: loss: 0.2186146229505539\n",
      "iteration 28671: loss: 0.2186143398284912\n",
      "iteration 28672: loss: 0.21861401200294495\n",
      "iteration 28673: loss: 0.21861371397972107\n",
      "iteration 28674: loss: 0.2186134308576584\n",
      "iteration 28675: loss: 0.21861322224140167\n",
      "iteration 28676: loss: 0.21861286461353302\n",
      "iteration 28677: loss: 0.21861255168914795\n",
      "iteration 28678: loss: 0.21861226856708527\n",
      "iteration 28679: loss: 0.21861200034618378\n",
      "iteration 28680: loss: 0.2186116874217987\n",
      "iteration 28681: loss: 0.21861140429973602\n",
      "iteration 28682: loss: 0.21861116588115692\n",
      "iteration 28683: loss: 0.21861088275909424\n",
      "iteration 28684: loss: 0.21861056983470917\n",
      "iteration 28685: loss: 0.2186102420091629\n",
      "iteration 28686: loss: 0.21860992908477783\n",
      "iteration 28687: loss: 0.21860964596271515\n",
      "iteration 28688: loss: 0.21860937774181366\n",
      "iteration 28689: loss: 0.21860909461975098\n",
      "iteration 28690: loss: 0.2186088114976883\n",
      "iteration 28691: loss: 0.2186085283756256\n",
      "iteration 28692: loss: 0.21860821545124054\n",
      "iteration 28693: loss: 0.21860790252685547\n",
      "iteration 28694: loss: 0.21860761940479279\n",
      "iteration 28695: loss: 0.2186073362827301\n",
      "iteration 28696: loss: 0.21860703825950623\n",
      "iteration 28697: loss: 0.21860674023628235\n",
      "iteration 28698: loss: 0.21860647201538086\n",
      "iteration 28699: loss: 0.2186061590909958\n",
      "iteration 28700: loss: 0.21860584616661072\n",
      "iteration 28701: loss: 0.21860560774803162\n",
      "iteration 28702: loss: 0.21860530972480774\n",
      "iteration 28703: loss: 0.21860499680042267\n",
      "iteration 28704: loss: 0.21860471367835999\n",
      "iteration 28705: loss: 0.21860437095165253\n",
      "iteration 28706: loss: 0.21860408782958984\n",
      "iteration 28707: loss: 0.21860380470752716\n",
      "iteration 28708: loss: 0.21860352158546448\n",
      "iteration 28709: loss: 0.218603253364563\n",
      "iteration 28710: loss: 0.2186029702425003\n",
      "iteration 28711: loss: 0.21860262751579285\n",
      "iteration 28712: loss: 0.21860238909721375\n",
      "iteration 28713: loss: 0.2186020314693451\n",
      "iteration 28714: loss: 0.2186017781496048\n",
      "iteration 28715: loss: 0.21860149502754211\n",
      "iteration 28716: loss: 0.21860118210315704\n",
      "iteration 28717: loss: 0.21860089898109436\n",
      "iteration 28718: loss: 0.21860063076019287\n",
      "iteration 28719: loss: 0.2186003178358078\n",
      "iteration 28720: loss: 0.21860003471374512\n",
      "iteration 28721: loss: 0.21859975159168243\n",
      "iteration 28722: loss: 0.21859951317310333\n",
      "iteration 28723: loss: 0.21859917044639587\n",
      "iteration 28724: loss: 0.2185988873243332\n",
      "iteration 28725: loss: 0.2185986042022705\n",
      "iteration 28726: loss: 0.21859827637672424\n",
      "iteration 28727: loss: 0.21859797835350037\n",
      "iteration 28728: loss: 0.21859769523143768\n",
      "iteration 28729: loss: 0.218597412109375\n",
      "iteration 28730: loss: 0.21859709918498993\n",
      "iteration 28731: loss: 0.21859681606292725\n",
      "iteration 28732: loss: 0.21859657764434814\n",
      "iteration 28733: loss: 0.2185962200164795\n",
      "iteration 28734: loss: 0.2185959815979004\n",
      "iteration 28735: loss: 0.2185956984758377\n",
      "iteration 28736: loss: 0.21859541535377502\n",
      "iteration 28737: loss: 0.21859511733055115\n",
      "iteration 28738: loss: 0.21859481930732727\n",
      "iteration 28739: loss: 0.2185945212841034\n",
      "iteration 28740: loss: 0.2185942381620407\n",
      "iteration 28741: loss: 0.21859392523765564\n",
      "iteration 28742: loss: 0.21859364211559296\n",
      "iteration 28743: loss: 0.21859338879585266\n",
      "iteration 28744: loss: 0.218593031167984\n",
      "iteration 28745: loss: 0.2185927927494049\n",
      "iteration 28746: loss: 0.21859245002269745\n",
      "iteration 28747: loss: 0.21859221160411835\n",
      "iteration 28748: loss: 0.21859189867973328\n",
      "iteration 28749: loss: 0.2185916006565094\n",
      "iteration 28750: loss: 0.2185913324356079\n",
      "iteration 28751: loss: 0.21859104931354523\n",
      "iteration 28752: loss: 0.21859070658683777\n",
      "iteration 28753: loss: 0.21859049797058105\n",
      "iteration 28754: loss: 0.21859018504619598\n",
      "iteration 28755: loss: 0.2185899317264557\n",
      "iteration 28756: loss: 0.21858961880207062\n",
      "iteration 28757: loss: 0.21858930587768555\n",
      "iteration 28758: loss: 0.21858897805213928\n",
      "iteration 28759: loss: 0.21858863532543182\n",
      "iteration 28760: loss: 0.2185884416103363\n",
      "iteration 28761: loss: 0.21858811378479004\n",
      "iteration 28762: loss: 0.21858783066272736\n",
      "iteration 28763: loss: 0.21858754754066467\n",
      "iteration 28764: loss: 0.21858730912208557\n",
      "iteration 28765: loss: 0.2185869663953781\n",
      "iteration 28766: loss: 0.21858660876750946\n",
      "iteration 28767: loss: 0.21858635544776917\n",
      "iteration 28768: loss: 0.21858613193035126\n",
      "iteration 28769: loss: 0.2185858190059662\n",
      "iteration 28770: loss: 0.2185855656862259\n",
      "iteration 28771: loss: 0.21858525276184082\n",
      "iteration 28772: loss: 0.21858496963977814\n",
      "iteration 28773: loss: 0.21858462691307068\n",
      "iteration 28774: loss: 0.218584343791008\n",
      "iteration 28775: loss: 0.2185841053724289\n",
      "iteration 28776: loss: 0.21858379244804382\n",
      "iteration 28777: loss: 0.21858350932598114\n",
      "iteration 28778: loss: 0.21858325600624084\n",
      "iteration 28779: loss: 0.21858295798301697\n",
      "iteration 28780: loss: 0.2185826599597931\n",
      "iteration 28781: loss: 0.21858234703540802\n",
      "iteration 28782: loss: 0.21858210861682892\n",
      "iteration 28783: loss: 0.21858179569244385\n",
      "iteration 28784: loss: 0.21858152747154236\n",
      "iteration 28785: loss: 0.21858124434947968\n",
      "iteration 28786: loss: 0.2185809165239334\n",
      "iteration 28787: loss: 0.2185806781053543\n",
      "iteration 28788: loss: 0.21858039498329163\n",
      "iteration 28789: loss: 0.21858003735542297\n",
      "iteration 28790: loss: 0.21857976913452148\n",
      "iteration 28791: loss: 0.21857950091362\n",
      "iteration 28792: loss: 0.2185792177915573\n",
      "iteration 28793: loss: 0.21857888996601105\n",
      "iteration 28794: loss: 0.21857872605323792\n",
      "iteration 28795: loss: 0.21857838332653046\n",
      "iteration 28796: loss: 0.21857810020446777\n",
      "iteration 28797: loss: 0.21857771277427673\n",
      "iteration 28798: loss: 0.2185775339603424\n",
      "iteration 28799: loss: 0.21857714653015137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 28800: loss: 0.21857687830924988\n",
      "iteration 28801: loss: 0.2185765951871872\n",
      "iteration 28802: loss: 0.2185763418674469\n",
      "iteration 28803: loss: 0.2185760736465454\n",
      "iteration 28804: loss: 0.21857571601867676\n",
      "iteration 28805: loss: 0.21857543289661407\n",
      "iteration 28806: loss: 0.21857509016990662\n",
      "iteration 28807: loss: 0.21857495605945587\n",
      "iteration 28808: loss: 0.2185746431350708\n",
      "iteration 28809: loss: 0.21857431530952454\n",
      "iteration 28810: loss: 0.21857404708862305\n",
      "iteration 28811: loss: 0.21857376396656036\n",
      "iteration 28812: loss: 0.2185734212398529\n",
      "iteration 28813: loss: 0.2185731679201126\n",
      "iteration 28814: loss: 0.21857289969921112\n",
      "iteration 28815: loss: 0.21857258677482605\n",
      "iteration 28816: loss: 0.21857228875160217\n",
      "iteration 28817: loss: 0.2185719907283783\n",
      "iteration 28818: loss: 0.218571737408638\n",
      "iteration 28819: loss: 0.2185714691877365\n",
      "iteration 28820: loss: 0.21857115626335144\n",
      "iteration 28821: loss: 0.21857090294361115\n",
      "iteration 28822: loss: 0.2185705602169037\n",
      "iteration 28823: loss: 0.21857032179832458\n",
      "iteration 28824: loss: 0.21856999397277832\n",
      "iteration 28825: loss: 0.21856968104839325\n",
      "iteration 28826: loss: 0.21856942772865295\n",
      "iteration 28827: loss: 0.21856912970542908\n",
      "iteration 28828: loss: 0.21856887638568878\n",
      "iteration 28829: loss: 0.2185685634613037\n",
      "iteration 28830: loss: 0.21856825053691864\n",
      "iteration 28831: loss: 0.21856799721717834\n",
      "iteration 28832: loss: 0.21856769919395447\n",
      "iteration 28833: loss: 0.21856741607189178\n",
      "iteration 28834: loss: 0.21856717765331268\n",
      "iteration 28835: loss: 0.2185668647289276\n",
      "iteration 28836: loss: 0.21856661140918732\n",
      "iteration 28837: loss: 0.21856626868247986\n",
      "iteration 28838: loss: 0.21856598556041718\n",
      "iteration 28839: loss: 0.2185656726360321\n",
      "iteration 28840: loss: 0.21856538951396942\n",
      "iteration 28841: loss: 0.21856513619422913\n",
      "iteration 28842: loss: 0.21856483817100525\n",
      "iteration 28843: loss: 0.21856459975242615\n",
      "iteration 28844: loss: 0.21856430172920227\n",
      "iteration 28845: loss: 0.21856403350830078\n",
      "iteration 28846: loss: 0.2185637503862381\n",
      "iteration 28847: loss: 0.21856346726417542\n",
      "iteration 28848: loss: 0.21856316924095154\n",
      "iteration 28849: loss: 0.21856288611888885\n",
      "iteration 28850: loss: 0.2185625582933426\n",
      "iteration 28851: loss: 0.21856224536895752\n",
      "iteration 28852: loss: 0.21856197714805603\n",
      "iteration 28853: loss: 0.21856173872947693\n",
      "iteration 28854: loss: 0.21856138110160828\n",
      "iteration 28855: loss: 0.21856112778186798\n",
      "iteration 28856: loss: 0.21856093406677246\n",
      "iteration 28857: loss: 0.21856053173542023\n",
      "iteration 28858: loss: 0.21856026351451874\n",
      "iteration 28859: loss: 0.21856001019477844\n",
      "iteration 28860: loss: 0.21855974197387695\n",
      "iteration 28861: loss: 0.21855945885181427\n",
      "iteration 28862: loss: 0.2185591161251068\n",
      "iteration 28863: loss: 0.2185588777065277\n",
      "iteration 28864: loss: 0.21855854988098145\n",
      "iteration 28865: loss: 0.21855831146240234\n",
      "iteration 28866: loss: 0.21855807304382324\n",
      "iteration 28867: loss: 0.21855774521827698\n",
      "iteration 28868: loss: 0.2185574471950531\n",
      "iteration 28869: loss: 0.21855714917182922\n",
      "iteration 28870: loss: 0.21855688095092773\n",
      "iteration 28871: loss: 0.21855659782886505\n",
      "iteration 28872: loss: 0.21855628490447998\n",
      "iteration 28873: loss: 0.21855604648590088\n",
      "iteration 28874: loss: 0.21855571866035461\n",
      "iteration 28875: loss: 0.21855542063713074\n",
      "iteration 28876: loss: 0.21855516731739044\n",
      "iteration 28877: loss: 0.21855488419532776\n",
      "iteration 28878: loss: 0.21855464577674866\n",
      "iteration 28879: loss: 0.2185543328523636\n",
      "iteration 28880: loss: 0.21855409443378448\n",
      "iteration 28881: loss: 0.2185538113117218\n",
      "iteration 28882: loss: 0.21855345368385315\n",
      "iteration 28883: loss: 0.21855315566062927\n",
      "iteration 28884: loss: 0.2185528725385666\n",
      "iteration 28885: loss: 0.2185526341199875\n",
      "iteration 28886: loss: 0.21855232119560242\n",
      "iteration 28887: loss: 0.21855206787586212\n",
      "iteration 28888: loss: 0.21855178475379944\n",
      "iteration 28889: loss: 0.21855144202709198\n",
      "iteration 28890: loss: 0.21855118870735168\n",
      "iteration 28891: loss: 0.2185509204864502\n",
      "iteration 28892: loss: 0.2185506522655487\n",
      "iteration 28893: loss: 0.21855032444000244\n",
      "iteration 28894: loss: 0.21855005621910095\n",
      "iteration 28895: loss: 0.2185496985912323\n",
      "iteration 28896: loss: 0.21854948997497559\n",
      "iteration 28897: loss: 0.21854925155639648\n",
      "iteration 28898: loss: 0.21854886412620544\n",
      "iteration 28899: loss: 0.21854862570762634\n",
      "iteration 28900: loss: 0.21854838728904724\n",
      "iteration 28901: loss: 0.21854805946350098\n",
      "iteration 28902: loss: 0.21854786574840546\n",
      "iteration 28903: loss: 0.2185475081205368\n",
      "iteration 28904: loss: 0.21854722499847412\n",
      "iteration 28905: loss: 0.21854686737060547\n",
      "iteration 28906: loss: 0.21854662895202637\n",
      "iteration 28907: loss: 0.21854639053344727\n",
      "iteration 28908: loss: 0.21854612231254578\n",
      "iteration 28909: loss: 0.21854586899280548\n",
      "iteration 28910: loss: 0.21854548156261444\n",
      "iteration 28911: loss: 0.21854527294635773\n",
      "iteration 28912: loss: 0.21854493021965027\n",
      "iteration 28913: loss: 0.21854467689990997\n",
      "iteration 28914: loss: 0.2185443937778473\n",
      "iteration 28915: loss: 0.2185441553592682\n",
      "iteration 28916: loss: 0.21854379773139954\n",
      "iteration 28917: loss: 0.21854355931282043\n",
      "iteration 28918: loss: 0.21854333579540253\n",
      "iteration 28919: loss: 0.21854300796985626\n",
      "iteration 28920: loss: 0.21854273974895477\n",
      "iteration 28921: loss: 0.2185424119234085\n",
      "iteration 28922: loss: 0.21854212880134583\n",
      "iteration 28923: loss: 0.21854190528392792\n",
      "iteration 28924: loss: 0.21854162216186523\n",
      "iteration 28925: loss: 0.21854126453399658\n",
      "iteration 28926: loss: 0.21854105591773987\n",
      "iteration 28927: loss: 0.218540757894516\n",
      "iteration 28928: loss: 0.21854040026664734\n",
      "iteration 28929: loss: 0.21854014694690704\n",
      "iteration 28930: loss: 0.21853987872600555\n",
      "iteration 28931: loss: 0.21853959560394287\n",
      "iteration 28932: loss: 0.21853938698768616\n",
      "iteration 28933: loss: 0.2185390740633011\n",
      "iteration 28934: loss: 0.21853876113891602\n",
      "iteration 28935: loss: 0.21853847801685333\n",
      "iteration 28936: loss: 0.21853816509246826\n",
      "iteration 28937: loss: 0.21853789687156677\n",
      "iteration 28938: loss: 0.21853764355182648\n",
      "iteration 28939: loss: 0.2185373306274414\n",
      "iteration 28940: loss: 0.2185370922088623\n",
      "iteration 28941: loss: 0.21853682398796082\n",
      "iteration 28942: loss: 0.21853654086589813\n",
      "iteration 28943: loss: 0.21853625774383545\n",
      "iteration 28944: loss: 0.21853597462177277\n",
      "iteration 28945: loss: 0.2185356318950653\n",
      "iteration 28946: loss: 0.21853534877300262\n",
      "iteration 28947: loss: 0.21853506565093994\n",
      "iteration 28948: loss: 0.21853478252887726\n",
      "iteration 28949: loss: 0.21853455901145935\n",
      "iteration 28950: loss: 0.2185342013835907\n",
      "iteration 28951: loss: 0.21853391826152802\n",
      "iteration 28952: loss: 0.2185337096452713\n",
      "iteration 28953: loss: 0.21853339672088623\n",
      "iteration 28954: loss: 0.21853318810462952\n",
      "iteration 28955: loss: 0.21853287518024445\n",
      "iteration 28956: loss: 0.21853256225585938\n",
      "iteration 28957: loss: 0.2185322791337967\n",
      "iteration 28958: loss: 0.2185320109128952\n",
      "iteration 28959: loss: 0.2185317575931549\n",
      "iteration 28960: loss: 0.21853141486644745\n",
      "iteration 28961: loss: 0.21853113174438477\n",
      "iteration 28962: loss: 0.21853086352348328\n",
      "iteration 28963: loss: 0.2185305804014206\n",
      "iteration 28964: loss: 0.2185302972793579\n",
      "iteration 28965: loss: 0.2185300588607788\n",
      "iteration 28966: loss: 0.21852977573871613\n",
      "iteration 28967: loss: 0.21852943301200867\n",
      "iteration 28968: loss: 0.21852914988994598\n",
      "iteration 28969: loss: 0.2185288667678833\n",
      "iteration 28970: loss: 0.21852867305278778\n",
      "iteration 28971: loss: 0.2185283601284027\n",
      "iteration 28972: loss: 0.21852806210517883\n",
      "iteration 28973: loss: 0.21852776408195496\n",
      "iteration 28974: loss: 0.21852752566337585\n",
      "iteration 28975: loss: 0.2185271680355072\n",
      "iteration 28976: loss: 0.21852688491344452\n",
      "iteration 28977: loss: 0.21852660179138184\n",
      "iteration 28978: loss: 0.21852636337280273\n",
      "iteration 28979: loss: 0.21852612495422363\n",
      "iteration 28980: loss: 0.21852581202983856\n",
      "iteration 28981: loss: 0.21852555871009827\n",
      "iteration 28982: loss: 0.2185252457857132\n",
      "iteration 28983: loss: 0.2185249626636505\n",
      "iteration 28984: loss: 0.2185247391462326\n",
      "iteration 28985: loss: 0.21852441132068634\n",
      "iteration 28986: loss: 0.21852418780326843\n",
      "iteration 28987: loss: 0.21852388978004456\n",
      "iteration 28988: loss: 0.21852362155914307\n",
      "iteration 28989: loss: 0.21852335333824158\n",
      "iteration 28990: loss: 0.21852299571037292\n",
      "iteration 28991: loss: 0.21852274239063263\n",
      "iteration 28992: loss: 0.21852245926856995\n",
      "iteration 28993: loss: 0.21852222084999084\n",
      "iteration 28994: loss: 0.21852190792560577\n",
      "iteration 28995: loss: 0.21852166950702667\n",
      "iteration 28996: loss: 0.2185213565826416\n",
      "iteration 28997: loss: 0.21852104365825653\n",
      "iteration 28998: loss: 0.21852083504199982\n",
      "iteration 28999: loss: 0.21852055191993713\n",
      "iteration 29000: loss: 0.21852023899555206\n",
      "iteration 29001: loss: 0.21851995587348938\n",
      "iteration 29002: loss: 0.2185196876525879\n",
      "iteration 29003: loss: 0.21851947903633118\n",
      "iteration 29004: loss: 0.21851912140846252\n",
      "iteration 29005: loss: 0.21851885318756104\n",
      "iteration 29006: loss: 0.21851854026317596\n",
      "iteration 29007: loss: 0.21851825714111328\n",
      "iteration 29008: loss: 0.21851801872253418\n",
      "iteration 29009: loss: 0.21851769089698792\n",
      "iteration 29010: loss: 0.2185174524784088\n",
      "iteration 29011: loss: 0.2185172140598297\n",
      "iteration 29012: loss: 0.21851691603660583\n",
      "iteration 29013: loss: 0.21851663291454315\n",
      "iteration 29014: loss: 0.2185163050889969\n",
      "iteration 29015: loss: 0.2185160368680954\n",
      "iteration 29016: loss: 0.2185157984495163\n",
      "iteration 29017: loss: 0.218515545129776\n",
      "iteration 29018: loss: 0.21851524710655212\n",
      "iteration 29019: loss: 0.21851494908332825\n",
      "iteration 29020: loss: 0.21851468086242676\n",
      "iteration 29021: loss: 0.21851439774036407\n",
      "iteration 29022: loss: 0.21851412951946259\n",
      "iteration 29023: loss: 0.21851381659507751\n",
      "iteration 29024: loss: 0.2185135781764984\n",
      "iteration 29025: loss: 0.21851325035095215\n",
      "iteration 29026: loss: 0.21851296722888947\n",
      "iteration 29027: loss: 0.21851277351379395\n",
      "iteration 29028: loss: 0.2185123860836029\n",
      "iteration 29029: loss: 0.2185121774673462\n",
      "iteration 29030: loss: 0.2185118943452835\n",
      "iteration 29031: loss: 0.21851158142089844\n",
      "iteration 29032: loss: 0.21851137280464172\n",
      "iteration 29033: loss: 0.21851103007793427\n",
      "iteration 29034: loss: 0.21851074695587158\n",
      "iteration 29035: loss: 0.2185104787349701\n",
      "iteration 29036: loss: 0.2185101956129074\n",
      "iteration 29037: loss: 0.21850991249084473\n",
      "iteration 29038: loss: 0.21850964426994324\n",
      "iteration 29039: loss: 0.21850936114788055\n",
      "iteration 29040: loss: 0.21850910782814026\n",
      "iteration 29041: loss: 0.21850886940956116\n",
      "iteration 29042: loss: 0.21850857138633728\n",
      "iteration 29043: loss: 0.2185082733631134\n",
      "iteration 29044: loss: 0.21850793063640594\n",
      "iteration 29045: loss: 0.21850769221782684\n",
      "iteration 29046: loss: 0.21850743889808655\n",
      "iteration 29047: loss: 0.21850717067718506\n",
      "iteration 29048: loss: 0.21850693225860596\n",
      "iteration 29049: loss: 0.2185065746307373\n",
      "iteration 29050: loss: 0.21850630640983582\n",
      "iteration 29051: loss: 0.2185061275959015\n",
      "iteration 29052: loss: 0.21850578486919403\n",
      "iteration 29053: loss: 0.21850553154945374\n",
      "iteration 29054: loss: 0.21850517392158508\n",
      "iteration 29055: loss: 0.21850495040416718\n",
      "iteration 29056: loss: 0.21850471198558807\n",
      "iteration 29057: loss: 0.2185044288635254\n",
      "iteration 29058: loss: 0.21850410103797913\n",
      "iteration 29059: loss: 0.21850387752056122\n",
      "iteration 29060: loss: 0.21850356459617615\n",
      "iteration 29061: loss: 0.21850331127643585\n",
      "iteration 29062: loss: 0.21850299835205078\n",
      "iteration 29063: loss: 0.2185027301311493\n",
      "iteration 29064: loss: 0.218502476811409\n",
      "iteration 29065: loss: 0.2185021936893463\n",
      "iteration 29066: loss: 0.21850188076496124\n",
      "iteration 29067: loss: 0.21850164234638214\n",
      "iteration 29068: loss: 0.21850132942199707\n",
      "iteration 29069: loss: 0.21850109100341797\n",
      "iteration 29070: loss: 0.21850080788135529\n",
      "iteration 29071: loss: 0.2185005247592926\n",
      "iteration 29072: loss: 0.2185002565383911\n",
      "iteration 29073: loss: 0.21849998831748962\n",
      "iteration 29074: loss: 0.21849970519542694\n",
      "iteration 29075: loss: 0.21849942207336426\n",
      "iteration 29076: loss: 0.21849915385246277\n",
      "iteration 29077: loss: 0.21849887073040009\n",
      "iteration 29078: loss: 0.21849855780601501\n",
      "iteration 29079: loss: 0.21849827468395233\n",
      "iteration 29080: loss: 0.21849799156188965\n",
      "iteration 29081: loss: 0.21849775314331055\n",
      "iteration 29082: loss: 0.21849747002124786\n",
      "iteration 29083: loss: 0.2184971570968628\n",
      "iteration 29084: loss: 0.2184969186782837\n",
      "iteration 29085: loss: 0.218496635556221\n",
      "iteration 29086: loss: 0.2184963971376419\n",
      "iteration 29087: loss: 0.21849611401557922\n",
      "iteration 29088: loss: 0.21849584579467773\n",
      "iteration 29089: loss: 0.21849563717842102\n",
      "iteration 29090: loss: 0.21849527955055237\n",
      "iteration 29091: loss: 0.21849501132965088\n",
      "iteration 29092: loss: 0.2184946984052658\n",
      "iteration 29093: loss: 0.2184944599866867\n",
      "iteration 29094: loss: 0.2184942215681076\n",
      "iteration 29095: loss: 0.21849389374256134\n",
      "iteration 29096: loss: 0.21849361062049866\n",
      "iteration 29097: loss: 0.21849331259727478\n",
      "iteration 29098: loss: 0.2184930294752121\n",
      "iteration 29099: loss: 0.218492791056633\n",
      "iteration 29100: loss: 0.2184925079345703\n",
      "iteration 29101: loss: 0.2184922993183136\n",
      "iteration 29102: loss: 0.21849195659160614\n",
      "iteration 29103: loss: 0.21849170327186584\n",
      "iteration 29104: loss: 0.21849143505096436\n",
      "iteration 29105: loss: 0.21849119663238525\n",
      "iteration 29106: loss: 0.21849088370800018\n",
      "iteration 29107: loss: 0.2184906005859375\n",
      "iteration 29108: loss: 0.21849028766155243\n",
      "iteration 29109: loss: 0.21849003434181213\n",
      "iteration 29110: loss: 0.21848979592323303\n",
      "iteration 29111: loss: 0.21848949790000916\n",
      "iteration 29112: loss: 0.2184891700744629\n",
      "iteration 29113: loss: 0.2184889316558838\n",
      "iteration 29114: loss: 0.2184886634349823\n",
      "iteration 29115: loss: 0.21848838031291962\n",
      "iteration 29116: loss: 0.21848812699317932\n",
      "iteration 29117: loss: 0.2184879034757614\n",
      "iteration 29118: loss: 0.21848757565021515\n",
      "iteration 29119: loss: 0.21848730742931366\n",
      "iteration 29120: loss: 0.21848702430725098\n",
      "iteration 29121: loss: 0.2184867411851883\n",
      "iteration 29122: loss: 0.2184864580631256\n",
      "iteration 29123: loss: 0.2184862345457077\n",
      "iteration 29124: loss: 0.21848595142364502\n",
      "iteration 29125: loss: 0.21848566830158234\n",
      "iteration 29126: loss: 0.21848540008068085\n",
      "iteration 29127: loss: 0.21848516166210175\n",
      "iteration 29128: loss: 0.21848487854003906\n",
      "iteration 29129: loss: 0.21848459541797638\n",
      "iteration 29130: loss: 0.2184843271970749\n",
      "iteration 29131: loss: 0.21848399937152863\n",
      "iteration 29132: loss: 0.21848377585411072\n",
      "iteration 29133: loss: 0.21848344802856445\n",
      "iteration 29134: loss: 0.21848317980766296\n",
      "iteration 29135: loss: 0.21848292648792267\n",
      "iteration 29136: loss: 0.21848265826702118\n",
      "iteration 29137: loss: 0.2184823453426361\n",
      "iteration 29138: loss: 0.218482106924057\n",
      "iteration 29139: loss: 0.21848180890083313\n",
      "iteration 29140: loss: 0.21848158538341522\n",
      "iteration 29141: loss: 0.21848130226135254\n",
      "iteration 29142: loss: 0.21848103404045105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 29143: loss: 0.21848079562187195\n",
      "iteration 29144: loss: 0.2184804379940033\n",
      "iteration 29145: loss: 0.2184801548719406\n",
      "iteration 29146: loss: 0.2184799164533615\n",
      "iteration 29147: loss: 0.21847966313362122\n",
      "iteration 29148: loss: 0.21847936511039734\n",
      "iteration 29149: loss: 0.21847912669181824\n",
      "iteration 29150: loss: 0.21847882866859436\n",
      "iteration 29151: loss: 0.21847856044769287\n",
      "iteration 29152: loss: 0.2184782773256302\n",
      "iteration 29153: loss: 0.2184779942035675\n",
      "iteration 29154: loss: 0.21847772598266602\n",
      "iteration 29155: loss: 0.21847744286060333\n",
      "iteration 29156: loss: 0.21847715973854065\n",
      "iteration 29157: loss: 0.21847693622112274\n",
      "iteration 29158: loss: 0.21847662329673767\n",
      "iteration 29159: loss: 0.21847638487815857\n",
      "iteration 29160: loss: 0.21847602725028992\n",
      "iteration 29161: loss: 0.2184758186340332\n",
      "iteration 29162: loss: 0.2184755802154541\n",
      "iteration 29163: loss: 0.21847525238990784\n",
      "iteration 29164: loss: 0.21847505867481232\n",
      "iteration 29165: loss: 0.21847474575042725\n",
      "iteration 29166: loss: 0.21847447752952576\n",
      "iteration 29167: loss: 0.21847417950630188\n",
      "iteration 29168: loss: 0.21847394108772278\n",
      "iteration 29169: loss: 0.2184736281633377\n",
      "iteration 29170: loss: 0.21847335994243622\n",
      "iteration 29171: loss: 0.21847307682037354\n",
      "iteration 29172: loss: 0.2184729129076004\n",
      "iteration 29173: loss: 0.21847257018089294\n",
      "iteration 29174: loss: 0.21847228705883026\n",
      "iteration 29175: loss: 0.21847204864025116\n",
      "iteration 29176: loss: 0.2184717357158661\n",
      "iteration 29177: loss: 0.2184714823961258\n",
      "iteration 29178: loss: 0.2184711992740631\n",
      "iteration 29179: loss: 0.2184709757566452\n",
      "iteration 29180: loss: 0.2184707224369049\n",
      "iteration 29181: loss: 0.21847037971019745\n",
      "iteration 29182: loss: 0.21847012639045715\n",
      "iteration 29183: loss: 0.21846982836723328\n",
      "iteration 29184: loss: 0.2184695452451706\n",
      "iteration 29185: loss: 0.21846933662891388\n",
      "iteration 29186: loss: 0.2184690237045288\n",
      "iteration 29187: loss: 0.21846874058246613\n",
      "iteration 29188: loss: 0.21846851706504822\n",
      "iteration 29189: loss: 0.21846815943717957\n",
      "iteration 29190: loss: 0.21846799552440643\n",
      "iteration 29191: loss: 0.21846763789653778\n",
      "iteration 29192: loss: 0.2184673547744751\n",
      "iteration 29193: loss: 0.2184670865535736\n",
      "iteration 29194: loss: 0.2184668332338333\n",
      "iteration 29195: loss: 0.2184666395187378\n",
      "iteration 29196: loss: 0.21846631169319153\n",
      "iteration 29197: loss: 0.21846601366996765\n",
      "iteration 29198: loss: 0.21846576035022736\n",
      "iteration 29199: loss: 0.21846547722816467\n",
      "iteration 29200: loss: 0.21846520900726318\n",
      "iteration 29201: loss: 0.2184649258852005\n",
      "iteration 29202: loss: 0.218464657664299\n",
      "iteration 29203: loss: 0.2184644639492035\n",
      "iteration 29204: loss: 0.2184641808271408\n",
      "iteration 29205: loss: 0.21846389770507812\n",
      "iteration 29206: loss: 0.21846361458301544\n",
      "iteration 29207: loss: 0.21846333146095276\n",
      "iteration 29208: loss: 0.21846313774585724\n",
      "iteration 29209: loss: 0.21846279501914978\n",
      "iteration 29210: loss: 0.2184625118970871\n",
      "iteration 29211: loss: 0.2184622585773468\n",
      "iteration 29212: loss: 0.2184619903564453\n",
      "iteration 29213: loss: 0.21846170723438263\n",
      "iteration 29214: loss: 0.21846146881580353\n",
      "iteration 29215: loss: 0.21846118569374084\n",
      "iteration 29216: loss: 0.2184608429670334\n",
      "iteration 29217: loss: 0.21846060454845428\n",
      "iteration 29218: loss: 0.21846036612987518\n",
      "iteration 29219: loss: 0.2184600830078125\n",
      "iteration 29220: loss: 0.2184598445892334\n",
      "iteration 29221: loss: 0.21845953166484833\n",
      "iteration 29222: loss: 0.21845927834510803\n",
      "iteration 29223: loss: 0.21845898032188416\n",
      "iteration 29224: loss: 0.21845872700214386\n",
      "iteration 29225: loss: 0.21845848858356476\n",
      "iteration 29226: loss: 0.21845822036266327\n",
      "iteration 29227: loss: 0.2184579074382782\n",
      "iteration 29228: loss: 0.21845762431621552\n",
      "iteration 29229: loss: 0.2184573858976364\n",
      "iteration 29230: loss: 0.21845713257789612\n",
      "iteration 29231: loss: 0.2184569090604782\n",
      "iteration 29232: loss: 0.21845658123493195\n",
      "iteration 29233: loss: 0.21845631301403046\n",
      "iteration 29234: loss: 0.21845602989196777\n",
      "iteration 29235: loss: 0.21845579147338867\n",
      "iteration 29236: loss: 0.2184554785490036\n",
      "iteration 29237: loss: 0.2184552699327469\n",
      "iteration 29238: loss: 0.21845492720603943\n",
      "iteration 29239: loss: 0.21845464408397675\n",
      "iteration 29240: loss: 0.21845436096191406\n",
      "iteration 29241: loss: 0.21845409274101257\n",
      "iteration 29242: loss: 0.21845388412475586\n",
      "iteration 29243: loss: 0.2184535562992096\n",
      "iteration 29244: loss: 0.2184533178806305\n",
      "iteration 29245: loss: 0.21845309436321259\n",
      "iteration 29246: loss: 0.2184528112411499\n",
      "iteration 29247: loss: 0.2184525430202484\n",
      "iteration 29248: loss: 0.21845224499702454\n",
      "iteration 29249: loss: 0.21845200657844543\n",
      "iteration 29250: loss: 0.21845169365406036\n",
      "iteration 29251: loss: 0.21845145523548126\n",
      "iteration 29252: loss: 0.21845117211341858\n",
      "iteration 29253: loss: 0.21845093369483948\n",
      "iteration 29254: loss: 0.2184506356716156\n",
      "iteration 29255: loss: 0.2184503972530365\n",
      "iteration 29256: loss: 0.21845011413097382\n",
      "iteration 29257: loss: 0.21844980120658875\n",
      "iteration 29258: loss: 0.21844959259033203\n",
      "iteration 29259: loss: 0.21844935417175293\n",
      "iteration 29260: loss: 0.21844899654388428\n",
      "iteration 29261: loss: 0.21844875812530518\n",
      "iteration 29262: loss: 0.21844851970672607\n",
      "iteration 29263: loss: 0.218448206782341\n",
      "iteration 29264: loss: 0.21844792366027832\n",
      "iteration 29265: loss: 0.21844768524169922\n",
      "iteration 29266: loss: 0.21844737231731415\n",
      "iteration 29267: loss: 0.21844716370105743\n",
      "iteration 29268: loss: 0.21844688057899475\n",
      "iteration 29269: loss: 0.21844665706157684\n",
      "iteration 29270: loss: 0.21844634413719177\n",
      "iteration 29271: loss: 0.21844609081745148\n",
      "iteration 29272: loss: 0.21844582259655\n",
      "iteration 29273: loss: 0.21844550967216492\n",
      "iteration 29274: loss: 0.21844522655010223\n",
      "iteration 29275: loss: 0.2184450626373291\n",
      "iteration 29276: loss: 0.21844473481178284\n",
      "iteration 29277: loss: 0.21844449639320374\n",
      "iteration 29278: loss: 0.21844418346881866\n",
      "iteration 29279: loss: 0.21844394505023956\n",
      "iteration 29280: loss: 0.21844370663166046\n",
      "iteration 29281: loss: 0.21844343841075897\n",
      "iteration 29282: loss: 0.2184431552886963\n",
      "iteration 29283: loss: 0.2184428870677948\n",
      "iteration 29284: loss: 0.21844252943992615\n",
      "iteration 29285: loss: 0.21844235062599182\n",
      "iteration 29286: loss: 0.21844211220741272\n",
      "iteration 29287: loss: 0.21844176948070526\n",
      "iteration 29288: loss: 0.21844153106212616\n",
      "iteration 29289: loss: 0.21844129264354706\n",
      "iteration 29290: loss: 0.218440979719162\n",
      "iteration 29291: loss: 0.2184407263994217\n",
      "iteration 29292: loss: 0.2184404581785202\n",
      "iteration 29293: loss: 0.21844017505645752\n",
      "iteration 29294: loss: 0.21843993663787842\n",
      "iteration 29295: loss: 0.21843962371349335\n",
      "iteration 29296: loss: 0.21843938529491425\n",
      "iteration 29297: loss: 0.21843913197517395\n",
      "iteration 29298: loss: 0.21843889355659485\n",
      "iteration 29299: loss: 0.21843862533569336\n",
      "iteration 29300: loss: 0.21843823790550232\n",
      "iteration 29301: loss: 0.2184380739927292\n",
      "iteration 29302: loss: 0.21843774616718292\n",
      "iteration 29303: loss: 0.21843752264976501\n",
      "iteration 29304: loss: 0.21843726933002472\n",
      "iteration 29305: loss: 0.21843692660331726\n",
      "iteration 29306: loss: 0.21843671798706055\n",
      "iteration 29307: loss: 0.21843643486499786\n",
      "iteration 29308: loss: 0.21843616664409637\n",
      "iteration 29309: loss: 0.21843591332435608\n",
      "iteration 29310: loss: 0.21843568980693817\n",
      "iteration 29311: loss: 0.2184353768825531\n",
      "iteration 29312: loss: 0.21843509376049042\n",
      "iteration 29313: loss: 0.2184348851442337\n",
      "iteration 29314: loss: 0.21843460202217102\n",
      "iteration 29315: loss: 0.21843433380126953\n",
      "iteration 29316: loss: 0.21843409538269043\n",
      "iteration 29317: loss: 0.21843381226062775\n",
      "iteration 29318: loss: 0.21843349933624268\n",
      "iteration 29319: loss: 0.21843326091766357\n",
      "iteration 29320: loss: 0.2184329777956009\n",
      "iteration 29321: loss: 0.21843275427818298\n",
      "iteration 29322: loss: 0.2184324711561203\n",
      "iteration 29323: loss: 0.21843221783638\n",
      "iteration 29324: loss: 0.21843190491199493\n",
      "iteration 29325: loss: 0.21843163669109344\n",
      "iteration 29326: loss: 0.21843139827251434\n",
      "iteration 29327: loss: 0.21843115985393524\n",
      "iteration 29328: loss: 0.2184308022260666\n",
      "iteration 29329: loss: 0.2184305489063263\n",
      "iteration 29330: loss: 0.2184302806854248\n",
      "iteration 29331: loss: 0.2184300720691681\n",
      "iteration 29332: loss: 0.2184297740459442\n",
      "iteration 29333: loss: 0.2184295356273651\n",
      "iteration 29334: loss: 0.21842928230762482\n",
      "iteration 29335: loss: 0.21842901408672333\n",
      "iteration 29336: loss: 0.21842876076698303\n",
      "iteration 29337: loss: 0.21842846274375916\n",
      "iteration 29338: loss: 0.21842820942401886\n",
      "iteration 29339: loss: 0.2184278964996338\n",
      "iteration 29340: loss: 0.2184276580810547\n",
      "iteration 29341: loss: 0.21842741966247559\n",
      "iteration 29342: loss: 0.2184271365404129\n",
      "iteration 29343: loss: 0.218426913022995\n",
      "iteration 29344: loss: 0.21842655539512634\n",
      "iteration 29345: loss: 0.21842631697654724\n",
      "iteration 29346: loss: 0.21842607855796814\n",
      "iteration 29347: loss: 0.21842584013938904\n",
      "iteration 29348: loss: 0.21842551231384277\n",
      "iteration 29349: loss: 0.21842530369758606\n",
      "iteration 29350: loss: 0.21842503547668457\n",
      "iteration 29351: loss: 0.21842476725578308\n",
      "iteration 29352: loss: 0.2184244692325592\n",
      "iteration 29353: loss: 0.2184242308139801\n",
      "iteration 29354: loss: 0.2184240072965622\n",
      "iteration 29355: loss: 0.21842369437217712\n",
      "iteration 29356: loss: 0.21842341125011444\n",
      "iteration 29357: loss: 0.21842312812805176\n",
      "iteration 29358: loss: 0.21842288970947266\n",
      "iteration 29359: loss: 0.21842262148857117\n",
      "iteration 29360: loss: 0.21842236816883087\n",
      "iteration 29361: loss: 0.21842209994792938\n",
      "iteration 29362: loss: 0.2184218466281891\n",
      "iteration 29363: loss: 0.2184215486049652\n",
      "iteration 29364: loss: 0.21842136979103088\n",
      "iteration 29365: loss: 0.21842101216316223\n",
      "iteration 29366: loss: 0.21842074394226074\n",
      "iteration 29367: loss: 0.21842050552368164\n",
      "iteration 29368: loss: 0.21842023730278015\n",
      "iteration 29369: loss: 0.21841993927955627\n",
      "iteration 29370: loss: 0.21841974556446075\n",
      "iteration 29371: loss: 0.21841947734355927\n",
      "iteration 29372: loss: 0.21841922402381897\n",
      "iteration 29373: loss: 0.21841886639595032\n",
      "iteration 29374: loss: 0.2184186428785324\n",
      "iteration 29375: loss: 0.21841847896575928\n",
      "iteration 29376: loss: 0.21841812133789062\n",
      "iteration 29377: loss: 0.21841785311698914\n",
      "iteration 29378: loss: 0.21841764450073242\n",
      "iteration 29379: loss: 0.21841736137866974\n",
      "iteration 29380: loss: 0.21841704845428467\n",
      "iteration 29381: loss: 0.21841678023338318\n",
      "iteration 29382: loss: 0.21841657161712646\n",
      "iteration 29383: loss: 0.21841630339622498\n",
      "iteration 29384: loss: 0.2184160202741623\n",
      "iteration 29385: loss: 0.21841582655906677\n",
      "iteration 29386: loss: 0.21841546893119812\n",
      "iteration 29387: loss: 0.2184152901172638\n",
      "iteration 29388: loss: 0.21841494739055634\n",
      "iteration 29389: loss: 0.21841470897197723\n",
      "iteration 29390: loss: 0.21841439604759216\n",
      "iteration 29391: loss: 0.21841414272785187\n",
      "iteration 29392: loss: 0.21841391921043396\n",
      "iteration 29393: loss: 0.21841368079185486\n",
      "iteration 29394: loss: 0.2184133529663086\n",
      "iteration 29395: loss: 0.21841314435005188\n",
      "iteration 29396: loss: 0.218412846326828\n",
      "iteration 29397: loss: 0.2184125930070877\n",
      "iteration 29398: loss: 0.2184123545885086\n",
      "iteration 29399: loss: 0.21841207146644592\n",
      "iteration 29400: loss: 0.21841180324554443\n",
      "iteration 29401: loss: 0.21841159462928772\n",
      "iteration 29402: loss: 0.21841128170490265\n",
      "iteration 29403: loss: 0.21841101348400116\n",
      "iteration 29404: loss: 0.21841077506542206\n",
      "iteration 29405: loss: 0.218410462141037\n",
      "iteration 29406: loss: 0.2184102088212967\n",
      "iteration 29407: loss: 0.2184099704027176\n",
      "iteration 29408: loss: 0.2184097021818161\n",
      "iteration 29409: loss: 0.2184094488620758\n",
      "iteration 29410: loss: 0.21840918064117432\n",
      "iteration 29411: loss: 0.21840886771678925\n",
      "iteration 29412: loss: 0.21840865910053253\n",
      "iteration 29413: loss: 0.21840843558311462\n",
      "iteration 29414: loss: 0.21840810775756836\n",
      "iteration 29415: loss: 0.21840782463550568\n",
      "iteration 29416: loss: 0.2184075564146042\n",
      "iteration 29417: loss: 0.21840734779834747\n",
      "iteration 29418: loss: 0.2184070646762848\n",
      "iteration 29419: loss: 0.21840687096118927\n",
      "iteration 29420: loss: 0.2184065282344818\n",
      "iteration 29421: loss: 0.2184063196182251\n",
      "iteration 29422: loss: 0.2184060513973236\n",
      "iteration 29423: loss: 0.2184057980775833\n",
      "iteration 29424: loss: 0.21840552985668182\n",
      "iteration 29425: loss: 0.21840521693229675\n",
      "iteration 29426: loss: 0.21840497851371765\n",
      "iteration 29427: loss: 0.21840479969978333\n",
      "iteration 29428: loss: 0.21840444207191467\n",
      "iteration 29429: loss: 0.21840424835681915\n",
      "iteration 29430: loss: 0.2184039056301117\n",
      "iteration 29431: loss: 0.218403622508049\n",
      "iteration 29432: loss: 0.2184034138917923\n",
      "iteration 29433: loss: 0.21840313076972961\n",
      "iteration 29434: loss: 0.21840281784534454\n",
      "iteration 29435: loss: 0.2184026539325714\n",
      "iteration 29436: loss: 0.21840238571166992\n",
      "iteration 29437: loss: 0.21840211749076843\n",
      "iteration 29438: loss: 0.21840178966522217\n",
      "iteration 29439: loss: 0.21840158104896545\n",
      "iteration 29440: loss: 0.21840131282806396\n",
      "iteration 29441: loss: 0.2184009999036789\n",
      "iteration 29442: loss: 0.21840080618858337\n",
      "iteration 29443: loss: 0.2184004783630371\n",
      "iteration 29444: loss: 0.218400239944458\n",
      "iteration 29445: loss: 0.2184000015258789\n",
      "iteration 29446: loss: 0.2183997631072998\n",
      "iteration 29447: loss: 0.21839945018291473\n",
      "iteration 29448: loss: 0.21839916706085205\n",
      "iteration 29449: loss: 0.21839892864227295\n",
      "iteration 29450: loss: 0.21839866042137146\n",
      "iteration 29451: loss: 0.21839840710163116\n",
      "iteration 29452: loss: 0.21839816868305206\n",
      "iteration 29453: loss: 0.21839794516563416\n",
      "iteration 29454: loss: 0.2183976173400879\n",
      "iteration 29455: loss: 0.2183973342180252\n",
      "iteration 29456: loss: 0.2183971405029297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 29457: loss: 0.21839690208435059\n",
      "iteration 29458: loss: 0.21839657425880432\n",
      "iteration 29459: loss: 0.2183963805437088\n",
      "iteration 29460: loss: 0.2183961421251297\n",
      "iteration 29461: loss: 0.21839585900306702\n",
      "iteration 29462: loss: 0.21839551627635956\n",
      "iteration 29463: loss: 0.21839527785778046\n",
      "iteration 29464: loss: 0.21839503943920135\n",
      "iteration 29465: loss: 0.21839472651481628\n",
      "iteration 29466: loss: 0.21839454770088196\n",
      "iteration 29467: loss: 0.21839427947998047\n",
      "iteration 29468: loss: 0.2183939665555954\n",
      "iteration 29469: loss: 0.2183937132358551\n",
      "iteration 29470: loss: 0.2183934897184372\n",
      "iteration 29471: loss: 0.21839316189289093\n",
      "iteration 29472: loss: 0.2183929681777954\n",
      "iteration 29473: loss: 0.2183927297592163\n",
      "iteration 29474: loss: 0.21839240193367004\n",
      "iteration 29475: loss: 0.21839216351509094\n",
      "iteration 29476: loss: 0.21839192509651184\n",
      "iteration 29477: loss: 0.21839168667793274\n",
      "iteration 29478: loss: 0.21839137375354767\n",
      "iteration 29479: loss: 0.21839115023612976\n",
      "iteration 29480: loss: 0.21839086711406708\n",
      "iteration 29481: loss: 0.21839062869548798\n",
      "iteration 29482: loss: 0.21839037537574768\n",
      "iteration 29483: loss: 0.21839013695716858\n",
      "iteration 29484: loss: 0.2183898687362671\n",
      "iteration 29485: loss: 0.21838955581188202\n",
      "iteration 29486: loss: 0.21838931739330292\n",
      "iteration 29487: loss: 0.21838903427124023\n",
      "iteration 29488: loss: 0.21838879585266113\n",
      "iteration 29489: loss: 0.21838851273059845\n",
      "iteration 29490: loss: 0.21838831901550293\n",
      "iteration 29491: loss: 0.21838803589344025\n",
      "iteration 29492: loss: 0.21838775277137756\n",
      "iteration 29493: loss: 0.21838748455047607\n",
      "iteration 29494: loss: 0.21838724613189697\n",
      "iteration 29495: loss: 0.2183869630098343\n",
      "iteration 29496: loss: 0.2183866798877716\n",
      "iteration 29497: loss: 0.2183864414691925\n",
      "iteration 29498: loss: 0.21838614344596863\n",
      "iteration 29499: loss: 0.21838593482971191\n",
      "iteration 29500: loss: 0.21838566660881042\n",
      "iteration 29501: loss: 0.21838541328907013\n",
      "iteration 29502: loss: 0.21838517487049103\n",
      "iteration 29503: loss: 0.21838486194610596\n",
      "iteration 29504: loss: 0.21838469803333282\n",
      "iteration 29505: loss: 0.21838441491127014\n",
      "iteration 29506: loss: 0.21838417649269104\n",
      "iteration 29507: loss: 0.2183838188648224\n",
      "iteration 29508: loss: 0.21838350594043732\n",
      "iteration 29509: loss: 0.21838335692882538\n",
      "iteration 29510: loss: 0.2183830738067627\n",
      "iteration 29511: loss: 0.2183828055858612\n",
      "iteration 29512: loss: 0.2183825671672821\n",
      "iteration 29513: loss: 0.2183823138475418\n",
      "iteration 29514: loss: 0.2183821201324463\n",
      "iteration 29515: loss: 0.2183818370103836\n",
      "iteration 29516: loss: 0.21838152408599854\n",
      "iteration 29517: loss: 0.21838125586509705\n",
      "iteration 29518: loss: 0.21838101744651794\n",
      "iteration 29519: loss: 0.21838073432445526\n",
      "iteration 29520: loss: 0.21838052570819855\n",
      "iteration 29521: loss: 0.21838025748729706\n",
      "iteration 29522: loss: 0.21838000416755676\n",
      "iteration 29523: loss: 0.2183796912431717\n",
      "iteration 29524: loss: 0.2183794528245926\n",
      "iteration 29525: loss: 0.2183792144060135\n",
      "iteration 29526: loss: 0.2183789759874344\n",
      "iteration 29527: loss: 0.21837863326072693\n",
      "iteration 29528: loss: 0.21837839484214783\n",
      "iteration 29529: loss: 0.2183781862258911\n",
      "iteration 29530: loss: 0.21837785840034485\n",
      "iteration 29531: loss: 0.21837763488292694\n",
      "iteration 29532: loss: 0.21837735176086426\n",
      "iteration 29533: loss: 0.21837715804576874\n",
      "iteration 29534: loss: 0.21837684512138367\n",
      "iteration 29535: loss: 0.21837656199932098\n",
      "iteration 29536: loss: 0.21837639808654785\n",
      "iteration 29537: loss: 0.21837611496448517\n",
      "iteration 29538: loss: 0.21837584674358368\n",
      "iteration 29539: loss: 0.21837559342384338\n",
      "iteration 29540: loss: 0.21837535500526428\n",
      "iteration 29541: loss: 0.2183750867843628\n",
      "iteration 29542: loss: 0.21837475895881653\n",
      "iteration 29543: loss: 0.2183745801448822\n",
      "iteration 29544: loss: 0.21837425231933594\n",
      "iteration 29545: loss: 0.218374103307724\n",
      "iteration 29546: loss: 0.21837374567985535\n",
      "iteration 29547: loss: 0.21837356686592102\n",
      "iteration 29548: loss: 0.21837322413921356\n",
      "iteration 29549: loss: 0.21837301552295685\n",
      "iteration 29550: loss: 0.21837273240089417\n",
      "iteration 29551: loss: 0.21837249398231506\n",
      "iteration 29552: loss: 0.21837225556373596\n",
      "iteration 29553: loss: 0.21837201714515686\n",
      "iteration 29554: loss: 0.21837174892425537\n",
      "iteration 29555: loss: 0.2183714359998703\n",
      "iteration 29556: loss: 0.21837115287780762\n",
      "iteration 29557: loss: 0.2183709591627121\n",
      "iteration 29558: loss: 0.21837067604064941\n",
      "iteration 29559: loss: 0.2183704674243927\n",
      "iteration 29560: loss: 0.21837016940116882\n",
      "iteration 29561: loss: 0.2183699607849121\n",
      "iteration 29562: loss: 0.21836967766284943\n",
      "iteration 29563: loss: 0.21836939454078674\n",
      "iteration 29564: loss: 0.21836915612220764\n",
      "iteration 29565: loss: 0.21836888790130615\n",
      "iteration 29566: loss: 0.21836860477924347\n",
      "iteration 29567: loss: 0.21836833655834198\n",
      "iteration 29568: loss: 0.21836812794208527\n",
      "iteration 29569: loss: 0.21836785972118378\n",
      "iteration 29570: loss: 0.2183675765991211\n",
      "iteration 29571: loss: 0.218367338180542\n",
      "iteration 29572: loss: 0.2183670997619629\n",
      "iteration 29573: loss: 0.2183668166399002\n",
      "iteration 29574: loss: 0.2183665782213211\n",
      "iteration 29575: loss: 0.21836631000041962\n",
      "iteration 29576: loss: 0.21836605668067932\n",
      "iteration 29577: loss: 0.21836578845977783\n",
      "iteration 29578: loss: 0.21836547553539276\n",
      "iteration 29579: loss: 0.21836528182029724\n",
      "iteration 29580: loss: 0.21836499869823456\n",
      "iteration 29581: loss: 0.21836480498313904\n",
      "iteration 29582: loss: 0.21836452186107635\n",
      "iteration 29583: loss: 0.21836423873901367\n",
      "iteration 29584: loss: 0.21836403012275696\n",
      "iteration 29585: loss: 0.2183637171983719\n",
      "iteration 29586: loss: 0.21836347877979279\n",
      "iteration 29587: loss: 0.21836325526237488\n",
      "iteration 29588: loss: 0.21836300194263458\n",
      "iteration 29589: loss: 0.2183627337217331\n",
      "iteration 29590: loss: 0.2183624804019928\n",
      "iteration 29591: loss: 0.2183622121810913\n",
      "iteration 29592: loss: 0.2183620184659958\n",
      "iteration 29593: loss: 0.21836166083812714\n",
      "iteration 29594: loss: 0.21836140751838684\n",
      "iteration 29595: loss: 0.21836118400096893\n",
      "iteration 29596: loss: 0.21836094558238983\n",
      "iteration 29597: loss: 0.2183607518672943\n",
      "iteration 29598: loss: 0.21836042404174805\n",
      "iteration 29599: loss: 0.21836018562316895\n",
      "iteration 29600: loss: 0.21835991740226746\n",
      "iteration 29601: loss: 0.21835963428020477\n",
      "iteration 29602: loss: 0.21835939586162567\n",
      "iteration 29603: loss: 0.21835915744304657\n",
      "iteration 29604: loss: 0.21835890412330627\n",
      "iteration 29605: loss: 0.21835866570472717\n",
      "iteration 29606: loss: 0.2183583527803421\n",
      "iteration 29607: loss: 0.218358114361763\n",
      "iteration 29608: loss: 0.2183578759431839\n",
      "iteration 29609: loss: 0.2183576077222824\n",
      "iteration 29610: loss: 0.2183573693037033\n",
      "iteration 29611: loss: 0.21835708618164062\n",
      "iteration 29612: loss: 0.21835684776306152\n",
      "iteration 29613: loss: 0.21835651993751526\n",
      "iteration 29614: loss: 0.21835634112358093\n",
      "iteration 29615: loss: 0.21835608780384064\n",
      "iteration 29616: loss: 0.21835584938526154\n",
      "iteration 29617: loss: 0.21835561096668243\n",
      "iteration 29618: loss: 0.21835532784461975\n",
      "iteration 29619: loss: 0.21835502982139587\n",
      "iteration 29620: loss: 0.21835479140281677\n",
      "iteration 29621: loss: 0.21835455298423767\n",
      "iteration 29622: loss: 0.218354269862175\n",
      "iteration 29623: loss: 0.21835406124591827\n",
      "iteration 29624: loss: 0.2183537781238556\n",
      "iteration 29625: loss: 0.2183535099029541\n",
      "iteration 29626: loss: 0.2183533012866974\n",
      "iteration 29627: loss: 0.2183530330657959\n",
      "iteration 29628: loss: 0.21835270524024963\n",
      "iteration 29629: loss: 0.2183525562286377\n",
      "iteration 29630: loss: 0.218352273106575\n",
      "iteration 29631: loss: 0.2183520346879959\n",
      "iteration 29632: loss: 0.21835176646709442\n",
      "iteration 29633: loss: 0.21835151314735413\n",
      "iteration 29634: loss: 0.21835121512413025\n",
      "iteration 29635: loss: 0.21835096180438995\n",
      "iteration 29636: loss: 0.21835073828697205\n",
      "iteration 29637: loss: 0.21835048496723175\n",
      "iteration 29638: loss: 0.21835026144981384\n",
      "iteration 29639: loss: 0.21834997832775116\n",
      "iteration 29640: loss: 0.21834976971149445\n",
      "iteration 29641: loss: 0.21834950149059296\n",
      "iteration 29642: loss: 0.21834918856620789\n",
      "iteration 29643: loss: 0.2183489352464676\n",
      "iteration 29644: loss: 0.2183486968278885\n",
      "iteration 29645: loss: 0.2183484584093094\n",
      "iteration 29646: loss: 0.2183481901884079\n",
      "iteration 29647: loss: 0.2183479368686676\n",
      "iteration 29648: loss: 0.2183476984500885\n",
      "iteration 29649: loss: 0.218347430229187\n",
      "iteration 29650: loss: 0.2183471918106079\n",
      "iteration 29651: loss: 0.21834690868854523\n",
      "iteration 29652: loss: 0.21834668517112732\n",
      "iteration 29653: loss: 0.21834640204906464\n",
      "iteration 29654: loss: 0.21834614872932434\n",
      "iteration 29655: loss: 0.21834592521190643\n",
      "iteration 29656: loss: 0.21834568679332733\n",
      "iteration 29657: loss: 0.21834544837474823\n",
      "iteration 29658: loss: 0.21834516525268555\n",
      "iteration 29659: loss: 0.21834492683410645\n",
      "iteration 29660: loss: 0.21834461390972137\n",
      "iteration 29661: loss: 0.21834440529346466\n",
      "iteration 29662: loss: 0.21834416687488556\n",
      "iteration 29663: loss: 0.21834388375282288\n",
      "iteration 29664: loss: 0.21834366023540497\n",
      "iteration 29665: loss: 0.2183433473110199\n",
      "iteration 29666: loss: 0.21834306418895721\n",
      "iteration 29667: loss: 0.21834290027618408\n",
      "iteration 29668: loss: 0.2183426171541214\n",
      "iteration 29669: loss: 0.2183423489332199\n",
      "iteration 29670: loss: 0.21834209561347961\n",
      "iteration 29671: loss: 0.2183418720960617\n",
      "iteration 29672: loss: 0.2183416336774826\n",
      "iteration 29673: loss: 0.21834135055541992\n",
      "iteration 29674: loss: 0.21834111213684082\n",
      "iteration 29675: loss: 0.21834082901477814\n",
      "iteration 29676: loss: 0.21834060549736023\n",
      "iteration 29677: loss: 0.21834035217761993\n",
      "iteration 29678: loss: 0.21834006905555725\n",
      "iteration 29679: loss: 0.21833983063697815\n",
      "iteration 29680: loss: 0.21833959221839905\n",
      "iteration 29681: loss: 0.21833932399749756\n",
      "iteration 29682: loss: 0.21833908557891846\n",
      "iteration 29683: loss: 0.21833884716033936\n",
      "iteration 29684: loss: 0.21833856403827667\n",
      "iteration 29685: loss: 0.21833834052085876\n",
      "iteration 29686: loss: 0.21833808720111847\n",
      "iteration 29687: loss: 0.21833781898021698\n",
      "iteration 29688: loss: 0.21833758056163788\n",
      "iteration 29689: loss: 0.2183372974395752\n",
      "iteration 29690: loss: 0.21833714842796326\n",
      "iteration 29691: loss: 0.2183367908000946\n",
      "iteration 29692: loss: 0.21833661198616028\n",
      "iteration 29693: loss: 0.2183363139629364\n",
      "iteration 29694: loss: 0.2183360606431961\n",
      "iteration 29695: loss: 0.2183358371257782\n",
      "iteration 29696: loss: 0.21833553910255432\n",
      "iteration 29697: loss: 0.21833530068397522\n",
      "iteration 29698: loss: 0.21833506226539612\n",
      "iteration 29699: loss: 0.21833482384681702\n",
      "iteration 29700: loss: 0.21833451092243195\n",
      "iteration 29701: loss: 0.21833428740501404\n",
      "iteration 29702: loss: 0.21833404898643494\n",
      "iteration 29703: loss: 0.21833375096321106\n",
      "iteration 29704: loss: 0.21833352744579315\n",
      "iteration 29705: loss: 0.21833333373069763\n",
      "iteration 29706: loss: 0.21833305060863495\n",
      "iteration 29707: loss: 0.21833273768424988\n",
      "iteration 29708: loss: 0.21833255887031555\n",
      "iteration 29709: loss: 0.21833224594593048\n",
      "iteration 29710: loss: 0.21833200752735138\n",
      "iteration 29711: loss: 0.2183317393064499\n",
      "iteration 29712: loss: 0.21833153069019318\n",
      "iteration 29713: loss: 0.2183312177658081\n",
      "iteration 29714: loss: 0.2183310091495514\n",
      "iteration 29715: loss: 0.21833071112632751\n",
      "iteration 29716: loss: 0.2183304727077484\n",
      "iteration 29717: loss: 0.2183302640914917\n",
      "iteration 29718: loss: 0.21832995116710663\n",
      "iteration 29719: loss: 0.21832971274852753\n",
      "iteration 29720: loss: 0.21832947432994843\n",
      "iteration 29721: loss: 0.21832922101020813\n",
      "iteration 29722: loss: 0.21832899749279022\n",
      "iteration 29723: loss: 0.21832874417304993\n",
      "iteration 29724: loss: 0.21832852065563202\n",
      "iteration 29725: loss: 0.21832819283008575\n",
      "iteration 29726: loss: 0.21832804381847382\n",
      "iteration 29727: loss: 0.21832771599292755\n",
      "iteration 29728: loss: 0.21832744777202606\n",
      "iteration 29729: loss: 0.21832728385925293\n",
      "iteration 29730: loss: 0.21832695603370667\n",
      "iteration 29731: loss: 0.21832673251628876\n",
      "iteration 29732: loss: 0.21832649409770966\n",
      "iteration 29733: loss: 0.21832625567913055\n",
      "iteration 29734: loss: 0.21832597255706787\n",
      "iteration 29735: loss: 0.21832573413848877\n",
      "iteration 29736: loss: 0.21832549571990967\n",
      "iteration 29737: loss: 0.21832521259784698\n",
      "iteration 29738: loss: 0.2183249443769455\n",
      "iteration 29739: loss: 0.21832475066184998\n",
      "iteration 29740: loss: 0.21832449734210968\n",
      "iteration 29741: loss: 0.2183242291212082\n",
      "iteration 29742: loss: 0.2183239907026291\n",
      "iteration 29743: loss: 0.2183237075805664\n",
      "iteration 29744: loss: 0.21832342445850372\n",
      "iteration 29745: loss: 0.2183232307434082\n",
      "iteration 29746: loss: 0.21832291781902313\n",
      "iteration 29747: loss: 0.21832275390625\n",
      "iteration 29748: loss: 0.21832247078418732\n",
      "iteration 29749: loss: 0.2183222472667694\n",
      "iteration 29750: loss: 0.2183219939470291\n",
      "iteration 29751: loss: 0.21832175552845\n",
      "iteration 29752: loss: 0.21832147240638733\n",
      "iteration 29753: loss: 0.21832123398780823\n",
      "iteration 29754: loss: 0.21832093596458435\n",
      "iteration 29755: loss: 0.21832069754600525\n",
      "iteration 29756: loss: 0.21832048892974854\n",
      "iteration 29757: loss: 0.21832029521465302\n",
      "iteration 29758: loss: 0.21831996738910675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 29759: loss: 0.21831969916820526\n",
      "iteration 29760: loss: 0.21831944584846497\n",
      "iteration 29761: loss: 0.21831922233104706\n",
      "iteration 29762: loss: 0.21831898391246796\n",
      "iteration 29763: loss: 0.21831874549388885\n",
      "iteration 29764: loss: 0.21831849217414856\n",
      "iteration 29765: loss: 0.21831825375556946\n",
      "iteration 29766: loss: 0.21831801533699036\n",
      "iteration 29767: loss: 0.21831774711608887\n",
      "iteration 29768: loss: 0.2183174341917038\n",
      "iteration 29769: loss: 0.21831722557544708\n",
      "iteration 29770: loss: 0.21831700205802917\n",
      "iteration 29771: loss: 0.21831674873828888\n",
      "iteration 29772: loss: 0.21831651031970978\n",
      "iteration 29773: loss: 0.2183162420988083\n",
      "iteration 29774: loss: 0.21831592917442322\n",
      "iteration 29775: loss: 0.21831579506397247\n",
      "iteration 29776: loss: 0.2183155119419098\n",
      "iteration 29777: loss: 0.21831519901752472\n",
      "iteration 29778: loss: 0.2183149755001068\n",
      "iteration 29779: loss: 0.21831472218036652\n",
      "iteration 29780: loss: 0.218314528465271\n",
      "iteration 29781: loss: 0.21831421554088593\n",
      "iteration 29782: loss: 0.2183140069246292\n",
      "iteration 29783: loss: 0.21831373870372772\n",
      "iteration 29784: loss: 0.21831350028514862\n",
      "iteration 29785: loss: 0.21831324696540833\n",
      "iteration 29786: loss: 0.2183130532503128\n",
      "iteration 29787: loss: 0.21831278502941132\n",
      "iteration 29788: loss: 0.21831247210502625\n",
      "iteration 29789: loss: 0.21831223368644714\n",
      "iteration 29790: loss: 0.21831202507019043\n",
      "iteration 29791: loss: 0.21831175684928894\n",
      "iteration 29792: loss: 0.21831151843070984\n",
      "iteration 29793: loss: 0.21831130981445312\n",
      "iteration 29794: loss: 0.21831098198890686\n",
      "iteration 29795: loss: 0.21831074357032776\n",
      "iteration 29796: loss: 0.21831056475639343\n",
      "iteration 29797: loss: 0.21831028163433075\n",
      "iteration 29798: loss: 0.21831002831459045\n",
      "iteration 29799: loss: 0.21830976009368896\n",
      "iteration 29800: loss: 0.21830955147743225\n",
      "iteration 29801: loss: 0.21830932796001434\n",
      "iteration 29802: loss: 0.2183089703321457\n",
      "iteration 29803: loss: 0.21830883622169495\n",
      "iteration 29804: loss: 0.21830853819847107\n",
      "iteration 29805: loss: 0.2183082401752472\n",
      "iteration 29806: loss: 0.21830801665782928\n",
      "iteration 29807: loss: 0.21830777823925018\n",
      "iteration 29808: loss: 0.21830753982067108\n",
      "iteration 29809: loss: 0.2183072566986084\n",
      "iteration 29810: loss: 0.21830704808235168\n",
      "iteration 29811: loss: 0.2183067798614502\n",
      "iteration 29812: loss: 0.2183065414428711\n",
      "iteration 29813: loss: 0.218306303024292\n",
      "iteration 29814: loss: 0.2183060646057129\n",
      "iteration 29815: loss: 0.2183057814836502\n",
      "iteration 29816: loss: 0.2183055430650711\n",
      "iteration 29817: loss: 0.218305304646492\n",
      "iteration 29818: loss: 0.21830499172210693\n",
      "iteration 29819: loss: 0.2183048278093338\n",
      "iteration 29820: loss: 0.2183045595884323\n",
      "iteration 29821: loss: 0.21830427646636963\n",
      "iteration 29822: loss: 0.21830406785011292\n",
      "iteration 29823: loss: 0.21830379962921143\n",
      "iteration 29824: loss: 0.21830353140830994\n",
      "iteration 29825: loss: 0.2183033525943756\n",
      "iteration 29826: loss: 0.21830305457115173\n",
      "iteration 29827: loss: 0.21830284595489502\n",
      "iteration 29828: loss: 0.2183026373386383\n",
      "iteration 29829: loss: 0.21830232441425323\n",
      "iteration 29830: loss: 0.21830210089683533\n",
      "iteration 29831: loss: 0.21830184757709503\n",
      "iteration 29832: loss: 0.21830153465270996\n",
      "iteration 29833: loss: 0.21830138564109802\n",
      "iteration 29834: loss: 0.21830113232135773\n",
      "iteration 29835: loss: 0.21830086410045624\n",
      "iteration 29836: loss: 0.21830061078071594\n",
      "iteration 29837: loss: 0.21830038726329803\n",
      "iteration 29838: loss: 0.21830010414123535\n",
      "iteration 29839: loss: 0.2182997763156891\n",
      "iteration 29840: loss: 0.21829959750175476\n",
      "iteration 29841: loss: 0.21829941868782043\n",
      "iteration 29842: loss: 0.21829912066459656\n",
      "iteration 29843: loss: 0.21829891204833984\n",
      "iteration 29844: loss: 0.21829859912395477\n",
      "iteration 29845: loss: 0.21829834580421448\n",
      "iteration 29846: loss: 0.218298077583313\n",
      "iteration 29847: loss: 0.21829786896705627\n",
      "iteration 29848: loss: 0.21829764544963837\n",
      "iteration 29849: loss: 0.21829740703105927\n",
      "iteration 29850: loss: 0.21829715371131897\n",
      "iteration 29851: loss: 0.2182968556880951\n",
      "iteration 29852: loss: 0.218296617269516\n",
      "iteration 29853: loss: 0.21829645335674286\n",
      "iteration 29854: loss: 0.2182960957288742\n",
      "iteration 29855: loss: 0.21829596161842346\n",
      "iteration 29856: loss: 0.21829566359519958\n",
      "iteration 29857: loss: 0.21829542517662048\n",
      "iteration 29858: loss: 0.21829518675804138\n",
      "iteration 29859: loss: 0.2182949036359787\n",
      "iteration 29860: loss: 0.21829469501972198\n",
      "iteration 29861: loss: 0.21829438209533691\n",
      "iteration 29862: loss: 0.21829411387443542\n",
      "iteration 29863: loss: 0.2182939350605011\n",
      "iteration 29864: loss: 0.21829374134540558\n",
      "iteration 29865: loss: 0.2182934284210205\n",
      "iteration 29866: loss: 0.2182932198047638\n",
      "iteration 29867: loss: 0.2182929813861847\n",
      "iteration 29868: loss: 0.2182927131652832\n",
      "iteration 29869: loss: 0.2182924449443817\n",
      "iteration 29870: loss: 0.218292236328125\n",
      "iteration 29871: loss: 0.21829192340373993\n",
      "iteration 29872: loss: 0.2182917594909668\n",
      "iteration 29873: loss: 0.2182914763689041\n",
      "iteration 29874: loss: 0.21829120814800262\n",
      "iteration 29875: loss: 0.21829096972942352\n",
      "iteration 29876: loss: 0.21829073131084442\n",
      "iteration 29877: loss: 0.21829047799110413\n",
      "iteration 29878: loss: 0.2182902842760086\n",
      "iteration 29879: loss: 0.2182900458574295\n",
      "iteration 29880: loss: 0.2182898074388504\n",
      "iteration 29881: loss: 0.21828952431678772\n",
      "iteration 29882: loss: 0.21828928589820862\n",
      "iteration 29883: loss: 0.21828904747962952\n",
      "iteration 29884: loss: 0.21828877925872803\n",
      "iteration 29885: loss: 0.21828851103782654\n",
      "iteration 29886: loss: 0.21828825771808624\n",
      "iteration 29887: loss: 0.21828797459602356\n",
      "iteration 29888: loss: 0.21828779578208923\n",
      "iteration 29889: loss: 0.21828751266002655\n",
      "iteration 29890: loss: 0.21828731894493103\n",
      "iteration 29891: loss: 0.21828708052635193\n",
      "iteration 29892: loss: 0.21828684210777283\n",
      "iteration 29893: loss: 0.21828658878803253\n",
      "iteration 29894: loss: 0.21828623116016388\n",
      "iteration 29895: loss: 0.21828608214855194\n",
      "iteration 29896: loss: 0.21828584372997284\n",
      "iteration 29897: loss: 0.21828556060791016\n",
      "iteration 29898: loss: 0.21828539669513702\n",
      "iteration 29899: loss: 0.21828508377075195\n",
      "iteration 29900: loss: 0.21828487515449524\n",
      "iteration 29901: loss: 0.21828460693359375\n",
      "iteration 29902: loss: 0.21828436851501465\n",
      "iteration 29903: loss: 0.21828413009643555\n",
      "iteration 29904: loss: 0.21828384697437286\n",
      "iteration 29905: loss: 0.21828360855579376\n",
      "iteration 29906: loss: 0.21828338503837585\n",
      "iteration 29907: loss: 0.21828313171863556\n",
      "iteration 29908: loss: 0.21828289330005646\n",
      "iteration 29909: loss: 0.21828262507915497\n",
      "iteration 29910: loss: 0.21828237175941467\n",
      "iteration 29911: loss: 0.21828213334083557\n",
      "iteration 29912: loss: 0.21828190982341766\n",
      "iteration 29913: loss: 0.21828167140483856\n",
      "iteration 29914: loss: 0.21828147768974304\n",
      "iteration 29915: loss: 0.21828117966651917\n",
      "iteration 29916: loss: 0.21828094124794006\n",
      "iteration 29917: loss: 0.21828067302703857\n",
      "iteration 29918: loss: 0.21828046441078186\n",
      "iteration 29919: loss: 0.21828027069568634\n",
      "iteration 29920: loss: 0.21827992796897888\n",
      "iteration 29921: loss: 0.21827971935272217\n",
      "iteration 29922: loss: 0.21827951073646545\n",
      "iteration 29923: loss: 0.21827921271324158\n",
      "iteration 29924: loss: 0.21827897429466248\n",
      "iteration 29925: loss: 0.21827873587608337\n",
      "iteration 29926: loss: 0.21827849745750427\n",
      "iteration 29927: loss: 0.2182782143354416\n",
      "iteration 29928: loss: 0.21827808022499084\n",
      "iteration 29929: loss: 0.21827776730060577\n",
      "iteration 29930: loss: 0.21827757358551025\n",
      "iteration 29931: loss: 0.21827729046344757\n",
      "iteration 29932: loss: 0.21827702224254608\n",
      "iteration 29933: loss: 0.2182767689228058\n",
      "iteration 29934: loss: 0.21827654540538788\n",
      "iteration 29935: loss: 0.21827630698680878\n",
      "iteration 29936: loss: 0.21827606856822968\n",
      "iteration 29937: loss: 0.21827581524848938\n",
      "iteration 29938: loss: 0.2182755470275879\n",
      "iteration 29939: loss: 0.21827533841133118\n",
      "iteration 29940: loss: 0.21827509999275208\n",
      "iteration 29941: loss: 0.21827483177185059\n",
      "iteration 29942: loss: 0.21827462315559387\n",
      "iteration 29943: loss: 0.2182743102312088\n",
      "iteration 29944: loss: 0.21827411651611328\n",
      "iteration 29945: loss: 0.21827392280101776\n",
      "iteration 29946: loss: 0.21827366948127747\n",
      "iteration 29947: loss: 0.2182733714580536\n",
      "iteration 29948: loss: 0.21827316284179688\n",
      "iteration 29949: loss: 0.2182728797197342\n",
      "iteration 29950: loss: 0.21827265620231628\n",
      "iteration 29951: loss: 0.218272402882576\n",
      "iteration 29952: loss: 0.2182721644639969\n",
      "iteration 29953: loss: 0.21827197074890137\n",
      "iteration 29954: loss: 0.21827168762683868\n",
      "iteration 29955: loss: 0.21827144920825958\n",
      "iteration 29956: loss: 0.2182711809873581\n",
      "iteration 29957: loss: 0.2182709276676178\n",
      "iteration 29958: loss: 0.2182706892490387\n",
      "iteration 29959: loss: 0.2182704657316208\n",
      "iteration 29960: loss: 0.2182702124118805\n",
      "iteration 29961: loss: 0.21827001869678497\n",
      "iteration 29962: loss: 0.21826979517936707\n",
      "iteration 29963: loss: 0.21826954185962677\n",
      "iteration 29964: loss: 0.2182692587375641\n",
      "iteration 29965: loss: 0.21826902031898499\n",
      "iteration 29966: loss: 0.2182687222957611\n",
      "iteration 29967: loss: 0.21826854348182678\n",
      "iteration 29968: loss: 0.21826830506324768\n",
      "iteration 29969: loss: 0.2182680368423462\n",
      "iteration 29970: loss: 0.2182677537202835\n",
      "iteration 29971: loss: 0.2182675153017044\n",
      "iteration 29972: loss: 0.2182672768831253\n",
      "iteration 29973: loss: 0.2182670533657074\n",
      "iteration 29974: loss: 0.21826688945293427\n",
      "iteration 29975: loss: 0.2182665765285492\n",
      "iteration 29976: loss: 0.2182663381099701\n",
      "iteration 29977: loss: 0.2182660847902298\n",
      "iteration 29978: loss: 0.2182658463716507\n",
      "iteration 29979: loss: 0.2182655781507492\n",
      "iteration 29980: loss: 0.2182653695344925\n",
      "iteration 29981: loss: 0.2182651311159134\n",
      "iteration 29982: loss: 0.21826490759849548\n",
      "iteration 29983: loss: 0.21826469898223877\n",
      "iteration 29984: loss: 0.21826446056365967\n",
      "iteration 29985: loss: 0.21826410293579102\n",
      "iteration 29986: loss: 0.21826386451721191\n",
      "iteration 29987: loss: 0.21826370060443878\n",
      "iteration 29988: loss: 0.21826346218585968\n",
      "iteration 29989: loss: 0.21826322376728058\n",
      "iteration 29990: loss: 0.21826298534870148\n",
      "iteration 29991: loss: 0.2182627022266388\n",
      "iteration 29992: loss: 0.2182624340057373\n",
      "iteration 29993: loss: 0.21826227009296417\n",
      "iteration 29994: loss: 0.21826191246509552\n",
      "iteration 29995: loss: 0.21826171875\n",
      "iteration 29996: loss: 0.2182614803314209\n",
      "iteration 29997: loss: 0.2182612419128418\n",
      "iteration 29998: loss: 0.21826104819774628\n",
      "iteration 29999: loss: 0.21826079487800598\n",
      "iteration 30000: loss: 0.2182605266571045\n",
      "iteration 30001: loss: 0.21826031804084778\n",
      "iteration 30002: loss: 0.2182600498199463\n",
      "iteration 30003: loss: 0.21825984120368958\n",
      "iteration 30004: loss: 0.21825961768627167\n",
      "iteration 30005: loss: 0.21825933456420898\n",
      "iteration 30006: loss: 0.21825909614562988\n",
      "iteration 30007: loss: 0.21825885772705078\n",
      "iteration 30008: loss: 0.21825861930847168\n",
      "iteration 30009: loss: 0.2182583063840866\n",
      "iteration 30010: loss: 0.21825814247131348\n",
      "iteration 30011: loss: 0.218257874250412\n",
      "iteration 30012: loss: 0.21825766563415527\n",
      "iteration 30013: loss: 0.21825742721557617\n",
      "iteration 30014: loss: 0.21825718879699707\n",
      "iteration 30015: loss: 0.2182569056749344\n",
      "iteration 30016: loss: 0.21825668215751648\n",
      "iteration 30017: loss: 0.2182563990354538\n",
      "iteration 30018: loss: 0.2182561457157135\n",
      "iteration 30019: loss: 0.21825595200061798\n",
      "iteration 30020: loss: 0.21825571358203888\n",
      "iteration 30021: loss: 0.21825547516345978\n",
      "iteration 30022: loss: 0.2182552069425583\n",
      "iteration 30023: loss: 0.21825499832630157\n",
      "iteration 30024: loss: 0.21825475990772247\n",
      "iteration 30025: loss: 0.21825452148914337\n",
      "iteration 30026: loss: 0.21825425326824188\n",
      "iteration 30027: loss: 0.21825408935546875\n",
      "iteration 30028: loss: 0.21825377643108368\n",
      "iteration 30029: loss: 0.21825356781482697\n",
      "iteration 30030: loss: 0.21825332939624786\n",
      "iteration 30031: loss: 0.21825306117534637\n",
      "iteration 30032: loss: 0.21825280785560608\n",
      "iteration 30033: loss: 0.21825256943702698\n",
      "iteration 30034: loss: 0.21825237572193146\n",
      "iteration 30035: loss: 0.21825213730335236\n",
      "iteration 30036: loss: 0.21825186908245087\n",
      "iteration 30037: loss: 0.21825166046619415\n",
      "iteration 30038: loss: 0.21825139224529266\n",
      "iteration 30039: loss: 0.21825118362903595\n",
      "iteration 30040: loss: 0.21825094521045685\n",
      "iteration 30041: loss: 0.21825066208839417\n",
      "iteration 30042: loss: 0.21825043857097626\n",
      "iteration 30043: loss: 0.21825020015239716\n",
      "iteration 30044: loss: 0.21824996173381805\n",
      "iteration 30045: loss: 0.21824975311756134\n",
      "iteration 30046: loss: 0.21824951469898224\n",
      "iteration 30047: loss: 0.21824923157691956\n",
      "iteration 30048: loss: 0.21824899315834045\n",
      "iteration 30049: loss: 0.21824876964092255\n",
      "iteration 30050: loss: 0.21824851632118225\n",
      "iteration 30051: loss: 0.21824832260608673\n",
      "iteration 30052: loss: 0.21824800968170166\n",
      "iteration 30053: loss: 0.21824781596660614\n",
      "iteration 30054: loss: 0.21824753284454346\n",
      "iteration 30055: loss: 0.21824729442596436\n",
      "iteration 30056: loss: 0.21824701130390167\n",
      "iteration 30057: loss: 0.21824684739112854\n",
      "iteration 30058: loss: 0.21824654936790466\n",
      "iteration 30059: loss: 0.21824637055397034\n",
      "iteration 30060: loss: 0.21824613213539124\n",
      "iteration 30061: loss: 0.21824589371681213\n",
      "iteration 30062: loss: 0.21824562549591064\n",
      "iteration 30063: loss: 0.21824538707733154\n",
      "iteration 30064: loss: 0.21824519336223602\n",
      "iteration 30065: loss: 0.21824488043785095\n",
      "iteration 30066: loss: 0.21824470162391663\n",
      "iteration 30067: loss: 0.2182445079088211\n",
      "iteration 30068: loss: 0.21824422478675842\n",
      "iteration 30069: loss: 0.21824398636817932\n",
      "iteration 30070: loss: 0.21824374794960022\n",
      "iteration 30071: loss: 0.21824350953102112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 30072: loss: 0.21824321150779724\n",
      "iteration 30073: loss: 0.21824303269386292\n",
      "iteration 30074: loss: 0.2182427942752838\n",
      "iteration 30075: loss: 0.21824249625205994\n",
      "iteration 30076: loss: 0.21824225783348083\n",
      "iteration 30077: loss: 0.2182420790195465\n",
      "iteration 30078: loss: 0.2182418406009674\n",
      "iteration 30079: loss: 0.2182416170835495\n",
      "iteration 30080: loss: 0.2182413637638092\n",
      "iteration 30081: loss: 0.2182411253452301\n",
      "iteration 30082: loss: 0.21824082732200623\n",
      "iteration 30083: loss: 0.21824069321155548\n",
      "iteration 30084: loss: 0.2182404100894928\n",
      "iteration 30085: loss: 0.2182401716709137\n",
      "iteration 30086: loss: 0.2182399332523346\n",
      "iteration 30087: loss: 0.2182396948337555\n",
      "iteration 30088: loss: 0.2182394564151764\n",
      "iteration 30089: loss: 0.21823915839195251\n",
      "iteration 30090: loss: 0.2182389199733734\n",
      "iteration 30091: loss: 0.21823875606060028\n",
      "iteration 30092: loss: 0.21823850274085999\n",
      "iteration 30093: loss: 0.2182382345199585\n",
      "iteration 30094: loss: 0.218237966299057\n",
      "iteration 30095: loss: 0.2182377576828003\n",
      "iteration 30096: loss: 0.2182374745607376\n",
      "iteration 30097: loss: 0.2182372808456421\n",
      "iteration 30098: loss: 0.21823708713054657\n",
      "iteration 30099: loss: 0.21823683381080627\n",
      "iteration 30100: loss: 0.21823661029338837\n",
      "iteration 30101: loss: 0.21823637187480927\n",
      "iteration 30102: loss: 0.2182360589504242\n",
      "iteration 30103: loss: 0.21823585033416748\n",
      "iteration 30104: loss: 0.21823561191558838\n",
      "iteration 30105: loss: 0.21823537349700928\n",
      "iteration 30106: loss: 0.21823513507843018\n",
      "iteration 30107: loss: 0.2182348221540451\n",
      "iteration 30108: loss: 0.21823468804359436\n",
      "iteration 30109: loss: 0.21823444962501526\n",
      "iteration 30110: loss: 0.21823421120643616\n",
      "iteration 30111: loss: 0.21823401749134064\n",
      "iteration 30112: loss: 0.21823373436927795\n",
      "iteration 30113: loss: 0.21823346614837646\n",
      "iteration 30114: loss: 0.21823325753211975\n",
      "iteration 30115: loss: 0.21823301911354065\n",
      "iteration 30116: loss: 0.21823278069496155\n",
      "iteration 30117: loss: 0.21823254227638245\n",
      "iteration 30118: loss: 0.21823230385780334\n",
      "iteration 30119: loss: 0.21823203563690186\n",
      "iteration 30120: loss: 0.21823179721832275\n",
      "iteration 30121: loss: 0.21823152899742126\n",
      "iteration 30122: loss: 0.21823132038116455\n",
      "iteration 30123: loss: 0.2182311713695526\n",
      "iteration 30124: loss: 0.2182309329509735\n",
      "iteration 30125: loss: 0.21823064982891083\n",
      "iteration 30126: loss: 0.21823039650917053\n",
      "iteration 30127: loss: 0.21823015809059143\n",
      "iteration 30128: loss: 0.21822988986968994\n",
      "iteration 30129: loss: 0.21822968125343323\n",
      "iteration 30130: loss: 0.21822944283485413\n",
      "iteration 30131: loss: 0.21822920441627502\n",
      "iteration 30132: loss: 0.21822898089885712\n",
      "iteration 30133: loss: 0.21822872757911682\n",
      "iteration 30134: loss: 0.21822848916053772\n",
      "iteration 30135: loss: 0.21822825074195862\n",
      "iteration 30136: loss: 0.2182280570268631\n",
      "iteration 30137: loss: 0.218227818608284\n",
      "iteration 30138: loss: 0.2182275354862213\n",
      "iteration 30139: loss: 0.2182273417711258\n",
      "iteration 30140: loss: 0.21822714805603027\n",
      "iteration 30141: loss: 0.2182268649339676\n",
      "iteration 30142: loss: 0.2182265967130661\n",
      "iteration 30143: loss: 0.21822640299797058\n",
      "iteration 30144: loss: 0.2182261198759079\n",
      "iteration 30145: loss: 0.2182258665561676\n",
      "iteration 30146: loss: 0.21822568774223328\n",
      "iteration 30147: loss: 0.21822543442249298\n",
      "iteration 30148: loss: 0.21822519600391388\n",
      "iteration 30149: loss: 0.21822495758533478\n",
      "iteration 30150: loss: 0.2182246893644333\n",
      "iteration 30151: loss: 0.21822449564933777\n",
      "iteration 30152: loss: 0.21822421252727509\n",
      "iteration 30153: loss: 0.21822404861450195\n",
      "iteration 30154: loss: 0.21822376549243927\n",
      "iteration 30155: loss: 0.21822352707386017\n",
      "iteration 30156: loss: 0.21822328865528107\n",
      "iteration 30157: loss: 0.21822300553321838\n",
      "iteration 30158: loss: 0.21822278201580048\n",
      "iteration 30159: loss: 0.21822257339954376\n",
      "iteration 30160: loss: 0.21822240948677063\n",
      "iteration 30161: loss: 0.21822209656238556\n",
      "iteration 30162: loss: 0.21822185814380646\n",
      "iteration 30163: loss: 0.21822158992290497\n",
      "iteration 30164: loss: 0.21822147071361542\n",
      "iteration 30165: loss: 0.21822114288806915\n",
      "iteration 30166: loss: 0.21822097897529602\n",
      "iteration 30167: loss: 0.21822066605091095\n",
      "iteration 30168: loss: 0.21822047233581543\n",
      "iteration 30169: loss: 0.21822020411491394\n",
      "iteration 30170: loss: 0.21822002530097961\n",
      "iteration 30171: loss: 0.21821975708007812\n",
      "iteration 30172: loss: 0.2182195633649826\n",
      "iteration 30173: loss: 0.21821928024291992\n",
      "iteration 30174: loss: 0.21821904182434082\n",
      "iteration 30175: loss: 0.2182188481092453\n",
      "iteration 30176: loss: 0.218218594789505\n",
      "iteration 30177: loss: 0.2182183712720871\n",
      "iteration 30178: loss: 0.21821816265583038\n",
      "iteration 30179: loss: 0.21821792423725128\n",
      "iteration 30180: loss: 0.2182176560163498\n",
      "iteration 30181: loss: 0.2182173728942871\n",
      "iteration 30182: loss: 0.218217134475708\n",
      "iteration 30183: loss: 0.21821697056293488\n",
      "iteration 30184: loss: 0.2182167023420334\n",
      "iteration 30185: loss: 0.21821650862693787\n",
      "iteration 30186: loss: 0.21821627020835876\n",
      "iteration 30187: loss: 0.2182159423828125\n",
      "iteration 30188: loss: 0.2182157337665558\n",
      "iteration 30189: loss: 0.21821551024913788\n",
      "iteration 30190: loss: 0.21821527183055878\n",
      "iteration 30191: loss: 0.21821506321430206\n",
      "iteration 30192: loss: 0.21821479499340057\n",
      "iteration 30193: loss: 0.21821460127830505\n",
      "iteration 30194: loss: 0.21821431815624237\n",
      "iteration 30195: loss: 0.21821412444114685\n",
      "iteration 30196: loss: 0.21821391582489014\n",
      "iteration 30197: loss: 0.21821367740631104\n",
      "iteration 30198: loss: 0.21821340918540955\n",
      "iteration 30199: loss: 0.21821317076683044\n",
      "iteration 30200: loss: 0.21821293234825134\n",
      "iteration 30201: loss: 0.21821275353431702\n",
      "iteration 30202: loss: 0.21821245551109314\n",
      "iteration 30203: loss: 0.21821221709251404\n",
      "iteration 30204: loss: 0.21821197867393494\n",
      "iteration 30205: loss: 0.21821174025535583\n",
      "iteration 30206: loss: 0.21821150183677673\n",
      "iteration 30207: loss: 0.2182113230228424\n",
      "iteration 30208: loss: 0.21821102499961853\n",
      "iteration 30209: loss: 0.21821081638336182\n",
      "iteration 30210: loss: 0.2182106077671051\n",
      "iteration 30211: loss: 0.21821030974388123\n",
      "iteration 30212: loss: 0.2182101458311081\n",
      "iteration 30213: loss: 0.2182098925113678\n",
      "iteration 30214: loss: 0.2182096689939499\n",
      "iteration 30215: loss: 0.2182094305753708\n",
      "iteration 30216: loss: 0.21820922195911407\n",
      "iteration 30217: loss: 0.21820895373821259\n",
      "iteration 30218: loss: 0.2182087004184723\n",
      "iteration 30219: loss: 0.2182084619998932\n",
      "iteration 30220: loss: 0.21820826828479767\n",
      "iteration 30221: loss: 0.21820798516273499\n",
      "iteration 30222: loss: 0.21820779144763947\n",
      "iteration 30223: loss: 0.21820755302906036\n",
      "iteration 30224: loss: 0.21820731461048126\n",
      "iteration 30225: loss: 0.21820709109306335\n",
      "iteration 30226: loss: 0.21820688247680664\n",
      "iteration 30227: loss: 0.21820661425590515\n",
      "iteration 30228: loss: 0.21820636093616486\n",
      "iteration 30229: loss: 0.21820616722106934\n",
      "iteration 30230: loss: 0.21820589900016785\n",
      "iteration 30231: loss: 0.21820564568042755\n",
      "iteration 30232: loss: 0.21820542216300964\n",
      "iteration 30233: loss: 0.21820516884326935\n",
      "iteration 30234: loss: 0.21820500493049622\n",
      "iteration 30235: loss: 0.21820473670959473\n",
      "iteration 30236: loss: 0.21820449829101562\n",
      "iteration 30237: loss: 0.2182042896747589\n",
      "iteration 30238: loss: 0.21820402145385742\n",
      "iteration 30239: loss: 0.2182038128376007\n",
      "iteration 30240: loss: 0.2182035893201828\n",
      "iteration 30241: loss: 0.2182033807039261\n",
      "iteration 30242: loss: 0.21820306777954102\n",
      "iteration 30243: loss: 0.2182028591632843\n",
      "iteration 30244: loss: 0.2182026356458664\n",
      "iteration 30245: loss: 0.21820244193077087\n",
      "iteration 30246: loss: 0.21820218861103058\n",
      "iteration 30247: loss: 0.2182019203901291\n",
      "iteration 30248: loss: 0.21820172667503357\n",
      "iteration 30249: loss: 0.21820151805877686\n",
      "iteration 30250: loss: 0.21820127964019775\n",
      "iteration 30251: loss: 0.21820107102394104\n",
      "iteration 30252: loss: 0.21820080280303955\n",
      "iteration 30253: loss: 0.21820056438446045\n",
      "iteration 30254: loss: 0.21820040047168732\n",
      "iteration 30255: loss: 0.21820005774497986\n",
      "iteration 30256: loss: 0.21819984912872314\n",
      "iteration 30257: loss: 0.21819965541362762\n",
      "iteration 30258: loss: 0.21819940209388733\n",
      "iteration 30259: loss: 0.21819916367530823\n",
      "iteration 30260: loss: 0.21819892525672913\n",
      "iteration 30261: loss: 0.21819862723350525\n",
      "iteration 30262: loss: 0.21819853782653809\n",
      "iteration 30263: loss: 0.21819821000099182\n",
      "iteration 30264: loss: 0.2181980162858963\n",
      "iteration 30265: loss: 0.2181977778673172\n",
      "iteration 30266: loss: 0.2181975096464157\n",
      "iteration 30267: loss: 0.2181973159313202\n",
      "iteration 30268: loss: 0.2181970626115799\n",
      "iteration 30269: loss: 0.218196839094162\n",
      "iteration 30270: loss: 0.2181965857744217\n",
      "iteration 30271: loss: 0.2181963473558426\n",
      "iteration 30272: loss: 0.21819612383842468\n",
      "iteration 30273: loss: 0.21819591522216797\n",
      "iteration 30274: loss: 0.21819567680358887\n",
      "iteration 30275: loss: 0.21819543838500977\n",
      "iteration 30276: loss: 0.21819519996643066\n",
      "iteration 30277: loss: 0.21819499135017395\n",
      "iteration 30278: loss: 0.21819476783275604\n",
      "iteration 30279: loss: 0.21819445490837097\n",
      "iteration 30280: loss: 0.21819424629211426\n",
      "iteration 30281: loss: 0.21819403767585754\n",
      "iteration 30282: loss: 0.21819381415843964\n",
      "iteration 30283: loss: 0.21819360554218292\n",
      "iteration 30284: loss: 0.21819329261779785\n",
      "iteration 30285: loss: 0.2181931436061859\n",
      "iteration 30286: loss: 0.21819286048412323\n",
      "iteration 30287: loss: 0.21819257736206055\n",
      "iteration 30288: loss: 0.21819241344928741\n",
      "iteration 30289: loss: 0.21819224953651428\n",
      "iteration 30290: loss: 0.2181919515132904\n",
      "iteration 30291: loss: 0.2181917428970337\n",
      "iteration 30292: loss: 0.2181914746761322\n",
      "iteration 30293: loss: 0.21819131076335907\n",
      "iteration 30294: loss: 0.21819105744361877\n",
      "iteration 30295: loss: 0.21819078922271729\n",
      "iteration 30296: loss: 0.2181905210018158\n",
      "iteration 30297: loss: 0.21819038689136505\n",
      "iteration 30298: loss: 0.21819007396697998\n",
      "iteration 30299: loss: 0.21818991005420685\n",
      "iteration 30300: loss: 0.21818964183330536\n",
      "iteration 30301: loss: 0.21818943321704865\n",
      "iteration 30302: loss: 0.21818916499614716\n",
      "iteration 30303: loss: 0.21818895637989044\n",
      "iteration 30304: loss: 0.21818873286247253\n",
      "iteration 30305: loss: 0.21818849444389343\n",
      "iteration 30306: loss: 0.21818825602531433\n",
      "iteration 30307: loss: 0.21818797290325165\n",
      "iteration 30308: loss: 0.21818776428699493\n",
      "iteration 30309: loss: 0.21818757057189941\n",
      "iteration 30310: loss: 0.2181873321533203\n",
      "iteration 30311: loss: 0.2181871235370636\n",
      "iteration 30312: loss: 0.21818682551383972\n",
      "iteration 30313: loss: 0.2181866466999054\n",
      "iteration 30314: loss: 0.2181864082813263\n",
      "iteration 30315: loss: 0.2181861698627472\n",
      "iteration 30316: loss: 0.21818597614765167\n",
      "iteration 30317: loss: 0.218185693025589\n",
      "iteration 30318: loss: 0.21818546950817108\n",
      "iteration 30319: loss: 0.21818526089191437\n",
      "iteration 30320: loss: 0.21818499267101288\n",
      "iteration 30321: loss: 0.21818479895591736\n",
      "iteration 30322: loss: 0.21818459033966064\n",
      "iteration 30323: loss: 0.21818432211875916\n",
      "iteration 30324: loss: 0.21818414330482483\n",
      "iteration 30325: loss: 0.21818391978740692\n",
      "iteration 30326: loss: 0.21818366646766663\n",
      "iteration 30327: loss: 0.21818336844444275\n",
      "iteration 30328: loss: 0.21818315982818604\n",
      "iteration 30329: loss: 0.21818289160728455\n",
      "iteration 30330: loss: 0.21818268299102783\n",
      "iteration 30331: loss: 0.21818247437477112\n",
      "iteration 30332: loss: 0.2181822806596756\n",
      "iteration 30333: loss: 0.2181820422410965\n",
      "iteration 30334: loss: 0.21818187832832336\n",
      "iteration 30335: loss: 0.2181815356016159\n",
      "iteration 30336: loss: 0.21818137168884277\n",
      "iteration 30337: loss: 0.21818113327026367\n",
      "iteration 30338: loss: 0.21818089485168457\n",
      "iteration 30339: loss: 0.21818062663078308\n",
      "iteration 30340: loss: 0.21818046271800995\n",
      "iteration 30341: loss: 0.21818017959594727\n",
      "iteration 30342: loss: 0.21817994117736816\n",
      "iteration 30343: loss: 0.21817970275878906\n",
      "iteration 30344: loss: 0.21817949414253235\n",
      "iteration 30345: loss: 0.21817925572395325\n",
      "iteration 30346: loss: 0.21817903220653534\n",
      "iteration 30347: loss: 0.21817879378795624\n",
      "iteration 30348: loss: 0.21817854046821594\n",
      "iteration 30349: loss: 0.21817834675312042\n",
      "iteration 30350: loss: 0.21817810833454132\n",
      "iteration 30351: loss: 0.2181778848171234\n",
      "iteration 30352: loss: 0.2181776463985443\n",
      "iteration 30353: loss: 0.21817739307880402\n",
      "iteration 30354: loss: 0.2181771695613861\n",
      "iteration 30355: loss: 0.2181769609451294\n",
      "iteration 30356: loss: 0.2181767225265503\n",
      "iteration 30357: loss: 0.21817655861377716\n",
      "iteration 30358: loss: 0.21817627549171448\n",
      "iteration 30359: loss: 0.21817603707313538\n",
      "iteration 30360: loss: 0.21817581355571747\n",
      "iteration 30361: loss: 0.21817560493946075\n",
      "iteration 30362: loss: 0.21817533671855927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 30363: loss: 0.21817514300346375\n",
      "iteration 30364: loss: 0.21817488968372345\n",
      "iteration 30365: loss: 0.21817469596862793\n",
      "iteration 30366: loss: 0.21817445755004883\n",
      "iteration 30367: loss: 0.21817418932914734\n",
      "iteration 30368: loss: 0.21817395091056824\n",
      "iteration 30369: loss: 0.2181737720966339\n",
      "iteration 30370: loss: 0.2181735336780548\n",
      "iteration 30371: loss: 0.2181733399629593\n",
      "iteration 30372: loss: 0.2181730717420578\n",
      "iteration 30373: loss: 0.2181728184223175\n",
      "iteration 30374: loss: 0.21817262470722198\n",
      "iteration 30375: loss: 0.21817238628864288\n",
      "iteration 30376: loss: 0.2181721180677414\n",
      "iteration 30377: loss: 0.21817190945148468\n",
      "iteration 30378: loss: 0.21817167103290558\n",
      "iteration 30379: loss: 0.21817152202129364\n",
      "iteration 30380: loss: 0.21817128360271454\n",
      "iteration 30381: loss: 0.21817097067832947\n",
      "iteration 30382: loss: 0.21817076206207275\n",
      "iteration 30383: loss: 0.21817049384117126\n",
      "iteration 30384: loss: 0.21817032992839813\n",
      "iteration 30385: loss: 0.21817007660865784\n",
      "iteration 30386: loss: 0.21816985309123993\n",
      "iteration 30387: loss: 0.21816964447498322\n",
      "iteration 30388: loss: 0.2181694060564041\n",
      "iteration 30389: loss: 0.2181691825389862\n",
      "iteration 30390: loss: 0.21816900372505188\n",
      "iteration 30391: loss: 0.218168705701828\n",
      "iteration 30392: loss: 0.2181684970855713\n",
      "iteration 30393: loss: 0.2181682139635086\n",
      "iteration 30394: loss: 0.21816802024841309\n",
      "iteration 30395: loss: 0.21816781163215637\n",
      "iteration 30396: loss: 0.21816757321357727\n",
      "iteration 30397: loss: 0.21816734969615936\n",
      "iteration 30398: loss: 0.21816718578338623\n",
      "iteration 30399: loss: 0.21816687285900116\n",
      "iteration 30400: loss: 0.21816666424274445\n",
      "iteration 30401: loss: 0.21816642582416534\n",
      "iteration 30402: loss: 0.21816623210906982\n",
      "iteration 30403: loss: 0.21816596388816833\n",
      "iteration 30404: loss: 0.21816575527191162\n",
      "iteration 30405: loss: 0.21816551685333252\n",
      "iteration 30406: loss: 0.218165323138237\n",
      "iteration 30407: loss: 0.2181650698184967\n",
      "iteration 30408: loss: 0.2181648463010788\n",
      "iteration 30409: loss: 0.2181645929813385\n",
      "iteration 30410: loss: 0.2181643694639206\n",
      "iteration 30411: loss: 0.21816417574882507\n",
      "iteration 30412: loss: 0.2181638926267624\n",
      "iteration 30413: loss: 0.21816368401050568\n",
      "iteration 30414: loss: 0.21816349029541016\n",
      "iteration 30415: loss: 0.21816322207450867\n",
      "iteration 30416: loss: 0.21816301345825195\n",
      "iteration 30417: loss: 0.21816273033618927\n",
      "iteration 30418: loss: 0.21816261112689972\n",
      "iteration 30419: loss: 0.2181623876094818\n",
      "iteration 30420: loss: 0.21816210448741913\n",
      "iteration 30421: loss: 0.21816189587116241\n",
      "iteration 30422: loss: 0.21816161274909973\n",
      "iteration 30423: loss: 0.21816138923168182\n",
      "iteration 30424: loss: 0.2181611955165863\n",
      "iteration 30425: loss: 0.2181609869003296\n",
      "iteration 30426: loss: 0.21816077828407288\n",
      "iteration 30427: loss: 0.21816055476665497\n",
      "iteration 30428: loss: 0.21816034615039825\n",
      "iteration 30429: loss: 0.21816007792949677\n",
      "iteration 30430: loss: 0.21815982460975647\n",
      "iteration 30431: loss: 0.21815960109233856\n",
      "iteration 30432: loss: 0.21815936267375946\n",
      "iteration 30433: loss: 0.21815916895866394\n",
      "iteration 30434: loss: 0.21815893054008484\n",
      "iteration 30435: loss: 0.21815867722034454\n",
      "iteration 30436: loss: 0.21815840899944305\n",
      "iteration 30437: loss: 0.21815824508666992\n",
      "iteration 30438: loss: 0.2181580364704132\n",
      "iteration 30439: loss: 0.21815776824951172\n",
      "iteration 30440: loss: 0.2181576043367386\n",
      "iteration 30441: loss: 0.2181573361158371\n",
      "iteration 30442: loss: 0.2181570827960968\n",
      "iteration 30443: loss: 0.21815690398216248\n",
      "iteration 30444: loss: 0.21815665066242218\n",
      "iteration 30445: loss: 0.21815641224384308\n",
      "iteration 30446: loss: 0.21815624833106995\n",
      "iteration 30447: loss: 0.21815602481365204\n",
      "iteration 30448: loss: 0.21815574169158936\n",
      "iteration 30449: loss: 0.21815550327301025\n",
      "iteration 30450: loss: 0.21815529465675354\n",
      "iteration 30451: loss: 0.2181551158428192\n",
      "iteration 30452: loss: 0.21815478801727295\n",
      "iteration 30453: loss: 0.21815462410449982\n",
      "iteration 30454: loss: 0.21815438568592072\n",
      "iteration 30455: loss: 0.2181541919708252\n",
      "iteration 30456: loss: 0.2181539237499237\n",
      "iteration 30457: loss: 0.2181536853313446\n",
      "iteration 30458: loss: 0.21815350651741028\n",
      "iteration 30459: loss: 0.2181532382965088\n",
      "iteration 30460: loss: 0.21815302968025208\n",
      "iteration 30461: loss: 0.21815280616283417\n",
      "iteration 30462: loss: 0.21815259754657745\n",
      "iteration 30463: loss: 0.21815235912799835\n",
      "iteration 30464: loss: 0.21815212070941925\n",
      "iteration 30465: loss: 0.21815189719200134\n",
      "iteration 30466: loss: 0.21815168857574463\n",
      "iteration 30467: loss: 0.21815145015716553\n",
      "iteration 30468: loss: 0.21815125644207\n",
      "iteration 30469: loss: 0.2181510031223297\n",
      "iteration 30470: loss: 0.2181507647037506\n",
      "iteration 30471: loss: 0.21815049648284912\n",
      "iteration 30472: loss: 0.2181502878665924\n",
      "iteration 30473: loss: 0.2181500941514969\n",
      "iteration 30474: loss: 0.2181498259305954\n",
      "iteration 30475: loss: 0.21814970672130585\n",
      "iteration 30476: loss: 0.21814939379692078\n",
      "iteration 30477: loss: 0.21814925968647003\n",
      "iteration 30478: loss: 0.21814897656440735\n",
      "iteration 30479: loss: 0.21814878284931183\n",
      "iteration 30480: loss: 0.21814855933189392\n",
      "iteration 30481: loss: 0.21814830601215363\n",
      "iteration 30482: loss: 0.21814808249473572\n",
      "iteration 30483: loss: 0.21814782917499542\n",
      "iteration 30484: loss: 0.21814759075641632\n",
      "iteration 30485: loss: 0.21814735233783722\n",
      "iteration 30486: loss: 0.2181471884250641\n",
      "iteration 30487: loss: 0.21814695000648499\n",
      "iteration 30488: loss: 0.2181466817855835\n",
      "iteration 30489: loss: 0.21814647316932678\n",
      "iteration 30490: loss: 0.2181461751461029\n",
      "iteration 30491: loss: 0.21814601123332977\n",
      "iteration 30492: loss: 0.21814581751823425\n",
      "iteration 30493: loss: 0.21814553439617157\n",
      "iteration 30494: loss: 0.21814532577991486\n",
      "iteration 30495: loss: 0.21814513206481934\n",
      "iteration 30496: loss: 0.21814493834972382\n",
      "iteration 30497: loss: 0.2181447297334671\n",
      "iteration 30498: loss: 0.21814444661140442\n",
      "iteration 30499: loss: 0.2181442230939865\n",
      "iteration 30500: loss: 0.2181439846754074\n",
      "iteration 30501: loss: 0.2181437909603119\n",
      "iteration 30502: loss: 0.2181435525417328\n",
      "iteration 30503: loss: 0.2181433141231537\n",
      "iteration 30504: loss: 0.21814307570457458\n",
      "iteration 30505: loss: 0.21814294159412384\n",
      "iteration 30506: loss: 0.21814270317554474\n",
      "iteration 30507: loss: 0.21814243495464325\n",
      "iteration 30508: loss: 0.21814218163490295\n",
      "iteration 30509: loss: 0.21814195811748505\n",
      "iteration 30510: loss: 0.21814176440238953\n",
      "iteration 30511: loss: 0.2181415557861328\n",
      "iteration 30512: loss: 0.2181413173675537\n",
      "iteration 30513: loss: 0.218141108751297\n",
      "iteration 30514: loss: 0.21814091503620148\n",
      "iteration 30515: loss: 0.2181406468153\n",
      "iteration 30516: loss: 0.2181404083967209\n",
      "iteration 30517: loss: 0.2181401252746582\n",
      "iteration 30518: loss: 0.21813993155956268\n",
      "iteration 30519: loss: 0.21813973784446716\n",
      "iteration 30520: loss: 0.21813952922821045\n",
      "iteration 30521: loss: 0.21813932061195374\n",
      "iteration 30522: loss: 0.21813908219337463\n",
      "iteration 30523: loss: 0.2181388884782791\n",
      "iteration 30524: loss: 0.2181386649608612\n",
      "iteration 30525: loss: 0.21813836693763733\n",
      "iteration 30526: loss: 0.2181381732225418\n",
      "iteration 30527: loss: 0.21813786029815674\n",
      "iteration 30528: loss: 0.2181376963853836\n",
      "iteration 30529: loss: 0.21813754737377167\n",
      "iteration 30530: loss: 0.21813729405403137\n",
      "iteration 30531: loss: 0.21813707053661346\n",
      "iteration 30532: loss: 0.21813686192035675\n",
      "iteration 30533: loss: 0.21813659369945526\n",
      "iteration 30534: loss: 0.21813635528087616\n",
      "iteration 30535: loss: 0.21813619136810303\n",
      "iteration 30536: loss: 0.21813592314720154\n",
      "iteration 30537: loss: 0.21813566982746124\n",
      "iteration 30538: loss: 0.21813547611236572\n",
      "iteration 30539: loss: 0.2181352823972702\n",
      "iteration 30540: loss: 0.2181350290775299\n",
      "iteration 30541: loss: 0.2181347906589508\n",
      "iteration 30542: loss: 0.2181345671415329\n",
      "iteration 30543: loss: 0.21813440322875977\n",
      "iteration 30544: loss: 0.21813412010669708\n",
      "iteration 30545: loss: 0.21813389658927917\n",
      "iteration 30546: loss: 0.21813373267650604\n",
      "iteration 30547: loss: 0.21813347935676575\n",
      "iteration 30548: loss: 0.21813321113586426\n",
      "iteration 30549: loss: 0.21813300251960754\n",
      "iteration 30550: loss: 0.21813280880451202\n",
      "iteration 30551: loss: 0.2181326448917389\n",
      "iteration 30552: loss: 0.21813234686851501\n",
      "iteration 30553: loss: 0.2181321680545807\n",
      "iteration 30554: loss: 0.2181318998336792\n",
      "iteration 30555: loss: 0.21813173592090607\n",
      "iteration 30556: loss: 0.21813145279884338\n",
      "iteration 30557: loss: 0.21813122928142548\n",
      "iteration 30558: loss: 0.21813106536865234\n",
      "iteration 30559: loss: 0.21813082695007324\n",
      "iteration 30560: loss: 0.21813058853149414\n",
      "iteration 30561: loss: 0.21813032031059265\n",
      "iteration 30562: loss: 0.21813015639781952\n",
      "iteration 30563: loss: 0.21812987327575684\n",
      "iteration 30564: loss: 0.21812966465950012\n",
      "iteration 30565: loss: 0.2181294709444046\n",
      "iteration 30566: loss: 0.2181292474269867\n",
      "iteration 30567: loss: 0.218128964304924\n",
      "iteration 30568: loss: 0.2181287556886673\n",
      "iteration 30569: loss: 0.21812859177589417\n",
      "iteration 30570: loss: 0.2181282788515091\n",
      "iteration 30571: loss: 0.21812811493873596\n",
      "iteration 30572: loss: 0.21812792122364044\n",
      "iteration 30573: loss: 0.21812765300273895\n",
      "iteration 30574: loss: 0.21812744438648224\n",
      "iteration 30575: loss: 0.2181272804737091\n",
      "iteration 30576: loss: 0.21812701225280762\n",
      "iteration 30577: loss: 0.2181268036365509\n",
      "iteration 30578: loss: 0.2181265652179718\n",
      "iteration 30579: loss: 0.2181263417005539\n",
      "iteration 30580: loss: 0.2181261032819748\n",
      "iteration 30581: loss: 0.2181258499622345\n",
      "iteration 30582: loss: 0.21812567114830017\n",
      "iteration 30583: loss: 0.21812543272972107\n",
      "iteration 30584: loss: 0.21812522411346436\n",
      "iteration 30585: loss: 0.21812501549720764\n",
      "iteration 30586: loss: 0.21812474727630615\n",
      "iteration 30587: loss: 0.21812458336353302\n",
      "iteration 30588: loss: 0.21812431514263153\n",
      "iteration 30589: loss: 0.21812410652637482\n",
      "iteration 30590: loss: 0.21812386810779572\n",
      "iteration 30591: loss: 0.2181236743927002\n",
      "iteration 30592: loss: 0.21812348067760468\n",
      "iteration 30593: loss: 0.21812324225902557\n",
      "iteration 30594: loss: 0.21812298893928528\n",
      "iteration 30595: loss: 0.21812281012535095\n",
      "iteration 30596: loss: 0.21812252700328827\n",
      "iteration 30597: loss: 0.21812233328819275\n",
      "iteration 30598: loss: 0.21812212467193604\n",
      "iteration 30599: loss: 0.21812193095684052\n",
      "iteration 30600: loss: 0.2181217223405838\n",
      "iteration 30601: loss: 0.21812143921852112\n",
      "iteration 30602: loss: 0.2181212157011032\n",
      "iteration 30603: loss: 0.2181210219860077\n",
      "iteration 30604: loss: 0.2181207835674286\n",
      "iteration 30605: loss: 0.2181205451488495\n",
      "iteration 30606: loss: 0.21812033653259277\n",
      "iteration 30607: loss: 0.21812014281749725\n",
      "iteration 30608: loss: 0.21811990439891815\n",
      "iteration 30609: loss: 0.21811966598033905\n",
      "iteration 30610: loss: 0.21811947226524353\n",
      "iteration 30611: loss: 0.21811918914318085\n",
      "iteration 30612: loss: 0.21811899542808533\n",
      "iteration 30613: loss: 0.2181187868118286\n",
      "iteration 30614: loss: 0.2181185781955719\n",
      "iteration 30615: loss: 0.218118354678154\n",
      "iteration 30616: loss: 0.21811816096305847\n",
      "iteration 30617: loss: 0.2181178629398346\n",
      "iteration 30618: loss: 0.21811766922473907\n",
      "iteration 30619: loss: 0.21811747550964355\n",
      "iteration 30620: loss: 0.21811723709106445\n",
      "iteration 30621: loss: 0.21811702847480774\n",
      "iteration 30622: loss: 0.21811680495738983\n",
      "iteration 30623: loss: 0.21811655163764954\n",
      "iteration 30624: loss: 0.21811632812023163\n",
      "iteration 30625: loss: 0.21811619400978088\n",
      "iteration 30626: loss: 0.218115895986557\n",
      "iteration 30627: loss: 0.21811561286449432\n",
      "iteration 30628: loss: 0.21811552345752716\n",
      "iteration 30629: loss: 0.2181152105331421\n",
      "iteration 30630: loss: 0.21811509132385254\n",
      "iteration 30631: loss: 0.21811485290527344\n",
      "iteration 30632: loss: 0.21811456978321075\n",
      "iteration 30633: loss: 0.21811437606811523\n",
      "iteration 30634: loss: 0.21811410784721375\n",
      "iteration 30635: loss: 0.21811392903327942\n",
      "iteration 30636: loss: 0.21811363101005554\n",
      "iteration 30637: loss: 0.21811354160308838\n",
      "iteration 30638: loss: 0.21811333298683167\n",
      "iteration 30639: loss: 0.2181130349636078\n",
      "iteration 30640: loss: 0.21811282634735107\n",
      "iteration 30641: loss: 0.21811255812644958\n",
      "iteration 30642: loss: 0.21811234951019287\n",
      "iteration 30643: loss: 0.21811218559741974\n",
      "iteration 30644: loss: 0.21811194717884064\n",
      "iteration 30645: loss: 0.21811175346374512\n",
      "iteration 30646: loss: 0.21811148524284363\n",
      "iteration 30647: loss: 0.21811127662658691\n",
      "iteration 30648: loss: 0.2181110829114914\n",
      "iteration 30649: loss: 0.2181108444929123\n",
      "iteration 30650: loss: 0.21811065077781677\n",
      "iteration 30651: loss: 0.21811039745807648\n",
      "iteration 30652: loss: 0.21811015903949738\n",
      "iteration 30653: loss: 0.21810993552207947\n",
      "iteration 30654: loss: 0.21810972690582275\n",
      "iteration 30655: loss: 0.21810945868492126\n",
      "iteration 30656: loss: 0.21810929477214813\n",
      "iteration 30657: loss: 0.21810901165008545\n",
      "iteration 30658: loss: 0.21810884773731232\n",
      "iteration 30659: loss: 0.2181086540222168\n",
      "iteration 30660: loss: 0.2181084156036377\n",
      "iteration 30661: loss: 0.2181081771850586\n",
      "iteration 30662: loss: 0.21810798346996307\n",
      "iteration 30663: loss: 0.21810774505138397\n",
      "iteration 30664: loss: 0.21810755133628845\n",
      "iteration 30665: loss: 0.21810726821422577\n",
      "iteration 30666: loss: 0.21810707449913025\n",
      "iteration 30667: loss: 0.2181069552898407\n",
      "iteration 30668: loss: 0.21810665726661682\n",
      "iteration 30669: loss: 0.2181064337491989\n",
      "iteration 30670: loss: 0.2181061953306198\n",
      "iteration 30671: loss: 0.21810606122016907\n",
      "iteration 30672: loss: 0.218105748295784\n",
      "iteration 30673: loss: 0.2181055247783661\n",
      "iteration 30674: loss: 0.21810536086559296\n",
      "iteration 30675: loss: 0.21810512244701385\n",
      "iteration 30676: loss: 0.21810488402843475\n",
      "iteration 30677: loss: 0.21810469031333923\n",
      "iteration 30678: loss: 0.21810445189476013\n",
      "iteration 30679: loss: 0.21810421347618103\n",
      "iteration 30680: loss: 0.2181040346622467\n",
      "iteration 30681: loss: 0.2181037962436676\n",
      "iteration 30682: loss: 0.21810360252857208\n",
      "iteration 30683: loss: 0.21810337901115417\n",
      "iteration 30684: loss: 0.21810312569141388\n",
      "iteration 30685: loss: 0.21810293197631836\n",
      "iteration 30686: loss: 0.21810266375541687\n",
      "iteration 30687: loss: 0.21810245513916016\n",
      "iteration 30688: loss: 0.21810224652290344\n",
      "iteration 30689: loss: 0.21810200810432434\n",
      "iteration 30690: loss: 0.21810181438922882\n",
      "iteration 30691: loss: 0.2181016504764557\n",
      "iteration 30692: loss: 0.2181014120578766\n",
      "iteration 30693: loss: 0.21810121834278107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 30694: loss: 0.21810093522071838\n",
      "iteration 30695: loss: 0.21810074150562286\n",
      "iteration 30696: loss: 0.21810050308704376\n",
      "iteration 30697: loss: 0.21810030937194824\n",
      "iteration 30698: loss: 0.21810007095336914\n",
      "iteration 30699: loss: 0.21809983253479004\n",
      "iteration 30700: loss: 0.21809962391853333\n",
      "iteration 30701: loss: 0.21809940040111542\n",
      "iteration 30702: loss: 0.21809926629066467\n",
      "iteration 30703: loss: 0.2180989533662796\n",
      "iteration 30704: loss: 0.21809878945350647\n",
      "iteration 30705: loss: 0.21809855103492737\n",
      "iteration 30706: loss: 0.21809831261634827\n",
      "iteration 30707: loss: 0.21809807419776917\n",
      "iteration 30708: loss: 0.21809785068035126\n",
      "iteration 30709: loss: 0.21809765696525574\n",
      "iteration 30710: loss: 0.21809744834899902\n",
      "iteration 30711: loss: 0.2180972397327423\n",
      "iteration 30712: loss: 0.21809697151184082\n",
      "iteration 30713: loss: 0.2180968075990677\n",
      "iteration 30714: loss: 0.21809661388397217\n",
      "iteration 30715: loss: 0.21809634566307068\n",
      "iteration 30716: loss: 0.21809618175029755\n",
      "iteration 30717: loss: 0.21809592843055725\n",
      "iteration 30718: loss: 0.21809569001197815\n",
      "iteration 30719: loss: 0.21809551119804382\n",
      "iteration 30720: loss: 0.21809525787830353\n",
      "iteration 30721: loss: 0.21809501945972443\n",
      "iteration 30722: loss: 0.2180948257446289\n",
      "iteration 30723: loss: 0.2180946171283722\n",
      "iteration 30724: loss: 0.21809439361095428\n",
      "iteration 30725: loss: 0.21809418499469757\n",
      "iteration 30726: loss: 0.21809396147727966\n",
      "iteration 30727: loss: 0.21809375286102295\n",
      "iteration 30728: loss: 0.21809355914592743\n",
      "iteration 30729: loss: 0.21809330582618713\n",
      "iteration 30730: loss: 0.2180931121110916\n",
      "iteration 30731: loss: 0.2180928885936737\n",
      "iteration 30732: loss: 0.218092679977417\n",
      "iteration 30733: loss: 0.2180924415588379\n",
      "iteration 30734: loss: 0.21809224784374237\n",
      "iteration 30735: loss: 0.21809200942516327\n",
      "iteration 30736: loss: 0.21809180080890656\n",
      "iteration 30737: loss: 0.21809153258800507\n",
      "iteration 30738: loss: 0.21809129416942596\n",
      "iteration 30739: loss: 0.21809116005897522\n",
      "iteration 30740: loss: 0.21809089183807373\n",
      "iteration 30741: loss: 0.2180906981229782\n",
      "iteration 30742: loss: 0.21809044480323792\n",
      "iteration 30743: loss: 0.2180902510881424\n",
      "iteration 30744: loss: 0.21809008717536926\n",
      "iteration 30745: loss: 0.21808984875679016\n",
      "iteration 30746: loss: 0.21808955073356628\n",
      "iteration 30747: loss: 0.21808938682079315\n",
      "iteration 30748: loss: 0.21808914840221405\n",
      "iteration 30749: loss: 0.21808895468711853\n",
      "iteration 30750: loss: 0.2180887758731842\n",
      "iteration 30751: loss: 0.21808850765228271\n",
      "iteration 30752: loss: 0.2180882692337036\n",
      "iteration 30753: loss: 0.21808812022209167\n",
      "iteration 30754: loss: 0.2180878221988678\n",
      "iteration 30755: loss: 0.21808762848377228\n",
      "iteration 30756: loss: 0.21808739006519318\n",
      "iteration 30757: loss: 0.21808727085590363\n",
      "iteration 30758: loss: 0.21808695793151855\n",
      "iteration 30759: loss: 0.21808674931526184\n",
      "iteration 30760: loss: 0.21808655560016632\n",
      "iteration 30761: loss: 0.21808631718158722\n",
      "iteration 30762: loss: 0.2180861234664917\n",
      "iteration 30763: loss: 0.21808595955371857\n",
      "iteration 30764: loss: 0.21808572113513947\n",
      "iteration 30765: loss: 0.21808548271656036\n",
      "iteration 30766: loss: 0.21808524429798126\n",
      "iteration 30767: loss: 0.21808500587940216\n",
      "iteration 30768: loss: 0.21808485686779022\n",
      "iteration 30769: loss: 0.21808461844921112\n",
      "iteration 30770: loss: 0.21808436512947083\n",
      "iteration 30771: loss: 0.2180842161178589\n",
      "iteration 30772: loss: 0.21808397769927979\n",
      "iteration 30773: loss: 0.2180837094783783\n",
      "iteration 30774: loss: 0.21808353066444397\n",
      "iteration 30775: loss: 0.21808335185050964\n",
      "iteration 30776: loss: 0.21808309853076935\n",
      "iteration 30777: loss: 0.21808286011219025\n",
      "iteration 30778: loss: 0.21808266639709473\n",
      "iteration 30779: loss: 0.21808242797851562\n",
      "iteration 30780: loss: 0.2180822342634201\n",
      "iteration 30781: loss: 0.21808204054832458\n",
      "iteration 30782: loss: 0.2180817872285843\n",
      "iteration 30783: loss: 0.21808163821697235\n",
      "iteration 30784: loss: 0.21808138489723206\n",
      "iteration 30785: loss: 0.21808111667633057\n",
      "iteration 30786: loss: 0.21808095276355743\n",
      "iteration 30787: loss: 0.21808071434497833\n",
      "iteration 30788: loss: 0.21808047592639923\n",
      "iteration 30789: loss: 0.2180802822113037\n",
      "iteration 30790: loss: 0.2180800884962082\n",
      "iteration 30791: loss: 0.2180798351764679\n",
      "iteration 30792: loss: 0.2180795967578888\n",
      "iteration 30793: loss: 0.21807941794395447\n",
      "iteration 30794: loss: 0.21807917952537537\n",
      "iteration 30795: loss: 0.21807900071144104\n",
      "iteration 30796: loss: 0.21807876229286194\n",
      "iteration 30797: loss: 0.21807856857776642\n",
      "iteration 30798: loss: 0.2180783748626709\n",
      "iteration 30799: loss: 0.2180781066417694\n",
      "iteration 30800: loss: 0.2180778682231903\n",
      "iteration 30801: loss: 0.2180776596069336\n",
      "iteration 30802: loss: 0.21807746589183807\n",
      "iteration 30803: loss: 0.21807722747325897\n",
      "iteration 30804: loss: 0.21807703375816345\n",
      "iteration 30805: loss: 0.21807679533958435\n",
      "iteration 30806: loss: 0.21807658672332764\n",
      "iteration 30807: loss: 0.21807637810707092\n",
      "iteration 30808: loss: 0.2180761843919754\n",
      "iteration 30809: loss: 0.2180759459733963\n",
      "iteration 30810: loss: 0.2180757224559784\n",
      "iteration 30811: loss: 0.21807554364204407\n",
      "iteration 30812: loss: 0.21807532012462616\n",
      "iteration 30813: loss: 0.21807508170604706\n",
      "iteration 30814: loss: 0.2180749475955963\n",
      "iteration 30815: loss: 0.21807464957237244\n",
      "iteration 30816: loss: 0.2180744856595993\n",
      "iteration 30817: loss: 0.21807429194450378\n",
      "iteration 30818: loss: 0.21807405352592468\n",
      "iteration 30819: loss: 0.2180737555027008\n",
      "iteration 30820: loss: 0.21807363629341125\n",
      "iteration 30821: loss: 0.21807339787483215\n",
      "iteration 30822: loss: 0.21807312965393066\n",
      "iteration 30823: loss: 0.21807289123535156\n",
      "iteration 30824: loss: 0.21807265281677246\n",
      "iteration 30825: loss: 0.2180725336074829\n",
      "iteration 30826: loss: 0.21807225048542023\n",
      "iteration 30827: loss: 0.2180721014738083\n",
      "iteration 30828: loss: 0.2180718630552292\n",
      "iteration 30829: loss: 0.21807165443897247\n",
      "iteration 30830: loss: 0.21807150542736053\n",
      "iteration 30831: loss: 0.21807117760181427\n",
      "iteration 30832: loss: 0.21807101368904114\n",
      "iteration 30833: loss: 0.21807077527046204\n",
      "iteration 30834: loss: 0.21807055175304413\n",
      "iteration 30835: loss: 0.21807034313678741\n",
      "iteration 30836: loss: 0.21807022392749786\n",
      "iteration 30837: loss: 0.2180698662996292\n",
      "iteration 30838: loss: 0.21806971728801727\n",
      "iteration 30839: loss: 0.21806950867176056\n",
      "iteration 30840: loss: 0.21806928515434265\n",
      "iteration 30841: loss: 0.21806903183460236\n",
      "iteration 30842: loss: 0.21806883811950684\n",
      "iteration 30843: loss: 0.21806871891021729\n",
      "iteration 30844: loss: 0.2180684506893158\n",
      "iteration 30845: loss: 0.21806824207305908\n",
      "iteration 30846: loss: 0.21806803345680237\n",
      "iteration 30847: loss: 0.2180677354335785\n",
      "iteration 30848: loss: 0.21806760132312775\n",
      "iteration 30849: loss: 0.21806733310222626\n",
      "iteration 30850: loss: 0.21806712448596954\n",
      "iteration 30851: loss: 0.2180669605731964\n",
      "iteration 30852: loss: 0.2180667221546173\n",
      "iteration 30853: loss: 0.2180664837360382\n",
      "iteration 30854: loss: 0.2180662453174591\n",
      "iteration 30855: loss: 0.21806609630584717\n",
      "iteration 30856: loss: 0.21806588768959045\n",
      "iteration 30857: loss: 0.21806561946868896\n",
      "iteration 30858: loss: 0.21806550025939941\n",
      "iteration 30859: loss: 0.21806523203849792\n",
      "iteration 30860: loss: 0.21806497871875763\n",
      "iteration 30861: loss: 0.2180648148059845\n",
      "iteration 30862: loss: 0.2180645763874054\n",
      "iteration 30863: loss: 0.21806438267230988\n",
      "iteration 30864: loss: 0.21806423366069794\n",
      "iteration 30865: loss: 0.21806392073631287\n",
      "iteration 30866: loss: 0.21806368231773376\n",
      "iteration 30867: loss: 0.21806350350379944\n",
      "iteration 30868: loss: 0.2180633246898651\n",
      "iteration 30869: loss: 0.2180631160736084\n",
      "iteration 30870: loss: 0.2180628478527069\n",
      "iteration 30871: loss: 0.2180626392364502\n",
      "iteration 30872: loss: 0.21806244552135468\n",
      "iteration 30873: loss: 0.21806220710277557\n",
      "iteration 30874: loss: 0.21806196868419647\n",
      "iteration 30875: loss: 0.21806177496910095\n",
      "iteration 30876: loss: 0.2180616557598114\n",
      "iteration 30877: loss: 0.21806129813194275\n",
      "iteration 30878: loss: 0.218061164021492\n",
      "iteration 30879: loss: 0.21806100010871887\n",
      "iteration 30880: loss: 0.21806073188781738\n",
      "iteration 30881: loss: 0.21806053817272186\n",
      "iteration 30882: loss: 0.21806025505065918\n",
      "iteration 30883: loss: 0.21806006133556366\n",
      "iteration 30884: loss: 0.21805985271930695\n",
      "iteration 30885: loss: 0.2180596888065338\n",
      "iteration 30886: loss: 0.2180594503879547\n",
      "iteration 30887: loss: 0.21805930137634277\n",
      "iteration 30888: loss: 0.21805894374847412\n",
      "iteration 30889: loss: 0.21805882453918457\n",
      "iteration 30890: loss: 0.21805855631828308\n",
      "iteration 30891: loss: 0.21805839240550995\n",
      "iteration 30892: loss: 0.21805819869041443\n",
      "iteration 30893: loss: 0.21805791556835175\n",
      "iteration 30894: loss: 0.21805772185325623\n",
      "iteration 30895: loss: 0.21805748343467712\n",
      "iteration 30896: loss: 0.2180573046207428\n",
      "iteration 30897: loss: 0.21805711090564728\n",
      "iteration 30898: loss: 0.21805687248706818\n",
      "iteration 30899: loss: 0.21805670857429504\n",
      "iteration 30900: loss: 0.21805641055107117\n",
      "iteration 30901: loss: 0.21805629134178162\n",
      "iteration 30902: loss: 0.21805603802204132\n",
      "iteration 30903: loss: 0.2180558145046234\n",
      "iteration 30904: loss: 0.2180556058883667\n",
      "iteration 30905: loss: 0.21805539727210999\n",
      "iteration 30906: loss: 0.21805517375469208\n",
      "iteration 30907: loss: 0.21805496513843536\n",
      "iteration 30908: loss: 0.21805481612682343\n",
      "iteration 30909: loss: 0.21805456280708313\n",
      "iteration 30910: loss: 0.2180543690919876\n",
      "iteration 30911: loss: 0.2180541753768921\n",
      "iteration 30912: loss: 0.2180538922548294\n",
      "iteration 30913: loss: 0.2180536985397339\n",
      "iteration 30914: loss: 0.21805350482463837\n",
      "iteration 30915: loss: 0.21805329620838165\n",
      "iteration 30916: loss: 0.21805302798748016\n",
      "iteration 30917: loss: 0.21805281937122345\n",
      "iteration 30918: loss: 0.2180526703596115\n",
      "iteration 30919: loss: 0.2180524319410324\n",
      "iteration 30920: loss: 0.21805217862129211\n",
      "iteration 30921: loss: 0.21805202960968018\n",
      "iteration 30922: loss: 0.21805182099342346\n",
      "iteration 30923: loss: 0.21805158257484436\n",
      "iteration 30924: loss: 0.21805138885974884\n",
      "iteration 30925: loss: 0.21805116534233093\n",
      "iteration 30926: loss: 0.21805092692375183\n",
      "iteration 30927: loss: 0.2180507630109787\n",
      "iteration 30928: loss: 0.21805055439472198\n",
      "iteration 30929: loss: 0.2180502861738205\n",
      "iteration 30930: loss: 0.21805007755756378\n",
      "iteration 30931: loss: 0.21804991364479065\n",
      "iteration 30932: loss: 0.21804969012737274\n",
      "iteration 30933: loss: 0.21804943680763245\n",
      "iteration 30934: loss: 0.21804921329021454\n",
      "iteration 30935: loss: 0.21804900467395782\n",
      "iteration 30936: loss: 0.2180488407611847\n",
      "iteration 30937: loss: 0.2180486023426056\n",
      "iteration 30938: loss: 0.21804837882518768\n",
      "iteration 30939: loss: 0.21804817020893097\n",
      "iteration 30940: loss: 0.21804794669151306\n",
      "iteration 30941: loss: 0.21804770827293396\n",
      "iteration 30942: loss: 0.2180475890636444\n",
      "iteration 30943: loss: 0.21804730594158173\n",
      "iteration 30944: loss: 0.2180471420288086\n",
      "iteration 30945: loss: 0.21804694831371307\n",
      "iteration 30946: loss: 0.2180466651916504\n",
      "iteration 30947: loss: 0.2180464267730713\n",
      "iteration 30948: loss: 0.21804633736610413\n",
      "iteration 30949: loss: 0.21804606914520264\n",
      "iteration 30950: loss: 0.21804586052894592\n",
      "iteration 30951: loss: 0.21804562211036682\n",
      "iteration 30952: loss: 0.2180453985929489\n",
      "iteration 30953: loss: 0.21804523468017578\n",
      "iteration 30954: loss: 0.21804499626159668\n",
      "iteration 30955: loss: 0.21804480254650116\n",
      "iteration 30956: loss: 0.21804460883140564\n",
      "iteration 30957: loss: 0.2180444449186325\n",
      "iteration 30958: loss: 0.21804413199424744\n",
      "iteration 30959: loss: 0.21804389357566833\n",
      "iteration 30960: loss: 0.2180437296628952\n",
      "iteration 30961: loss: 0.2180435210466385\n",
      "iteration 30962: loss: 0.21804329752922058\n",
      "iteration 30963: loss: 0.21804311871528625\n",
      "iteration 30964: loss: 0.21804289519786835\n",
      "iteration 30965: loss: 0.21804268658161163\n",
      "iteration 30966: loss: 0.21804246306419373\n",
      "iteration 30967: loss: 0.2180422842502594\n",
      "iteration 30968: loss: 0.2180420458316803\n",
      "iteration 30969: loss: 0.2180418074131012\n",
      "iteration 30970: loss: 0.21804161369800568\n",
      "iteration 30971: loss: 0.21804141998291016\n",
      "iteration 30972: loss: 0.21804113686084747\n",
      "iteration 30973: loss: 0.21804094314575195\n",
      "iteration 30974: loss: 0.21804074943065643\n",
      "iteration 30975: loss: 0.2180406153202057\n",
      "iteration 30976: loss: 0.21804039180278778\n",
      "iteration 30977: loss: 0.21804013848304749\n",
      "iteration 30978: loss: 0.21803994476795197\n",
      "iteration 30979: loss: 0.21803970634937286\n",
      "iteration 30980: loss: 0.21803955733776093\n",
      "iteration 30981: loss: 0.21803930401802063\n",
      "iteration 30982: loss: 0.2180391252040863\n",
      "iteration 30983: loss: 0.2180389165878296\n",
      "iteration 30984: loss: 0.21803872287273407\n",
      "iteration 30985: loss: 0.2180384397506714\n",
      "iteration 30986: loss: 0.21803823113441467\n",
      "iteration 30987: loss: 0.21803805232048035\n",
      "iteration 30988: loss: 0.21803784370422363\n",
      "iteration 30989: loss: 0.21803763508796692\n",
      "iteration 30990: loss: 0.21803739666938782\n",
      "iteration 30991: loss: 0.2180372178554535\n",
      "iteration 30992: loss: 0.21803703904151917\n",
      "iteration 30993: loss: 0.21803677082061768\n",
      "iteration 30994: loss: 0.21803653240203857\n",
      "iteration 30995: loss: 0.21803636848926544\n",
      "iteration 30996: loss: 0.21803617477416992\n",
      "iteration 30997: loss: 0.21803589165210724\n",
      "iteration 30998: loss: 0.2180357277393341\n",
      "iteration 30999: loss: 0.2180355042219162\n",
      "iteration 31000: loss: 0.21803537011146545\n",
      "iteration 31001: loss: 0.21803507208824158\n",
      "iteration 31002: loss: 0.21803493797779083\n",
      "iteration 31003: loss: 0.21803466975688934\n",
      "iteration 31004: loss: 0.21803446114063263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 31005: loss: 0.2180342972278595\n",
      "iteration 31006: loss: 0.218034029006958\n",
      "iteration 31007: loss: 0.2180338352918625\n",
      "iteration 31008: loss: 0.21803364157676697\n",
      "iteration 31009: loss: 0.21803340315818787\n",
      "iteration 31010: loss: 0.21803322434425354\n",
      "iteration 31011: loss: 0.21803298592567444\n",
      "iteration 31012: loss: 0.2180328071117401\n",
      "iteration 31013: loss: 0.21803250908851624\n",
      "iteration 31014: loss: 0.2180323600769043\n",
      "iteration 31015: loss: 0.21803219616413116\n",
      "iteration 31016: loss: 0.21803197264671326\n",
      "iteration 31017: loss: 0.21803179383277893\n",
      "iteration 31018: loss: 0.2180316001176834\n",
      "iteration 31019: loss: 0.21803131699562073\n",
      "iteration 31020: loss: 0.21803107857704163\n",
      "iteration 31021: loss: 0.2180309295654297\n",
      "iteration 31022: loss: 0.21803069114685059\n",
      "iteration 31023: loss: 0.21803048253059387\n",
      "iteration 31024: loss: 0.21803028881549835\n",
      "iteration 31025: loss: 0.21803005039691925\n",
      "iteration 31026: loss: 0.21802988648414612\n",
      "iteration 31027: loss: 0.21802964806556702\n",
      "iteration 31028: loss: 0.2180294543504715\n",
      "iteration 31029: loss: 0.21802926063537598\n",
      "iteration 31030: loss: 0.2180289924144745\n",
      "iteration 31031: loss: 0.21802881360054016\n",
      "iteration 31032: loss: 0.21802859008312225\n",
      "iteration 31033: loss: 0.21802833676338196\n",
      "iteration 31034: loss: 0.2180282175540924\n",
      "iteration 31035: loss: 0.2180279940366745\n",
      "iteration 31036: loss: 0.21802778542041779\n",
      "iteration 31037: loss: 0.2180275171995163\n",
      "iteration 31038: loss: 0.21802732348442078\n",
      "iteration 31039: loss: 0.21802718937397003\n",
      "iteration 31040: loss: 0.21802692115306854\n",
      "iteration 31041: loss: 0.21802671253681183\n",
      "iteration 31042: loss: 0.21802644431591034\n",
      "iteration 31043: loss: 0.21802625060081482\n",
      "iteration 31044: loss: 0.2180260866880417\n",
      "iteration 31045: loss: 0.21802587807178497\n",
      "iteration 31046: loss: 0.21802571415901184\n",
      "iteration 31047: loss: 0.21802547574043274\n",
      "iteration 31048: loss: 0.21802528202533722\n",
      "iteration 31049: loss: 0.2180251181125641\n",
      "iteration 31050: loss: 0.2180248498916626\n",
      "iteration 31051: loss: 0.21802464127540588\n",
      "iteration 31052: loss: 0.21802440285682678\n",
      "iteration 31053: loss: 0.21802422404289246\n",
      "iteration 31054: loss: 0.21802398562431335\n",
      "iteration 31055: loss: 0.21802382171154022\n",
      "iteration 31056: loss: 0.2180236577987671\n",
      "iteration 31057: loss: 0.21802334487438202\n",
      "iteration 31058: loss: 0.21802322566509247\n",
      "iteration 31059: loss: 0.21802294254302979\n",
      "iteration 31060: loss: 0.21802277863025665\n",
      "iteration 31061: loss: 0.21802255511283875\n",
      "iteration 31062: loss: 0.21802237629890442\n",
      "iteration 31063: loss: 0.21802210807800293\n",
      "iteration 31064: loss: 0.2180219441652298\n",
      "iteration 31065: loss: 0.21802166104316711\n",
      "iteration 31066: loss: 0.21802151203155518\n",
      "iteration 31067: loss: 0.21802130341529846\n",
      "iteration 31068: loss: 0.21802110970020294\n",
      "iteration 31069: loss: 0.21802084147930145\n",
      "iteration 31070: loss: 0.21802067756652832\n",
      "iteration 31071: loss: 0.2180204838514328\n",
      "iteration 31072: loss: 0.21802029013633728\n",
      "iteration 31073: loss: 0.21802003681659698\n",
      "iteration 31074: loss: 0.21801981329917908\n",
      "iteration 31075: loss: 0.21801963448524475\n",
      "iteration 31076: loss: 0.21801944077014923\n",
      "iteration 31077: loss: 0.21801920235157013\n",
      "iteration 31078: loss: 0.21801896393299103\n",
      "iteration 31079: loss: 0.2180188000202179\n",
      "iteration 31080: loss: 0.21801860630512238\n",
      "iteration 31081: loss: 0.21801845729351044\n",
      "iteration 31082: loss: 0.21801820397377014\n",
      "iteration 31083: loss: 0.21801798045635223\n",
      "iteration 31084: loss: 0.21801777184009552\n",
      "iteration 31085: loss: 0.21801762282848358\n",
      "iteration 31086: loss: 0.2180173397064209\n",
      "iteration 31087: loss: 0.21801714599132538\n",
      "iteration 31088: loss: 0.21801693737506866\n",
      "iteration 31089: loss: 0.21801677346229553\n",
      "iteration 31090: loss: 0.21801650524139404\n",
      "iteration 31091: loss: 0.21801631152629852\n",
      "iteration 31092: loss: 0.21801607310771942\n",
      "iteration 31093: loss: 0.2180159091949463\n",
      "iteration 31094: loss: 0.2180156707763672\n",
      "iteration 31095: loss: 0.21801547706127167\n",
      "iteration 31096: loss: 0.21801523864269257\n",
      "iteration 31097: loss: 0.21801503002643585\n",
      "iteration 31098: loss: 0.21801486611366272\n",
      "iteration 31099: loss: 0.21801462769508362\n",
      "iteration 31100: loss: 0.2180144488811493\n",
      "iteration 31101: loss: 0.21801427006721497\n",
      "iteration 31102: loss: 0.21801404654979706\n",
      "iteration 31103: loss: 0.21801385283470154\n",
      "iteration 31104: loss: 0.21801359951496124\n",
      "iteration 31105: loss: 0.21801337599754333\n",
      "iteration 31106: loss: 0.218013197183609\n",
      "iteration 31107: loss: 0.2180130034685135\n",
      "iteration 31108: loss: 0.2180127650499344\n",
      "iteration 31109: loss: 0.21801269054412842\n",
      "iteration 31110: loss: 0.21801237761974335\n",
      "iteration 31111: loss: 0.21801213920116425\n",
      "iteration 31112: loss: 0.21801193058490753\n",
      "iteration 31113: loss: 0.21801181137561798\n",
      "iteration 31114: loss: 0.2180115282535553\n",
      "iteration 31115: loss: 0.2180112898349762\n",
      "iteration 31116: loss: 0.21801109611988068\n",
      "iteration 31117: loss: 0.21801094710826874\n",
      "iteration 31118: loss: 0.21801075339317322\n",
      "iteration 31119: loss: 0.21801050007343292\n",
      "iteration 31120: loss: 0.2180103063583374\n",
      "iteration 31121: loss: 0.2180100679397583\n",
      "iteration 31122: loss: 0.21800991892814636\n",
      "iteration 31123: loss: 0.21800962090492249\n",
      "iteration 31124: loss: 0.21800950169563293\n",
      "iteration 31125: loss: 0.21800930798053741\n",
      "iteration 31126: loss: 0.2180090695619583\n",
      "iteration 31127: loss: 0.2180088460445404\n",
      "iteration 31128: loss: 0.21800866723060608\n",
      "iteration 31129: loss: 0.21800847351551056\n",
      "iteration 31130: loss: 0.21800823509693146\n",
      "iteration 31131: loss: 0.21800801157951355\n",
      "iteration 31132: loss: 0.21800780296325684\n",
      "iteration 31133: loss: 0.21800760924816132\n",
      "iteration 31134: loss: 0.2180074155330658\n",
      "iteration 31135: loss: 0.2180071771144867\n",
      "iteration 31136: loss: 0.21800701320171356\n",
      "iteration 31137: loss: 0.21800677478313446\n",
      "iteration 31138: loss: 0.21800653636455536\n",
      "iteration 31139: loss: 0.21800637245178223\n",
      "iteration 31140: loss: 0.21800613403320312\n",
      "iteration 31141: loss: 0.2180059850215912\n",
      "iteration 31142: loss: 0.2180057317018509\n",
      "iteration 31143: loss: 0.21800556778907776\n",
      "iteration 31144: loss: 0.21800538897514343\n",
      "iteration 31145: loss: 0.21800513565540314\n",
      "iteration 31146: loss: 0.21800494194030762\n",
      "iteration 31147: loss: 0.2180047482252121\n",
      "iteration 31148: loss: 0.21800455451011658\n",
      "iteration 31149: loss: 0.21800437569618225\n",
      "iteration 31150: loss: 0.21800410747528076\n",
      "iteration 31151: loss: 0.21800386905670166\n",
      "iteration 31152: loss: 0.2180037498474121\n",
      "iteration 31153: loss: 0.218003511428833\n",
      "iteration 31154: loss: 0.2180032730102539\n",
      "iteration 31155: loss: 0.2180030643939972\n",
      "iteration 31156: loss: 0.21800284087657928\n",
      "iteration 31157: loss: 0.21800264716148376\n",
      "iteration 31158: loss: 0.21800246834754944\n",
      "iteration 31159: loss: 0.21800227463245392\n",
      "iteration 31160: loss: 0.21800203621387482\n",
      "iteration 31161: loss: 0.2180018424987793\n",
      "iteration 31162: loss: 0.21800164878368378\n",
      "iteration 31163: loss: 0.21800144016742706\n",
      "iteration 31164: loss: 0.21800121665000916\n",
      "iteration 31165: loss: 0.21800103783607483\n",
      "iteration 31166: loss: 0.21800081431865692\n",
      "iteration 31167: loss: 0.2180006504058838\n",
      "iteration 31168: loss: 0.2180003821849823\n",
      "iteration 31169: loss: 0.21800021827220917\n",
      "iteration 31170: loss: 0.21799997985363007\n",
      "iteration 31171: loss: 0.21799978613853455\n",
      "iteration 31172: loss: 0.2179996222257614\n",
      "iteration 31173: loss: 0.2179993838071823\n",
      "iteration 31174: loss: 0.21799921989440918\n",
      "iteration 31175: loss: 0.2179989367723465\n",
      "iteration 31176: loss: 0.21799877285957336\n",
      "iteration 31177: loss: 0.21799854934215546\n",
      "iteration 31178: loss: 0.21799838542938232\n",
      "iteration 31179: loss: 0.2179981768131256\n",
      "iteration 31180: loss: 0.21799790859222412\n",
      "iteration 31181: loss: 0.21799778938293457\n",
      "iteration 31182: loss: 0.21799755096435547\n",
      "iteration 31183: loss: 0.21799735724925995\n",
      "iteration 31184: loss: 0.21799711883068085\n",
      "iteration 31185: loss: 0.21799692511558533\n",
      "iteration 31186: loss: 0.217996746301651\n",
      "iteration 31187: loss: 0.21799655258655548\n",
      "iteration 31188: loss: 0.21799631416797638\n",
      "iteration 31189: loss: 0.21799607574939728\n",
      "iteration 31190: loss: 0.21799591183662415\n",
      "iteration 31191: loss: 0.21799568831920624\n",
      "iteration 31192: loss: 0.21799549460411072\n",
      "iteration 31193: loss: 0.21799533069133759\n",
      "iteration 31194: loss: 0.2179950773715973\n",
      "iteration 31195: loss: 0.21799488365650177\n",
      "iteration 31196: loss: 0.21799468994140625\n",
      "iteration 31197: loss: 0.21799449622631073\n",
      "iteration 31198: loss: 0.21799428761005402\n",
      "iteration 31199: loss: 0.2179940938949585\n",
      "iteration 31200: loss: 0.21799388527870178\n",
      "iteration 31201: loss: 0.2179935872554779\n",
      "iteration 31202: loss: 0.21799345314502716\n",
      "iteration 31203: loss: 0.21799325942993164\n",
      "iteration 31204: loss: 0.2179931104183197\n",
      "iteration 31205: loss: 0.2179928719997406\n",
      "iteration 31206: loss: 0.2179926335811615\n",
      "iteration 31207: loss: 0.21799242496490479\n",
      "iteration 31208: loss: 0.21799218654632568\n",
      "iteration 31209: loss: 0.21799197793006897\n",
      "iteration 31210: loss: 0.21799179911613464\n",
      "iteration 31211: loss: 0.2179916352033615\n",
      "iteration 31212: loss: 0.217991441488266\n",
      "iteration 31213: loss: 0.21799126267433167\n",
      "iteration 31214: loss: 0.21799102425575256\n",
      "iteration 31215: loss: 0.21799078583717346\n",
      "iteration 31216: loss: 0.21799054741859436\n",
      "iteration 31217: loss: 0.21799035370349884\n",
      "iteration 31218: loss: 0.2179901897907257\n",
      "iteration 31219: loss: 0.2179899513721466\n",
      "iteration 31220: loss: 0.2179897278547287\n",
      "iteration 31221: loss: 0.21798953413963318\n",
      "iteration 31222: loss: 0.21798929572105408\n",
      "iteration 31223: loss: 0.21798913180828094\n",
      "iteration 31224: loss: 0.21798893809318542\n",
      "iteration 31225: loss: 0.2179887592792511\n",
      "iteration 31226: loss: 0.2179885357618332\n",
      "iteration 31227: loss: 0.21798840165138245\n",
      "iteration 31228: loss: 0.21798816323280334\n",
      "iteration 31229: loss: 0.21798793971538544\n",
      "iteration 31230: loss: 0.21798773109912872\n",
      "iteration 31231: loss: 0.2179875373840332\n",
      "iteration 31232: loss: 0.2179872691631317\n",
      "iteration 31233: loss: 0.21798710525035858\n",
      "iteration 31234: loss: 0.21798686683177948\n",
      "iteration 31235: loss: 0.21798677742481232\n",
      "iteration 31236: loss: 0.21798649430274963\n",
      "iteration 31237: loss: 0.2179863154888153\n",
      "iteration 31238: loss: 0.217986062169075\n",
      "iteration 31239: loss: 0.21798589825630188\n",
      "iteration 31240: loss: 0.21798567473888397\n",
      "iteration 31241: loss: 0.21798551082611084\n",
      "iteration 31242: loss: 0.21798527240753174\n",
      "iteration 31243: loss: 0.2179851233959198\n",
      "iteration 31244: loss: 0.2179848700761795\n",
      "iteration 31245: loss: 0.21798467636108398\n",
      "iteration 31246: loss: 0.21798451244831085\n",
      "iteration 31247: loss: 0.21798424422740936\n",
      "iteration 31248: loss: 0.2179841250181198\n",
      "iteration 31249: loss: 0.21798387169837952\n",
      "iteration 31250: loss: 0.2179836481809616\n",
      "iteration 31251: loss: 0.21798351407051086\n",
      "iteration 31252: loss: 0.217983216047287\n",
      "iteration 31253: loss: 0.21798300743103027\n",
      "iteration 31254: loss: 0.21798288822174072\n",
      "iteration 31255: loss: 0.21798262000083923\n",
      "iteration 31256: loss: 0.21798241138458252\n",
      "iteration 31257: loss: 0.2179822027683258\n",
      "iteration 31258: loss: 0.21798208355903625\n",
      "iteration 31259: loss: 0.21798186004161835\n",
      "iteration 31260: loss: 0.21798160672187805\n",
      "iteration 31261: loss: 0.21798142790794373\n",
      "iteration 31262: loss: 0.21798117458820343\n",
      "iteration 31263: loss: 0.21798105537891388\n",
      "iteration 31264: loss: 0.2179807871580124\n",
      "iteration 31265: loss: 0.21798059344291687\n",
      "iteration 31266: loss: 0.21798035502433777\n",
      "iteration 31267: loss: 0.21798022091388702\n",
      "iteration 31268: loss: 0.2179800570011139\n",
      "iteration 31269: loss: 0.21797986328601837\n",
      "iteration 31270: loss: 0.2179795801639557\n",
      "iteration 31271: loss: 0.21797943115234375\n",
      "iteration 31272: loss: 0.21797922253608704\n",
      "iteration 31273: loss: 0.21797899901866913\n",
      "iteration 31274: loss: 0.21797879040241241\n",
      "iteration 31275: loss: 0.21797862648963928\n",
      "iteration 31276: loss: 0.21797840297222137\n",
      "iteration 31277: loss: 0.21797823905944824\n",
      "iteration 31278: loss: 0.21797797083854675\n",
      "iteration 31279: loss: 0.21797779202461243\n",
      "iteration 31280: loss: 0.2179776430130005\n",
      "iteration 31281: loss: 0.21797731518745422\n",
      "iteration 31282: loss: 0.21797719597816467\n",
      "iteration 31283: loss: 0.21797700226306915\n",
      "iteration 31284: loss: 0.21797680854797363\n",
      "iteration 31285: loss: 0.21797654032707214\n",
      "iteration 31286: loss: 0.217976376414299\n",
      "iteration 31287: loss: 0.21797621250152588\n",
      "iteration 31288: loss: 0.21797597408294678\n",
      "iteration 31289: loss: 0.21797576546669006\n",
      "iteration 31290: loss: 0.21797552704811096\n",
      "iteration 31291: loss: 0.2179754078388214\n",
      "iteration 31292: loss: 0.2179752141237259\n",
      "iteration 31293: loss: 0.2179749459028244\n",
      "iteration 31294: loss: 0.21797478199005127\n",
      "iteration 31295: loss: 0.21797454357147217\n",
      "iteration 31296: loss: 0.21797433495521545\n",
      "iteration 31297: loss: 0.21797411143779755\n",
      "iteration 31298: loss: 0.21797394752502441\n",
      "iteration 31299: loss: 0.2179737091064453\n",
      "iteration 31300: loss: 0.21797354519367218\n",
      "iteration 31301: loss: 0.21797335147857666\n",
      "iteration 31302: loss: 0.21797314286231995\n",
      "iteration 31303: loss: 0.21797296404838562\n",
      "iteration 31304: loss: 0.21797268092632294\n",
      "iteration 31305: loss: 0.2179725617170334\n",
      "iteration 31306: loss: 0.2179723083972931\n",
      "iteration 31307: loss: 0.21797215938568115\n",
      "iteration 31308: loss: 0.21797189116477966\n",
      "iteration 31309: loss: 0.2179717719554901\n",
      "iteration 31310: loss: 0.21797148883342743\n",
      "iteration 31311: loss: 0.2179713249206543\n",
      "iteration 31312: loss: 0.2179710865020752\n",
      "iteration 31313: loss: 0.21797089278697968\n",
      "iteration 31314: loss: 0.21797075867652893\n",
      "iteration 31315: loss: 0.21797053515911102\n",
      "iteration 31316: loss: 0.21797028183937073\n",
      "iteration 31317: loss: 0.2179700881242752\n",
      "iteration 31318: loss: 0.21796993911266327\n",
      "iteration 31319: loss: 0.21796968579292297\n",
      "iteration 31320: loss: 0.21796946227550507\n",
      "iteration 31321: loss: 0.21796929836273193\n",
      "iteration 31322: loss: 0.21796908974647522\n",
      "iteration 31323: loss: 0.21796885132789612\n",
      "iteration 31324: loss: 0.2179686725139618\n",
      "iteration 31325: loss: 0.21796846389770508\n",
      "iteration 31326: loss: 0.21796829998493195\n",
      "iteration 31327: loss: 0.21796810626983643\n",
      "iteration 31328: loss: 0.2179679125547409\n",
      "iteration 31329: loss: 0.2179677039384842\n",
      "iteration 31330: loss: 0.21796751022338867\n",
      "iteration 31331: loss: 0.21796730160713196\n",
      "iteration 31332: loss: 0.21796707808971405\n",
      "iteration 31333: loss: 0.21796691417694092\n",
      "iteration 31334: loss: 0.2179667055606842\n",
      "iteration 31335: loss: 0.21796651184558868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 31336: loss: 0.2179662436246872\n",
      "iteration 31337: loss: 0.21796610951423645\n",
      "iteration 31338: loss: 0.21796591579914093\n",
      "iteration 31339: loss: 0.21796569228172302\n",
      "iteration 31340: loss: 0.2179655134677887\n",
      "iteration 31341: loss: 0.2179652750492096\n",
      "iteration 31342: loss: 0.21796508133411407\n",
      "iteration 31343: loss: 0.21796485781669617\n",
      "iteration 31344: loss: 0.21796469390392303\n",
      "iteration 31345: loss: 0.21796450018882751\n",
      "iteration 31346: loss: 0.2179643213748932\n",
      "iteration 31347: loss: 0.21796409785747528\n",
      "iteration 31348: loss: 0.21796388924121857\n",
      "iteration 31349: loss: 0.21796372532844543\n",
      "iteration 31350: loss: 0.21796348690986633\n",
      "iteration 31351: loss: 0.217963308095932\n",
      "iteration 31352: loss: 0.2179630696773529\n",
      "iteration 31353: loss: 0.21796289086341858\n",
      "iteration 31354: loss: 0.21796274185180664\n",
      "iteration 31355: loss: 0.21796247363090515\n",
      "iteration 31356: loss: 0.21796226501464844\n",
      "iteration 31357: loss: 0.2179621011018753\n",
      "iteration 31358: loss: 0.21796190738677979\n",
      "iteration 31359: loss: 0.21796169877052307\n",
      "iteration 31360: loss: 0.21796146035194397\n",
      "iteration 31361: loss: 0.21796128153800964\n",
      "iteration 31362: loss: 0.21796110272407532\n",
      "iteration 31363: loss: 0.21796086430549622\n",
      "iteration 31364: loss: 0.21796074509620667\n",
      "iteration 31365: loss: 0.2179604470729828\n",
      "iteration 31366: loss: 0.21796028316020966\n",
      "iteration 31367: loss: 0.21796008944511414\n",
      "iteration 31368: loss: 0.21795985102653503\n",
      "iteration 31369: loss: 0.21795973181724548\n",
      "iteration 31370: loss: 0.2179594337940216\n",
      "iteration 31371: loss: 0.21795932948589325\n",
      "iteration 31372: loss: 0.21795904636383057\n",
      "iteration 31373: loss: 0.21795888245105743\n",
      "iteration 31374: loss: 0.21795865893363953\n",
      "iteration 31375: loss: 0.21795852482318878\n",
      "iteration 31376: loss: 0.21795828640460968\n",
      "iteration 31377: loss: 0.21795813739299774\n",
      "iteration 31378: loss: 0.21795789897441864\n",
      "iteration 31379: loss: 0.21795770525932312\n",
      "iteration 31380: loss: 0.21795745193958282\n",
      "iteration 31381: loss: 0.2179572880268097\n",
      "iteration 31382: loss: 0.2179570496082306\n",
      "iteration 31383: loss: 0.21795690059661865\n",
      "iteration 31384: loss: 0.21795673668384552\n",
      "iteration 31385: loss: 0.21795649826526642\n",
      "iteration 31386: loss: 0.21795625984668732\n",
      "iteration 31387: loss: 0.2179560661315918\n",
      "iteration 31388: loss: 0.21795591711997986\n",
      "iteration 31389: loss: 0.21795567870140076\n",
      "iteration 31390: loss: 0.21795549988746643\n",
      "iteration 31391: loss: 0.2179553061723709\n",
      "iteration 31392: loss: 0.2179550677537918\n",
      "iteration 31393: loss: 0.2179548740386963\n",
      "iteration 31394: loss: 0.21795468032360077\n",
      "iteration 31395: loss: 0.21795451641082764\n",
      "iteration 31396: loss: 0.21795424818992615\n",
      "iteration 31397: loss: 0.21795408427715302\n",
      "iteration 31398: loss: 0.2179538756608963\n",
      "iteration 31399: loss: 0.21795368194580078\n",
      "iteration 31400: loss: 0.21795347332954407\n",
      "iteration 31401: loss: 0.21795323491096497\n",
      "iteration 31402: loss: 0.2179531306028366\n",
      "iteration 31403: loss: 0.2179528921842575\n",
      "iteration 31404: loss: 0.217952698469162\n",
      "iteration 31405: loss: 0.21795248985290527\n",
      "iteration 31406: loss: 0.21795229613780975\n",
      "iteration 31407: loss: 0.21795210242271423\n",
      "iteration 31408: loss: 0.2179519236087799\n",
      "iteration 31409: loss: 0.2179516851902008\n",
      "iteration 31410: loss: 0.21795156598091125\n",
      "iteration 31411: loss: 0.21795137226581573\n",
      "iteration 31412: loss: 0.21795117855072021\n",
      "iteration 31413: loss: 0.21795089542865753\n",
      "iteration 31414: loss: 0.217950701713562\n",
      "iteration 31415: loss: 0.2179504930973053\n",
      "iteration 31416: loss: 0.21795034408569336\n",
      "iteration 31417: loss: 0.21795013546943665\n",
      "iteration 31418: loss: 0.21794991195201874\n",
      "iteration 31419: loss: 0.21794967353343964\n",
      "iteration 31420: loss: 0.21794958412647247\n",
      "iteration 31421: loss: 0.21794931590557098\n",
      "iteration 31422: loss: 0.21794907748699188\n",
      "iteration 31423: loss: 0.21794891357421875\n",
      "iteration 31424: loss: 0.21794871985912323\n",
      "iteration 31425: loss: 0.21794851124286652\n",
      "iteration 31426: loss: 0.2179482877254486\n",
      "iteration 31427: loss: 0.2179480791091919\n",
      "iteration 31428: loss: 0.21794793009757996\n",
      "iteration 31429: loss: 0.21794767677783966\n",
      "iteration 31430: loss: 0.21794751286506653\n",
      "iteration 31431: loss: 0.217947319149971\n",
      "iteration 31432: loss: 0.21794715523719788\n",
      "iteration 31433: loss: 0.21794693171977997\n",
      "iteration 31434: loss: 0.21794672310352325\n",
      "iteration 31435: loss: 0.21794655919075012\n",
      "iteration 31436: loss: 0.21794633567333221\n",
      "iteration 31437: loss: 0.21794617176055908\n",
      "iteration 31438: loss: 0.2179459035396576\n",
      "iteration 31439: loss: 0.2179456651210785\n",
      "iteration 31440: loss: 0.21794560551643372\n",
      "iteration 31441: loss: 0.21794533729553223\n",
      "iteration 31442: loss: 0.2179451286792755\n",
      "iteration 31443: loss: 0.21794493496418\n",
      "iteration 31444: loss: 0.21794477105140686\n",
      "iteration 31445: loss: 0.21794454753398895\n",
      "iteration 31446: loss: 0.21794433891773224\n",
      "iteration 31447: loss: 0.21794411540031433\n",
      "iteration 31448: loss: 0.2179439812898636\n",
      "iteration 31449: loss: 0.21794378757476807\n",
      "iteration 31450: loss: 0.21794357895851135\n",
      "iteration 31451: loss: 0.21794335544109344\n",
      "iteration 31452: loss: 0.2179431915283203\n",
      "iteration 31453: loss: 0.21794292330741882\n",
      "iteration 31454: loss: 0.21794280409812927\n",
      "iteration 31455: loss: 0.21794256567955017\n",
      "iteration 31456: loss: 0.21794238686561584\n",
      "iteration 31457: loss: 0.21794220805168152\n",
      "iteration 31458: loss: 0.2179419994354248\n",
      "iteration 31459: loss: 0.2179417610168457\n",
      "iteration 31460: loss: 0.21794156730175018\n",
      "iteration 31461: loss: 0.21794140338897705\n",
      "iteration 31462: loss: 0.21794119477272034\n",
      "iteration 31463: loss: 0.21794095635414124\n",
      "iteration 31464: loss: 0.21794073283672333\n",
      "iteration 31465: loss: 0.2179405689239502\n",
      "iteration 31466: loss: 0.21794040501117706\n",
      "iteration 31467: loss: 0.21794016659259796\n",
      "iteration 31468: loss: 0.21794001758098602\n",
      "iteration 31469: loss: 0.2179398238658905\n",
      "iteration 31470: loss: 0.2179395854473114\n",
      "iteration 31471: loss: 0.21793940663337708\n",
      "iteration 31472: loss: 0.21793918311595917\n",
      "iteration 31473: loss: 0.21793906390666962\n",
      "iteration 31474: loss: 0.21793881058692932\n",
      "iteration 31475: loss: 0.2179385870695114\n",
      "iteration 31476: loss: 0.21793845295906067\n",
      "iteration 31477: loss: 0.2179381549358368\n",
      "iteration 31478: loss: 0.21793802082538605\n",
      "iteration 31479: loss: 0.21793785691261292\n",
      "iteration 31480: loss: 0.2179376184940338\n",
      "iteration 31481: loss: 0.2179374396800995\n",
      "iteration 31482: loss: 0.21793727576732635\n",
      "iteration 31483: loss: 0.21793702244758606\n",
      "iteration 31484: loss: 0.21793684363365173\n",
      "iteration 31485: loss: 0.21793660521507263\n",
      "iteration 31486: loss: 0.2179364711046219\n",
      "iteration 31487: loss: 0.21793630719184875\n",
      "iteration 31488: loss: 0.21793611347675323\n",
      "iteration 31489: loss: 0.21793588995933533\n",
      "iteration 31490: loss: 0.2179356813430786\n",
      "iteration 31491: loss: 0.2179354727268219\n",
      "iteration 31492: loss: 0.2179352343082428\n",
      "iteration 31493: loss: 0.21793505549430847\n",
      "iteration 31494: loss: 0.21793492138385773\n",
      "iteration 31495: loss: 0.21793465316295624\n",
      "iteration 31496: loss: 0.21793445944786072\n",
      "iteration 31497: loss: 0.21793429553508759\n",
      "iteration 31498: loss: 0.21793413162231445\n",
      "iteration 31499: loss: 0.21793389320373535\n",
      "iteration 31500: loss: 0.21793372929096222\n",
      "iteration 31501: loss: 0.2179335057735443\n",
      "iteration 31502: loss: 0.2179332673549652\n",
      "iteration 31503: loss: 0.21793313324451447\n",
      "iteration 31504: loss: 0.21793290972709656\n",
      "iteration 31505: loss: 0.21793274581432343\n",
      "iteration 31506: loss: 0.2179325520992279\n",
      "iteration 31507: loss: 0.2179323136806488\n",
      "iteration 31508: loss: 0.21793217957019806\n",
      "iteration 31509: loss: 0.21793194115161896\n",
      "iteration 31510: loss: 0.21793171763420105\n",
      "iteration 31511: loss: 0.21793153882026672\n",
      "iteration 31512: loss: 0.21793131530284882\n",
      "iteration 31513: loss: 0.21793119609355927\n",
      "iteration 31514: loss: 0.21793094277381897\n",
      "iteration 31515: loss: 0.21793076395988464\n",
      "iteration 31516: loss: 0.21793052554130554\n",
      "iteration 31517: loss: 0.2179303616285324\n",
      "iteration 31518: loss: 0.2179301679134369\n",
      "iteration 31519: loss: 0.21793000400066376\n",
      "iteration 31520: loss: 0.21792975068092346\n",
      "iteration 31521: loss: 0.21792957186698914\n",
      "iteration 31522: loss: 0.217929407954216\n",
      "iteration 31523: loss: 0.2179291695356369\n",
      "iteration 31524: loss: 0.21792903542518616\n",
      "iteration 31525: loss: 0.21792881190776825\n",
      "iteration 31526: loss: 0.21792852878570557\n",
      "iteration 31527: loss: 0.2179284393787384\n",
      "iteration 31528: loss: 0.2179282009601593\n",
      "iteration 31529: loss: 0.21792802214622498\n",
      "iteration 31530: loss: 0.21792778372764587\n",
      "iteration 31531: loss: 0.21792764961719513\n",
      "iteration 31532: loss: 0.2179274559020996\n",
      "iteration 31533: loss: 0.2179272174835205\n",
      "iteration 31534: loss: 0.2179269790649414\n",
      "iteration 31535: loss: 0.21792683005332947\n",
      "iteration 31536: loss: 0.21792669594287872\n",
      "iteration 31537: loss: 0.21792645752429962\n",
      "iteration 31538: loss: 0.2179262936115265\n",
      "iteration 31539: loss: 0.217926025390625\n",
      "iteration 31540: loss: 0.21792583167552948\n",
      "iteration 31541: loss: 0.21792566776275635\n",
      "iteration 31542: loss: 0.21792550384998322\n",
      "iteration 31543: loss: 0.2179253101348877\n",
      "iteration 31544: loss: 0.217925027012825\n",
      "iteration 31545: loss: 0.21792492270469666\n",
      "iteration 31546: loss: 0.21792468428611755\n",
      "iteration 31547: loss: 0.21792447566986084\n",
      "iteration 31548: loss: 0.2179243564605713\n",
      "iteration 31549: loss: 0.2179240733385086\n",
      "iteration 31550: loss: 0.21792390942573547\n",
      "iteration 31551: loss: 0.21792368590831757\n",
      "iteration 31552: loss: 0.21792349219322205\n",
      "iteration 31553: loss: 0.21792331337928772\n",
      "iteration 31554: loss: 0.2179231196641922\n",
      "iteration 31555: loss: 0.21792295575141907\n",
      "iteration 31556: loss: 0.21792273223400116\n",
      "iteration 31557: loss: 0.21792249381542206\n",
      "iteration 31558: loss: 0.2179224044084549\n",
      "iteration 31559: loss: 0.2179221361875534\n",
      "iteration 31560: loss: 0.2179219275712967\n",
      "iteration 31561: loss: 0.21792177855968475\n",
      "iteration 31562: loss: 0.21792149543762207\n",
      "iteration 31563: loss: 0.2179214060306549\n",
      "iteration 31564: loss: 0.21792113780975342\n",
      "iteration 31565: loss: 0.21792101860046387\n",
      "iteration 31566: loss: 0.21792078018188477\n",
      "iteration 31567: loss: 0.21792058646678925\n",
      "iteration 31568: loss: 0.21792039275169373\n",
      "iteration 31569: loss: 0.2179202139377594\n",
      "iteration 31570: loss: 0.2179199904203415\n",
      "iteration 31571: loss: 0.21791979670524597\n",
      "iteration 31572: loss: 0.21791963279247284\n",
      "iteration 31573: loss: 0.21791942417621613\n",
      "iteration 31574: loss: 0.2179192304611206\n",
      "iteration 31575: loss: 0.21791906654834747\n",
      "iteration 31576: loss: 0.21791882812976837\n",
      "iteration 31577: loss: 0.21791863441467285\n",
      "iteration 31578: loss: 0.21791847050189972\n",
      "iteration 31579: loss: 0.2179182767868042\n",
      "iteration 31580: loss: 0.21791808307170868\n",
      "iteration 31581: loss: 0.217917799949646\n",
      "iteration 31582: loss: 0.21791765093803406\n",
      "iteration 31583: loss: 0.2179175317287445\n",
      "iteration 31584: loss: 0.2179173231124878\n",
      "iteration 31585: loss: 0.2179170846939087\n",
      "iteration 31586: loss: 0.21791689097881317\n",
      "iteration 31587: loss: 0.21791675686836243\n",
      "iteration 31588: loss: 0.21791651844978333\n",
      "iteration 31589: loss: 0.21791629493236542\n",
      "iteration 31590: loss: 0.21791617572307587\n",
      "iteration 31591: loss: 0.21791592240333557\n",
      "iteration 31592: loss: 0.21791572868824005\n",
      "iteration 31593: loss: 0.21791550517082214\n",
      "iteration 31594: loss: 0.21791532635688782\n",
      "iteration 31595: loss: 0.2179151326417923\n",
      "iteration 31596: loss: 0.2179149091243744\n",
      "iteration 31597: loss: 0.21791477501392365\n",
      "iteration 31598: loss: 0.21791458129882812\n",
      "iteration 31599: loss: 0.217914417386055\n",
      "iteration 31600: loss: 0.2179141491651535\n",
      "iteration 31601: loss: 0.21791401505470276\n",
      "iteration 31602: loss: 0.21791379153728485\n",
      "iteration 31603: loss: 0.21791358292102814\n",
      "iteration 31604: loss: 0.21791338920593262\n",
      "iteration 31605: loss: 0.21791322529315948\n",
      "iteration 31606: loss: 0.21791298687458038\n",
      "iteration 31607: loss: 0.21791282296180725\n",
      "iteration 31608: loss: 0.21791262924671173\n",
      "iteration 31609: loss: 0.21791240572929382\n",
      "iteration 31610: loss: 0.2179121971130371\n",
      "iteration 31611: loss: 0.21791204810142517\n",
      "iteration 31612: loss: 0.21791179478168488\n",
      "iteration 31613: loss: 0.21791164577007294\n",
      "iteration 31614: loss: 0.21791143715381622\n",
      "iteration 31615: loss: 0.21791128814220428\n",
      "iteration 31616: loss: 0.21791109442710876\n",
      "iteration 31617: loss: 0.21791085600852966\n",
      "iteration 31618: loss: 0.21791067719459534\n",
      "iteration 31619: loss: 0.21791048347949982\n",
      "iteration 31620: loss: 0.21791033446788788\n",
      "iteration 31621: loss: 0.2179100513458252\n",
      "iteration 31622: loss: 0.21790990233421326\n",
      "iteration 31623: loss: 0.21790969371795654\n",
      "iteration 31624: loss: 0.2179095447063446\n",
      "iteration 31625: loss: 0.21790926158428192\n",
      "iteration 31626: loss: 0.21790912747383118\n",
      "iteration 31627: loss: 0.21790894865989685\n",
      "iteration 31628: loss: 0.21790874004364014\n",
      "iteration 31629: loss: 0.21790862083435059\n",
      "iteration 31630: loss: 0.21790838241577148\n",
      "iteration 31631: loss: 0.21790814399719238\n",
      "iteration 31632: loss: 0.21790802478790283\n",
      "iteration 31633: loss: 0.21790778636932373\n",
      "iteration 31634: loss: 0.21790757775306702\n",
      "iteration 31635: loss: 0.21790742874145508\n",
      "iteration 31636: loss: 0.21790719032287598\n",
      "iteration 31637: loss: 0.21790702641010284\n",
      "iteration 31638: loss: 0.2179068624973297\n",
      "iteration 31639: loss: 0.2179066687822342\n",
      "iteration 31640: loss: 0.21790647506713867\n",
      "iteration 31641: loss: 0.21790623664855957\n",
      "iteration 31642: loss: 0.21790604293346405\n",
      "iteration 31643: loss: 0.21790587902069092\n",
      "iteration 31644: loss: 0.21790571510791779\n",
      "iteration 31645: loss: 0.21790547668933868\n",
      "iteration 31646: loss: 0.21790525317192078\n",
      "iteration 31647: loss: 0.21790511906147003\n",
      "iteration 31648: loss: 0.21790489554405212\n",
      "iteration 31649: loss: 0.2179047167301178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 31650: loss: 0.21790452301502228\n",
      "iteration 31651: loss: 0.21790432929992676\n",
      "iteration 31652: loss: 0.21790413558483124\n",
      "iteration 31653: loss: 0.2179039716720581\n",
      "iteration 31654: loss: 0.217903733253479\n",
      "iteration 31655: loss: 0.21790353953838348\n",
      "iteration 31656: loss: 0.21790342032909393\n",
      "iteration 31657: loss: 0.21790310740470886\n",
      "iteration 31658: loss: 0.21790297329425812\n",
      "iteration 31659: loss: 0.21790280938148499\n",
      "iteration 31660: loss: 0.21790261566638947\n",
      "iteration 31661: loss: 0.21790234744548798\n",
      "iteration 31662: loss: 0.21790218353271484\n",
      "iteration 31663: loss: 0.2179020196199417\n",
      "iteration 31664: loss: 0.2179018259048462\n",
      "iteration 31665: loss: 0.21790166199207306\n",
      "iteration 31666: loss: 0.21790142357349396\n",
      "iteration 31667: loss: 0.21790125966072083\n",
      "iteration 31668: loss: 0.2179010808467865\n",
      "iteration 31669: loss: 0.21790090203285217\n",
      "iteration 31670: loss: 0.21790067851543427\n",
      "iteration 31671: loss: 0.21790048480033875\n",
      "iteration 31672: loss: 0.21790027618408203\n",
      "iteration 31673: loss: 0.2179000824689865\n",
      "iteration 31674: loss: 0.2178998738527298\n",
      "iteration 31675: loss: 0.21789970993995667\n",
      "iteration 31676: loss: 0.21789948642253876\n",
      "iteration 31677: loss: 0.21789929270744324\n",
      "iteration 31678: loss: 0.21789908409118652\n",
      "iteration 31679: loss: 0.2178989201784134\n",
      "iteration 31680: loss: 0.21789877116680145\n",
      "iteration 31681: loss: 0.21789851784706116\n",
      "iteration 31682: loss: 0.21789832413196564\n",
      "iteration 31683: loss: 0.2178982049226761\n",
      "iteration 31684: loss: 0.21789789199829102\n",
      "iteration 31685: loss: 0.21789774298667908\n",
      "iteration 31686: loss: 0.21789756417274475\n",
      "iteration 31687: loss: 0.2178974449634552\n",
      "iteration 31688: loss: 0.21789714694023132\n",
      "iteration 31689: loss: 0.2178969830274582\n",
      "iteration 31690: loss: 0.21789677441120148\n",
      "iteration 31691: loss: 0.21789665520191193\n",
      "iteration 31692: loss: 0.21789641678333282\n",
      "iteration 31693: loss: 0.21789619326591492\n",
      "iteration 31694: loss: 0.21789602935314178\n",
      "iteration 31695: loss: 0.21789586544036865\n",
      "iteration 31696: loss: 0.21789565682411194\n",
      "iteration 31697: loss: 0.21789546310901642\n",
      "iteration 31698: loss: 0.21789522469043732\n",
      "iteration 31699: loss: 0.21789507567882538\n",
      "iteration 31700: loss: 0.21789488196372986\n",
      "iteration 31701: loss: 0.2178947478532791\n",
      "iteration 31702: loss: 0.2178945094347\n",
      "iteration 31703: loss: 0.2178943157196045\n",
      "iteration 31704: loss: 0.21789410710334778\n",
      "iteration 31705: loss: 0.21789391338825226\n",
      "iteration 31706: loss: 0.21789374947547913\n",
      "iteration 31707: loss: 0.2178936004638672\n",
      "iteration 31708: loss: 0.2178933322429657\n",
      "iteration 31709: loss: 0.21789315342903137\n",
      "iteration 31710: loss: 0.21789297461509705\n",
      "iteration 31711: loss: 0.2178928107023239\n",
      "iteration 31712: loss: 0.2178926020860672\n",
      "iteration 31713: loss: 0.21789243817329407\n",
      "iteration 31714: loss: 0.21789219975471497\n",
      "iteration 31715: loss: 0.21789202094078064\n",
      "iteration 31716: loss: 0.2178918421268463\n",
      "iteration 31717: loss: 0.2178916484117508\n",
      "iteration 31718: loss: 0.21789148449897766\n",
      "iteration 31719: loss: 0.21789124608039856\n",
      "iteration 31720: loss: 0.21789109706878662\n",
      "iteration 31721: loss: 0.21789082884788513\n",
      "iteration 31722: loss: 0.217890664935112\n",
      "iteration 31723: loss: 0.21789050102233887\n",
      "iteration 31724: loss: 0.21789033710956573\n",
      "iteration 31725: loss: 0.21789011359214783\n",
      "iteration 31726: loss: 0.2178899049758911\n",
      "iteration 31727: loss: 0.217889666557312\n",
      "iteration 31728: loss: 0.21788950264453888\n",
      "iteration 31729: loss: 0.21788930892944336\n",
      "iteration 31730: loss: 0.2178892195224762\n",
      "iteration 31731: loss: 0.2178889513015747\n",
      "iteration 31732: loss: 0.2178887575864792\n",
      "iteration 31733: loss: 0.21788854897022247\n",
      "iteration 31734: loss: 0.21788838505744934\n",
      "iteration 31735: loss: 0.21788816154003143\n",
      "iteration 31736: loss: 0.2178879976272583\n",
      "iteration 31737: loss: 0.21788784861564636\n",
      "iteration 31738: loss: 0.21788759529590607\n",
      "iteration 31739: loss: 0.21788740158081055\n",
      "iteration 31740: loss: 0.21788723766803741\n",
      "iteration 31741: loss: 0.2178870141506195\n",
      "iteration 31742: loss: 0.21788683533668518\n",
      "iteration 31743: loss: 0.21788668632507324\n",
      "iteration 31744: loss: 0.2178865224123001\n",
      "iteration 31745: loss: 0.2178863286972046\n",
      "iteration 31746: loss: 0.21788612008094788\n",
      "iteration 31747: loss: 0.217885822057724\n",
      "iteration 31748: loss: 0.21788570284843445\n",
      "iteration 31749: loss: 0.2178855687379837\n",
      "iteration 31750: loss: 0.2178853452205658\n",
      "iteration 31751: loss: 0.21788516640663147\n",
      "iteration 31752: loss: 0.21788492798805237\n",
      "iteration 31753: loss: 0.21788474917411804\n",
      "iteration 31754: loss: 0.2178846150636673\n",
      "iteration 31755: loss: 0.21788445115089417\n",
      "iteration 31756: loss: 0.21788422763347626\n",
      "iteration 31757: loss: 0.21788403391838074\n",
      "iteration 31758: loss: 0.21788378059864044\n",
      "iteration 31759: loss: 0.2178836166858673\n",
      "iteration 31760: loss: 0.21788343787193298\n",
      "iteration 31761: loss: 0.21788319945335388\n",
      "iteration 31762: loss: 0.21788311004638672\n",
      "iteration 31763: loss: 0.21788282692432404\n",
      "iteration 31764: loss: 0.21788263320922852\n",
      "iteration 31765: loss: 0.21788251399993896\n",
      "iteration 31766: loss: 0.21788230538368225\n",
      "iteration 31767: loss: 0.21788212656974792\n",
      "iteration 31768: loss: 0.2178819626569748\n",
      "iteration 31769: loss: 0.2178817242383957\n",
      "iteration 31770: loss: 0.21788156032562256\n",
      "iteration 31771: loss: 0.21788135170936584\n",
      "iteration 31772: loss: 0.21788112819194794\n",
      "iteration 31773: loss: 0.21788091957569122\n",
      "iteration 31774: loss: 0.21788077056407928\n",
      "iteration 31775: loss: 0.21788060665130615\n",
      "iteration 31776: loss: 0.21788044273853302\n",
      "iteration 31777: loss: 0.2178802490234375\n",
      "iteration 31778: loss: 0.2178800404071808\n",
      "iteration 31779: loss: 0.21787981688976288\n",
      "iteration 31780: loss: 0.21787962317466736\n",
      "iteration 31781: loss: 0.21787944436073303\n",
      "iteration 31782: loss: 0.2178792953491211\n",
      "iteration 31783: loss: 0.21787910163402557\n",
      "iteration 31784: loss: 0.21787886321544647\n",
      "iteration 31785: loss: 0.21787874400615692\n",
      "iteration 31786: loss: 0.21787849068641663\n",
      "iteration 31787: loss: 0.2178783118724823\n",
      "iteration 31788: loss: 0.21787817776203156\n",
      "iteration 31789: loss: 0.21787793934345245\n",
      "iteration 31790: loss: 0.21787777543067932\n",
      "iteration 31791: loss: 0.2178775519132614\n",
      "iteration 31792: loss: 0.2178773581981659\n",
      "iteration 31793: loss: 0.21787719428539276\n",
      "iteration 31794: loss: 0.21787695586681366\n",
      "iteration 31795: loss: 0.21787676215171814\n",
      "iteration 31796: loss: 0.217876598238945\n",
      "iteration 31797: loss: 0.21787646412849426\n",
      "iteration 31798: loss: 0.21787628531455994\n",
      "iteration 31799: loss: 0.21787598729133606\n",
      "iteration 31800: loss: 0.2178758829832077\n",
      "iteration 31801: loss: 0.2178756296634674\n",
      "iteration 31802: loss: 0.2178754359483719\n",
      "iteration 31803: loss: 0.21787524223327637\n",
      "iteration 31804: loss: 0.21787507832050323\n",
      "iteration 31805: loss: 0.2178749293088913\n",
      "iteration 31806: loss: 0.21787472069263458\n",
      "iteration 31807: loss: 0.21787457168102264\n",
      "iteration 31808: loss: 0.21787433326244354\n",
      "iteration 31809: loss: 0.21787413954734802\n",
      "iteration 31810: loss: 0.21787402033805847\n",
      "iteration 31811: loss: 0.21787378191947937\n",
      "iteration 31812: loss: 0.21787360310554504\n",
      "iteration 31813: loss: 0.21787342429161072\n",
      "iteration 31814: loss: 0.217873215675354\n",
      "iteration 31815: loss: 0.2178730070590973\n",
      "iteration 31816: loss: 0.21787285804748535\n",
      "iteration 31817: loss: 0.21787266433238983\n",
      "iteration 31818: loss: 0.21787242591381073\n",
      "iteration 31819: loss: 0.21787229180335999\n",
      "iteration 31820: loss: 0.21787209808826447\n",
      "iteration 31821: loss: 0.21787185966968536\n",
      "iteration 31822: loss: 0.21787163615226746\n",
      "iteration 31823: loss: 0.21787142753601074\n",
      "iteration 31824: loss: 0.2178712785243988\n",
      "iteration 31825: loss: 0.21787115931510925\n",
      "iteration 31826: loss: 0.21787095069885254\n",
      "iteration 31827: loss: 0.21787075698375702\n",
      "iteration 31828: loss: 0.21787051856517792\n",
      "iteration 31829: loss: 0.21787039935588837\n",
      "iteration 31830: loss: 0.21787019073963165\n",
      "iteration 31831: loss: 0.21786996722221375\n",
      "iteration 31832: loss: 0.217869833111763\n",
      "iteration 31833: loss: 0.2178696095943451\n",
      "iteration 31834: loss: 0.21786943078041077\n",
      "iteration 31835: loss: 0.21786928176879883\n",
      "iteration 31836: loss: 0.21786904335021973\n",
      "iteration 31837: loss: 0.2178688794374466\n",
      "iteration 31838: loss: 0.21786871552467346\n",
      "iteration 31839: loss: 0.21786847710609436\n",
      "iteration 31840: loss: 0.2178683578968048\n",
      "iteration 31841: loss: 0.2178681194782257\n",
      "iteration 31842: loss: 0.2178678959608078\n",
      "iteration 31843: loss: 0.21786773204803467\n",
      "iteration 31844: loss: 0.21786753833293915\n",
      "iteration 31845: loss: 0.2178674191236496\n",
      "iteration 31846: loss: 0.21786721050739288\n",
      "iteration 31847: loss: 0.21786697208881378\n",
      "iteration 31848: loss: 0.21786685287952423\n",
      "iteration 31849: loss: 0.21786658465862274\n",
      "iteration 31850: loss: 0.21786637604236603\n",
      "iteration 31851: loss: 0.2178662270307541\n",
      "iteration 31852: loss: 0.21786609292030334\n",
      "iteration 31853: loss: 0.21786585450172424\n",
      "iteration 31854: loss: 0.21786561608314514\n",
      "iteration 31855: loss: 0.21786543726921082\n",
      "iteration 31856: loss: 0.21786527335643768\n",
      "iteration 31857: loss: 0.21786507964134216\n",
      "iteration 31858: loss: 0.21786490082740784\n",
      "iteration 31859: loss: 0.21786479651927948\n",
      "iteration 31860: loss: 0.21786455810070038\n",
      "iteration 31861: loss: 0.21786436438560486\n",
      "iteration 31862: loss: 0.21786415576934814\n",
      "iteration 31863: loss: 0.2178640365600586\n",
      "iteration 31864: loss: 0.21786382794380188\n",
      "iteration 31865: loss: 0.21786358952522278\n",
      "iteration 31866: loss: 0.21786341071128845\n",
      "iteration 31867: loss: 0.21786324679851532\n",
      "iteration 31868: loss: 0.2178630381822586\n",
      "iteration 31869: loss: 0.21786287426948547\n",
      "iteration 31870: loss: 0.21786269545555115\n",
      "iteration 31871: loss: 0.21786251664161682\n",
      "iteration 31872: loss: 0.21786227822303772\n",
      "iteration 31873: loss: 0.21786217391490936\n",
      "iteration 31874: loss: 0.21786189079284668\n",
      "iteration 31875: loss: 0.21786174178123474\n",
      "iteration 31876: loss: 0.21786156296730042\n",
      "iteration 31877: loss: 0.21786141395568848\n",
      "iteration 31878: loss: 0.21786122024059296\n",
      "iteration 31879: loss: 0.21786101162433624\n",
      "iteration 31880: loss: 0.21786081790924072\n",
      "iteration 31881: loss: 0.2178606241941452\n",
      "iteration 31882: loss: 0.21786046028137207\n",
      "iteration 31883: loss: 0.21786025166511536\n",
      "iteration 31884: loss: 0.21786007285118103\n",
      "iteration 31885: loss: 0.21785981953144073\n",
      "iteration 31886: loss: 0.2178596556186676\n",
      "iteration 31887: loss: 0.21785947680473328\n",
      "iteration 31888: loss: 0.21785931289196014\n",
      "iteration 31889: loss: 0.21785911917686462\n",
      "iteration 31890: loss: 0.2178589403629303\n",
      "iteration 31891: loss: 0.2178587019443512\n",
      "iteration 31892: loss: 0.21785850822925568\n",
      "iteration 31893: loss: 0.21785831451416016\n",
      "iteration 31894: loss: 0.2178581953048706\n",
      "iteration 31895: loss: 0.21785800158977509\n",
      "iteration 31896: loss: 0.21785779297351837\n",
      "iteration 31897: loss: 0.21785762906074524\n",
      "iteration 31898: loss: 0.21785740554332733\n",
      "iteration 31899: loss: 0.2178572714328766\n",
      "iteration 31900: loss: 0.21785703301429749\n",
      "iteration 31901: loss: 0.21785685420036316\n",
      "iteration 31902: loss: 0.21785664558410645\n",
      "iteration 31903: loss: 0.2178564965724945\n",
      "iteration 31904: loss: 0.21785633265972137\n",
      "iteration 31905: loss: 0.21785609424114227\n",
      "iteration 31906: loss: 0.21785596013069153\n",
      "iteration 31907: loss: 0.21785573661327362\n",
      "iteration 31908: loss: 0.2178555428981781\n",
      "iteration 31909: loss: 0.21785536408424377\n",
      "iteration 31910: loss: 0.21785517036914825\n",
      "iteration 31911: loss: 0.21785500645637512\n",
      "iteration 31912: loss: 0.2178548127412796\n",
      "iteration 31913: loss: 0.21785464882850647\n",
      "iteration 31914: loss: 0.21785445511341095\n",
      "iteration 31915: loss: 0.21785426139831543\n",
      "iteration 31916: loss: 0.21785402297973633\n",
      "iteration 31917: loss: 0.21785393357276917\n",
      "iteration 31918: loss: 0.21785371005535126\n",
      "iteration 31919: loss: 0.2178535759449005\n",
      "iteration 31920: loss: 0.2178533524274826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 31921: loss: 0.21785318851470947\n",
      "iteration 31922: loss: 0.21785297989845276\n",
      "iteration 31923: loss: 0.21785278618335724\n",
      "iteration 31924: loss: 0.2178526222705841\n",
      "iteration 31925: loss: 0.2178523987531662\n",
      "iteration 31926: loss: 0.21785219013690948\n",
      "iteration 31927: loss: 0.21785199642181396\n",
      "iteration 31928: loss: 0.21785187721252441\n",
      "iteration 31929: loss: 0.2178516685962677\n",
      "iteration 31930: loss: 0.21785148978233337\n",
      "iteration 31931: loss: 0.21785128116607666\n",
      "iteration 31932: loss: 0.21785113215446472\n",
      "iteration 31933: loss: 0.217850923538208\n",
      "iteration 31934: loss: 0.21785075962543488\n",
      "iteration 31935: loss: 0.21785053610801697\n",
      "iteration 31936: loss: 0.21785040199756622\n",
      "iteration 31937: loss: 0.21785016357898712\n",
      "iteration 31938: loss: 0.2178499400615692\n",
      "iteration 31939: loss: 0.21784980595111847\n",
      "iteration 31940: loss: 0.21784961223602295\n",
      "iteration 31941: loss: 0.21784941852092743\n",
      "iteration 31942: loss: 0.21784920990467072\n",
      "iteration 31943: loss: 0.2178490161895752\n",
      "iteration 31944: loss: 0.21784885227680206\n",
      "iteration 31945: loss: 0.2178487330675125\n",
      "iteration 31946: loss: 0.21784846484661102\n",
      "iteration 31947: loss: 0.2178483009338379\n",
      "iteration 31948: loss: 0.21784810721874237\n",
      "iteration 31949: loss: 0.21784791350364685\n",
      "iteration 31950: loss: 0.2178477793931961\n",
      "iteration 31951: loss: 0.21784758567810059\n",
      "iteration 31952: loss: 0.21784737706184387\n",
      "iteration 31953: loss: 0.21784719824790955\n",
      "iteration 31954: loss: 0.21784701943397522\n",
      "iteration 31955: loss: 0.2178468257188797\n",
      "iteration 31956: loss: 0.21784654259681702\n",
      "iteration 31957: loss: 0.21784643828868866\n",
      "iteration 31958: loss: 0.21784630417823792\n",
      "iteration 31959: loss: 0.2178460657596588\n",
      "iteration 31960: loss: 0.21784591674804688\n",
      "iteration 31961: loss: 0.21784567832946777\n",
      "iteration 31962: loss: 0.21784551441669464\n",
      "iteration 31963: loss: 0.21784541010856628\n",
      "iteration 31964: loss: 0.2178451269865036\n",
      "iteration 31965: loss: 0.21784496307373047\n",
      "iteration 31966: loss: 0.21784476935863495\n",
      "iteration 31967: loss: 0.2178446352481842\n",
      "iteration 31968: loss: 0.2178443968296051\n",
      "iteration 31969: loss: 0.21784424781799316\n",
      "iteration 31970: loss: 0.21784405410289764\n",
      "iteration 31971: loss: 0.2178439199924469\n",
      "iteration 31972: loss: 0.2178436815738678\n",
      "iteration 31973: loss: 0.2178434431552887\n",
      "iteration 31974: loss: 0.21784329414367676\n",
      "iteration 31975: loss: 0.21784310042858124\n",
      "iteration 31976: loss: 0.2178429663181305\n",
      "iteration 31977: loss: 0.2178427278995514\n",
      "iteration 31978: loss: 0.21784254908561707\n",
      "iteration 31979: loss: 0.21784242987632751\n",
      "iteration 31980: loss: 0.21784217655658722\n",
      "iteration 31981: loss: 0.21784202754497528\n",
      "iteration 31982: loss: 0.21784181892871857\n",
      "iteration 31983: loss: 0.21784162521362305\n",
      "iteration 31984: loss: 0.2178415060043335\n",
      "iteration 31985: loss: 0.2178412675857544\n",
      "iteration 31986: loss: 0.21784110367298126\n",
      "iteration 31987: loss: 0.21784090995788574\n",
      "iteration 31988: loss: 0.21784070134162903\n",
      "iteration 31989: loss: 0.2178405076265335\n",
      "iteration 31990: loss: 0.217840313911438\n",
      "iteration 31991: loss: 0.21784019470214844\n",
      "iteration 31992: loss: 0.21784000098705292\n",
      "iteration 31993: loss: 0.2178397923707962\n",
      "iteration 31994: loss: 0.2178395688533783\n",
      "iteration 31995: loss: 0.21783939003944397\n",
      "iteration 31996: loss: 0.21783924102783203\n",
      "iteration 31997: loss: 0.2178390473127365\n",
      "iteration 31998: loss: 0.21783891320228577\n",
      "iteration 31999: loss: 0.21783867478370667\n",
      "iteration 32000: loss: 0.21783849596977234\n",
      "iteration 32001: loss: 0.2178383618593216\n",
      "iteration 32002: loss: 0.2178381383419037\n",
      "iteration 32003: loss: 0.21783792972564697\n",
      "iteration 32004: loss: 0.21783781051635742\n",
      "iteration 32005: loss: 0.21783754229545593\n",
      "iteration 32006: loss: 0.2178373634815216\n",
      "iteration 32007: loss: 0.21783718466758728\n",
      "iteration 32008: loss: 0.21783700585365295\n",
      "iteration 32009: loss: 0.21783681213855743\n",
      "iteration 32010: loss: 0.21783669292926788\n",
      "iteration 32011: loss: 0.21783652901649475\n",
      "iteration 32012: loss: 0.21783630549907684\n",
      "iteration 32013: loss: 0.21783609688282013\n",
      "iteration 32014: loss: 0.2178359478712082\n",
      "iteration 32015: loss: 0.2178357094526291\n",
      "iteration 32016: loss: 0.21783557534217834\n",
      "iteration 32017: loss: 0.21783535182476044\n",
      "iteration 32018: loss: 0.21783515810966492\n",
      "iteration 32019: loss: 0.2178349792957306\n",
      "iteration 32020: loss: 0.21783486008644104\n",
      "iteration 32021: loss: 0.21783462166786194\n",
      "iteration 32022: loss: 0.2178344428539276\n",
      "iteration 32023: loss: 0.21783427894115448\n",
      "iteration 32024: loss: 0.21783407032489777\n",
      "iteration 32025: loss: 0.21783390641212463\n",
      "iteration 32026: loss: 0.21783368289470673\n",
      "iteration 32027: loss: 0.21783356368541718\n",
      "iteration 32028: loss: 0.21783332526683807\n",
      "iteration 32029: loss: 0.21783313155174255\n",
      "iteration 32030: loss: 0.217833012342453\n",
      "iteration 32031: loss: 0.21783284842967987\n",
      "iteration 32032: loss: 0.21783259510993958\n",
      "iteration 32033: loss: 0.21783241629600525\n",
      "iteration 32034: loss: 0.21783220767974854\n",
      "iteration 32035: loss: 0.21783208847045898\n",
      "iteration 32036: loss: 0.21783185005187988\n",
      "iteration 32037: loss: 0.21783168613910675\n",
      "iteration 32038: loss: 0.21783146262168884\n",
      "iteration 32039: loss: 0.2178313434123993\n",
      "iteration 32040: loss: 0.21783113479614258\n",
      "iteration 32041: loss: 0.21783092617988586\n",
      "iteration 32042: loss: 0.2178308218717575\n",
      "iteration 32043: loss: 0.2178306132555008\n",
      "iteration 32044: loss: 0.21783038973808289\n",
      "iteration 32045: loss: 0.21783021092414856\n",
      "iteration 32046: loss: 0.21783003211021423\n",
      "iteration 32047: loss: 0.21782979369163513\n",
      "iteration 32048: loss: 0.21782967448234558\n",
      "iteration 32049: loss: 0.21782946586608887\n",
      "iteration 32050: loss: 0.21782930195331573\n",
      "iteration 32051: loss: 0.2178291529417038\n",
      "iteration 32052: loss: 0.21782894432544708\n",
      "iteration 32053: loss: 0.21782875061035156\n",
      "iteration 32054: loss: 0.21782860159873962\n",
      "iteration 32055: loss: 0.2178284227848053\n",
      "iteration 32056: loss: 0.2178281992673874\n",
      "iteration 32057: loss: 0.21782800555229187\n",
      "iteration 32058: loss: 0.21782787144184113\n",
      "iteration 32059: loss: 0.2178276777267456\n",
      "iteration 32060: loss: 0.21782748401165009\n",
      "iteration 32061: loss: 0.21782736480236053\n",
      "iteration 32062: loss: 0.21782712638378143\n",
      "iteration 32063: loss: 0.21782691776752472\n",
      "iteration 32064: loss: 0.2178267538547516\n",
      "iteration 32065: loss: 0.21782657504081726\n",
      "iteration 32066: loss: 0.21782641112804413\n",
      "iteration 32067: loss: 0.21782617270946503\n",
      "iteration 32068: loss: 0.2178259789943695\n",
      "iteration 32069: loss: 0.21782581508159637\n",
      "iteration 32070: loss: 0.21782569587230682\n",
      "iteration 32071: loss: 0.21782545745372772\n",
      "iteration 32072: loss: 0.2178252637386322\n",
      "iteration 32073: loss: 0.21782512962818146\n",
      "iteration 32074: loss: 0.21782493591308594\n",
      "iteration 32075: loss: 0.21782474219799042\n",
      "iteration 32076: loss: 0.21782460808753967\n",
      "iteration 32077: loss: 0.21782436966896057\n",
      "iteration 32078: loss: 0.21782422065734863\n",
      "iteration 32079: loss: 0.21782393753528595\n",
      "iteration 32080: loss: 0.2178238183259964\n",
      "iteration 32081: loss: 0.21782362461090088\n",
      "iteration 32082: loss: 0.21782350540161133\n",
      "iteration 32083: loss: 0.21782322227954865\n",
      "iteration 32084: loss: 0.2178230583667755\n",
      "iteration 32085: loss: 0.2178228199481964\n",
      "iteration 32086: loss: 0.21782274544239044\n",
      "iteration 32087: loss: 0.21782255172729492\n",
      "iteration 32088: loss: 0.2178223431110382\n",
      "iteration 32089: loss: 0.21782216429710388\n",
      "iteration 32090: loss: 0.21782200038433075\n",
      "iteration 32091: loss: 0.21782180666923523\n",
      "iteration 32092: loss: 0.2178216278553009\n",
      "iteration 32093: loss: 0.21782144904136658\n",
      "iteration 32094: loss: 0.21782131493091583\n",
      "iteration 32095: loss: 0.21782109141349792\n",
      "iteration 32096: loss: 0.2178209275007248\n",
      "iteration 32097: loss: 0.21782079339027405\n",
      "iteration 32098: loss: 0.21782056987285614\n",
      "iteration 32099: loss: 0.217820405960083\n",
      "iteration 32100: loss: 0.2178201675415039\n",
      "iteration 32101: loss: 0.21782000362873077\n",
      "iteration 32102: loss: 0.21781978011131287\n",
      "iteration 32103: loss: 0.21781961619853973\n",
      "iteration 32104: loss: 0.2178194522857666\n",
      "iteration 32105: loss: 0.21781928837299347\n",
      "iteration 32106: loss: 0.21781909465789795\n",
      "iteration 32107: loss: 0.21781888604164124\n",
      "iteration 32108: loss: 0.2178187370300293\n",
      "iteration 32109: loss: 0.21781854331493378\n",
      "iteration 32110: loss: 0.21781837940216064\n",
      "iteration 32111: loss: 0.21781814098358154\n",
      "iteration 32112: loss: 0.21781794726848602\n",
      "iteration 32113: loss: 0.2178177833557129\n",
      "iteration 32114: loss: 0.21781757473945618\n",
      "iteration 32115: loss: 0.21781745553016663\n",
      "iteration 32116: loss: 0.2178173065185547\n",
      "iteration 32117: loss: 0.21781711280345917\n",
      "iteration 32118: loss: 0.21781685948371887\n",
      "iteration 32119: loss: 0.21781674027442932\n",
      "iteration 32120: loss: 0.21781650185585022\n",
      "iteration 32121: loss: 0.2178163230419159\n",
      "iteration 32122: loss: 0.21781614422798157\n",
      "iteration 32123: loss: 0.21781599521636963\n",
      "iteration 32124: loss: 0.2178158313035965\n",
      "iteration 32125: loss: 0.21781563758850098\n",
      "iteration 32126: loss: 0.21781547367572784\n",
      "iteration 32127: loss: 0.2178153544664383\n",
      "iteration 32128: loss: 0.2178150862455368\n",
      "iteration 32129: loss: 0.21781489253044128\n",
      "iteration 32130: loss: 0.21781471371650696\n",
      "iteration 32131: loss: 0.21781449019908905\n",
      "iteration 32132: loss: 0.2178143709897995\n",
      "iteration 32133: loss: 0.21781423687934875\n",
      "iteration 32134: loss: 0.21781393885612488\n",
      "iteration 32135: loss: 0.21781381964683533\n",
      "iteration 32136: loss: 0.217813640832901\n",
      "iteration 32137: loss: 0.21781344711780548\n",
      "iteration 32138: loss: 0.21781322360038757\n",
      "iteration 32139: loss: 0.21781308948993683\n",
      "iteration 32140: loss: 0.2178129255771637\n",
      "iteration 32141: loss: 0.21781274676322937\n",
      "iteration 32142: loss: 0.21781258285045624\n",
      "iteration 32143: loss: 0.21781234443187714\n",
      "iteration 32144: loss: 0.21781215071678162\n",
      "iteration 32145: loss: 0.21781203150749207\n",
      "iteration 32146: loss: 0.21781185269355774\n",
      "iteration 32147: loss: 0.21781165897846222\n",
      "iteration 32148: loss: 0.21781142055988312\n",
      "iteration 32149: loss: 0.21781118214130402\n",
      "iteration 32150: loss: 0.21781113743782043\n",
      "iteration 32151: loss: 0.2178109586238861\n",
      "iteration 32152: loss: 0.21781077980995178\n",
      "iteration 32153: loss: 0.21781055629253387\n",
      "iteration 32154: loss: 0.21781039237976074\n",
      "iteration 32155: loss: 0.21781019866466522\n",
      "iteration 32156: loss: 0.2178099900484085\n",
      "iteration 32157: loss: 0.2178097665309906\n",
      "iteration 32158: loss: 0.21780963242053986\n",
      "iteration 32159: loss: 0.21780940890312195\n",
      "iteration 32160: loss: 0.21780924499034882\n",
      "iteration 32161: loss: 0.21780912578105927\n",
      "iteration 32162: loss: 0.21780891716480255\n",
      "iteration 32163: loss: 0.21780872344970703\n",
      "iteration 32164: loss: 0.2178085297346115\n",
      "iteration 32165: loss: 0.21780841052532196\n",
      "iteration 32166: loss: 0.21780815720558167\n",
      "iteration 32167: loss: 0.21780797839164734\n",
      "iteration 32168: loss: 0.2178078591823578\n",
      "iteration 32169: loss: 0.2178075760602951\n",
      "iteration 32170: loss: 0.21780745685100555\n",
      "iteration 32171: loss: 0.21780724823474884\n",
      "iteration 32172: loss: 0.2178070843219757\n",
      "iteration 32173: loss: 0.21780700981616974\n",
      "iteration 32174: loss: 0.21780672669410706\n",
      "iteration 32175: loss: 0.2178066223859787\n",
      "iteration 32176: loss: 0.2178063690662384\n",
      "iteration 32177: loss: 0.21780622005462646\n",
      "iteration 32178: loss: 0.21780605614185333\n",
      "iteration 32179: loss: 0.21780583262443542\n",
      "iteration 32180: loss: 0.2178056687116623\n",
      "iteration 32181: loss: 0.21780550479888916\n",
      "iteration 32182: loss: 0.21780529618263245\n",
      "iteration 32183: loss: 0.21780510246753693\n",
      "iteration 32184: loss: 0.217804953455925\n",
      "iteration 32185: loss: 0.21780475974082947\n",
      "iteration 32186: loss: 0.21780455112457275\n",
      "iteration 32187: loss: 0.2178044319152832\n",
      "iteration 32188: loss: 0.2178041934967041\n",
      "iteration 32189: loss: 0.21780404448509216\n",
      "iteration 32190: loss: 0.21780386567115784\n",
      "iteration 32191: loss: 0.21780367195606232\n",
      "iteration 32192: loss: 0.2178034782409668\n",
      "iteration 32193: loss: 0.21780338883399963\n",
      "iteration 32194: loss: 0.21780309081077576\n",
      "iteration 32195: loss: 0.2178030014038086\n",
      "iteration 32196: loss: 0.2178027629852295\n",
      "iteration 32197: loss: 0.21780256927013397\n",
      "iteration 32198: loss: 0.21780240535736084\n",
      "iteration 32199: loss: 0.21780221164226532\n",
      "iteration 32200: loss: 0.21780207753181458\n",
      "iteration 32201: loss: 0.21780189871788025\n",
      "iteration 32202: loss: 0.21780173480510712\n",
      "iteration 32203: loss: 0.21780157089233398\n",
      "iteration 32204: loss: 0.21780140697956085\n",
      "iteration 32205: loss: 0.21780116856098175\n",
      "iteration 32206: loss: 0.21780094504356384\n",
      "iteration 32207: loss: 0.21780076622962952\n",
      "iteration 32208: loss: 0.2178005874156952\n",
      "iteration 32209: loss: 0.21780045330524445\n",
      "iteration 32210: loss: 0.21780022978782654\n",
      "iteration 32211: loss: 0.21780017018318176\n",
      "iteration 32212: loss: 0.2177998572587967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 32213: loss: 0.21779970824718475\n",
      "iteration 32214: loss: 0.21779954433441162\n",
      "iteration 32215: loss: 0.2177993506193161\n",
      "iteration 32216: loss: 0.21779918670654297\n",
      "iteration 32217: loss: 0.21779902279376984\n",
      "iteration 32218: loss: 0.21779879927635193\n",
      "iteration 32219: loss: 0.21779866516590118\n",
      "iteration 32220: loss: 0.21779850125312805\n",
      "iteration 32221: loss: 0.21779827773571014\n",
      "iteration 32222: loss: 0.217798113822937\n",
      "iteration 32223: loss: 0.2177979052066803\n",
      "iteration 32224: loss: 0.21779775619506836\n",
      "iteration 32225: loss: 0.21779751777648926\n",
      "iteration 32226: loss: 0.2177973985671997\n",
      "iteration 32227: loss: 0.217797189950943\n",
      "iteration 32228: loss: 0.21779704093933105\n",
      "iteration 32229: loss: 0.21779684722423553\n",
      "iteration 32230: loss: 0.21779660880565643\n",
      "iteration 32231: loss: 0.2177964448928833\n",
      "iteration 32232: loss: 0.21779635548591614\n",
      "iteration 32233: loss: 0.21779613196849823\n",
      "iteration 32234: loss: 0.2177959382534027\n",
      "iteration 32235: loss: 0.21779577434062958\n",
      "iteration 32236: loss: 0.21779561042785645\n",
      "iteration 32237: loss: 0.21779540181159973\n",
      "iteration 32238: loss: 0.2177952229976654\n",
      "iteration 32239: loss: 0.2177950143814087\n",
      "iteration 32240: loss: 0.21779486536979675\n",
      "iteration 32241: loss: 0.21779470145702362\n",
      "iteration 32242: loss: 0.2177945375442505\n",
      "iteration 32243: loss: 0.21779434382915497\n",
      "iteration 32244: loss: 0.21779422461986542\n",
      "iteration 32245: loss: 0.2177940309047699\n",
      "iteration 32246: loss: 0.2177937775850296\n",
      "iteration 32247: loss: 0.21779365837574005\n",
      "iteration 32248: loss: 0.21779343485832214\n",
      "iteration 32249: loss: 0.217793270945549\n",
      "iteration 32250: loss: 0.2177930772304535\n",
      "iteration 32251: loss: 0.21779294312000275\n",
      "iteration 32252: loss: 0.21779274940490723\n",
      "iteration 32253: loss: 0.2177925556898117\n",
      "iteration 32254: loss: 0.21779239177703857\n",
      "iteration 32255: loss: 0.21779218316078186\n",
      "iteration 32256: loss: 0.21779203414916992\n",
      "iteration 32257: loss: 0.2177918404340744\n",
      "iteration 32258: loss: 0.21779164671897888\n",
      "iteration 32259: loss: 0.21779148280620575\n",
      "iteration 32260: loss: 0.2177913635969162\n",
      "iteration 32261: loss: 0.2177911102771759\n",
      "iteration 32262: loss: 0.21779099106788635\n",
      "iteration 32263: loss: 0.21779075264930725\n",
      "iteration 32264: loss: 0.2177906036376953\n",
      "iteration 32265: loss: 0.2177904099225998\n",
      "iteration 32266: loss: 0.21779027581214905\n",
      "iteration 32267: loss: 0.21779008209705353\n",
      "iteration 32268: loss: 0.2177899330854416\n",
      "iteration 32269: loss: 0.21778972446918488\n",
      "iteration 32270: loss: 0.21778948605060577\n",
      "iteration 32271: loss: 0.21778933703899384\n",
      "iteration 32272: loss: 0.2177892029285431\n",
      "iteration 32273: loss: 0.21778900921344757\n",
      "iteration 32274: loss: 0.21778878569602966\n",
      "iteration 32275: loss: 0.21778865158557892\n",
      "iteration 32276: loss: 0.2177884578704834\n",
      "iteration 32277: loss: 0.21778830885887146\n",
      "iteration 32278: loss: 0.21778813004493713\n",
      "iteration 32279: loss: 0.21778789162635803\n",
      "iteration 32280: loss: 0.2177877128124237\n",
      "iteration 32281: loss: 0.21778759360313416\n",
      "iteration 32282: loss: 0.21778741478919983\n",
      "iteration 32283: loss: 0.2177872210741043\n",
      "iteration 32284: loss: 0.2177870273590088\n",
      "iteration 32285: loss: 0.21778683364391327\n",
      "iteration 32286: loss: 0.21778669953346252\n",
      "iteration 32287: loss: 0.21778655052185059\n",
      "iteration 32288: loss: 0.2177862823009491\n",
      "iteration 32289: loss: 0.21778616309165955\n",
      "iteration 32290: loss: 0.21778598427772522\n",
      "iteration 32291: loss: 0.2177857905626297\n",
      "iteration 32292: loss: 0.21778562664985657\n",
      "iteration 32293: loss: 0.21778543293476105\n",
      "iteration 32294: loss: 0.21778526902198792\n",
      "iteration 32295: loss: 0.2177850306034088\n",
      "iteration 32296: loss: 0.21778491139411926\n",
      "iteration 32297: loss: 0.21778471767902374\n",
      "iteration 32298: loss: 0.21778452396392822\n",
      "iteration 32299: loss: 0.2177843302488327\n",
      "iteration 32300: loss: 0.21778424084186554\n",
      "iteration 32301: loss: 0.21778400242328644\n",
      "iteration 32302: loss: 0.2177838534116745\n",
      "iteration 32303: loss: 0.21778364479541779\n",
      "iteration 32304: loss: 0.21778349578380585\n",
      "iteration 32305: loss: 0.21778333187103271\n",
      "iteration 32306: loss: 0.217783123254776\n",
      "iteration 32307: loss: 0.21778292953968048\n",
      "iteration 32308: loss: 0.21778282523155212\n",
      "iteration 32309: loss: 0.21778258681297302\n",
      "iteration 32310: loss: 0.2177824229001999\n",
      "iteration 32311: loss: 0.21778221428394318\n",
      "iteration 32312: loss: 0.21778206527233124\n",
      "iteration 32313: loss: 0.21778187155723572\n",
      "iteration 32314: loss: 0.2177816927433014\n",
      "iteration 32315: loss: 0.21778154373168945\n",
      "iteration 32316: loss: 0.21778135001659393\n",
      "iteration 32317: loss: 0.2177811563014984\n",
      "iteration 32318: loss: 0.2177809476852417\n",
      "iteration 32319: loss: 0.21778078377246857\n",
      "iteration 32320: loss: 0.21778063476085663\n",
      "iteration 32321: loss: 0.2177804410457611\n",
      "iteration 32322: loss: 0.2177802324295044\n",
      "iteration 32323: loss: 0.21778011322021484\n",
      "iteration 32324: loss: 0.21777991950511932\n",
      "iteration 32325: loss: 0.2177797257900238\n",
      "iteration 32326: loss: 0.21777959167957306\n",
      "iteration 32327: loss: 0.21777942776679993\n",
      "iteration 32328: loss: 0.21777920424938202\n",
      "iteration 32329: loss: 0.2177790105342865\n",
      "iteration 32330: loss: 0.21777887642383575\n",
      "iteration 32331: loss: 0.21777868270874023\n",
      "iteration 32332: loss: 0.2177785336971283\n",
      "iteration 32333: loss: 0.21777832508087158\n",
      "iteration 32334: loss: 0.21777813136577606\n",
      "iteration 32335: loss: 0.21777793765068054\n",
      "iteration 32336: loss: 0.217777818441391\n",
      "iteration 32337: loss: 0.21777760982513428\n",
      "iteration 32338: loss: 0.21777740120887756\n",
      "iteration 32339: loss: 0.21777722239494324\n",
      "iteration 32340: loss: 0.2177770882844925\n",
      "iteration 32341: loss: 0.21777686476707458\n",
      "iteration 32342: loss: 0.21777673065662384\n",
      "iteration 32343: loss: 0.2177765667438507\n",
      "iteration 32344: loss: 0.21777644753456116\n",
      "iteration 32345: loss: 0.21777622401714325\n",
      "iteration 32346: loss: 0.21777606010437012\n",
      "iteration 32347: loss: 0.2177758663892746\n",
      "iteration 32348: loss: 0.21777570247650146\n",
      "iteration 32349: loss: 0.21777553856372833\n",
      "iteration 32350: loss: 0.2177753448486328\n",
      "iteration 32351: loss: 0.21777518093585968\n",
      "iteration 32352: loss: 0.21777494251728058\n",
      "iteration 32353: loss: 0.21777482330799103\n",
      "iteration 32354: loss: 0.21777458488941193\n",
      "iteration 32355: loss: 0.21777448058128357\n",
      "iteration 32356: loss: 0.21777422726154327\n",
      "iteration 32357: loss: 0.21777406334877014\n",
      "iteration 32358: loss: 0.2177739143371582\n",
      "iteration 32359: loss: 0.21777375042438507\n",
      "iteration 32360: loss: 0.21777358651161194\n",
      "iteration 32361: loss: 0.21777339279651642\n",
      "iteration 32362: loss: 0.2177731692790985\n",
      "iteration 32363: loss: 0.21777305006980896\n",
      "iteration 32364: loss: 0.21777288615703583\n",
      "iteration 32365: loss: 0.21777264773845673\n",
      "iteration 32366: loss: 0.2177724838256836\n",
      "iteration 32367: loss: 0.21777229011058807\n",
      "iteration 32368: loss: 0.21777217090129852\n",
      "iteration 32369: loss: 0.21777203679084778\n",
      "iteration 32370: loss: 0.21777179837226868\n",
      "iteration 32371: loss: 0.21777161955833435\n",
      "iteration 32372: loss: 0.2177715301513672\n",
      "iteration 32373: loss: 0.21777129173278809\n",
      "iteration 32374: loss: 0.2177710235118866\n",
      "iteration 32375: loss: 0.21777088940143585\n",
      "iteration 32376: loss: 0.21777066588401794\n",
      "iteration 32377: loss: 0.21777057647705078\n",
      "iteration 32378: loss: 0.21777036786079407\n",
      "iteration 32379: loss: 0.21777021884918213\n",
      "iteration 32380: loss: 0.217770054936409\n",
      "iteration 32381: loss: 0.21776989102363586\n",
      "iteration 32382: loss: 0.21776965260505676\n",
      "iteration 32383: loss: 0.2177695482969284\n",
      "iteration 32384: loss: 0.2177693396806717\n",
      "iteration 32385: loss: 0.21776917576789856\n",
      "iteration 32386: loss: 0.21776898205280304\n",
      "iteration 32387: loss: 0.2177688181400299\n",
      "iteration 32388: loss: 0.2177685797214508\n",
      "iteration 32389: loss: 0.21776847541332245\n",
      "iteration 32390: loss: 0.21776828169822693\n",
      "iteration 32391: loss: 0.21776807308197021\n",
      "iteration 32392: loss: 0.21776790916919708\n",
      "iteration 32393: loss: 0.21776774525642395\n",
      "iteration 32394: loss: 0.21776752173900604\n",
      "iteration 32395: loss: 0.2177674025297165\n",
      "iteration 32396: loss: 0.21776720881462097\n",
      "iteration 32397: loss: 0.21776704490184784\n",
      "iteration 32398: loss: 0.2177668809890747\n",
      "iteration 32399: loss: 0.217766672372818\n",
      "iteration 32400: loss: 0.21776656806468964\n",
      "iteration 32401: loss: 0.21776635944843292\n",
      "iteration 32402: loss: 0.21776612102985382\n",
      "iteration 32403: loss: 0.21776601672172546\n",
      "iteration 32404: loss: 0.21776585280895233\n",
      "iteration 32405: loss: 0.21776564419269562\n",
      "iteration 32406: loss: 0.21776548027992249\n",
      "iteration 32407: loss: 0.21776530146598816\n",
      "iteration 32408: loss: 0.21776506304740906\n",
      "iteration 32409: loss: 0.2177649289369583\n",
      "iteration 32410: loss: 0.2177647352218628\n",
      "iteration 32411: loss: 0.21776454150676727\n",
      "iteration 32412: loss: 0.2177644670009613\n",
      "iteration 32413: loss: 0.217764213681221\n",
      "iteration 32414: loss: 0.21776404976844788\n",
      "iteration 32415: loss: 0.21776393055915833\n",
      "iteration 32416: loss: 0.217763751745224\n",
      "iteration 32417: loss: 0.21776354312896729\n",
      "iteration 32418: loss: 0.21776333451271057\n",
      "iteration 32419: loss: 0.21776314079761505\n",
      "iteration 32420: loss: 0.21776297688484192\n",
      "iteration 32421: loss: 0.2177627980709076\n",
      "iteration 32422: loss: 0.21776261925697327\n",
      "iteration 32423: loss: 0.2177625149488449\n",
      "iteration 32424: loss: 0.2177623063325882\n",
      "iteration 32425: loss: 0.21776214241981506\n",
      "iteration 32426: loss: 0.21776196360588074\n",
      "iteration 32427: loss: 0.2177617847919464\n",
      "iteration 32428: loss: 0.2177615910768509\n",
      "iteration 32429: loss: 0.21776142716407776\n",
      "iteration 32430: loss: 0.21776127815246582\n",
      "iteration 32431: loss: 0.2177611142396927\n",
      "iteration 32432: loss: 0.21776092052459717\n",
      "iteration 32433: loss: 0.21776075661182404\n",
      "iteration 32434: loss: 0.21776056289672852\n",
      "iteration 32435: loss: 0.2177603542804718\n",
      "iteration 32436: loss: 0.21776020526885986\n",
      "iteration 32437: loss: 0.21776004135608673\n",
      "iteration 32438: loss: 0.2177598476409912\n",
      "iteration 32439: loss: 0.21775969862937927\n",
      "iteration 32440: loss: 0.21775956451892853\n",
      "iteration 32441: loss: 0.2177594006061554\n",
      "iteration 32442: loss: 0.2177591770887375\n",
      "iteration 32443: loss: 0.21775898337364197\n",
      "iteration 32444: loss: 0.21775884926319122\n",
      "iteration 32445: loss: 0.21775856614112854\n",
      "iteration 32446: loss: 0.21775846183300018\n",
      "iteration 32447: loss: 0.21775826811790466\n",
      "iteration 32448: loss: 0.21775810420513153\n",
      "iteration 32449: loss: 0.2177579700946808\n",
      "iteration 32450: loss: 0.21775782108306885\n",
      "iteration 32451: loss: 0.21775755286216736\n",
      "iteration 32452: loss: 0.21775738894939423\n",
      "iteration 32453: loss: 0.21775726974010468\n",
      "iteration 32454: loss: 0.21775706112384796\n",
      "iteration 32455: loss: 0.21775691211223602\n",
      "iteration 32456: loss: 0.2177567034959793\n",
      "iteration 32457: loss: 0.21775653958320618\n",
      "iteration 32458: loss: 0.21775631606578827\n",
      "iteration 32459: loss: 0.21775615215301514\n",
      "iteration 32460: loss: 0.21775606274604797\n",
      "iteration 32461: loss: 0.21775583922863007\n",
      "iteration 32462: loss: 0.21775567531585693\n",
      "iteration 32463: loss: 0.2177554816007614\n",
      "iteration 32464: loss: 0.21775534749031067\n",
      "iteration 32465: loss: 0.21775515377521515\n",
      "iteration 32466: loss: 0.21775498986244202\n",
      "iteration 32467: loss: 0.2177547961473465\n",
      "iteration 32468: loss: 0.21775460243225098\n",
      "iteration 32469: loss: 0.21775443851947784\n",
      "iteration 32470: loss: 0.2177542746067047\n",
      "iteration 32471: loss: 0.2177540361881256\n",
      "iteration 32472: loss: 0.21775393187999725\n",
      "iteration 32473: loss: 0.21775376796722412\n",
      "iteration 32474: loss: 0.217753604054451\n",
      "iteration 32475: loss: 0.21775341033935547\n",
      "iteration 32476: loss: 0.21775321662425995\n",
      "iteration 32477: loss: 0.21775302290916443\n",
      "iteration 32478: loss: 0.2177528440952301\n",
      "iteration 32479: loss: 0.21775269508361816\n",
      "iteration 32480: loss: 0.21775253117084503\n",
      "iteration 32481: loss: 0.2177523672580719\n",
      "iteration 32482: loss: 0.21775221824645996\n",
      "iteration 32483: loss: 0.21775205433368683\n",
      "iteration 32484: loss: 0.2177518606185913\n",
      "iteration 32485: loss: 0.2177516222000122\n",
      "iteration 32486: loss: 0.21775147318840027\n",
      "iteration 32487: loss: 0.21775135397911072\n",
      "iteration 32488: loss: 0.21775111556053162\n",
      "iteration 32489: loss: 0.2177509367465973\n",
      "iteration 32490: loss: 0.21775078773498535\n",
      "iteration 32491: loss: 0.2177506387233734\n",
      "iteration 32492: loss: 0.2177504301071167\n",
      "iteration 32493: loss: 0.21775026619434357\n",
      "iteration 32494: loss: 0.21775010228157043\n",
      "iteration 32495: loss: 0.21774990856647491\n",
      "iteration 32496: loss: 0.217749685049057\n",
      "iteration 32497: loss: 0.21774962544441223\n",
      "iteration 32498: loss: 0.21774938702583313\n",
      "iteration 32499: loss: 0.21774926781654358\n",
      "iteration 32500: loss: 0.21774908900260925\n",
      "iteration 32501: loss: 0.21774883568286896\n",
      "iteration 32502: loss: 0.2177487164735794\n",
      "iteration 32503: loss: 0.21774856746196747\n",
      "iteration 32504: loss: 0.21774832904338837\n",
      "iteration 32505: loss: 0.2177482396364212\n",
      "iteration 32506: loss: 0.21774804592132568\n",
      "iteration 32507: loss: 0.21774780750274658\n",
      "iteration 32508: loss: 0.21774765849113464\n",
      "iteration 32509: loss: 0.2177475392818451\n",
      "iteration 32510: loss: 0.21774733066558838\n",
      "iteration 32511: loss: 0.21774713695049286\n",
      "iteration 32512: loss: 0.21774692833423615\n",
      "iteration 32513: loss: 0.2177467793226242\n",
      "iteration 32514: loss: 0.2177465856075287\n",
      "iteration 32515: loss: 0.21774642169475555\n",
      "iteration 32516: loss: 0.21774625778198242\n",
      "iteration 32517: loss: 0.21774610877037048\n",
      "iteration 32518: loss: 0.21774592995643616\n",
      "iteration 32519: loss: 0.21774575114250183\n",
      "iteration 32520: loss: 0.2177456170320511\n",
      "iteration 32521: loss: 0.21774545311927795\n",
      "iteration 32522: loss: 0.21774525940418243\n",
      "iteration 32523: loss: 0.2177450954914093\n",
      "iteration 32524: loss: 0.21774491667747498\n",
      "iteration 32525: loss: 0.21774470806121826\n",
      "iteration 32526: loss: 0.2177445888519287\n",
      "iteration 32527: loss: 0.2177443951368332\n",
      "iteration 32528: loss: 0.21774418652057648\n",
      "iteration 32529: loss: 0.21774399280548096\n",
      "iteration 32530: loss: 0.217743918299675\n",
      "iteration 32531: loss: 0.21774370968341827\n",
      "iteration 32532: loss: 0.21774351596832275\n",
      "iteration 32533: loss: 0.21774332225322723\n",
      "iteration 32534: loss: 0.2177431285381317\n",
      "iteration 32535: loss: 0.21774296462535858\n",
      "iteration 32536: loss: 0.21774283051490784\n",
      "iteration 32537: loss: 0.2177426517009735\n",
      "iteration 32538: loss: 0.21774248778820038\n",
      "iteration 32539: loss: 0.21774232387542725\n",
      "iteration 32540: loss: 0.21774211525917053\n",
      "iteration 32541: loss: 0.2177419662475586\n",
      "iteration 32542: loss: 0.21774180233478546\n",
      "iteration 32543: loss: 0.21774157881736755\n",
      "iteration 32544: loss: 0.2177414894104004\n",
      "iteration 32545: loss: 0.21774129569530487\n",
      "iteration 32546: loss: 0.21774113178253174\n",
      "iteration 32547: loss: 0.21774089336395264\n",
      "iteration 32548: loss: 0.21774077415466309\n",
      "iteration 32549: loss: 0.21774058043956757\n",
      "iteration 32550: loss: 0.21774037182331085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 32551: loss: 0.2177402526140213\n",
      "iteration 32552: loss: 0.21774005889892578\n",
      "iteration 32553: loss: 0.21773986518383026\n",
      "iteration 32554: loss: 0.2177397459745407\n",
      "iteration 32555: loss: 0.21773958206176758\n",
      "iteration 32556: loss: 0.21773938834667206\n",
      "iteration 32557: loss: 0.2177392542362213\n",
      "iteration 32558: loss: 0.2177390158176422\n",
      "iteration 32559: loss: 0.21773886680603027\n",
      "iteration 32560: loss: 0.21773870289325714\n",
      "iteration 32561: loss: 0.217738538980484\n",
      "iteration 32562: loss: 0.2177383154630661\n",
      "iteration 32563: loss: 0.21773819625377655\n",
      "iteration 32564: loss: 0.21773798763751984\n",
      "iteration 32565: loss: 0.21773779392242432\n",
      "iteration 32566: loss: 0.21773764491081238\n",
      "iteration 32567: loss: 0.21773751080036163\n",
      "iteration 32568: loss: 0.2177373170852661\n",
      "iteration 32569: loss: 0.21773716807365417\n",
      "iteration 32570: loss: 0.21773692965507507\n",
      "iteration 32571: loss: 0.21773679554462433\n",
      "iteration 32572: loss: 0.2177366316318512\n",
      "iteration 32573: loss: 0.2177363932132721\n",
      "iteration 32574: loss: 0.21773627400398254\n",
      "iteration 32575: loss: 0.21773609519004822\n",
      "iteration 32576: loss: 0.2177358865737915\n",
      "iteration 32577: loss: 0.21773579716682434\n",
      "iteration 32578: loss: 0.21773557364940643\n",
      "iteration 32579: loss: 0.2177354097366333\n",
      "iteration 32580: loss: 0.21773524582386017\n",
      "iteration 32581: loss: 0.21773509681224823\n",
      "iteration 32582: loss: 0.21773496270179749\n",
      "iteration 32583: loss: 0.21773472428321838\n",
      "iteration 32584: loss: 0.21773461997509003\n",
      "iteration 32585: loss: 0.21773433685302734\n",
      "iteration 32586: loss: 0.2177342176437378\n",
      "iteration 32587: loss: 0.21773402392864227\n",
      "iteration 32588: loss: 0.21773388981819153\n",
      "iteration 32589: loss: 0.2177337408065796\n",
      "iteration 32590: loss: 0.21773354709148407\n",
      "iteration 32591: loss: 0.21773335337638855\n",
      "iteration 32592: loss: 0.2177332192659378\n",
      "iteration 32593: loss: 0.2177329808473587\n",
      "iteration 32594: loss: 0.21773281693458557\n",
      "iteration 32595: loss: 0.21773259341716766\n",
      "iteration 32596: loss: 0.2177325040102005\n",
      "iteration 32597: loss: 0.21773234009742737\n",
      "iteration 32598: loss: 0.21773219108581543\n",
      "iteration 32599: loss: 0.2177319973707199\n",
      "iteration 32600: loss: 0.21773183345794678\n",
      "iteration 32601: loss: 0.21773162484169006\n",
      "iteration 32602: loss: 0.2177315056324005\n",
      "iteration 32603: loss: 0.2177313268184662\n",
      "iteration 32604: loss: 0.21773108839988708\n",
      "iteration 32605: loss: 0.21773099899291992\n",
      "iteration 32606: loss: 0.2177308052778244\n",
      "iteration 32607: loss: 0.21773061156272888\n",
      "iteration 32608: loss: 0.21773043274879456\n",
      "iteration 32609: loss: 0.21773028373718262\n",
      "iteration 32610: loss: 0.21773013472557068\n",
      "iteration 32611: loss: 0.21772995591163635\n",
      "iteration 32612: loss: 0.21772976219654083\n",
      "iteration 32613: loss: 0.21772965788841248\n",
      "iteration 32614: loss: 0.21772940456867218\n",
      "iteration 32615: loss: 0.21772924065589905\n",
      "iteration 32616: loss: 0.21772906184196472\n",
      "iteration 32617: loss: 0.21772892773151398\n",
      "iteration 32618: loss: 0.21772877871990204\n",
      "iteration 32619: loss: 0.21772858500480652\n",
      "iteration 32620: loss: 0.2177283763885498\n",
      "iteration 32621: loss: 0.21772822737693787\n",
      "iteration 32622: loss: 0.21772801876068115\n",
      "iteration 32623: loss: 0.21772785484790802\n",
      "iteration 32624: loss: 0.21772773563861847\n",
      "iteration 32625: loss: 0.21772751212120056\n",
      "iteration 32626: loss: 0.21772734820842743\n",
      "iteration 32627: loss: 0.2177271544933319\n",
      "iteration 32628: loss: 0.21772702038288116\n",
      "iteration 32629: loss: 0.21772682666778564\n",
      "iteration 32630: loss: 0.2177266776561737\n",
      "iteration 32631: loss: 0.21772651374340057\n",
      "iteration 32632: loss: 0.21772632002830505\n",
      "iteration 32633: loss: 0.2177261859178543\n",
      "iteration 32634: loss: 0.21772602200508118\n",
      "iteration 32635: loss: 0.21772579848766327\n",
      "iteration 32636: loss: 0.21772560477256775\n",
      "iteration 32637: loss: 0.217725470662117\n",
      "iteration 32638: loss: 0.21772532165050507\n",
      "iteration 32639: loss: 0.21772515773773193\n",
      "iteration 32640: loss: 0.21772494912147522\n",
      "iteration 32641: loss: 0.2177247703075409\n",
      "iteration 32642: loss: 0.21772459149360657\n",
      "iteration 32643: loss: 0.21772444248199463\n",
      "iteration 32644: loss: 0.21772432327270508\n",
      "iteration 32645: loss: 0.2177240401506424\n",
      "iteration 32646: loss: 0.21772393584251404\n",
      "iteration 32647: loss: 0.21772384643554688\n",
      "iteration 32648: loss: 0.21772365272045135\n",
      "iteration 32649: loss: 0.21772344410419464\n",
      "iteration 32650: loss: 0.21772325038909912\n",
      "iteration 32651: loss: 0.21772316098213196\n",
      "iteration 32652: loss: 0.21772293746471405\n",
      "iteration 32653: loss: 0.21772274374961853\n",
      "iteration 32654: loss: 0.2177225798368454\n",
      "iteration 32655: loss: 0.21772244572639465\n",
      "iteration 32656: loss: 0.21772222220897675\n",
      "iteration 32657: loss: 0.217722088098526\n",
      "iteration 32658: loss: 0.21772193908691406\n",
      "iteration 32659: loss: 0.21772170066833496\n",
      "iteration 32660: loss: 0.21772153675556183\n",
      "iteration 32661: loss: 0.2177213728427887\n",
      "iteration 32662: loss: 0.21772122383117676\n",
      "iteration 32663: loss: 0.21772105991840363\n",
      "iteration 32664: loss: 0.21772094070911407\n",
      "iteration 32665: loss: 0.21772070229053497\n",
      "iteration 32666: loss: 0.21772050857543945\n",
      "iteration 32667: loss: 0.21772035956382751\n",
      "iteration 32668: loss: 0.21772024035453796\n",
      "iteration 32669: loss: 0.21771998703479767\n",
      "iteration 32670: loss: 0.21771986782550812\n",
      "iteration 32671: loss: 0.21771971881389618\n",
      "iteration 32672: loss: 0.21771951019763947\n",
      "iteration 32673: loss: 0.21771936118602753\n",
      "iteration 32674: loss: 0.21771922707557678\n",
      "iteration 32675: loss: 0.21771898865699768\n",
      "iteration 32676: loss: 0.21771880984306335\n",
      "iteration 32677: loss: 0.2177186757326126\n",
      "iteration 32678: loss: 0.21771851181983948\n",
      "iteration 32679: loss: 0.21771831810474396\n",
      "iteration 32680: loss: 0.21771812438964844\n",
      "iteration 32681: loss: 0.2177179753780365\n",
      "iteration 32682: loss: 0.21771779656410217\n",
      "iteration 32683: loss: 0.21771760284900665\n",
      "iteration 32684: loss: 0.2177174985408783\n",
      "iteration 32685: loss: 0.2177172601222992\n",
      "iteration 32686: loss: 0.21771714091300964\n",
      "iteration 32687: loss: 0.2177169770002365\n",
      "iteration 32688: loss: 0.21771684288978577\n",
      "iteration 32689: loss: 0.21771664917469025\n",
      "iteration 32690: loss: 0.21771642565727234\n",
      "iteration 32691: loss: 0.2177163064479828\n",
      "iteration 32692: loss: 0.21771612763404846\n",
      "iteration 32693: loss: 0.21771594882011414\n",
      "iteration 32694: loss: 0.2177158147096634\n",
      "iteration 32695: loss: 0.21771562099456787\n",
      "iteration 32696: loss: 0.21771545708179474\n",
      "iteration 32697: loss: 0.2177152931690216\n",
      "iteration 32698: loss: 0.21771511435508728\n",
      "iteration 32699: loss: 0.21771495044231415\n",
      "iteration 32700: loss: 0.21771475672721863\n",
      "iteration 32701: loss: 0.2177145928144455\n",
      "iteration 32702: loss: 0.21771450340747833\n",
      "iteration 32703: loss: 0.21771422028541565\n",
      "iteration 32704: loss: 0.2177140712738037\n",
      "iteration 32705: loss: 0.21771392226219177\n",
      "iteration 32706: loss: 0.21771374344825745\n",
      "iteration 32707: loss: 0.21771354973316193\n",
      "iteration 32708: loss: 0.21771340072155\n",
      "iteration 32709: loss: 0.21771320700645447\n",
      "iteration 32710: loss: 0.21771307289600372\n",
      "iteration 32711: loss: 0.21771296858787537\n",
      "iteration 32712: loss: 0.21771278977394104\n",
      "iteration 32713: loss: 0.21771255135536194\n",
      "iteration 32714: loss: 0.21771235764026642\n",
      "iteration 32715: loss: 0.2177121639251709\n",
      "iteration 32716: loss: 0.21771200001239777\n",
      "iteration 32717: loss: 0.21771188080310822\n",
      "iteration 32718: loss: 0.21771173179149628\n",
      "iteration 32719: loss: 0.21771159768104553\n",
      "iteration 32720: loss: 0.2177114188671112\n",
      "iteration 32721: loss: 0.21771128475666046\n",
      "iteration 32722: loss: 0.21771100163459778\n",
      "iteration 32723: loss: 0.21771085262298584\n",
      "iteration 32724: loss: 0.2177107036113739\n",
      "iteration 32725: loss: 0.21771056950092316\n",
      "iteration 32726: loss: 0.21771034598350525\n",
      "iteration 32727: loss: 0.2177102118730545\n",
      "iteration 32728: loss: 0.21771006286144257\n",
      "iteration 32729: loss: 0.21770986914634705\n",
      "iteration 32730: loss: 0.21770969033241272\n",
      "iteration 32731: loss: 0.21770942211151123\n",
      "iteration 32732: loss: 0.21770933270454407\n",
      "iteration 32733: loss: 0.21770918369293213\n",
      "iteration 32734: loss: 0.217709019780159\n",
      "iteration 32735: loss: 0.21770887076854706\n",
      "iteration 32736: loss: 0.2177087515592575\n",
      "iteration 32737: loss: 0.2177084982395172\n",
      "iteration 32738: loss: 0.21770839393138885\n",
      "iteration 32739: loss: 0.21770815551280975\n",
      "iteration 32740: loss: 0.21770799160003662\n",
      "iteration 32741: loss: 0.2177078276872635\n",
      "iteration 32742: loss: 0.21770763397216797\n",
      "iteration 32743: loss: 0.21770748496055603\n",
      "iteration 32744: loss: 0.21770735085010529\n",
      "iteration 32745: loss: 0.21770718693733215\n",
      "iteration 32746: loss: 0.21770696341991425\n",
      "iteration 32747: loss: 0.2177068293094635\n",
      "iteration 32748: loss: 0.21770663559436798\n",
      "iteration 32749: loss: 0.21770647168159485\n",
      "iteration 32750: loss: 0.21770629286766052\n",
      "iteration 32751: loss: 0.21770620346069336\n",
      "iteration 32752: loss: 0.21770593523979187\n",
      "iteration 32753: loss: 0.21770575642585754\n",
      "iteration 32754: loss: 0.217705637216568\n",
      "iteration 32755: loss: 0.21770548820495605\n",
      "iteration 32756: loss: 0.21770533919334412\n",
      "iteration 32757: loss: 0.2177051305770874\n",
      "iteration 32758: loss: 0.21770498156547546\n",
      "iteration 32759: loss: 0.21770481765270233\n",
      "iteration 32760: loss: 0.2177046835422516\n",
      "iteration 32761: loss: 0.21770444512367249\n",
      "iteration 32762: loss: 0.21770429611206055\n",
      "iteration 32763: loss: 0.2177041471004486\n",
      "iteration 32764: loss: 0.2177039384841919\n",
      "iteration 32765: loss: 0.21770378947257996\n",
      "iteration 32766: loss: 0.21770358085632324\n",
      "iteration 32767: loss: 0.2177034318447113\n",
      "iteration 32768: loss: 0.21770329773426056\n",
      "iteration 32769: loss: 0.21770314872264862\n",
      "iteration 32770: loss: 0.2177029550075531\n",
      "iteration 32771: loss: 0.217702716588974\n",
      "iteration 32772: loss: 0.21770258247852325\n",
      "iteration 32773: loss: 0.2177024632692337\n",
      "iteration 32774: loss: 0.21770229935646057\n",
      "iteration 32775: loss: 0.21770210564136505\n",
      "iteration 32776: loss: 0.2177019566297531\n",
      "iteration 32777: loss: 0.2177017629146576\n",
      "iteration 32778: loss: 0.21770159900188446\n",
      "iteration 32779: loss: 0.21770140528678894\n",
      "iteration 32780: loss: 0.2177012860774994\n",
      "iteration 32781: loss: 0.21770107746124268\n",
      "iteration 32782: loss: 0.21770086884498596\n",
      "iteration 32783: loss: 0.21770071983337402\n",
      "iteration 32784: loss: 0.21770057082176208\n",
      "iteration 32785: loss: 0.21770039200782776\n",
      "iteration 32786: loss: 0.2177003175020218\n",
      "iteration 32787: loss: 0.21770012378692627\n",
      "iteration 32788: loss: 0.21769991517066956\n",
      "iteration 32789: loss: 0.21769979596138\n",
      "iteration 32790: loss: 0.21769960224628448\n",
      "iteration 32791: loss: 0.21769937872886658\n",
      "iteration 32792: loss: 0.21769928932189941\n",
      "iteration 32793: loss: 0.21769912540912628\n",
      "iteration 32794: loss: 0.21769890189170837\n",
      "iteration 32795: loss: 0.21769872307777405\n",
      "iteration 32796: loss: 0.2176985740661621\n",
      "iteration 32797: loss: 0.2176983803510666\n",
      "iteration 32798: loss: 0.21769824624061584\n",
      "iteration 32799: loss: 0.2176980972290039\n",
      "iteration 32800: loss: 0.21769794821739197\n",
      "iteration 32801: loss: 0.21769778430461884\n",
      "iteration 32802: loss: 0.21769757568836212\n",
      "iteration 32803: loss: 0.217697411775589\n",
      "iteration 32804: loss: 0.21769721806049347\n",
      "iteration 32805: loss: 0.21769706904888153\n",
      "iteration 32806: loss: 0.2176969051361084\n",
      "iteration 32807: loss: 0.21769675612449646\n",
      "iteration 32808: loss: 0.21769662201404572\n",
      "iteration 32809: loss: 0.2176964282989502\n",
      "iteration 32810: loss: 0.21769627928733826\n",
      "iteration 32811: loss: 0.21769607067108154\n",
      "iteration 32812: loss: 0.2176959067583084\n",
      "iteration 32813: loss: 0.21769578754901886\n",
      "iteration 32814: loss: 0.21769556403160095\n",
      "iteration 32815: loss: 0.2176954299211502\n",
      "iteration 32816: loss: 0.21769526600837708\n",
      "iteration 32817: loss: 0.21769507229328156\n",
      "iteration 32818: loss: 0.21769492328166962\n",
      "iteration 32819: loss: 0.2176947295665741\n",
      "iteration 32820: loss: 0.21769456565380096\n",
      "iteration 32821: loss: 0.21769437193870544\n",
      "iteration 32822: loss: 0.2176942080259323\n",
      "iteration 32823: loss: 0.2176940143108368\n",
      "iteration 32824: loss: 0.21769395470619202\n",
      "iteration 32825: loss: 0.21769371628761292\n",
      "iteration 32826: loss: 0.21769356727600098\n",
      "iteration 32827: loss: 0.21769340336322784\n",
      "iteration 32828: loss: 0.21769317984580994\n",
      "iteration 32829: loss: 0.21769306063652039\n",
      "iteration 32830: loss: 0.21769294142723083\n",
      "iteration 32831: loss: 0.21769273281097412\n",
      "iteration 32832: loss: 0.2176925390958786\n",
      "iteration 32833: loss: 0.21769241988658905\n",
      "iteration 32834: loss: 0.2176922857761383\n",
      "iteration 32835: loss: 0.2176920622587204\n",
      "iteration 32836: loss: 0.21769186854362488\n",
      "iteration 32837: loss: 0.21769168972969055\n",
      "iteration 32838: loss: 0.2176915854215622\n",
      "iteration 32839: loss: 0.21769142150878906\n",
      "iteration 32840: loss: 0.21769127249717712\n",
      "iteration 32841: loss: 0.21769098937511444\n",
      "iteration 32842: loss: 0.2176908701658249\n",
      "iteration 32843: loss: 0.21769073605537415\n",
      "iteration 32844: loss: 0.21769054234027863\n",
      "iteration 32845: loss: 0.21769042313098907\n",
      "iteration 32846: loss: 0.21769022941589355\n",
      "iteration 32847: loss: 0.21769008040428162\n",
      "iteration 32848: loss: 0.21768994629383087\n",
      "iteration 32849: loss: 0.21768975257873535\n",
      "iteration 32850: loss: 0.21768954396247864\n",
      "iteration 32851: loss: 0.2176893651485443\n",
      "iteration 32852: loss: 0.21768923103809357\n",
      "iteration 32853: loss: 0.21768906712532043\n",
      "iteration 32854: loss: 0.2176888883113861\n",
      "iteration 32855: loss: 0.21768875420093536\n",
      "iteration 32856: loss: 0.217688649892807\n",
      "iteration 32857: loss: 0.21768836677074432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 32858: loss: 0.21768824756145477\n",
      "iteration 32859: loss: 0.21768803894519806\n",
      "iteration 32860: loss: 0.2176879197359085\n",
      "iteration 32861: loss: 0.2176876962184906\n",
      "iteration 32862: loss: 0.21768756210803986\n",
      "iteration 32863: loss: 0.21768739819526672\n",
      "iteration 32864: loss: 0.2176872193813324\n",
      "iteration 32865: loss: 0.21768710017204285\n",
      "iteration 32866: loss: 0.21768689155578613\n",
      "iteration 32867: loss: 0.2176867425441742\n",
      "iteration 32868: loss: 0.21768660843372345\n",
      "iteration 32869: loss: 0.21768641471862793\n",
      "iteration 32870: loss: 0.2176862508058548\n",
      "iteration 32871: loss: 0.21768610179424286\n",
      "iteration 32872: loss: 0.21768596768379211\n",
      "iteration 32873: loss: 0.2176857441663742\n",
      "iteration 32874: loss: 0.21768558025360107\n",
      "iteration 32875: loss: 0.21768541634082794\n",
      "iteration 32876: loss: 0.217685267329216\n",
      "iteration 32877: loss: 0.21768510341644287\n",
      "iteration 32878: loss: 0.21768490970134735\n",
      "iteration 32879: loss: 0.21768474578857422\n",
      "iteration 32880: loss: 0.2176845818758011\n",
      "iteration 32881: loss: 0.21768435835838318\n",
      "iteration 32882: loss: 0.2176842987537384\n",
      "iteration 32883: loss: 0.21768411993980408\n",
      "iteration 32884: loss: 0.21768394112586975\n",
      "iteration 32885: loss: 0.21768370270729065\n",
      "iteration 32886: loss: 0.2176835834980011\n",
      "iteration 32887: loss: 0.21768346428871155\n",
      "iteration 32888: loss: 0.21768328547477722\n",
      "iteration 32889: loss: 0.2176830768585205\n",
      "iteration 32890: loss: 0.21768298745155334\n",
      "iteration 32891: loss: 0.21768271923065186\n",
      "iteration 32892: loss: 0.21768257021903992\n",
      "iteration 32893: loss: 0.21768240630626678\n",
      "iteration 32894: loss: 0.21768227219581604\n",
      "iteration 32895: loss: 0.2176820933818817\n",
      "iteration 32896: loss: 0.21768195927143097\n",
      "iteration 32897: loss: 0.21768176555633545\n",
      "iteration 32898: loss: 0.21768160164356232\n",
      "iteration 32899: loss: 0.2176814079284668\n",
      "iteration 32900: loss: 0.21768125891685486\n",
      "iteration 32901: loss: 0.21768108010292053\n",
      "iteration 32902: loss: 0.21768097579479218\n",
      "iteration 32903: loss: 0.21768073737621307\n",
      "iteration 32904: loss: 0.21768061816692352\n",
      "iteration 32905: loss: 0.2176804542541504\n",
      "iteration 32906: loss: 0.21768024563789368\n",
      "iteration 32907: loss: 0.21768005192279816\n",
      "iteration 32908: loss: 0.2176799476146698\n",
      "iteration 32909: loss: 0.21767973899841309\n",
      "iteration 32910: loss: 0.21767958998680115\n",
      "iteration 32911: loss: 0.21767938137054443\n",
      "iteration 32912: loss: 0.2176792323589325\n",
      "iteration 32913: loss: 0.21767914295196533\n",
      "iteration 32914: loss: 0.2176789492368698\n",
      "iteration 32915: loss: 0.21767883002758026\n",
      "iteration 32916: loss: 0.21767857670783997\n",
      "iteration 32917: loss: 0.21767845749855042\n",
      "iteration 32918: loss: 0.2176782637834549\n",
      "iteration 32919: loss: 0.21767807006835938\n",
      "iteration 32920: loss: 0.2176779806613922\n",
      "iteration 32921: loss: 0.21767780184745789\n",
      "iteration 32922: loss: 0.21767759323120117\n",
      "iteration 32923: loss: 0.21767739951610565\n",
      "iteration 32924: loss: 0.2176772654056549\n",
      "iteration 32925: loss: 0.21767711639404297\n",
      "iteration 32926: loss: 0.21767687797546387\n",
      "iteration 32927: loss: 0.21767675876617432\n",
      "iteration 32928: loss: 0.21767663955688477\n",
      "iteration 32929: loss: 0.21767644584178925\n",
      "iteration 32930: loss: 0.21767635643482208\n",
      "iteration 32931: loss: 0.21767613291740417\n",
      "iteration 32932: loss: 0.21767599880695343\n",
      "iteration 32933: loss: 0.21767576038837433\n",
      "iteration 32934: loss: 0.21767564117908478\n",
      "iteration 32935: loss: 0.21767552196979523\n",
      "iteration 32936: loss: 0.21767529845237732\n",
      "iteration 32937: loss: 0.217675119638443\n",
      "iteration 32938: loss: 0.21767497062683105\n",
      "iteration 32939: loss: 0.2176748812198639\n",
      "iteration 32940: loss: 0.21767468750476837\n",
      "iteration 32941: loss: 0.21767449378967285\n",
      "iteration 32942: loss: 0.21767432987689972\n",
      "iteration 32943: loss: 0.21767418086528778\n",
      "iteration 32944: loss: 0.21767397224903107\n",
      "iteration 32945: loss: 0.21767385303974152\n",
      "iteration 32946: loss: 0.217673659324646\n",
      "iteration 32947: loss: 0.21767346560955048\n",
      "iteration 32948: loss: 0.21767334640026093\n",
      "iteration 32949: loss: 0.2176731526851654\n",
      "iteration 32950: loss: 0.21767298877239227\n",
      "iteration 32951: loss: 0.21767282485961914\n",
      "iteration 32952: loss: 0.21767263114452362\n",
      "iteration 32953: loss: 0.21767251193523407\n",
      "iteration 32954: loss: 0.21767237782478333\n",
      "iteration 32955: loss: 0.2176722288131714\n",
      "iteration 32956: loss: 0.21767203509807587\n",
      "iteration 32957: loss: 0.21767184138298035\n",
      "iteration 32958: loss: 0.21767167747020721\n",
      "iteration 32959: loss: 0.21767154335975647\n",
      "iteration 32960: loss: 0.21767134964466095\n",
      "iteration 32961: loss: 0.217671200633049\n",
      "iteration 32962: loss: 0.21767106652259827\n",
      "iteration 32963: loss: 0.21767087280750275\n",
      "iteration 32964: loss: 0.2176707237958908\n",
      "iteration 32965: loss: 0.2176705300807953\n",
      "iteration 32966: loss: 0.21767035126686096\n",
      "iteration 32967: loss: 0.21767020225524902\n",
      "iteration 32968: loss: 0.21767005324363708\n",
      "iteration 32969: loss: 0.21766988933086395\n",
      "iteration 32970: loss: 0.21766972541809082\n",
      "iteration 32971: loss: 0.21766948699951172\n",
      "iteration 32972: loss: 0.21766933798789978\n",
      "iteration 32973: loss: 0.21766924858093262\n",
      "iteration 32974: loss: 0.21766901016235352\n",
      "iteration 32975: loss: 0.21766886115074158\n",
      "iteration 32976: loss: 0.21766872704029083\n",
      "iteration 32977: loss: 0.21766850352287292\n",
      "iteration 32978: loss: 0.21766844391822815\n",
      "iteration 32979: loss: 0.21766820549964905\n",
      "iteration 32980: loss: 0.2176680564880371\n",
      "iteration 32981: loss: 0.21766793727874756\n",
      "iteration 32982: loss: 0.21766774356365204\n",
      "iteration 32983: loss: 0.2176676094532013\n",
      "iteration 32984: loss: 0.21766741573810577\n",
      "iteration 32985: loss: 0.21766725182533264\n",
      "iteration 32986: loss: 0.21766707301139832\n",
      "iteration 32987: loss: 0.21766693890094757\n",
      "iteration 32988: loss: 0.21766671538352966\n",
      "iteration 32989: loss: 0.21766655147075653\n",
      "iteration 32990: loss: 0.2176664173603058\n",
      "iteration 32991: loss: 0.21766623854637146\n",
      "iteration 32992: loss: 0.21766610443592072\n",
      "iteration 32993: loss: 0.2176659107208252\n",
      "iteration 32994: loss: 0.21766571700572968\n",
      "iteration 32995: loss: 0.2176656275987625\n",
      "iteration 32996: loss: 0.217665433883667\n",
      "iteration 32997: loss: 0.21766528487205505\n",
      "iteration 32998: loss: 0.2176651507616043\n",
      "iteration 32999: loss: 0.21766500174999237\n",
      "iteration 33000: loss: 0.21766480803489685\n",
      "iteration 33001: loss: 0.21766462922096252\n",
      "iteration 33002: loss: 0.21766448020935059\n",
      "iteration 33003: loss: 0.2176641970872879\n",
      "iteration 33004: loss: 0.21766415238380432\n",
      "iteration 33005: loss: 0.21766391396522522\n",
      "iteration 33006: loss: 0.21766380965709686\n",
      "iteration 33007: loss: 0.21766367554664612\n",
      "iteration 33008: loss: 0.2176634818315506\n",
      "iteration 33009: loss: 0.21766333281993866\n",
      "iteration 33010: loss: 0.21766313910484314\n",
      "iteration 33011: loss: 0.2176630049943924\n",
      "iteration 33012: loss: 0.21766281127929688\n",
      "iteration 33013: loss: 0.21766269207000732\n",
      "iteration 33014: loss: 0.21766254305839539\n",
      "iteration 33015: loss: 0.21766236424446106\n",
      "iteration 33016: loss: 0.21766214072704315\n",
      "iteration 33017: loss: 0.2176620066165924\n",
      "iteration 33018: loss: 0.21766185760498047\n",
      "iteration 33019: loss: 0.21766169369220734\n",
      "iteration 33020: loss: 0.2176615297794342\n",
      "iteration 33021: loss: 0.2176612913608551\n",
      "iteration 33022: loss: 0.21766118705272675\n",
      "iteration 33023: loss: 0.217661052942276\n",
      "iteration 33024: loss: 0.21766087412834167\n",
      "iteration 33025: loss: 0.21766075491905212\n",
      "iteration 33026: loss: 0.2176605761051178\n",
      "iteration 33027: loss: 0.21766042709350586\n",
      "iteration 33028: loss: 0.21766023337841034\n",
      "iteration 33029: loss: 0.21766003966331482\n",
      "iteration 33030: loss: 0.2176598608493805\n",
      "iteration 33031: loss: 0.21765978634357452\n",
      "iteration 33032: loss: 0.217659592628479\n",
      "iteration 33033: loss: 0.2176593840122223\n",
      "iteration 33034: loss: 0.21765923500061035\n",
      "iteration 33035: loss: 0.21765904128551483\n",
      "iteration 33036: loss: 0.21765892207622528\n",
      "iteration 33037: loss: 0.21765871345996857\n",
      "iteration 33038: loss: 0.21765856444835663\n",
      "iteration 33039: loss: 0.2176583707332611\n",
      "iteration 33040: loss: 0.21765828132629395\n",
      "iteration 33041: loss: 0.21765808761119843\n",
      "iteration 33042: loss: 0.2176579236984253\n",
      "iteration 33043: loss: 0.21765775978565216\n",
      "iteration 33044: loss: 0.2176576405763626\n",
      "iteration 33045: loss: 0.21765749156475067\n",
      "iteration 33046: loss: 0.21765728294849396\n",
      "iteration 33047: loss: 0.21765708923339844\n",
      "iteration 33048: loss: 0.2176569253206253\n",
      "iteration 33049: loss: 0.21765680611133575\n",
      "iteration 33050: loss: 0.2176567018032074\n",
      "iteration 33051: loss: 0.21765640377998352\n",
      "iteration 33052: loss: 0.21765632927417755\n",
      "iteration 33053: loss: 0.21765616536140442\n",
      "iteration 33054: loss: 0.2176559716463089\n",
      "iteration 33055: loss: 0.21765582263469696\n",
      "iteration 33056: loss: 0.21765558421611786\n",
      "iteration 33057: loss: 0.2176555097103119\n",
      "iteration 33058: loss: 0.21765533089637756\n",
      "iteration 33059: loss: 0.21765510737895966\n",
      "iteration 33060: loss: 0.2176549881696701\n",
      "iteration 33061: loss: 0.21765482425689697\n",
      "iteration 33062: loss: 0.21765466034412384\n",
      "iteration 33063: loss: 0.2176545113325119\n",
      "iteration 33064: loss: 0.21765437722206116\n",
      "iteration 33065: loss: 0.21765418350696564\n",
      "iteration 33066: loss: 0.2176540344953537\n",
      "iteration 33067: loss: 0.21765384078025818\n",
      "iteration 33068: loss: 0.21765366196632385\n",
      "iteration 33069: loss: 0.21765351295471191\n",
      "iteration 33070: loss: 0.2176533192396164\n",
      "iteration 33071: loss: 0.21765320003032684\n",
      "iteration 33072: loss: 0.2176530808210373\n",
      "iteration 33073: loss: 0.21765287220478058\n",
      "iteration 33074: loss: 0.21765272319316864\n",
      "iteration 33075: loss: 0.21765252947807312\n",
      "iteration 33076: loss: 0.21765239536762238\n",
      "iteration 33077: loss: 0.21765223145484924\n",
      "iteration 33078: loss: 0.2176520824432373\n",
      "iteration 33079: loss: 0.2176518440246582\n",
      "iteration 33080: loss: 0.21765175461769104\n",
      "iteration 33081: loss: 0.21765156090259552\n",
      "iteration 33082: loss: 0.2176513671875\n",
      "iteration 33083: loss: 0.21765121817588806\n",
      "iteration 33084: loss: 0.2176510989665985\n",
      "iteration 33085: loss: 0.2176508903503418\n",
      "iteration 33086: loss: 0.21765077114105225\n",
      "iteration 33087: loss: 0.2176506519317627\n",
      "iteration 33088: loss: 0.21765050292015076\n",
      "iteration 33089: loss: 0.21765029430389404\n",
      "iteration 33090: loss: 0.2176501303911209\n",
      "iteration 33091: loss: 0.21764996647834778\n",
      "iteration 33092: loss: 0.21764981746673584\n",
      "iteration 33093: loss: 0.21764960885047913\n",
      "iteration 33094: loss: 0.2176494598388672\n",
      "iteration 33095: loss: 0.21764931082725525\n",
      "iteration 33096: loss: 0.2176491767168045\n",
      "iteration 33097: loss: 0.21764901280403137\n",
      "iteration 33098: loss: 0.21764883399009705\n",
      "iteration 33099: loss: 0.21764865517616272\n",
      "iteration 33100: loss: 0.21764850616455078\n",
      "iteration 33101: loss: 0.21764831244945526\n",
      "iteration 33102: loss: 0.2176482379436493\n",
      "iteration 33103: loss: 0.2176479995250702\n",
      "iteration 33104: loss: 0.21764786541461945\n",
      "iteration 33105: loss: 0.2176477015018463\n",
      "iteration 33106: loss: 0.21764755249023438\n",
      "iteration 33107: loss: 0.21764734387397766\n",
      "iteration 33108: loss: 0.21764719486236572\n",
      "iteration 33109: loss: 0.21764710545539856\n",
      "iteration 33110: loss: 0.21764683723449707\n",
      "iteration 33111: loss: 0.21764671802520752\n",
      "iteration 33112: loss: 0.21764659881591797\n",
      "iteration 33113: loss: 0.21764639019966125\n",
      "iteration 33114: loss: 0.21764624118804932\n",
      "iteration 33115: loss: 0.21764607727527618\n",
      "iteration 33116: loss: 0.21764592826366425\n",
      "iteration 33117: loss: 0.21764573454856873\n",
      "iteration 33118: loss: 0.21764567494392395\n",
      "iteration 33119: loss: 0.21764545142650604\n",
      "iteration 33120: loss: 0.21764521300792694\n",
      "iteration 33121: loss: 0.2176450937986374\n",
      "iteration 33122: loss: 0.21764492988586426\n",
      "iteration 33123: loss: 0.21764476597309113\n",
      "iteration 33124: loss: 0.21764464676380157\n",
      "iteration 33125: loss: 0.21764449775218964\n",
      "iteration 33126: loss: 0.21764430403709412\n",
      "iteration 33127: loss: 0.2176441252231598\n",
      "iteration 33128: loss: 0.21764397621154785\n",
      "iteration 33129: loss: 0.2176438271999359\n",
      "iteration 33130: loss: 0.2176436185836792\n",
      "iteration 33131: loss: 0.21764346957206726\n",
      "iteration 33132: loss: 0.21764333546161652\n",
      "iteration 33133: loss: 0.217643141746521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 33134: loss: 0.21764305233955383\n",
      "iteration 33135: loss: 0.21764282882213593\n",
      "iteration 33136: loss: 0.21764269471168518\n",
      "iteration 33137: loss: 0.21764254570007324\n",
      "iteration 33138: loss: 0.21764245629310608\n",
      "iteration 33139: loss: 0.2176421582698822\n",
      "iteration 33140: loss: 0.21764203906059265\n",
      "iteration 33141: loss: 0.2176419049501419\n",
      "iteration 33142: loss: 0.2176417112350464\n",
      "iteration 33143: loss: 0.21764159202575684\n",
      "iteration 33144: loss: 0.21764138340950012\n",
      "iteration 33145: loss: 0.21764123439788818\n",
      "iteration 33146: loss: 0.21764108538627625\n",
      "iteration 33147: loss: 0.21764083206653595\n",
      "iteration 33148: loss: 0.2176407277584076\n",
      "iteration 33149: loss: 0.21764059364795685\n",
      "iteration 33150: loss: 0.2176404744386673\n",
      "iteration 33151: loss: 0.2176402509212494\n",
      "iteration 33152: loss: 0.21764007210731506\n",
      "iteration 33153: loss: 0.2176399677991867\n",
      "iteration 33154: loss: 0.21763975918293\n",
      "iteration 33155: loss: 0.21763959527015686\n",
      "iteration 33156: loss: 0.2176394760608673\n",
      "iteration 33157: loss: 0.21763929724693298\n",
      "iteration 33158: loss: 0.21763917803764343\n",
      "iteration 33159: loss: 0.21763892471790314\n",
      "iteration 33160: loss: 0.21763882040977478\n",
      "iteration 33161: loss: 0.21763868629932404\n",
      "iteration 33162: loss: 0.21763846278190613\n",
      "iteration 33163: loss: 0.21763832867145538\n",
      "iteration 33164: loss: 0.21763816475868225\n",
      "iteration 33165: loss: 0.2176380157470703\n",
      "iteration 33166: loss: 0.21763786673545837\n",
      "iteration 33167: loss: 0.21763768792152405\n",
      "iteration 33168: loss: 0.2176375687122345\n",
      "iteration 33169: loss: 0.21763738989830017\n",
      "iteration 33170: loss: 0.21763721108436584\n",
      "iteration 33171: loss: 0.21763701736927032\n",
      "iteration 33172: loss: 0.2176368683576584\n",
      "iteration 33173: loss: 0.21763670444488525\n",
      "iteration 33174: loss: 0.21763654053211212\n",
      "iteration 33175: loss: 0.21763643622398376\n",
      "iteration 33176: loss: 0.21763625741004944\n",
      "iteration 33177: loss: 0.21763603389263153\n",
      "iteration 33178: loss: 0.21763591468334198\n",
      "iteration 33179: loss: 0.21763579547405243\n",
      "iteration 33180: loss: 0.2176356315612793\n",
      "iteration 33181: loss: 0.21763548254966736\n",
      "iteration 33182: loss: 0.21763530373573303\n",
      "iteration 33183: loss: 0.2176351249217987\n",
      "iteration 33184: loss: 0.21763496100902557\n",
      "iteration 33185: loss: 0.21763482689857483\n",
      "iteration 33186: loss: 0.2176346480846405\n",
      "iteration 33187: loss: 0.21763451397418976\n",
      "iteration 33188: loss: 0.21763427555561066\n",
      "iteration 33189: loss: 0.2176341563463211\n",
      "iteration 33190: loss: 0.21763403713703156\n",
      "iteration 33191: loss: 0.21763387322425842\n",
      "iteration 33192: loss: 0.2176336944103241\n",
      "iteration 33193: loss: 0.21763351559638977\n",
      "iteration 33194: loss: 0.2176334112882614\n",
      "iteration 33195: loss: 0.2176332026720047\n",
      "iteration 33196: loss: 0.21763305366039276\n",
      "iteration 33197: loss: 0.21763291954994202\n",
      "iteration 33198: loss: 0.2176327258348465\n",
      "iteration 33199: loss: 0.21763257682323456\n",
      "iteration 33200: loss: 0.21763233840465546\n",
      "iteration 33201: loss: 0.2176322489976883\n",
      "iteration 33202: loss: 0.21763205528259277\n",
      "iteration 33203: loss: 0.21763190627098083\n",
      "iteration 33204: loss: 0.2176317721605301\n",
      "iteration 33205: loss: 0.21763165295124054\n",
      "iteration 33206: loss: 0.21763142943382263\n",
      "iteration 33207: loss: 0.21763131022453308\n",
      "iteration 33208: loss: 0.21763105690479279\n",
      "iteration 33209: loss: 0.2176310122013092\n",
      "iteration 33210: loss: 0.21763074398040771\n",
      "iteration 33211: loss: 0.21763058006763458\n",
      "iteration 33212: loss: 0.21763047575950623\n",
      "iteration 33213: loss: 0.21763034164905548\n",
      "iteration 33214: loss: 0.21763019263744354\n",
      "iteration 33215: loss: 0.2176300287246704\n",
      "iteration 33216: loss: 0.21762986481189728\n",
      "iteration 33217: loss: 0.21762970089912415\n",
      "iteration 33218: loss: 0.21762947738170624\n",
      "iteration 33219: loss: 0.21762938797473907\n",
      "iteration 33220: loss: 0.21762919425964355\n",
      "iteration 33221: loss: 0.217629075050354\n",
      "iteration 33222: loss: 0.21762888133525848\n",
      "iteration 33223: loss: 0.21762868762016296\n",
      "iteration 33224: loss: 0.2176285684108734\n",
      "iteration 33225: loss: 0.21762844920158386\n",
      "iteration 33226: loss: 0.21762827038764954\n",
      "iteration 33227: loss: 0.2176281213760376\n",
      "iteration 33228: loss: 0.21762791275978088\n",
      "iteration 33229: loss: 0.21762779355049133\n",
      "iteration 33230: loss: 0.2176276445388794\n",
      "iteration 33231: loss: 0.21762745082378387\n",
      "iteration 33232: loss: 0.21762733161449432\n",
      "iteration 33233: loss: 0.21762709319591522\n",
      "iteration 33234: loss: 0.2176268994808197\n",
      "iteration 33235: loss: 0.21762685477733612\n",
      "iteration 33236: loss: 0.21762660145759583\n",
      "iteration 33237: loss: 0.21762649714946747\n",
      "iteration 33238: loss: 0.21762636303901672\n",
      "iteration 33239: loss: 0.21762612462043762\n",
      "iteration 33240: loss: 0.21762600541114807\n",
      "iteration 33241: loss: 0.21762585639953613\n",
      "iteration 33242: loss: 0.21762576699256897\n",
      "iteration 33243: loss: 0.21762552857398987\n",
      "iteration 33244: loss: 0.21762537956237793\n",
      "iteration 33245: loss: 0.217625230550766\n",
      "iteration 33246: loss: 0.2176249921321869\n",
      "iteration 33247: loss: 0.21762493252754211\n",
      "iteration 33248: loss: 0.2176247537136078\n",
      "iteration 33249: loss: 0.2176245152950287\n",
      "iteration 33250: loss: 0.2176244556903839\n",
      "iteration 33251: loss: 0.217624232172966\n",
      "iteration 33252: loss: 0.21762409806251526\n",
      "iteration 33253: loss: 0.21762390434741974\n",
      "iteration 33254: loss: 0.21762380003929138\n",
      "iteration 33255: loss: 0.21762362122535706\n",
      "iteration 33256: loss: 0.21762347221374512\n",
      "iteration 33257: loss: 0.21762330830097198\n",
      "iteration 33258: loss: 0.21762314438819885\n",
      "iteration 33259: loss: 0.21762295067310333\n",
      "iteration 33260: loss: 0.21762284636497498\n",
      "iteration 33261: loss: 0.21762268245220184\n",
      "iteration 33262: loss: 0.2176225185394287\n",
      "iteration 33263: loss: 0.21762236952781677\n",
      "iteration 33264: loss: 0.21762216091156006\n",
      "iteration 33265: loss: 0.21762201189994812\n",
      "iteration 33266: loss: 0.2176218330860138\n",
      "iteration 33267: loss: 0.21762171387672424\n",
      "iteration 33268: loss: 0.2176215648651123\n",
      "iteration 33269: loss: 0.21762137115001678\n",
      "iteration 33270: loss: 0.21762120723724365\n",
      "iteration 33271: loss: 0.2176210582256317\n",
      "iteration 33272: loss: 0.21762093901634216\n",
      "iteration 33273: loss: 0.21762070059776306\n",
      "iteration 33274: loss: 0.2176206409931183\n",
      "iteration 33275: loss: 0.21762046217918396\n",
      "iteration 33276: loss: 0.21762025356292725\n",
      "iteration 33277: loss: 0.2176201343536377\n",
      "iteration 33278: loss: 0.21761997044086456\n",
      "iteration 33279: loss: 0.217619851231575\n",
      "iteration 33280: loss: 0.2176196575164795\n",
      "iteration 33281: loss: 0.21761946380138397\n",
      "iteration 33282: loss: 0.21761934459209442\n",
      "iteration 33283: loss: 0.21761925518512726\n",
      "iteration 33284: loss: 0.21761901676654816\n",
      "iteration 33285: loss: 0.21761885285377502\n",
      "iteration 33286: loss: 0.2176186591386795\n",
      "iteration 33287: loss: 0.21761855483055115\n",
      "iteration 33288: loss: 0.2176184207201004\n",
      "iteration 33289: loss: 0.2176181972026825\n",
      "iteration 33290: loss: 0.21761807799339294\n",
      "iteration 33291: loss: 0.2176179140806198\n",
      "iteration 33292: loss: 0.21761775016784668\n",
      "iteration 33293: loss: 0.21761754155158997\n",
      "iteration 33294: loss: 0.2176174372434616\n",
      "iteration 33295: loss: 0.21761727333068848\n",
      "iteration 33296: loss: 0.21761712431907654\n",
      "iteration 33297: loss: 0.21761691570281982\n",
      "iteration 33298: loss: 0.21761684119701385\n",
      "iteration 33299: loss: 0.21761660277843475\n",
      "iteration 33300: loss: 0.217616468667984\n",
      "iteration 33301: loss: 0.21761628985404968\n",
      "iteration 33302: loss: 0.21761612594127655\n",
      "iteration 33303: loss: 0.217616006731987\n",
      "iteration 33304: loss: 0.21761587262153625\n",
      "iteration 33305: loss: 0.21761563420295715\n",
      "iteration 33306: loss: 0.2176155298948288\n",
      "iteration 33307: loss: 0.21761539578437805\n",
      "iteration 33308: loss: 0.21761521697044373\n",
      "iteration 33309: loss: 0.21761509776115417\n",
      "iteration 33310: loss: 0.21761491894721985\n",
      "iteration 33311: loss: 0.21761472523212433\n",
      "iteration 33312: loss: 0.2176145613193512\n",
      "iteration 33313: loss: 0.21761441230773926\n",
      "iteration 33314: loss: 0.21761424839496613\n",
      "iteration 33315: loss: 0.21761412918567657\n",
      "iteration 33316: loss: 0.21761390566825867\n",
      "iteration 33317: loss: 0.21761378645896912\n",
      "iteration 33318: loss: 0.21761366724967957\n",
      "iteration 33319: loss: 0.21761348843574524\n",
      "iteration 33320: loss: 0.2176133692264557\n",
      "iteration 33321: loss: 0.21761317551136017\n",
      "iteration 33322: loss: 0.21761305630207062\n",
      "iteration 33323: loss: 0.2176128327846527\n",
      "iteration 33324: loss: 0.21761266887187958\n",
      "iteration 33325: loss: 0.21761250495910645\n",
      "iteration 33326: loss: 0.2176123559474945\n",
      "iteration 33327: loss: 0.21761217713356018\n",
      "iteration 33328: loss: 0.2176121175289154\n",
      "iteration 33329: loss: 0.2176118642091751\n",
      "iteration 33330: loss: 0.21761171519756317\n",
      "iteration 33331: loss: 0.21761159598827362\n",
      "iteration 33332: loss: 0.21761135756969452\n",
      "iteration 33333: loss: 0.21761123836040497\n",
      "iteration 33334: loss: 0.21761107444763184\n",
      "iteration 33335: loss: 0.2176109254360199\n",
      "iteration 33336: loss: 0.21761076152324677\n",
      "iteration 33337: loss: 0.21761062741279602\n",
      "iteration 33338: loss: 0.21761047840118408\n",
      "iteration 33339: loss: 0.21761028468608856\n",
      "iteration 33340: loss: 0.21761015057563782\n",
      "iteration 33341: loss: 0.2176099568605423\n",
      "iteration 33342: loss: 0.21760985255241394\n",
      "iteration 33343: loss: 0.2176096886396408\n",
      "iteration 33344: loss: 0.21760952472686768\n",
      "iteration 33345: loss: 0.21760933101177216\n",
      "iteration 33346: loss: 0.2176091969013214\n",
      "iteration 33347: loss: 0.21760904788970947\n",
      "iteration 33348: loss: 0.21760888397693634\n",
      "iteration 33349: loss: 0.2176087200641632\n",
      "iteration 33350: loss: 0.21760860085487366\n",
      "iteration 33351: loss: 0.21760842204093933\n",
      "iteration 33352: loss: 0.2176082879304886\n",
      "iteration 33353: loss: 0.21760809421539307\n",
      "iteration 33354: loss: 0.21760794520378113\n",
      "iteration 33355: loss: 0.21760782599449158\n",
      "iteration 33356: loss: 0.21760764718055725\n",
      "iteration 33357: loss: 0.2176074981689453\n",
      "iteration 33358: loss: 0.2176073044538498\n",
      "iteration 33359: loss: 0.21760721504688263\n",
      "iteration 33360: loss: 0.21760697662830353\n",
      "iteration 33361: loss: 0.2176068276166916\n",
      "iteration 33362: loss: 0.21760666370391846\n",
      "iteration 33363: loss: 0.2176065444946289\n",
      "iteration 33364: loss: 0.2176063358783722\n",
      "iteration 33365: loss: 0.21760615706443787\n",
      "iteration 33366: loss: 0.2176060974597931\n",
      "iteration 33367: loss: 0.21760591864585876\n",
      "iteration 33368: loss: 0.21760578453540802\n",
      "iteration 33369: loss: 0.2176055908203125\n",
      "iteration 33370: loss: 0.21760542690753937\n",
      "iteration 33371: loss: 0.21760520339012146\n",
      "iteration 33372: loss: 0.2176050841808319\n",
      "iteration 33373: loss: 0.21760490536689758\n",
      "iteration 33374: loss: 0.21760478615760803\n",
      "iteration 33375: loss: 0.21760472655296326\n",
      "iteration 33376: loss: 0.21760448813438416\n",
      "iteration 33377: loss: 0.2176043689250946\n",
      "iteration 33378: loss: 0.2176041603088379\n",
      "iteration 33379: loss: 0.21760404109954834\n",
      "iteration 33380: loss: 0.21760383248329163\n",
      "iteration 33381: loss: 0.21760377287864685\n",
      "iteration 33382: loss: 0.21760353446006775\n",
      "iteration 33383: loss: 0.2176034152507782\n",
      "iteration 33384: loss: 0.2176031768321991\n",
      "iteration 33385: loss: 0.21760308742523193\n",
      "iteration 33386: loss: 0.2176029235124588\n",
      "iteration 33387: loss: 0.21760275959968567\n",
      "iteration 33388: loss: 0.21760258078575134\n",
      "iteration 33389: loss: 0.2176024615764618\n",
      "iteration 33390: loss: 0.21760229766368866\n",
      "iteration 33391: loss: 0.21760210394859314\n",
      "iteration 33392: loss: 0.21760194003582\n",
      "iteration 33393: loss: 0.21760182082653046\n",
      "iteration 33394: loss: 0.2176016867160797\n",
      "iteration 33395: loss: 0.2176014930009842\n",
      "iteration 33396: loss: 0.21760129928588867\n",
      "iteration 33397: loss: 0.21760115027427673\n",
      "iteration 33398: loss: 0.217601016163826\n",
      "iteration 33399: loss: 0.21760086715221405\n",
      "iteration 33400: loss: 0.2176007479429245\n",
      "iteration 33401: loss: 0.2176005095243454\n",
      "iteration 33402: loss: 0.21760046482086182\n",
      "iteration 33403: loss: 0.21760019659996033\n",
      "iteration 33404: loss: 0.21760013699531555\n",
      "iteration 33405: loss: 0.21759989857673645\n",
      "iteration 33406: loss: 0.2175997942686081\n",
      "iteration 33407: loss: 0.21759958565235138\n",
      "iteration 33408: loss: 0.2175995111465454\n",
      "iteration 33409: loss: 0.2175993025302887\n",
      "iteration 33410: loss: 0.21759919822216034\n",
      "iteration 33411: loss: 0.21759895980358124\n",
      "iteration 33412: loss: 0.21759887039661407\n",
      "iteration 33413: loss: 0.21759863197803497\n",
      "iteration 33414: loss: 0.217598557472229\n",
      "iteration 33415: loss: 0.2175983488559723\n",
      "iteration 33416: loss: 0.21759822964668274\n",
      "iteration 33417: loss: 0.2175981104373932\n",
      "iteration 33418: loss: 0.21759791672229767\n",
      "iteration 33419: loss: 0.21759772300720215\n",
      "iteration 33420: loss: 0.21759752929210663\n",
      "iteration 33421: loss: 0.21759748458862305\n",
      "iteration 33422: loss: 0.21759732067584991\n",
      "iteration 33423: loss: 0.2175971269607544\n",
      "iteration 33424: loss: 0.21759696304798126\n",
      "iteration 33425: loss: 0.21759681403636932\n",
      "iteration 33426: loss: 0.21759667992591858\n",
      "iteration 33427: loss: 0.21759653091430664\n",
      "iteration 33428: loss: 0.21759632229804993\n",
      "iteration 33429: loss: 0.217596173286438\n",
      "iteration 33430: loss: 0.21759600937366486\n",
      "iteration 33431: loss: 0.21759584546089172\n",
      "iteration 33432: loss: 0.21759574115276337\n",
      "iteration 33433: loss: 0.21759557723999023\n",
      "iteration 33434: loss: 0.21759538352489471\n",
      "iteration 33435: loss: 0.217595174908638\n",
      "iteration 33436: loss: 0.21759510040283203\n",
      "iteration 33437: loss: 0.21759498119354248\n",
      "iteration 33438: loss: 0.21759481728076935\n",
      "iteration 33439: loss: 0.21759457886219025\n",
      "iteration 33440: loss: 0.2175944596529007\n",
      "iteration 33441: loss: 0.21759434044361115\n",
      "iteration 33442: loss: 0.21759410202503204\n",
      "iteration 33443: loss: 0.2175939530134201\n",
      "iteration 33444: loss: 0.21759383380413055\n",
      "iteration 33445: loss: 0.2175936996936798\n",
      "iteration 33446: loss: 0.21759352087974548\n",
      "iteration 33447: loss: 0.21759335696697235\n",
      "iteration 33448: loss: 0.21759319305419922\n",
      "iteration 33449: loss: 0.21759304404258728\n",
      "iteration 33450: loss: 0.21759290993213654\n",
      "iteration 33451: loss: 0.21759271621704102\n",
      "iteration 33452: loss: 0.21759255230426788\n",
      "iteration 33453: loss: 0.21759240329265594\n",
      "iteration 33454: loss: 0.2175922691822052\n",
      "iteration 33455: loss: 0.21759214997291565\n",
      "iteration 33456: loss: 0.21759195625782013\n",
      "iteration 33457: loss: 0.2175918072462082\n",
      "iteration 33458: loss: 0.21759167313575745\n",
      "iteration 33459: loss: 0.21759159862995148\n",
      "iteration 33460: loss: 0.21759133040905\n",
      "iteration 33461: loss: 0.21759121119976044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 33462: loss: 0.2175910472869873\n",
      "iteration 33463: loss: 0.21759095788002014\n",
      "iteration 33464: loss: 0.21759065985679626\n",
      "iteration 33465: loss: 0.2175906002521515\n",
      "iteration 33466: loss: 0.21759042143821716\n",
      "iteration 33467: loss: 0.21759024262428284\n",
      "iteration 33468: loss: 0.2175901234149933\n",
      "iteration 33469: loss: 0.21758994460105896\n",
      "iteration 33470: loss: 0.21758981049060822\n",
      "iteration 33471: loss: 0.21758964657783508\n",
      "iteration 33472: loss: 0.21758945286273956\n",
      "iteration 33473: loss: 0.2175893783569336\n",
      "iteration 33474: loss: 0.21758916974067688\n",
      "iteration 33475: loss: 0.21758905053138733\n",
      "iteration 33476: loss: 0.21758881211280823\n",
      "iteration 33477: loss: 0.21758870780467987\n",
      "iteration 33478: loss: 0.21758854389190674\n",
      "iteration 33479: loss: 0.21758835017681122\n",
      "iteration 33480: loss: 0.21758826076984406\n",
      "iteration 33481: loss: 0.21758802235126495\n",
      "iteration 33482: loss: 0.21758794784545898\n",
      "iteration 33483: loss: 0.21758775413036346\n",
      "iteration 33484: loss: 0.2175876349210739\n",
      "iteration 33485: loss: 0.2175874412059784\n",
      "iteration 33486: loss: 0.21758732199668884\n",
      "iteration 33487: loss: 0.21758714318275452\n",
      "iteration 33488: loss: 0.2175869643688202\n",
      "iteration 33489: loss: 0.21758687496185303\n",
      "iteration 33490: loss: 0.2175867110490799\n",
      "iteration 33491: loss: 0.21758651733398438\n",
      "iteration 33492: loss: 0.2175864279270172\n",
      "iteration 33493: loss: 0.2175862342119217\n",
      "iteration 33494: loss: 0.21758601069450378\n",
      "iteration 33495: loss: 0.21758589148521423\n",
      "iteration 33496: loss: 0.21758577227592468\n",
      "iteration 33497: loss: 0.21758563816547394\n",
      "iteration 33498: loss: 0.21758541464805603\n",
      "iteration 33499: loss: 0.21758528053760529\n",
      "iteration 33500: loss: 0.21758511662483215\n",
      "iteration 33501: loss: 0.21758496761322021\n",
      "iteration 33502: loss: 0.21758480370044708\n",
      "iteration 33503: loss: 0.21758468449115753\n",
      "iteration 33504: loss: 0.217584490776062\n",
      "iteration 33505: loss: 0.21758434176445007\n",
      "iteration 33506: loss: 0.2175842523574829\n",
      "iteration 33507: loss: 0.2175840586423874\n",
      "iteration 33508: loss: 0.21758389472961426\n",
      "iteration 33509: loss: 0.21758374571800232\n",
      "iteration 33510: loss: 0.2175835818052292\n",
      "iteration 33511: loss: 0.21758341789245605\n",
      "iteration 33512: loss: 0.21758325397968292\n",
      "iteration 33513: loss: 0.21758310496807098\n",
      "iteration 33514: loss: 0.21758297085762024\n",
      "iteration 33515: loss: 0.2175828218460083\n",
      "iteration 33516: loss: 0.21758265793323517\n",
      "iteration 33517: loss: 0.21758249402046204\n",
      "iteration 33518: loss: 0.2175823450088501\n",
      "iteration 33519: loss: 0.21758219599723816\n",
      "iteration 33520: loss: 0.2175820767879486\n",
      "iteration 33521: loss: 0.21758191287517548\n",
      "iteration 33522: loss: 0.21758171916007996\n",
      "iteration 33523: loss: 0.21758155524730682\n",
      "iteration 33524: loss: 0.2175813913345337\n",
      "iteration 33525: loss: 0.21758131682872772\n",
      "iteration 33526: loss: 0.217581108212471\n",
      "iteration 33527: loss: 0.21758094429969788\n",
      "iteration 33528: loss: 0.21758083999156952\n",
      "iteration 33529: loss: 0.2175806313753128\n",
      "iteration 33530: loss: 0.21758051216602325\n",
      "iteration 33531: loss: 0.2175803929567337\n",
      "iteration 33532: loss: 0.2175801545381546\n",
      "iteration 33533: loss: 0.21758003532886505\n",
      "iteration 33534: loss: 0.2175798863172531\n",
      "iteration 33535: loss: 0.2175796777009964\n",
      "iteration 33536: loss: 0.21757948398590088\n",
      "iteration 33537: loss: 0.2175794392824173\n",
      "iteration 33538: loss: 0.21757924556732178\n",
      "iteration 33539: loss: 0.21757908165454865\n",
      "iteration 33540: loss: 0.2175789624452591\n",
      "iteration 33541: loss: 0.2175787389278412\n",
      "iteration 33542: loss: 0.21757864952087402\n",
      "iteration 33543: loss: 0.2175784856081009\n",
      "iteration 33544: loss: 0.21757832169532776\n",
      "iteration 33545: loss: 0.21757817268371582\n",
      "iteration 33546: loss: 0.21757808327674866\n",
      "iteration 33547: loss: 0.21757788956165314\n",
      "iteration 33548: loss: 0.21757769584655762\n",
      "iteration 33549: loss: 0.21757760643959045\n",
      "iteration 33550: loss: 0.21757741272449493\n",
      "iteration 33551: loss: 0.2175772488117218\n",
      "iteration 33552: loss: 0.21757706999778748\n",
      "iteration 33553: loss: 0.2175769805908203\n",
      "iteration 33554: loss: 0.21757681667804718\n",
      "iteration 33555: loss: 0.21757665276527405\n",
      "iteration 33556: loss: 0.2175765037536621\n",
      "iteration 33557: loss: 0.21757633984088898\n",
      "iteration 33558: loss: 0.21757617592811584\n",
      "iteration 33559: loss: 0.21757598221302032\n",
      "iteration 33560: loss: 0.21757587790489197\n",
      "iteration 33561: loss: 0.21757569909095764\n",
      "iteration 33562: loss: 0.2175755500793457\n",
      "iteration 33563: loss: 0.21757538616657257\n",
      "iteration 33564: loss: 0.21757522225379944\n",
      "iteration 33565: loss: 0.2175751030445099\n",
      "iteration 33566: loss: 0.21757487952709198\n",
      "iteration 33567: loss: 0.21757479012012482\n",
      "iteration 33568: loss: 0.21757462620735168\n",
      "iteration 33569: loss: 0.21757443249225616\n",
      "iteration 33570: loss: 0.2175743281841278\n",
      "iteration 33571: loss: 0.21757419407367706\n",
      "iteration 33572: loss: 0.21757397055625916\n",
      "iteration 33573: loss: 0.2175738513469696\n",
      "iteration 33574: loss: 0.21757371723651886\n",
      "iteration 33575: loss: 0.21757352352142334\n",
      "iteration 33576: loss: 0.2175733745098114\n",
      "iteration 33577: loss: 0.21757321059703827\n",
      "iteration 33578: loss: 0.2175731211900711\n",
      "iteration 33579: loss: 0.21757292747497559\n",
      "iteration 33580: loss: 0.21757283806800842\n",
      "iteration 33581: loss: 0.2175726443529129\n",
      "iteration 33582: loss: 0.21757249534130096\n",
      "iteration 33583: loss: 0.21757230162620544\n",
      "iteration 33584: loss: 0.21757221221923828\n",
      "iteration 33585: loss: 0.21757204830646515\n",
      "iteration 33586: loss: 0.21757182478904724\n",
      "iteration 33587: loss: 0.2175717055797577\n",
      "iteration 33588: loss: 0.21757152676582336\n",
      "iteration 33589: loss: 0.2175714075565338\n",
      "iteration 33590: loss: 0.2175712138414383\n",
      "iteration 33591: loss: 0.21757110953330994\n",
      "iteration 33592: loss: 0.2175709754228592\n",
      "iteration 33593: loss: 0.21757081151008606\n",
      "iteration 33594: loss: 0.21757066249847412\n",
      "iteration 33595: loss: 0.2175704538822174\n",
      "iteration 33596: loss: 0.21757033467292786\n",
      "iteration 33597: loss: 0.21757018566131592\n",
      "iteration 33598: loss: 0.2175699919462204\n",
      "iteration 33599: loss: 0.21756982803344727\n",
      "iteration 33600: loss: 0.2175697386264801\n",
      "iteration 33601: loss: 0.21756963431835175\n",
      "iteration 33602: loss: 0.21756942570209503\n",
      "iteration 33603: loss: 0.2175692766904831\n",
      "iteration 33604: loss: 0.217569038271904\n",
      "iteration 33605: loss: 0.21756896376609802\n",
      "iteration 33606: loss: 0.2175687998533249\n",
      "iteration 33607: loss: 0.21756859123706818\n",
      "iteration 33608: loss: 0.21756848692893982\n",
      "iteration 33609: loss: 0.21756836771965027\n",
      "iteration 33610: loss: 0.21756815910339355\n",
      "iteration 33611: loss: 0.21756799519062042\n",
      "iteration 33612: loss: 0.21756792068481445\n",
      "iteration 33613: loss: 0.21756775677204132\n",
      "iteration 33614: loss: 0.2175675928592682\n",
      "iteration 33615: loss: 0.21756744384765625\n",
      "iteration 33616: loss: 0.21756725013256073\n",
      "iteration 33617: loss: 0.21756716072559357\n",
      "iteration 33618: loss: 0.21756693720817566\n",
      "iteration 33619: loss: 0.2175668179988861\n",
      "iteration 33620: loss: 0.21756665408611298\n",
      "iteration 33621: loss: 0.21756649017333984\n",
      "iteration 33622: loss: 0.2175663709640503\n",
      "iteration 33623: loss: 0.21756616234779358\n",
      "iteration 33624: loss: 0.21756605803966522\n",
      "iteration 33625: loss: 0.21756592392921448\n",
      "iteration 33626: loss: 0.21756574511528015\n",
      "iteration 33627: loss: 0.21756556630134583\n",
      "iteration 33628: loss: 0.2175654172897339\n",
      "iteration 33629: loss: 0.21756520867347717\n",
      "iteration 33630: loss: 0.21756510436534882\n",
      "iteration 33631: loss: 0.21756494045257568\n",
      "iteration 33632: loss: 0.21756479144096375\n",
      "iteration 33633: loss: 0.217564657330513\n",
      "iteration 33634: loss: 0.21756449341773987\n",
      "iteration 33635: loss: 0.21756431460380554\n",
      "iteration 33636: loss: 0.2175641804933548\n",
      "iteration 33637: loss: 0.21756410598754883\n",
      "iteration 33638: loss: 0.21756389737129211\n",
      "iteration 33639: loss: 0.2175637185573578\n",
      "iteration 33640: loss: 0.21756359934806824\n",
      "iteration 33641: loss: 0.2175634652376175\n",
      "iteration 33642: loss: 0.21756324172019958\n",
      "iteration 33643: loss: 0.21756315231323242\n",
      "iteration 33644: loss: 0.21756291389465332\n",
      "iteration 33645: loss: 0.21756282448768616\n",
      "iteration 33646: loss: 0.21756264567375183\n",
      "iteration 33647: loss: 0.2175625115633011\n",
      "iteration 33648: loss: 0.21756234765052795\n",
      "iteration 33649: loss: 0.21756219863891602\n",
      "iteration 33650: loss: 0.21756207942962646\n",
      "iteration 33651: loss: 0.21756193041801453\n",
      "iteration 33652: loss: 0.2175617665052414\n",
      "iteration 33653: loss: 0.21756163239479065\n",
      "iteration 33654: loss: 0.21756143867969513\n",
      "iteration 33655: loss: 0.21756131947040558\n",
      "iteration 33656: loss: 0.21756115555763245\n",
      "iteration 33657: loss: 0.2175610065460205\n",
      "iteration 33658: loss: 0.21756084263324738\n",
      "iteration 33659: loss: 0.21756067872047424\n",
      "iteration 33660: loss: 0.2175605744123459\n",
      "iteration 33661: loss: 0.21756041049957275\n",
      "iteration 33662: loss: 0.21756020188331604\n",
      "iteration 33663: loss: 0.2175600826740265\n",
      "iteration 33664: loss: 0.21755990386009216\n",
      "iteration 33665: loss: 0.21755976974964142\n",
      "iteration 33666: loss: 0.21755962073802948\n",
      "iteration 33667: loss: 0.21755953133106232\n",
      "iteration 33668: loss: 0.2175593078136444\n",
      "iteration 33669: loss: 0.21755917370319366\n",
      "iteration 33670: loss: 0.21755895018577576\n",
      "iteration 33671: loss: 0.2175588309764862\n",
      "iteration 33672: loss: 0.21755871176719666\n",
      "iteration 33673: loss: 0.21755853295326233\n",
      "iteration 33674: loss: 0.2175583839416504\n",
      "iteration 33675: loss: 0.21755823493003845\n",
      "iteration 33676: loss: 0.2175581008195877\n",
      "iteration 33677: loss: 0.21755793690681458\n",
      "iteration 33678: loss: 0.21755775809288025\n",
      "iteration 33679: loss: 0.2175576388835907\n",
      "iteration 33680: loss: 0.21755746006965637\n",
      "iteration 33681: loss: 0.2175573855638504\n",
      "iteration 33682: loss: 0.2175571620464325\n",
      "iteration 33683: loss: 0.21755695343017578\n",
      "iteration 33684: loss: 0.2175568789243698\n",
      "iteration 33685: loss: 0.21755674481391907\n",
      "iteration 33686: loss: 0.21755659580230713\n",
      "iteration 33687: loss: 0.2175564467906952\n",
      "iteration 33688: loss: 0.21755631268024445\n",
      "iteration 33689: loss: 0.21755611896514893\n",
      "iteration 33690: loss: 0.21755599975585938\n",
      "iteration 33691: loss: 0.21755588054656982\n",
      "iteration 33692: loss: 0.21755561232566833\n",
      "iteration 33693: loss: 0.21755552291870117\n",
      "iteration 33694: loss: 0.21755540370941162\n",
      "iteration 33695: loss: 0.2175551950931549\n",
      "iteration 33696: loss: 0.21755504608154297\n",
      "iteration 33697: loss: 0.21755485236644745\n",
      "iteration 33698: loss: 0.21755468845367432\n",
      "iteration 33699: loss: 0.21755453944206238\n",
      "iteration 33700: loss: 0.21755445003509521\n",
      "iteration 33701: loss: 0.2175542563199997\n",
      "iteration 33702: loss: 0.21755412220954895\n",
      "iteration 33703: loss: 0.2175540030002594\n",
      "iteration 33704: loss: 0.21755385398864746\n",
      "iteration 33705: loss: 0.21755366027355194\n",
      "iteration 33706: loss: 0.2175535410642624\n",
      "iteration 33707: loss: 0.21755337715148926\n",
      "iteration 33708: loss: 0.21755321323871613\n",
      "iteration 33709: loss: 0.2175530642271042\n",
      "iteration 33710: loss: 0.21755290031433105\n",
      "iteration 33711: loss: 0.2175527811050415\n",
      "iteration 33712: loss: 0.21755261719226837\n",
      "iteration 33713: loss: 0.21755246818065643\n",
      "iteration 33714: loss: 0.2175522744655609\n",
      "iteration 33715: loss: 0.21755218505859375\n",
      "iteration 33716: loss: 0.2175520360469818\n",
      "iteration 33717: loss: 0.21755187213420868\n",
      "iteration 33718: loss: 0.21755167841911316\n",
      "iteration 33719: loss: 0.21755151450634003\n",
      "iteration 33720: loss: 0.21755146980285645\n",
      "iteration 33721: loss: 0.2175513505935669\n",
      "iteration 33722: loss: 0.21755118668079376\n",
      "iteration 33723: loss: 0.21755094826221466\n",
      "iteration 33724: loss: 0.2175508439540863\n",
      "iteration 33725: loss: 0.21755066514015198\n",
      "iteration 33726: loss: 0.21755056083202362\n",
      "iteration 33727: loss: 0.2175503969192505\n",
      "iteration 33728: loss: 0.21755020320415497\n",
      "iteration 33729: loss: 0.21755006909370422\n",
      "iteration 33730: loss: 0.2175498902797699\n",
      "iteration 33731: loss: 0.21754977107048035\n",
      "iteration 33732: loss: 0.21754956245422363\n",
      "iteration 33733: loss: 0.21754944324493408\n",
      "iteration 33734: loss: 0.21754932403564453\n",
      "iteration 33735: loss: 0.2175491750240326\n",
      "iteration 33736: loss: 0.21754899621009827\n",
      "iteration 33737: loss: 0.21754881739616394\n",
      "iteration 33738: loss: 0.2175486832857132\n",
      "iteration 33739: loss: 0.21754853427410126\n",
      "iteration 33740: loss: 0.2175484150648117\n",
      "iteration 33741: loss: 0.21754828095436096\n",
      "iteration 33742: loss: 0.21754808723926544\n",
      "iteration 33743: loss: 0.2175479382276535\n",
      "iteration 33744: loss: 0.2175477296113968\n",
      "iteration 33745: loss: 0.21754762530326843\n",
      "iteration 33746: loss: 0.2175474464893341\n",
      "iteration 33747: loss: 0.21754726767539978\n",
      "iteration 33748: loss: 0.21754717826843262\n",
      "iteration 33749: loss: 0.21754701435565948\n",
      "iteration 33750: loss: 0.21754691004753113\n",
      "iteration 33751: loss: 0.21754670143127441\n",
      "iteration 33752: loss: 0.21754658222198486\n",
      "iteration 33753: loss: 0.21754641830921173\n",
      "iteration 33754: loss: 0.21754629909992218\n",
      "iteration 33755: loss: 0.21754613518714905\n",
      "iteration 33756: loss: 0.2175459861755371\n",
      "iteration 33757: loss: 0.2175457775592804\n",
      "iteration 33758: loss: 0.21754562854766846\n",
      "iteration 33759: loss: 0.21754546463489532\n",
      "iteration 33760: loss: 0.2175453156232834\n",
      "iteration 33761: loss: 0.21754518151283264\n",
      "iteration 33762: loss: 0.21754512190818787\n",
      "iteration 33763: loss: 0.21754491329193115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 33764: loss: 0.2175447940826416\n",
      "iteration 33765: loss: 0.21754463016986847\n",
      "iteration 33766: loss: 0.21754443645477295\n",
      "iteration 33767: loss: 0.2175443172454834\n",
      "iteration 33768: loss: 0.21754419803619385\n",
      "iteration 33769: loss: 0.21754398941993713\n",
      "iteration 33770: loss: 0.2175438106060028\n",
      "iteration 33771: loss: 0.21754375100135803\n",
      "iteration 33772: loss: 0.2175436019897461\n",
      "iteration 33773: loss: 0.21754340827465057\n",
      "iteration 33774: loss: 0.21754327416419983\n",
      "iteration 33775: loss: 0.2175430953502655\n",
      "iteration 33776: loss: 0.2175428867340088\n",
      "iteration 33777: loss: 0.21754276752471924\n",
      "iteration 33778: loss: 0.2175426483154297\n",
      "iteration 33779: loss: 0.21754248440265656\n",
      "iteration 33780: loss: 0.217542365193367\n",
      "iteration 33781: loss: 0.21754220128059387\n",
      "iteration 33782: loss: 0.21754200756549835\n",
      "iteration 33783: loss: 0.2175418585538864\n",
      "iteration 33784: loss: 0.21754178404808044\n",
      "iteration 33785: loss: 0.21754160523414612\n",
      "iteration 33786: loss: 0.21754145622253418\n",
      "iteration 33787: loss: 0.21754130721092224\n",
      "iteration 33788: loss: 0.21754112839698792\n",
      "iteration 33789: loss: 0.21754097938537598\n",
      "iteration 33790: loss: 0.21754081547260284\n",
      "iteration 33791: loss: 0.2175406664609909\n",
      "iteration 33792: loss: 0.21754062175750732\n",
      "iteration 33793: loss: 0.2175404280424118\n",
      "iteration 33794: loss: 0.21754026412963867\n",
      "iteration 33795: loss: 0.21754010021686554\n",
      "iteration 33796: loss: 0.21753999590873718\n",
      "iteration 33797: loss: 0.21753978729248047\n",
      "iteration 33798: loss: 0.2175397127866745\n",
      "iteration 33799: loss: 0.21753957867622375\n",
      "iteration 33800: loss: 0.21753938496112823\n",
      "iteration 33801: loss: 0.2175392210483551\n",
      "iteration 33802: loss: 0.21753904223442078\n",
      "iteration 33803: loss: 0.21753890812397003\n",
      "iteration 33804: loss: 0.2175387591123581\n",
      "iteration 33805: loss: 0.21753859519958496\n",
      "iteration 33806: loss: 0.21753844618797302\n",
      "iteration 33807: loss: 0.21753831207752228\n",
      "iteration 33808: loss: 0.21753811836242676\n",
      "iteration 33809: loss: 0.2175380289554596\n",
      "iteration 33810: loss: 0.21753783524036407\n",
      "iteration 33811: loss: 0.21753767132759094\n",
      "iteration 33812: loss: 0.2175375521183014\n",
      "iteration 33813: loss: 0.21753740310668945\n",
      "iteration 33814: loss: 0.21753723919391632\n",
      "iteration 33815: loss: 0.21753711998462677\n",
      "iteration 33816: loss: 0.21753697097301483\n",
      "iteration 33817: loss: 0.2175368070602417\n",
      "iteration 33818: loss: 0.21753664314746857\n",
      "iteration 33819: loss: 0.2175365388393402\n",
      "iteration 33820: loss: 0.2175363302230835\n",
      "iteration 33821: loss: 0.21753621101379395\n",
      "iteration 33822: loss: 0.2175360471010208\n",
      "iteration 33823: loss: 0.2175358235836029\n",
      "iteration 33824: loss: 0.21753573417663574\n",
      "iteration 33825: loss: 0.21753564476966858\n",
      "iteration 33826: loss: 0.2175353765487671\n",
      "iteration 33827: loss: 0.21753528714179993\n",
      "iteration 33828: loss: 0.217535138130188\n",
      "iteration 33829: loss: 0.21753497421741486\n",
      "iteration 33830: loss: 0.2175348699092865\n",
      "iteration 33831: loss: 0.21753475069999695\n",
      "iteration 33832: loss: 0.21753454208374023\n",
      "iteration 33833: loss: 0.2175343930721283\n",
      "iteration 33834: loss: 0.21753425896167755\n",
      "iteration 33835: loss: 0.21753403544425964\n",
      "iteration 33836: loss: 0.2175339162349701\n",
      "iteration 33837: loss: 0.21753378212451935\n",
      "iteration 33838: loss: 0.2175336629152298\n",
      "iteration 33839: loss: 0.2175334393978119\n",
      "iteration 33840: loss: 0.21753337979316711\n",
      "iteration 33841: loss: 0.21753326058387756\n",
      "iteration 33842: loss: 0.21753311157226562\n",
      "iteration 33843: loss: 0.21753284335136414\n",
      "iteration 33844: loss: 0.21753272414207458\n",
      "iteration 33845: loss: 0.21753260493278503\n",
      "iteration 33846: loss: 0.2175324410200119\n",
      "iteration 33847: loss: 0.21753230690956116\n",
      "iteration 33848: loss: 0.2175321877002716\n",
      "iteration 33849: loss: 0.2175319939851761\n",
      "iteration 33850: loss: 0.21753187477588654\n",
      "iteration 33851: loss: 0.21753168106079102\n",
      "iteration 33852: loss: 0.21753151714801788\n",
      "iteration 33853: loss: 0.21753141283988953\n",
      "iteration 33854: loss: 0.2175312042236328\n",
      "iteration 33855: loss: 0.21753108501434326\n",
      "iteration 33856: loss: 0.2175309658050537\n",
      "iteration 33857: loss: 0.21753081679344177\n",
      "iteration 33858: loss: 0.21753065288066864\n",
      "iteration 33859: loss: 0.2175304889678955\n",
      "iteration 33860: loss: 0.2175302952528\n",
      "iteration 33861: loss: 0.21753017604351044\n",
      "iteration 33862: loss: 0.2175300419330597\n",
      "iteration 33863: loss: 0.21752986311912537\n",
      "iteration 33864: loss: 0.2175297737121582\n",
      "iteration 33865: loss: 0.21752962470054626\n",
      "iteration 33866: loss: 0.21752949059009552\n",
      "iteration 33867: loss: 0.2175292670726776\n",
      "iteration 33868: loss: 0.21752920746803284\n",
      "iteration 33869: loss: 0.21752901375293732\n",
      "iteration 33870: loss: 0.21752884984016418\n",
      "iteration 33871: loss: 0.21752873063087463\n",
      "iteration 33872: loss: 0.2175285816192627\n",
      "iteration 33873: loss: 0.21752841770648956\n",
      "iteration 33874: loss: 0.21752825379371643\n",
      "iteration 33875: loss: 0.2175281047821045\n",
      "iteration 33876: loss: 0.21752795577049255\n",
      "iteration 33877: loss: 0.2175278663635254\n",
      "iteration 33878: loss: 0.21752774715423584\n",
      "iteration 33879: loss: 0.21752753853797913\n",
      "iteration 33880: loss: 0.2175273448228836\n",
      "iteration 33881: loss: 0.21752719581127167\n",
      "iteration 33882: loss: 0.2175271064043045\n",
      "iteration 33883: loss: 0.2175268828868866\n",
      "iteration 33884: loss: 0.21752679347991943\n",
      "iteration 33885: loss: 0.2175265997648239\n",
      "iteration 33886: loss: 0.21752646565437317\n",
      "iteration 33887: loss: 0.21752634644508362\n",
      "iteration 33888: loss: 0.2175261527299881\n",
      "iteration 33889: loss: 0.21752604842185974\n",
      "iteration 33890: loss: 0.21752586960792542\n",
      "iteration 33891: loss: 0.2175256758928299\n",
      "iteration 33892: loss: 0.21752557158470154\n",
      "iteration 33893: loss: 0.2175254374742508\n",
      "iteration 33894: loss: 0.21752531826496124\n",
      "iteration 33895: loss: 0.2175251692533493\n",
      "iteration 33896: loss: 0.21752497553825378\n",
      "iteration 33897: loss: 0.21752481162548065\n",
      "iteration 33898: loss: 0.2175247222185135\n",
      "iteration 33899: loss: 0.21752449870109558\n",
      "iteration 33900: loss: 0.21752440929412842\n",
      "iteration 33901: loss: 0.21752426028251648\n",
      "iteration 33902: loss: 0.21752405166625977\n",
      "iteration 33903: loss: 0.21752402186393738\n",
      "iteration 33904: loss: 0.21752385795116425\n",
      "iteration 33905: loss: 0.21752361953258514\n",
      "iteration 33906: loss: 0.2175234854221344\n",
      "iteration 33907: loss: 0.21752338111400604\n",
      "iteration 33908: loss: 0.21752317249774933\n",
      "iteration 33909: loss: 0.21752305328845978\n",
      "iteration 33910: loss: 0.21752285957336426\n",
      "iteration 33911: loss: 0.2175227403640747\n",
      "iteration 33912: loss: 0.21752257645130157\n",
      "iteration 33913: loss: 0.21752247214317322\n",
      "iteration 33914: loss: 0.21752230823040009\n",
      "iteration 33915: loss: 0.21752221882343292\n",
      "iteration 33916: loss: 0.2175220251083374\n",
      "iteration 33917: loss: 0.21752190589904785\n",
      "iteration 33918: loss: 0.21752174198627472\n",
      "iteration 33919: loss: 0.2175215780735016\n",
      "iteration 33920: loss: 0.21752142906188965\n",
      "iteration 33921: loss: 0.21752122044563293\n",
      "iteration 33922: loss: 0.21752116084098816\n",
      "iteration 33923: loss: 0.21752099692821503\n",
      "iteration 33924: loss: 0.2175207883119583\n",
      "iteration 33925: loss: 0.21752062439918518\n",
      "iteration 33926: loss: 0.2175205498933792\n",
      "iteration 33927: loss: 0.21752038598060608\n",
      "iteration 33928: loss: 0.21752023696899414\n",
      "iteration 33929: loss: 0.2175200879573822\n",
      "iteration 33930: loss: 0.21751992404460907\n",
      "iteration 33931: loss: 0.21751976013183594\n",
      "iteration 33932: loss: 0.217519611120224\n",
      "iteration 33933: loss: 0.21751947700977325\n",
      "iteration 33934: loss: 0.2175193578004837\n",
      "iteration 33935: loss: 0.21751920878887177\n",
      "iteration 33936: loss: 0.21751901507377625\n",
      "iteration 33937: loss: 0.2175188958644867\n",
      "iteration 33938: loss: 0.21751871705055237\n",
      "iteration 33939: loss: 0.21751859784126282\n",
      "iteration 33940: loss: 0.2175184190273285\n",
      "iteration 33941: loss: 0.21751832962036133\n",
      "iteration 33942: loss: 0.2175181359052658\n",
      "iteration 33943: loss: 0.21751806139945984\n",
      "iteration 33944: loss: 0.21751785278320312\n",
      "iteration 33945: loss: 0.2175177037715912\n",
      "iteration 33946: loss: 0.21751756966114044\n",
      "iteration 33947: loss: 0.2175174206495285\n",
      "iteration 33948: loss: 0.21751725673675537\n",
      "iteration 33949: loss: 0.21751706302165985\n",
      "iteration 33950: loss: 0.21751698851585388\n",
      "iteration 33951: loss: 0.21751685440540314\n",
      "iteration 33952: loss: 0.21751663088798523\n",
      "iteration 33953: loss: 0.21751651167869568\n",
      "iteration 33954: loss: 0.21751634776592255\n",
      "iteration 33955: loss: 0.217516228556633\n",
      "iteration 33956: loss: 0.21751613914966583\n",
      "iteration 33957: loss: 0.2175159901380539\n",
      "iteration 33958: loss: 0.21751578152179718\n",
      "iteration 33959: loss: 0.21751566231250763\n",
      "iteration 33960: loss: 0.21751554310321808\n",
      "iteration 33961: loss: 0.21751537919044495\n",
      "iteration 33962: loss: 0.21751520037651062\n",
      "iteration 33963: loss: 0.21751506626605988\n",
      "iteration 33964: loss: 0.21751490235328674\n",
      "iteration 33965: loss: 0.2175147831439972\n",
      "iteration 33966: loss: 0.21751455962657928\n",
      "iteration 33967: loss: 0.21751442551612854\n",
      "iteration 33968: loss: 0.21751435101032257\n",
      "iteration 33969: loss: 0.21751411259174347\n",
      "iteration 33970: loss: 0.2175140082836151\n",
      "iteration 33971: loss: 0.2175137996673584\n",
      "iteration 33972: loss: 0.21751371026039124\n",
      "iteration 33973: loss: 0.21751360595226288\n",
      "iteration 33974: loss: 0.2175132930278778\n",
      "iteration 33975: loss: 0.21751324832439423\n",
      "iteration 33976: loss: 0.21751312911510468\n",
      "iteration 33977: loss: 0.21751300990581512\n",
      "iteration 33978: loss: 0.21751287579536438\n",
      "iteration 33979: loss: 0.21751265227794647\n",
      "iteration 33980: loss: 0.21751245856285095\n",
      "iteration 33981: loss: 0.21751239895820618\n",
      "iteration 33982: loss: 0.21751224994659424\n",
      "iteration 33983: loss: 0.2175120860338211\n",
      "iteration 33984: loss: 0.21751193702220917\n",
      "iteration 33985: loss: 0.21751180291175842\n",
      "iteration 33986: loss: 0.21751165390014648\n",
      "iteration 33987: loss: 0.21751156449317932\n",
      "iteration 33988: loss: 0.2175113707780838\n",
      "iteration 33989: loss: 0.21751117706298828\n",
      "iteration 33990: loss: 0.21751105785369873\n",
      "iteration 33991: loss: 0.2175108939409256\n",
      "iteration 33992: loss: 0.21751073002815247\n",
      "iteration 33993: loss: 0.21751061081886292\n",
      "iteration 33994: loss: 0.21751046180725098\n",
      "iteration 33995: loss: 0.2175103724002838\n",
      "iteration 33996: loss: 0.21751022338867188\n",
      "iteration 33997: loss: 0.21751002967357635\n",
      "iteration 33998: loss: 0.21750983595848083\n",
      "iteration 33999: loss: 0.21750974655151367\n",
      "iteration 34000: loss: 0.21750955283641815\n",
      "iteration 34001: loss: 0.2175094187259674\n",
      "iteration 34002: loss: 0.21750929951667786\n",
      "iteration 34003: loss: 0.21750915050506592\n",
      "iteration 34004: loss: 0.21750900149345398\n",
      "iteration 34005: loss: 0.21750882267951965\n",
      "iteration 34006: loss: 0.2175087034702301\n",
      "iteration 34007: loss: 0.21750852465629578\n",
      "iteration 34008: loss: 0.21750836074352264\n",
      "iteration 34009: loss: 0.21750827133655548\n",
      "iteration 34010: loss: 0.21750807762145996\n",
      "iteration 34011: loss: 0.217508003115654\n",
      "iteration 34012: loss: 0.21750780940055847\n",
      "iteration 34013: loss: 0.21750769019126892\n",
      "iteration 34014: loss: 0.21750755608081818\n",
      "iteration 34015: loss: 0.21750736236572266\n",
      "iteration 34016: loss: 0.2175072729587555\n",
      "iteration 34017: loss: 0.21750704944133759\n",
      "iteration 34018: loss: 0.21750688552856445\n",
      "iteration 34019: loss: 0.2175067961215973\n",
      "iteration 34020: loss: 0.21750664710998535\n",
      "iteration 34021: loss: 0.2175065279006958\n",
      "iteration 34022: loss: 0.21750636398792267\n",
      "iteration 34023: loss: 0.21750617027282715\n",
      "iteration 34024: loss: 0.21750609576702118\n",
      "iteration 34025: loss: 0.21750596165657043\n",
      "iteration 34026: loss: 0.21750573813915253\n",
      "iteration 34027: loss: 0.21750560402870178\n",
      "iteration 34028: loss: 0.21750545501708984\n",
      "iteration 34029: loss: 0.2175053060054779\n",
      "iteration 34030: loss: 0.21750521659851074\n",
      "iteration 34031: loss: 0.21750500798225403\n",
      "iteration 34032: loss: 0.2175048142671585\n",
      "iteration 34033: loss: 0.21750470995903015\n",
      "iteration 34034: loss: 0.2175045758485794\n",
      "iteration 34035: loss: 0.21750445663928986\n",
      "iteration 34036: loss: 0.21750429272651672\n",
      "iteration 34037: loss: 0.21750417351722717\n",
      "iteration 34038: loss: 0.21750393509864807\n",
      "iteration 34039: loss: 0.2175038605928421\n",
      "iteration 34040: loss: 0.21750369668006897\n",
      "iteration 34041: loss: 0.2175035923719406\n",
      "iteration 34042: loss: 0.21750345826148987\n",
      "iteration 34043: loss: 0.21750326454639435\n",
      "iteration 34044: loss: 0.2175031155347824\n",
      "iteration 34045: loss: 0.2175029218196869\n",
      "iteration 34046: loss: 0.21750283241271973\n",
      "iteration 34047: loss: 0.2175026386976242\n",
      "iteration 34048: loss: 0.21750254929065704\n",
      "iteration 34049: loss: 0.2175024002790451\n",
      "iteration 34050: loss: 0.21750228106975555\n",
      "iteration 34051: loss: 0.21750207245349884\n",
      "iteration 34052: loss: 0.2175019234418869\n",
      "iteration 34053: loss: 0.21750178933143616\n",
      "iteration 34054: loss: 0.21750164031982422\n",
      "iteration 34055: loss: 0.21750149130821228\n",
      "iteration 34056: loss: 0.21750132739543915\n",
      "iteration 34057: loss: 0.21750123798847198\n",
      "iteration 34058: loss: 0.21750107407569885\n",
      "iteration 34059: loss: 0.2175009548664093\n",
      "iteration 34060: loss: 0.21750080585479736\n",
      "iteration 34061: loss: 0.2175006866455078\n",
      "iteration 34062: loss: 0.21750040352344513\n",
      "iteration 34063: loss: 0.21750032901763916\n",
      "iteration 34064: loss: 0.21750016510486603\n",
      "iteration 34065: loss: 0.21750006079673767\n",
      "iteration 34066: loss: 0.21749992668628693\n",
      "iteration 34067: loss: 0.2174997329711914\n",
      "iteration 34068: loss: 0.21749961376190186\n",
      "iteration 34069: loss: 0.21749944984912872\n",
      "iteration 34070: loss: 0.2174992561340332\n",
      "iteration 34071: loss: 0.21749916672706604\n",
      "iteration 34072: loss: 0.2174990177154541\n",
      "iteration 34073: loss: 0.21749892830848694\n",
      "iteration 34074: loss: 0.21749873459339142\n",
      "iteration 34075: loss: 0.21749858558177948\n",
      "iteration 34076: loss: 0.21749842166900635\n",
      "iteration 34077: loss: 0.21749833226203918\n",
      "iteration 34078: loss: 0.21749815344810486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 34079: loss: 0.2174980193376541\n",
      "iteration 34080: loss: 0.2174977958202362\n",
      "iteration 34081: loss: 0.21749763190746307\n",
      "iteration 34082: loss: 0.21749761700630188\n",
      "iteration 34083: loss: 0.21749739348888397\n",
      "iteration 34084: loss: 0.21749725937843323\n",
      "iteration 34085: loss: 0.2174971103668213\n",
      "iteration 34086: loss: 0.21749694645404816\n",
      "iteration 34087: loss: 0.21749679744243622\n",
      "iteration 34088: loss: 0.21749667823314667\n",
      "iteration 34089: loss: 0.21749648451805115\n",
      "iteration 34090: loss: 0.21749639511108398\n",
      "iteration 34091: loss: 0.21749627590179443\n",
      "iteration 34092: loss: 0.2174961119890213\n",
      "iteration 34093: loss: 0.21749600768089294\n",
      "iteration 34094: loss: 0.21749579906463623\n",
      "iteration 34095: loss: 0.2174956053495407\n",
      "iteration 34096: loss: 0.21749551594257355\n",
      "iteration 34097: loss: 0.217495396733284\n",
      "iteration 34098: loss: 0.21749523282051086\n",
      "iteration 34099: loss: 0.21749508380889893\n",
      "iteration 34100: loss: 0.217494934797287\n",
      "iteration 34101: loss: 0.21749477088451385\n",
      "iteration 34102: loss: 0.2174946367740631\n",
      "iteration 34103: loss: 0.21749445796012878\n",
      "iteration 34104: loss: 0.21749433875083923\n",
      "iteration 34105: loss: 0.2174942046403885\n",
      "iteration 34106: loss: 0.21749404072761536\n",
      "iteration 34107: loss: 0.21749389171600342\n",
      "iteration 34108: loss: 0.21749381721019745\n",
      "iteration 34109: loss: 0.21749362349510193\n",
      "iteration 34110: loss: 0.21749348938465118\n",
      "iteration 34111: loss: 0.21749329566955566\n",
      "iteration 34112: loss: 0.2174931764602661\n",
      "iteration 34113: loss: 0.21749301254749298\n",
      "iteration 34114: loss: 0.21749284863471985\n",
      "iteration 34115: loss: 0.21749281883239746\n",
      "iteration 34116: loss: 0.21749262511730194\n",
      "iteration 34117: loss: 0.21749243140220642\n",
      "iteration 34118: loss: 0.21749237179756165\n",
      "iteration 34119: loss: 0.21749213337898254\n",
      "iteration 34120: loss: 0.217492014169693\n",
      "iteration 34121: loss: 0.21749186515808105\n",
      "iteration 34122: loss: 0.21749171614646912\n",
      "iteration 34123: loss: 0.21749158203601837\n",
      "iteration 34124: loss: 0.21749143302440643\n",
      "iteration 34125: loss: 0.2174912989139557\n",
      "iteration 34126: loss: 0.21749114990234375\n",
      "iteration 34127: loss: 0.2174910008907318\n",
      "iteration 34128: loss: 0.2174907922744751\n",
      "iteration 34129: loss: 0.21749074757099152\n",
      "iteration 34130: loss: 0.21749059855937958\n",
      "iteration 34131: loss: 0.21749038994312286\n",
      "iteration 34132: loss: 0.2174902707338333\n",
      "iteration 34133: loss: 0.21749015152454376\n",
      "iteration 34134: loss: 0.21748992800712585\n",
      "iteration 34135: loss: 0.2174898087978363\n",
      "iteration 34136: loss: 0.21748968958854675\n",
      "iteration 34137: loss: 0.217489555478096\n",
      "iteration 34138: loss: 0.2174893617630005\n",
      "iteration 34139: loss: 0.21748919785022736\n",
      "iteration 34140: loss: 0.2174891233444214\n",
      "iteration 34141: loss: 0.21748895943164825\n",
      "iteration 34142: loss: 0.21748876571655273\n",
      "iteration 34143: loss: 0.21748867630958557\n",
      "iteration 34144: loss: 0.21748855710029602\n",
      "iteration 34145: loss: 0.2174883633852005\n",
      "iteration 34146: loss: 0.21748824417591095\n",
      "iteration 34147: loss: 0.217488095164299\n",
      "iteration 34148: loss: 0.21748793125152588\n",
      "iteration 34149: loss: 0.21748784184455872\n",
      "iteration 34150: loss: 0.2174876183271408\n",
      "iteration 34151: loss: 0.21748752892017365\n",
      "iteration 34152: loss: 0.21748733520507812\n",
      "iteration 34153: loss: 0.21748724579811096\n",
      "iteration 34154: loss: 0.21748705208301544\n",
      "iteration 34155: loss: 0.2174869030714035\n",
      "iteration 34156: loss: 0.21748682856559753\n",
      "iteration 34157: loss: 0.2174866646528244\n",
      "iteration 34158: loss: 0.21748650074005127\n",
      "iteration 34159: loss: 0.21748638153076172\n",
      "iteration 34160: loss: 0.2174862176179886\n",
      "iteration 34161: loss: 0.21748606860637665\n",
      "iteration 34162: loss: 0.2174859493970871\n",
      "iteration 34163: loss: 0.21748574078083038\n",
      "iteration 34164: loss: 0.21748562157154083\n",
      "iteration 34165: loss: 0.2174854725599289\n",
      "iteration 34166: loss: 0.21748533844947815\n",
      "iteration 34167: loss: 0.21748515963554382\n",
      "iteration 34168: loss: 0.21748504042625427\n",
      "iteration 34169: loss: 0.21748492121696472\n",
      "iteration 34170: loss: 0.21748478710651398\n",
      "iteration 34171: loss: 0.21748459339141846\n",
      "iteration 34172: loss: 0.21748444437980652\n",
      "iteration 34173: loss: 0.2174842804670334\n",
      "iteration 34174: loss: 0.21748416125774384\n",
      "iteration 34175: loss: 0.21748396754264832\n",
      "iteration 34176: loss: 0.21748390793800354\n",
      "iteration 34177: loss: 0.21748371422290802\n",
      "iteration 34178: loss: 0.21748356521129608\n",
      "iteration 34179: loss: 0.21748343110084534\n",
      "iteration 34180: loss: 0.2174832820892334\n",
      "iteration 34181: loss: 0.21748316287994385\n",
      "iteration 34182: loss: 0.2174830436706543\n",
      "iteration 34183: loss: 0.21748287975788116\n",
      "iteration 34184: loss: 0.2174827605485916\n",
      "iteration 34185: loss: 0.21748261153697968\n",
      "iteration 34186: loss: 0.21748244762420654\n",
      "iteration 34187: loss: 0.2174822837114334\n",
      "iteration 34188: loss: 0.21748213469982147\n",
      "iteration 34189: loss: 0.21748200058937073\n",
      "iteration 34190: loss: 0.21748188138008118\n",
      "iteration 34191: loss: 0.21748170256614685\n",
      "iteration 34192: loss: 0.2174815684556961\n",
      "iteration 34193: loss: 0.21748140454292297\n",
      "iteration 34194: loss: 0.21748128533363342\n",
      "iteration 34195: loss: 0.21748116612434387\n",
      "iteration 34196: loss: 0.21748104691505432\n",
      "iteration 34197: loss: 0.2174808233976364\n",
      "iteration 34198: loss: 0.21748065948486328\n",
      "iteration 34199: loss: 0.2174805849790573\n",
      "iteration 34200: loss: 0.2174803763628006\n",
      "iteration 34201: loss: 0.21748027205467224\n",
      "iteration 34202: loss: 0.21748018264770508\n",
      "iteration 34203: loss: 0.21747994422912598\n",
      "iteration 34204: loss: 0.21747978031635284\n",
      "iteration 34205: loss: 0.2174796313047409\n",
      "iteration 34206: loss: 0.21747951209545135\n",
      "iteration 34207: loss: 0.2174793928861618\n",
      "iteration 34208: loss: 0.21747927367687225\n",
      "iteration 34209: loss: 0.21747907996177673\n",
      "iteration 34210: loss: 0.21747902035713196\n",
      "iteration 34211: loss: 0.21747887134552002\n",
      "iteration 34212: loss: 0.2174787074327469\n",
      "iteration 34213: loss: 0.21747855842113495\n",
      "iteration 34214: loss: 0.21747834980487823\n",
      "iteration 34215: loss: 0.21747824549674988\n",
      "iteration 34216: loss: 0.21747811138629913\n",
      "iteration 34217: loss: 0.2174779623746872\n",
      "iteration 34218: loss: 0.21747776865959167\n",
      "iteration 34219: loss: 0.2174777239561081\n",
      "iteration 34220: loss: 0.21747751533985138\n",
      "iteration 34221: loss: 0.21747739613056183\n",
      "iteration 34222: loss: 0.2174772322177887\n",
      "iteration 34223: loss: 0.21747712790966034\n",
      "iteration 34224: loss: 0.2174769937992096\n",
      "iteration 34225: loss: 0.21747687458992004\n",
      "iteration 34226: loss: 0.21747663617134094\n",
      "iteration 34227: loss: 0.2174765169620514\n",
      "iteration 34228: loss: 0.21747639775276184\n",
      "iteration 34229: loss: 0.2174762785434723\n",
      "iteration 34230: loss: 0.21747605502605438\n",
      "iteration 34231: loss: 0.21747592091560364\n",
      "iteration 34232: loss: 0.2174758017063141\n",
      "iteration 34233: loss: 0.21747565269470215\n",
      "iteration 34234: loss: 0.21747544407844543\n",
      "iteration 34235: loss: 0.21747533977031708\n",
      "iteration 34236: loss: 0.21747520565986633\n",
      "iteration 34237: loss: 0.21747508645057678\n",
      "iteration 34238: loss: 0.21747493743896484\n",
      "iteration 34239: loss: 0.2174747884273529\n",
      "iteration 34240: loss: 0.2174745798110962\n",
      "iteration 34241: loss: 0.21747449040412903\n",
      "iteration 34242: loss: 0.2174743115901947\n",
      "iteration 34243: loss: 0.21747419238090515\n",
      "iteration 34244: loss: 0.2174740582704544\n",
      "iteration 34245: loss: 0.21747395396232605\n",
      "iteration 34246: loss: 0.2174738347530365\n",
      "iteration 34247: loss: 0.21747362613677979\n",
      "iteration 34248: loss: 0.21747341752052307\n",
      "iteration 34249: loss: 0.2174733579158783\n",
      "iteration 34250: loss: 0.21747323870658875\n",
      "iteration 34251: loss: 0.21747305989265442\n",
      "iteration 34252: loss: 0.21747294068336487\n",
      "iteration 34253: loss: 0.21747274696826935\n",
      "iteration 34254: loss: 0.21747258305549622\n",
      "iteration 34255: loss: 0.21747250854969025\n",
      "iteration 34256: loss: 0.2174723595380783\n",
      "iteration 34257: loss: 0.21747219562530518\n",
      "iteration 34258: loss: 0.21747203171253204\n",
      "iteration 34259: loss: 0.2174719125032425\n",
      "iteration 34260: loss: 0.21747179329395294\n",
      "iteration 34261: loss: 0.21747159957885742\n",
      "iteration 34262: loss: 0.21747148036956787\n",
      "iteration 34263: loss: 0.2174713909626007\n",
      "iteration 34264: loss: 0.21747121214866638\n",
      "iteration 34265: loss: 0.21747107803821564\n",
      "iteration 34266: loss: 0.21747088432312012\n",
      "iteration 34267: loss: 0.21747076511383057\n",
      "iteration 34268: loss: 0.21747061610221863\n",
      "iteration 34269: loss: 0.2174704372882843\n",
      "iteration 34270: loss: 0.21747036278247833\n",
      "iteration 34271: loss: 0.21747009456157684\n",
      "iteration 34272: loss: 0.21747002005577087\n",
      "iteration 34273: loss: 0.2174699604511261\n",
      "iteration 34274: loss: 0.217469722032547\n",
      "iteration 34275: loss: 0.21746960282325745\n",
      "iteration 34276: loss: 0.21746954321861267\n",
      "iteration 34277: loss: 0.21746928989887238\n",
      "iteration 34278: loss: 0.21746917068958282\n",
      "iteration 34279: loss: 0.2174690216779709\n",
      "iteration 34280: loss: 0.21746885776519775\n",
      "iteration 34281: loss: 0.21746870875358582\n",
      "iteration 34282: loss: 0.21746861934661865\n",
      "iteration 34283: loss: 0.21746845543384552\n",
      "iteration 34284: loss: 0.21746830642223358\n",
      "iteration 34285: loss: 0.21746818721294403\n",
      "iteration 34286: loss: 0.2174679934978485\n",
      "iteration 34287: loss: 0.21746782958507538\n",
      "iteration 34288: loss: 0.21746771037578583\n",
      "iteration 34289: loss: 0.21746762096881866\n",
      "iteration 34290: loss: 0.21746745705604553\n",
      "iteration 34291: loss: 0.2174673080444336\n",
      "iteration 34292: loss: 0.21746715903282166\n",
      "iteration 34293: loss: 0.2174670696258545\n",
      "iteration 34294: loss: 0.21746686100959778\n",
      "iteration 34295: loss: 0.21746671199798584\n",
      "iteration 34296: loss: 0.21746662259101868\n",
      "iteration 34297: loss: 0.21746647357940674\n",
      "iteration 34298: loss: 0.2174663096666336\n",
      "iteration 34299: loss: 0.21746614575386047\n",
      "iteration 34300: loss: 0.21746604144573212\n",
      "iteration 34301: loss: 0.21746592223644257\n",
      "iteration 34302: loss: 0.21746572852134705\n",
      "iteration 34303: loss: 0.21746554970741272\n",
      "iteration 34304: loss: 0.21746547520160675\n",
      "iteration 34305: loss: 0.21746525168418884\n",
      "iteration 34306: loss: 0.21746516227722168\n",
      "iteration 34307: loss: 0.21746501326560974\n",
      "iteration 34308: loss: 0.217464879155159\n",
      "iteration 34309: loss: 0.21746475994586945\n",
      "iteration 34310: loss: 0.2174646109342575\n",
      "iteration 34311: loss: 0.2174644023180008\n",
      "iteration 34312: loss: 0.2174643725156784\n",
      "iteration 34313: loss: 0.21746417880058289\n",
      "iteration 34314: loss: 0.21746405959129333\n",
      "iteration 34315: loss: 0.21746382117271423\n",
      "iteration 34316: loss: 0.21746373176574707\n",
      "iteration 34317: loss: 0.21746358275413513\n",
      "iteration 34318: loss: 0.21746349334716797\n",
      "iteration 34319: loss: 0.21746332943439484\n",
      "iteration 34320: loss: 0.2174631655216217\n",
      "iteration 34321: loss: 0.21746301651000977\n",
      "iteration 34322: loss: 0.21746286749839783\n",
      "iteration 34323: loss: 0.21746273338794708\n",
      "iteration 34324: loss: 0.21746262907981873\n",
      "iteration 34325: loss: 0.21746239066123962\n",
      "iteration 34326: loss: 0.21746233105659485\n",
      "iteration 34327: loss: 0.21746213734149933\n",
      "iteration 34328: loss: 0.21746210753917694\n",
      "iteration 34329: loss: 0.21746189892292023\n",
      "iteration 34330: loss: 0.21746167540550232\n",
      "iteration 34331: loss: 0.21746163070201874\n",
      "iteration 34332: loss: 0.21746139228343964\n",
      "iteration 34333: loss: 0.21746130287647247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 34334: loss: 0.21746115386486053\n",
      "iteration 34335: loss: 0.21746106445789337\n",
      "iteration 34336: loss: 0.21746084094047546\n",
      "iteration 34337: loss: 0.2174607515335083\n",
      "iteration 34338: loss: 0.21746063232421875\n",
      "iteration 34339: loss: 0.21746046841144562\n",
      "iteration 34340: loss: 0.2174602746963501\n",
      "iteration 34341: loss: 0.21746015548706055\n",
      "iteration 34342: loss: 0.21746006608009338\n",
      "iteration 34343: loss: 0.21745991706848145\n",
      "iteration 34344: loss: 0.21745970845222473\n",
      "iteration 34345: loss: 0.21745958924293518\n",
      "iteration 34346: loss: 0.21745947003364563\n",
      "iteration 34347: loss: 0.21745935082435608\n",
      "iteration 34348: loss: 0.21745917201042175\n",
      "iteration 34349: loss: 0.217459037899971\n",
      "iteration 34350: loss: 0.21745887398719788\n",
      "iteration 34351: loss: 0.21745872497558594\n",
      "iteration 34352: loss: 0.217458575963974\n",
      "iteration 34353: loss: 0.21745851635932922\n",
      "iteration 34354: loss: 0.21745829284191132\n",
      "iteration 34355: loss: 0.21745820343494415\n",
      "iteration 34356: loss: 0.21745805442333221\n",
      "iteration 34357: loss: 0.2174578458070755\n",
      "iteration 34358: loss: 0.21745774149894714\n",
      "iteration 34359: loss: 0.2174576073884964\n",
      "iteration 34360: loss: 0.21745753288269043\n",
      "iteration 34361: loss: 0.2174573391675949\n",
      "iteration 34362: loss: 0.2174571305513382\n",
      "iteration 34363: loss: 0.21745705604553223\n",
      "iteration 34364: loss: 0.2174568921327591\n",
      "iteration 34365: loss: 0.21745677292346954\n",
      "iteration 34366: loss: 0.21745665371418\n",
      "iteration 34367: loss: 0.21745650470256805\n",
      "iteration 34368: loss: 0.21745634078979492\n",
      "iteration 34369: loss: 0.2174561321735382\n",
      "iteration 34370: loss: 0.21745602786540985\n",
      "iteration 34371: loss: 0.2174559384584427\n",
      "iteration 34372: loss: 0.21745578944683075\n",
      "iteration 34373: loss: 0.21745559573173523\n",
      "iteration 34374: loss: 0.21745547652244568\n",
      "iteration 34375: loss: 0.21745529770851135\n",
      "iteration 34376: loss: 0.2174551784992218\n",
      "iteration 34377: loss: 0.21745505928993225\n",
      "iteration 34378: loss: 0.21745488047599792\n",
      "iteration 34379: loss: 0.2174546718597412\n",
      "iteration 34380: loss: 0.2174547016620636\n",
      "iteration 34381: loss: 0.21745450794696808\n",
      "iteration 34382: loss: 0.21745431423187256\n",
      "iteration 34383: loss: 0.217454195022583\n",
      "iteration 34384: loss: 0.21745404601097107\n",
      "iteration 34385: loss: 0.21745391190052032\n",
      "iteration 34386: loss: 0.21745379269123077\n",
      "iteration 34387: loss: 0.21745367348194122\n",
      "iteration 34388: loss: 0.2174534797668457\n",
      "iteration 34389: loss: 0.21745336055755615\n",
      "iteration 34390: loss: 0.21745319664478302\n",
      "iteration 34391: loss: 0.21745304763317108\n",
      "iteration 34392: loss: 0.21745292842388153\n",
      "iteration 34393: loss: 0.21745271980762482\n",
      "iteration 34394: loss: 0.21745257079601288\n",
      "iteration 34395: loss: 0.21745248138904572\n",
      "iteration 34396: loss: 0.21745236217975616\n",
      "iteration 34397: loss: 0.21745219826698303\n",
      "iteration 34398: loss: 0.2174520194530487\n",
      "iteration 34399: loss: 0.21745197474956512\n",
      "iteration 34400: loss: 0.2174517661333084\n",
      "iteration 34401: loss: 0.21745160222053528\n",
      "iteration 34402: loss: 0.21745149791240692\n",
      "iteration 34403: loss: 0.21745136380195618\n",
      "iteration 34404: loss: 0.21745118498802185\n",
      "iteration 34405: loss: 0.2174510955810547\n",
      "iteration 34406: loss: 0.21745088696479797\n",
      "iteration 34407: loss: 0.21745078265666962\n",
      "iteration 34408: loss: 0.21745061874389648\n",
      "iteration 34409: loss: 0.21745046973228455\n",
      "iteration 34410: loss: 0.217450350522995\n",
      "iteration 34411: loss: 0.21745018661022186\n",
      "iteration 34412: loss: 0.2174501121044159\n",
      "iteration 34413: loss: 0.21744990348815918\n",
      "iteration 34414: loss: 0.21744981408119202\n",
      "iteration 34415: loss: 0.2174496203660965\n",
      "iteration 34416: loss: 0.21744951605796814\n",
      "iteration 34417: loss: 0.217449352145195\n",
      "iteration 34418: loss: 0.21744918823242188\n",
      "iteration 34419: loss: 0.2174491435289383\n",
      "iteration 34420: loss: 0.21744899451732635\n",
      "iteration 34421: loss: 0.2174488604068756\n",
      "iteration 34422: loss: 0.21744868159294128\n",
      "iteration 34423: loss: 0.21744851768016815\n",
      "iteration 34424: loss: 0.2174483835697174\n",
      "iteration 34425: loss: 0.2174481898546219\n",
      "iteration 34426: loss: 0.21744811534881592\n",
      "iteration 34427: loss: 0.21744795143604279\n",
      "iteration 34428: loss: 0.21744783222675323\n",
      "iteration 34429: loss: 0.21744760870933533\n",
      "iteration 34430: loss: 0.21744754910469055\n",
      "iteration 34431: loss: 0.21744735538959503\n",
      "iteration 34432: loss: 0.21744731068611145\n",
      "iteration 34433: loss: 0.21744711697101593\n",
      "iteration 34434: loss: 0.21744699776172638\n",
      "iteration 34435: loss: 0.21744683384895325\n",
      "iteration 34436: loss: 0.21744664013385773\n",
      "iteration 34437: loss: 0.21744661033153534\n",
      "iteration 34438: loss: 0.21744641661643982\n",
      "iteration 34439: loss: 0.21744629740715027\n",
      "iteration 34440: loss: 0.21744608879089355\n",
      "iteration 34441: loss: 0.2174459993839264\n",
      "iteration 34442: loss: 0.21744588017463684\n",
      "iteration 34443: loss: 0.21744570136070251\n",
      "iteration 34444: loss: 0.21744556725025177\n",
      "iteration 34445: loss: 0.21744544804096222\n",
      "iteration 34446: loss: 0.2174452543258667\n",
      "iteration 34447: loss: 0.21744513511657715\n",
      "iteration 34448: loss: 0.21744497120380402\n",
      "iteration 34449: loss: 0.21744485199451447\n",
      "iteration 34450: loss: 0.21744470298290253\n",
      "iteration 34451: loss: 0.21744461357593536\n",
      "iteration 34452: loss: 0.21744446456432343\n",
      "iteration 34453: loss: 0.21744433045387268\n",
      "iteration 34454: loss: 0.21744415163993835\n",
      "iteration 34455: loss: 0.2174440175294876\n",
      "iteration 34456: loss: 0.21744394302368164\n",
      "iteration 34457: loss: 0.21744367480278015\n",
      "iteration 34458: loss: 0.2174435406923294\n",
      "iteration 34459: loss: 0.21744339168071747\n",
      "iteration 34460: loss: 0.21744322776794434\n",
      "iteration 34461: loss: 0.21744318306446075\n",
      "iteration 34462: loss: 0.21744301915168762\n",
      "iteration 34463: loss: 0.21744287014007568\n",
      "iteration 34464: loss: 0.21744272112846375\n",
      "iteration 34465: loss: 0.217442587018013\n",
      "iteration 34466: loss: 0.21744251251220703\n",
      "iteration 34467: loss: 0.2174423635005951\n",
      "iteration 34468: loss: 0.21744218468666077\n",
      "iteration 34469: loss: 0.21744199097156525\n",
      "iteration 34470: loss: 0.21744191646575928\n",
      "iteration 34471: loss: 0.21744175255298615\n",
      "iteration 34472: loss: 0.2174416035413742\n",
      "iteration 34473: loss: 0.21744146943092346\n",
      "iteration 34474: loss: 0.2174413502216339\n",
      "iteration 34475: loss: 0.21744120121002197\n",
      "iteration 34476: loss: 0.21744103729724884\n",
      "iteration 34477: loss: 0.2174408733844757\n",
      "iteration 34478: loss: 0.21744075417518616\n",
      "iteration 34479: loss: 0.21744060516357422\n",
      "iteration 34480: loss: 0.2174404412508011\n",
      "iteration 34481: loss: 0.21744032204151154\n",
      "iteration 34482: loss: 0.2174401730298996\n",
      "iteration 34483: loss: 0.21744005382061005\n",
      "iteration 34484: loss: 0.21743988990783691\n",
      "iteration 34485: loss: 0.21743977069854736\n",
      "iteration 34486: loss: 0.21743960678577423\n",
      "iteration 34487: loss: 0.21743948757648468\n",
      "iteration 34488: loss: 0.21743933856487274\n",
      "iteration 34489: loss: 0.217439204454422\n",
      "iteration 34490: loss: 0.21743910014629364\n",
      "iteration 34491: loss: 0.21743902564048767\n",
      "iteration 34492: loss: 0.21743878722190857\n",
      "iteration 34493: loss: 0.21743865311145782\n",
      "iteration 34494: loss: 0.2174385040998459\n",
      "iteration 34495: loss: 0.21743831038475037\n",
      "iteration 34496: loss: 0.2174382209777832\n",
      "iteration 34497: loss: 0.21743807196617126\n",
      "iteration 34498: loss: 0.2174380123615265\n",
      "iteration 34499: loss: 0.21743778884410858\n",
      "iteration 34500: loss: 0.21743759512901306\n",
      "iteration 34501: loss: 0.2174375355243683\n",
      "iteration 34502: loss: 0.21743735671043396\n",
      "iteration 34503: loss: 0.2174372375011444\n",
      "iteration 34504: loss: 0.2174370288848877\n",
      "iteration 34505: loss: 0.2174369841814041\n",
      "iteration 34506: loss: 0.2174367904663086\n",
      "iteration 34507: loss: 0.21743670105934143\n",
      "iteration 34508: loss: 0.2174365073442459\n",
      "iteration 34509: loss: 0.21743646264076233\n",
      "iteration 34510: loss: 0.2174363136291504\n",
      "iteration 34511: loss: 0.21743616461753845\n",
      "iteration 34512: loss: 0.21743598580360413\n",
      "iteration 34513: loss: 0.2174358069896698\n",
      "iteration 34514: loss: 0.21743568778038025\n",
      "iteration 34515: loss: 0.2174355536699295\n",
      "iteration 34516: loss: 0.2174353152513504\n",
      "iteration 34517: loss: 0.21743524074554443\n",
      "iteration 34518: loss: 0.21743512153625488\n",
      "iteration 34519: loss: 0.21743497252464294\n",
      "iteration 34520: loss: 0.21743488311767578\n",
      "iteration 34521: loss: 0.21743467450141907\n",
      "iteration 34522: loss: 0.21743452548980713\n",
      "iteration 34523: loss: 0.21743445098400116\n",
      "iteration 34524: loss: 0.2174343317747116\n",
      "iteration 34525: loss: 0.21743416786193848\n",
      "iteration 34526: loss: 0.21743401885032654\n",
      "iteration 34527: loss: 0.217433899641037\n",
      "iteration 34528: loss: 0.21743373572826385\n",
      "iteration 34529: loss: 0.21743357181549072\n",
      "iteration 34530: loss: 0.21743348240852356\n",
      "iteration 34531: loss: 0.21743333339691162\n",
      "iteration 34532: loss: 0.21743321418762207\n",
      "iteration 34533: loss: 0.21743305027484894\n",
      "iteration 34534: loss: 0.2174328863620758\n",
      "iteration 34535: loss: 0.21743273735046387\n",
      "iteration 34536: loss: 0.21743258833885193\n",
      "iteration 34537: loss: 0.21743254363536835\n",
      "iteration 34538: loss: 0.21743234992027283\n",
      "iteration 34539: loss: 0.21743221580982208\n",
      "iteration 34540: loss: 0.21743206679821014\n",
      "iteration 34541: loss: 0.2174319326877594\n",
      "iteration 34542: loss: 0.2174317091703415\n",
      "iteration 34543: loss: 0.21743157505989075\n",
      "iteration 34544: loss: 0.21743150055408478\n",
      "iteration 34545: loss: 0.21743135154247284\n",
      "iteration 34546: loss: 0.2174312174320221\n",
      "iteration 34547: loss: 0.21743103861808777\n",
      "iteration 34548: loss: 0.21743091940879822\n",
      "iteration 34549: loss: 0.21743078529834747\n",
      "iteration 34550: loss: 0.21743063628673553\n",
      "iteration 34551: loss: 0.21743051707744598\n",
      "iteration 34552: loss: 0.21743035316467285\n",
      "iteration 34553: loss: 0.2174302339553833\n",
      "iteration 34554: loss: 0.21743008494377136\n",
      "iteration 34555: loss: 0.21742990612983704\n",
      "iteration 34556: loss: 0.21742983162403107\n",
      "iteration 34557: loss: 0.21742963790893555\n",
      "iteration 34558: loss: 0.217429518699646\n",
      "iteration 34559: loss: 0.21742944419384003\n",
      "iteration 34560: loss: 0.2174292504787445\n",
      "iteration 34561: loss: 0.21742911636829376\n",
      "iteration 34562: loss: 0.21742896735668182\n",
      "iteration 34563: loss: 0.21742883324623108\n",
      "iteration 34564: loss: 0.21742872893810272\n",
      "iteration 34565: loss: 0.2174285650253296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 34566: loss: 0.21742840111255646\n",
      "iteration 34567: loss: 0.21742825210094452\n",
      "iteration 34568: loss: 0.2174280881881714\n",
      "iteration 34569: loss: 0.21742801368236542\n",
      "iteration 34570: loss: 0.2174278199672699\n",
      "iteration 34571: loss: 0.21742773056030273\n",
      "iteration 34572: loss: 0.2174275666475296\n",
      "iteration 34573: loss: 0.21742741763591766\n",
      "iteration 34574: loss: 0.2174273431301117\n",
      "iteration 34575: loss: 0.217427060008049\n",
      "iteration 34576: loss: 0.21742701530456543\n",
      "iteration 34577: loss: 0.21742680668830872\n",
      "iteration 34578: loss: 0.21742670238018036\n",
      "iteration 34579: loss: 0.2174266129732132\n",
      "iteration 34580: loss: 0.21742644906044006\n",
      "iteration 34581: loss: 0.2174263298511505\n",
      "iteration 34582: loss: 0.2174261063337326\n",
      "iteration 34583: loss: 0.21742603182792664\n",
      "iteration 34584: loss: 0.2174258679151535\n",
      "iteration 34585: loss: 0.21742574870586395\n",
      "iteration 34586: loss: 0.21742558479309082\n",
      "iteration 34587: loss: 0.21742549538612366\n",
      "iteration 34588: loss: 0.21742534637451172\n",
      "iteration 34589: loss: 0.2174251526594162\n",
      "iteration 34590: loss: 0.21742503345012665\n",
      "iteration 34591: loss: 0.2174249142408371\n",
      "iteration 34592: loss: 0.21742479503154755\n",
      "iteration 34593: loss: 0.21742460131645203\n",
      "iteration 34594: loss: 0.21742454171180725\n",
      "iteration 34595: loss: 0.21742434799671173\n",
      "iteration 34596: loss: 0.21742424368858337\n",
      "iteration 34597: loss: 0.21742410957813263\n",
      "iteration 34598: loss: 0.2174239605665207\n",
      "iteration 34599: loss: 0.21742384135723114\n",
      "iteration 34600: loss: 0.217423677444458\n",
      "iteration 34601: loss: 0.21742358803749084\n",
      "iteration 34602: loss: 0.21742339432239532\n",
      "iteration 34603: loss: 0.2174232006072998\n",
      "iteration 34604: loss: 0.21742312610149384\n",
      "iteration 34605: loss: 0.2174229621887207\n",
      "iteration 34606: loss: 0.21742281317710876\n",
      "iteration 34607: loss: 0.2174226939678192\n",
      "iteration 34608: loss: 0.21742257475852966\n",
      "iteration 34609: loss: 0.21742241084575653\n",
      "iteration 34610: loss: 0.2174222469329834\n",
      "iteration 34611: loss: 0.21742212772369385\n",
      "iteration 34612: loss: 0.2174220085144043\n",
      "iteration 34613: loss: 0.21742185950279236\n",
      "iteration 34614: loss: 0.21742165088653564\n",
      "iteration 34615: loss: 0.21742156147956848\n",
      "iteration 34616: loss: 0.2174214869737625\n",
      "iteration 34617: loss: 0.21742133796215057\n",
      "iteration 34618: loss: 0.21742117404937744\n",
      "iteration 34619: loss: 0.2174210250377655\n",
      "iteration 34620: loss: 0.21742090582847595\n",
      "iteration 34621: loss: 0.2174208164215088\n",
      "iteration 34622: loss: 0.2174205780029297\n",
      "iteration 34623: loss: 0.21742045879364014\n",
      "iteration 34624: loss: 0.21742033958435059\n",
      "iteration 34625: loss: 0.21742022037506104\n",
      "iteration 34626: loss: 0.2174200564622879\n",
      "iteration 34627: loss: 0.21741986274719238\n",
      "iteration 34628: loss: 0.21741977334022522\n",
      "iteration 34629: loss: 0.21741965413093567\n",
      "iteration 34630: loss: 0.21741953492164612\n",
      "iteration 34631: loss: 0.2174193561077118\n",
      "iteration 34632: loss: 0.21741926670074463\n",
      "iteration 34633: loss: 0.2174191027879715\n",
      "iteration 34634: loss: 0.21741895377635956\n",
      "iteration 34635: loss: 0.2174188196659088\n",
      "iteration 34636: loss: 0.21741871535778046\n",
      "iteration 34637: loss: 0.2174185812473297\n",
      "iteration 34638: loss: 0.2174183577299118\n",
      "iteration 34639: loss: 0.21741823852062225\n",
      "iteration 34640: loss: 0.2174181193113327\n",
      "iteration 34641: loss: 0.21741802990436554\n",
      "iteration 34642: loss: 0.21741780638694763\n",
      "iteration 34643: loss: 0.2174176722764969\n",
      "iteration 34644: loss: 0.21741752326488495\n",
      "iteration 34645: loss: 0.21741744875907898\n",
      "iteration 34646: loss: 0.21741731464862823\n",
      "iteration 34647: loss: 0.21741712093353271\n",
      "iteration 34648: loss: 0.2174169272184372\n",
      "iteration 34649: loss: 0.2174168825149536\n",
      "iteration 34650: loss: 0.21741671860218048\n",
      "iteration 34651: loss: 0.21741656959056854\n",
      "iteration 34652: loss: 0.21741648018360138\n",
      "iteration 34653: loss: 0.21741637587547302\n",
      "iteration 34654: loss: 0.2174161672592163\n",
      "iteration 34655: loss: 0.21741604804992676\n",
      "iteration 34656: loss: 0.21741589903831482\n",
      "iteration 34657: loss: 0.21741566061973572\n",
      "iteration 34658: loss: 0.21741566061973572\n",
      "iteration 34659: loss: 0.21741552650928497\n",
      "iteration 34660: loss: 0.21741533279418945\n",
      "iteration 34661: loss: 0.21741525828838348\n",
      "iteration 34662: loss: 0.2174150049686432\n",
      "iteration 34663: loss: 0.21741490066051483\n",
      "iteration 34664: loss: 0.21741478145122528\n",
      "iteration 34665: loss: 0.21741466224193573\n",
      "iteration 34666: loss: 0.2174144983291626\n",
      "iteration 34667: loss: 0.21741437911987305\n",
      "iteration 34668: loss: 0.21741417050361633\n",
      "iteration 34669: loss: 0.21741411089897156\n",
      "iteration 34670: loss: 0.21741405129432678\n",
      "iteration 34671: loss: 0.2174137532711029\n",
      "iteration 34672: loss: 0.21741369366645813\n",
      "iteration 34673: loss: 0.21741358935832977\n",
      "iteration 34674: loss: 0.21741342544555664\n",
      "iteration 34675: loss: 0.21741323173046112\n",
      "iteration 34676: loss: 0.21741314232349396\n",
      "iteration 34677: loss: 0.2174130231142044\n",
      "iteration 34678: loss: 0.2174127995967865\n",
      "iteration 34679: loss: 0.21741275489330292\n",
      "iteration 34680: loss: 0.2174125611782074\n",
      "iteration 34681: loss: 0.21741239726543427\n",
      "iteration 34682: loss: 0.21741227805614471\n",
      "iteration 34683: loss: 0.21741214394569397\n",
      "iteration 34684: loss: 0.2174120843410492\n",
      "iteration 34685: loss: 0.21741190552711487\n",
      "iteration 34686: loss: 0.21741178631782532\n",
      "iteration 34687: loss: 0.2174115628004074\n",
      "iteration 34688: loss: 0.21741142868995667\n",
      "iteration 34689: loss: 0.2174113243818283\n",
      "iteration 34690: loss: 0.21741123497486115\n",
      "iteration 34691: loss: 0.217411071062088\n",
      "iteration 34692: loss: 0.2174108773469925\n",
      "iteration 34693: loss: 0.2174108326435089\n",
      "iteration 34694: loss: 0.21741065382957458\n",
      "iteration 34695: loss: 0.21741051971912384\n",
      "iteration 34696: loss: 0.2174104005098343\n",
      "iteration 34697: loss: 0.21741017699241638\n",
      "iteration 34698: loss: 0.21741008758544922\n",
      "iteration 34699: loss: 0.21740996837615967\n",
      "iteration 34700: loss: 0.21740980446338654\n",
      "iteration 34701: loss: 0.2174096405506134\n",
      "iteration 34702: loss: 0.21740956604480743\n",
      "iteration 34703: loss: 0.2174094021320343\n",
      "iteration 34704: loss: 0.21740928292274475\n",
      "iteration 34705: loss: 0.21740910410881042\n",
      "iteration 34706: loss: 0.21740898489952087\n",
      "iteration 34707: loss: 0.21740885078907013\n",
      "iteration 34708: loss: 0.2174087017774582\n",
      "iteration 34709: loss: 0.21740853786468506\n",
      "iteration 34710: loss: 0.2174084186553955\n",
      "iteration 34711: loss: 0.21740829944610596\n",
      "iteration 34712: loss: 0.21740813553333282\n",
      "iteration 34713: loss: 0.21740803122520447\n",
      "iteration 34714: loss: 0.21740785241127014\n",
      "iteration 34715: loss: 0.21740777790546417\n",
      "iteration 34716: loss: 0.21740761399269104\n",
      "iteration 34717: loss: 0.21740750968456268\n",
      "iteration 34718: loss: 0.21740731596946716\n",
      "iteration 34719: loss: 0.21740718185901642\n",
      "iteration 34720: loss: 0.21740706264972687\n",
      "iteration 34721: loss: 0.21740691363811493\n",
      "iteration 34722: loss: 0.21740677952766418\n",
      "iteration 34723: loss: 0.21740667521953583\n",
      "iteration 34724: loss: 0.2174065113067627\n",
      "iteration 34725: loss: 0.21740636229515076\n",
      "iteration 34726: loss: 0.2174062728881836\n",
      "iteration 34727: loss: 0.21740610897541046\n",
      "iteration 34728: loss: 0.21740595996379852\n",
      "iteration 34729: loss: 0.21740582585334778\n",
      "iteration 34730: loss: 0.21740564703941345\n",
      "iteration 34731: loss: 0.2174055129289627\n",
      "iteration 34732: loss: 0.21740536391735077\n",
      "iteration 34733: loss: 0.2174052894115448\n",
      "iteration 34734: loss: 0.21740517020225525\n",
      "iteration 34735: loss: 0.21740496158599854\n",
      "iteration 34736: loss: 0.2174048125743866\n",
      "iteration 34737: loss: 0.21740469336509705\n",
      "iteration 34738: loss: 0.2174045741558075\n",
      "iteration 34739: loss: 0.21740441024303436\n",
      "iteration 34740: loss: 0.2174043208360672\n",
      "iteration 34741: loss: 0.21740412712097168\n",
      "iteration 34742: loss: 0.21740400791168213\n",
      "iteration 34743: loss: 0.2174038589000702\n",
      "iteration 34744: loss: 0.21740373969078064\n",
      "iteration 34745: loss: 0.21740365028381348\n",
      "iteration 34746: loss: 0.21740345656871796\n",
      "iteration 34747: loss: 0.2174033671617508\n",
      "iteration 34748: loss: 0.21740317344665527\n",
      "iteration 34749: loss: 0.21740305423736572\n",
      "iteration 34750: loss: 0.21740296483039856\n",
      "iteration 34751: loss: 0.21740269660949707\n",
      "iteration 34752: loss: 0.21740266680717468\n",
      "iteration 34753: loss: 0.21740254759788513\n",
      "iteration 34754: loss: 0.21740233898162842\n",
      "iteration 34755: loss: 0.21740221977233887\n",
      "iteration 34756: loss: 0.2174021303653717\n",
      "iteration 34757: loss: 0.21740195155143738\n",
      "iteration 34758: loss: 0.21740183234214783\n",
      "iteration 34759: loss: 0.21740174293518066\n",
      "iteration 34760: loss: 0.21740153431892395\n",
      "iteration 34761: loss: 0.217401385307312\n",
      "iteration 34762: loss: 0.21740126609802246\n",
      "iteration 34763: loss: 0.21740111708641052\n",
      "iteration 34764: loss: 0.2174009531736374\n",
      "iteration 34765: loss: 0.21740086376667023\n",
      "iteration 34766: loss: 0.21740074455738068\n",
      "iteration 34767: loss: 0.21740062534809113\n",
      "iteration 34768: loss: 0.2174004316329956\n",
      "iteration 34769: loss: 0.21740026772022247\n",
      "iteration 34770: loss: 0.21740016341209412\n",
      "iteration 34771: loss: 0.21740002930164337\n",
      "iteration 34772: loss: 0.21739988029003143\n",
      "iteration 34773: loss: 0.21739979088306427\n",
      "iteration 34774: loss: 0.21739959716796875\n",
      "iteration 34775: loss: 0.21739952266216278\n",
      "iteration 34776: loss: 0.21739935874938965\n",
      "iteration 34777: loss: 0.21739919483661652\n",
      "iteration 34778: loss: 0.217399001121521\n",
      "iteration 34779: loss: 0.21739895641803741\n",
      "iteration 34780: loss: 0.21739879250526428\n",
      "iteration 34781: loss: 0.21739867329597473\n",
      "iteration 34782: loss: 0.21739856898784637\n",
      "iteration 34783: loss: 0.21739844977855682\n",
      "iteration 34784: loss: 0.21739821135997772\n",
      "iteration 34785: loss: 0.21739816665649414\n",
      "iteration 34786: loss: 0.2173980176448822\n",
      "iteration 34787: loss: 0.21739783883094788\n",
      "iteration 34788: loss: 0.21739771962165833\n",
      "iteration 34789: loss: 0.217397540807724\n",
      "iteration 34790: loss: 0.21739740669727325\n",
      "iteration 34791: loss: 0.21739725768566132\n",
      "iteration 34792: loss: 0.21739716827869415\n",
      "iteration 34793: loss: 0.21739701926708221\n",
      "iteration 34794: loss: 0.21739688515663147\n",
      "iteration 34795: loss: 0.21739676594734192\n",
      "iteration 34796: loss: 0.2173965722322464\n",
      "iteration 34797: loss: 0.21739649772644043\n",
      "iteration 34798: loss: 0.21739637851715088\n",
      "iteration 34799: loss: 0.21739625930786133\n",
      "iteration 34800: loss: 0.2173960655927658\n",
      "iteration 34801: loss: 0.21739594638347626\n",
      "iteration 34802: loss: 0.21739578247070312\n",
      "iteration 34803: loss: 0.21739569306373596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 34804: loss: 0.21739551424980164\n",
      "iteration 34805: loss: 0.2173953801393509\n",
      "iteration 34806: loss: 0.21739526093006134\n",
      "iteration 34807: loss: 0.2173950970172882\n",
      "iteration 34808: loss: 0.21739494800567627\n",
      "iteration 34809: loss: 0.21739482879638672\n",
      "iteration 34810: loss: 0.21739467978477478\n",
      "iteration 34811: loss: 0.21739454567432404\n",
      "iteration 34812: loss: 0.21739444136619568\n",
      "iteration 34813: loss: 0.21739426255226135\n",
      "iteration 34814: loss: 0.217394158244133\n",
      "iteration 34815: loss: 0.21739402413368225\n",
      "iteration 34816: loss: 0.2173938751220703\n",
      "iteration 34817: loss: 0.2173936814069748\n",
      "iteration 34818: loss: 0.21739356219768524\n",
      "iteration 34819: loss: 0.21739347279071808\n",
      "iteration 34820: loss: 0.21739335358142853\n",
      "iteration 34821: loss: 0.2173932045698166\n",
      "iteration 34822: loss: 0.21739307045936584\n",
      "iteration 34823: loss: 0.2173929661512375\n",
      "iteration 34824: loss: 0.21739284694194794\n",
      "iteration 34825: loss: 0.21739265322685242\n",
      "iteration 34826: loss: 0.21739251911640167\n",
      "iteration 34827: loss: 0.21739239990711212\n",
      "iteration 34828: loss: 0.21739228069782257\n",
      "iteration 34829: loss: 0.21739213168621063\n",
      "iteration 34830: loss: 0.2173919677734375\n",
      "iteration 34831: loss: 0.21739187836647034\n",
      "iteration 34832: loss: 0.21739168465137482\n",
      "iteration 34833: loss: 0.21739156544208527\n",
      "iteration 34834: loss: 0.21739141643047333\n",
      "iteration 34835: loss: 0.21739132702350616\n",
      "iteration 34836: loss: 0.21739110350608826\n",
      "iteration 34837: loss: 0.2173910140991211\n",
      "iteration 34838: loss: 0.21739093959331512\n",
      "iteration 34839: loss: 0.2173907458782196\n",
      "iteration 34840: loss: 0.21739056706428528\n",
      "iteration 34841: loss: 0.21739044785499573\n",
      "iteration 34842: loss: 0.21739032864570618\n",
      "iteration 34843: loss: 0.21739017963409424\n",
      "iteration 34844: loss: 0.21739009022712708\n",
      "iteration 34845: loss: 0.21738989651203156\n",
      "iteration 34846: loss: 0.2173897922039032\n",
      "iteration 34847: loss: 0.21738965809345245\n",
      "iteration 34848: loss: 0.21738950908184052\n",
      "iteration 34849: loss: 0.21738941967487335\n",
      "iteration 34850: loss: 0.21738925576210022\n",
      "iteration 34851: loss: 0.21738913655281067\n",
      "iteration 34852: loss: 0.21738895773887634\n",
      "iteration 34853: loss: 0.2173888236284256\n",
      "iteration 34854: loss: 0.21738871932029724\n",
      "iteration 34855: loss: 0.2173885554075241\n",
      "iteration 34856: loss: 0.21738842129707336\n",
      "iteration 34857: loss: 0.2173883020877838\n",
      "iteration 34858: loss: 0.2173881083726883\n",
      "iteration 34859: loss: 0.21738798916339874\n",
      "iteration 34860: loss: 0.2173878699541092\n",
      "iteration 34861: loss: 0.21738767623901367\n",
      "iteration 34862: loss: 0.21738755702972412\n",
      "iteration 34863: loss: 0.21738743782043457\n",
      "iteration 34864: loss: 0.2173873484134674\n",
      "iteration 34865: loss: 0.21738724410533905\n",
      "iteration 34866: loss: 0.2173871099948883\n",
      "iteration 34867: loss: 0.2173868715763092\n",
      "iteration 34868: loss: 0.21738679707050323\n",
      "iteration 34869: loss: 0.21738660335540771\n",
      "iteration 34870: loss: 0.21738648414611816\n",
      "iteration 34871: loss: 0.217386394739151\n",
      "iteration 34872: loss: 0.21738624572753906\n",
      "iteration 34873: loss: 0.2173861712217331\n",
      "iteration 34874: loss: 0.21738596260547638\n",
      "iteration 34875: loss: 0.21738581359386444\n",
      "iteration 34876: loss: 0.21738576889038086\n",
      "iteration 34877: loss: 0.21738557517528534\n",
      "iteration 34878: loss: 0.2173854410648346\n",
      "iteration 34879: loss: 0.21738532185554504\n",
      "iteration 34880: loss: 0.2173851728439331\n",
      "iteration 34881: loss: 0.21738497912883759\n",
      "iteration 34882: loss: 0.21738484501838684\n",
      "iteration 34883: loss: 0.21738477051258087\n",
      "iteration 34884: loss: 0.21738460659980774\n",
      "iteration 34885: loss: 0.21738453209400177\n",
      "iteration 34886: loss: 0.21738430857658386\n",
      "iteration 34887: loss: 0.21738417446613312\n",
      "iteration 34888: loss: 0.21738412976264954\n",
      "iteration 34889: loss: 0.2173839807510376\n",
      "iteration 34890: loss: 0.21738378703594208\n",
      "iteration 34891: loss: 0.21738362312316895\n",
      "iteration 34892: loss: 0.217383474111557\n",
      "iteration 34893: loss: 0.21738342940807343\n",
      "iteration 34894: loss: 0.21738329529762268\n",
      "iteration 34895: loss: 0.21738310158252716\n",
      "iteration 34896: loss: 0.21738295257091522\n",
      "iteration 34897: loss: 0.21738281846046448\n",
      "iteration 34898: loss: 0.21738266944885254\n",
      "iteration 34899: loss: 0.21738258004188538\n",
      "iteration 34900: loss: 0.21738247573375702\n",
      "iteration 34901: loss: 0.21738235652446747\n",
      "iteration 34902: loss: 0.21738223731517792\n",
      "iteration 34903: loss: 0.2173820286989212\n",
      "iteration 34904: loss: 0.21738190948963165\n",
      "iteration 34905: loss: 0.21738176047801971\n",
      "iteration 34906: loss: 0.21738167107105255\n",
      "iteration 34907: loss: 0.21738147735595703\n",
      "iteration 34908: loss: 0.21738140285015106\n",
      "iteration 34909: loss: 0.2173812836408615\n",
      "iteration 34910: loss: 0.21738111972808838\n",
      "iteration 34911: loss: 0.21738095581531525\n",
      "iteration 34912: loss: 0.21738079190254211\n",
      "iteration 34913: loss: 0.2173806130886078\n",
      "iteration 34914: loss: 0.2173805683851242\n",
      "iteration 34915: loss: 0.2173803746700287\n",
      "iteration 34916: loss: 0.21738028526306152\n",
      "iteration 34917: loss: 0.21738013625144958\n",
      "iteration 34918: loss: 0.21737997233867645\n",
      "iteration 34919: loss: 0.2173798531293869\n",
      "iteration 34920: loss: 0.21737973392009735\n",
      "iteration 34921: loss: 0.2173795998096466\n",
      "iteration 34922: loss: 0.21737945079803467\n",
      "iteration 34923: loss: 0.21737933158874512\n",
      "iteration 34924: loss: 0.21737918257713318\n",
      "iteration 34925: loss: 0.21737904846668243\n",
      "iteration 34926: loss: 0.21737892925739288\n",
      "iteration 34927: loss: 0.21737881004810333\n",
      "iteration 34928: loss: 0.2173786610364914\n",
      "iteration 34929: loss: 0.21737846732139587\n",
      "iteration 34930: loss: 0.21737833321094513\n",
      "iteration 34931: loss: 0.21737821400165558\n",
      "iteration 34932: loss: 0.21737810969352722\n",
      "iteration 34933: loss: 0.21737797558307648\n",
      "iteration 34934: loss: 0.21737787127494812\n",
      "iteration 34935: loss: 0.21737773716449738\n",
      "iteration 34936: loss: 0.21737758815288544\n",
      "iteration 34937: loss: 0.2173774689435959\n",
      "iteration 34938: loss: 0.21737726032733917\n",
      "iteration 34939: loss: 0.2173771858215332\n",
      "iteration 34940: loss: 0.2173769772052765\n",
      "iteration 34941: loss: 0.21737690269947052\n",
      "iteration 34942: loss: 0.21737675368785858\n",
      "iteration 34943: loss: 0.21737658977508545\n",
      "iteration 34944: loss: 0.21737651526927948\n",
      "iteration 34945: loss: 0.21737632155418396\n",
      "iteration 34946: loss: 0.21737626194953918\n",
      "iteration 34947: loss: 0.21737606823444366\n",
      "iteration 34948: loss: 0.2173759639263153\n",
      "iteration 34949: loss: 0.21737582981586456\n",
      "iteration 34950: loss: 0.21737566590309143\n",
      "iteration 34951: loss: 0.21737556159496307\n",
      "iteration 34952: loss: 0.21737535297870636\n",
      "iteration 34953: loss: 0.2173752784729004\n",
      "iteration 34954: loss: 0.21737512946128845\n",
      "iteration 34955: loss: 0.2173749953508377\n",
      "iteration 34956: loss: 0.21737487614154816\n",
      "iteration 34957: loss: 0.21737468242645264\n",
      "iteration 34958: loss: 0.21737456321716309\n",
      "iteration 34959: loss: 0.21737448871135712\n",
      "iteration 34960: loss: 0.21737435460090637\n",
      "iteration 34961: loss: 0.21737417578697205\n",
      "iteration 34962: loss: 0.21737408638000488\n",
      "iteration 34963: loss: 0.21737392246723175\n",
      "iteration 34964: loss: 0.21737375855445862\n",
      "iteration 34965: loss: 0.21737363934516907\n",
      "iteration 34966: loss: 0.21737349033355713\n",
      "iteration 34967: loss: 0.21737340092658997\n",
      "iteration 34968: loss: 0.21737328171730042\n",
      "iteration 34969: loss: 0.21737313270568848\n",
      "iteration 34970: loss: 0.21737298369407654\n",
      "iteration 34971: loss: 0.2173728495836258\n",
      "iteration 34972: loss: 0.21737273037433624\n",
      "iteration 34973: loss: 0.2173725664615631\n",
      "iteration 34974: loss: 0.21737246215343475\n",
      "iteration 34975: loss: 0.217372328042984\n",
      "iteration 34976: loss: 0.21737217903137207\n",
      "iteration 34977: loss: 0.21737201511859894\n",
      "iteration 34978: loss: 0.217371866106987\n",
      "iteration 34979: loss: 0.21737170219421387\n",
      "iteration 34980: loss: 0.2173716127872467\n",
      "iteration 34981: loss: 0.21737149357795715\n",
      "iteration 34982: loss: 0.2173713743686676\n",
      "iteration 34983: loss: 0.21737118065357208\n",
      "iteration 34984: loss: 0.21737103164196014\n",
      "iteration 34985: loss: 0.21737101674079895\n",
      "iteration 34986: loss: 0.21737079322338104\n",
      "iteration 34987: loss: 0.2173706591129303\n",
      "iteration 34988: loss: 0.21737055480480194\n",
      "iteration 34989: loss: 0.21737046539783478\n",
      "iteration 34990: loss: 0.21737031638622284\n",
      "iteration 34991: loss: 0.21737010776996613\n",
      "iteration 34992: loss: 0.21737006306648254\n",
      "iteration 34993: loss: 0.21736986935138702\n",
      "iteration 34994: loss: 0.21736975014209747\n",
      "iteration 34995: loss: 0.21736964583396912\n",
      "iteration 34996: loss: 0.2173694372177124\n",
      "iteration 34997: loss: 0.21736936271190643\n",
      "iteration 34998: loss: 0.21736924350261688\n",
      "iteration 34999: loss: 0.21736904978752136\n",
      "iteration 35000: loss: 0.21736900508403778\n",
      "iteration 35001: loss: 0.21736879646778107\n",
      "iteration 35002: loss: 0.21736867725849152\n",
      "iteration 35003: loss: 0.21736852824687958\n",
      "iteration 35004: loss: 0.21736840903759003\n",
      "iteration 35005: loss: 0.2173682451248169\n",
      "iteration 35006: loss: 0.21736812591552734\n",
      "iteration 35007: loss: 0.2173679769039154\n",
      "iteration 35008: loss: 0.21736781299114227\n",
      "iteration 35009: loss: 0.21736769378185272\n",
      "iteration 35010: loss: 0.21736755967140198\n",
      "iteration 35011: loss: 0.217367485165596\n",
      "iteration 35012: loss: 0.2173672914505005\n",
      "iteration 35013: loss: 0.21736714243888855\n",
      "iteration 35014: loss: 0.21736708283424377\n",
      "iteration 35015: loss: 0.21736690402030945\n",
      "iteration 35016: loss: 0.21736674010753632\n",
      "iteration 35017: loss: 0.21736666560173035\n",
      "iteration 35018: loss: 0.21736650168895721\n",
      "iteration 35019: loss: 0.21736641228199005\n",
      "iteration 35020: loss: 0.21736621856689453\n",
      "iteration 35021: loss: 0.2173660695552826\n",
      "iteration 35022: loss: 0.21736598014831543\n",
      "iteration 35023: loss: 0.21736589074134827\n",
      "iteration 35024: loss: 0.21736565232276917\n",
      "iteration 35025: loss: 0.2173655480146408\n",
      "iteration 35026: loss: 0.21736545860767365\n",
      "iteration 35027: loss: 0.21736526489257812\n",
      "iteration 35028: loss: 0.21736519038677216\n",
      "iteration 35029: loss: 0.2173650711774826\n",
      "iteration 35030: loss: 0.21736495196819305\n",
      "iteration 35031: loss: 0.21736474335193634\n",
      "iteration 35032: loss: 0.21736463904380798\n",
      "iteration 35033: loss: 0.21736451983451843\n",
      "iteration 35034: loss: 0.21736431121826172\n",
      "iteration 35035: loss: 0.21736423671245575\n",
      "iteration 35036: loss: 0.217364102602005\n",
      "iteration 35037: loss: 0.21736399829387665\n",
      "iteration 35038: loss: 0.21736383438110352\n",
      "iteration 35039: loss: 0.21736371517181396\n",
      "iteration 35040: loss: 0.21736355125904083\n",
      "iteration 35041: loss: 0.2173634022474289\n",
      "iteration 35042: loss: 0.21736328303813934\n",
      "iteration 35043: loss: 0.21736320853233337\n",
      "iteration 35044: loss: 0.21736302971839905\n",
      "iteration 35045: loss: 0.2173628807067871\n",
      "iteration 35046: loss: 0.21736279129981995\n",
      "iteration 35047: loss: 0.2173626869916916\n",
      "iteration 35048: loss: 0.21736247837543488\n",
      "iteration 35049: loss: 0.21736237406730652\n",
      "iteration 35050: loss: 0.2173622101545334\n",
      "iteration 35051: loss: 0.21736207604408264\n",
      "iteration 35052: loss: 0.2173619568347931\n",
      "iteration 35053: loss: 0.21736185252666473\n",
      "iteration 35054: loss: 0.2173616588115692\n",
      "iteration 35055: loss: 0.21736153960227966\n",
      "iteration 35056: loss: 0.21736137568950653\n",
      "iteration 35057: loss: 0.21736125648021698\n",
      "iteration 35058: loss: 0.21736112236976624\n",
      "iteration 35059: loss: 0.21736101806163788\n",
      "iteration 35060: loss: 0.21736082434654236\n",
      "iteration 35061: loss: 0.2173607051372528\n",
      "iteration 35062: loss: 0.21736064553260803\n",
      "iteration 35063: loss: 0.21736054122447968\n",
      "iteration 35064: loss: 0.21736034750938416\n",
      "iteration 35065: loss: 0.21736018359661102\n",
      "iteration 35066: loss: 0.21736006438732147\n",
      "iteration 35067: loss: 0.2173599749803543\n",
      "iteration 35068: loss: 0.21735981106758118\n",
      "iteration 35069: loss: 0.21735963225364685\n",
      "iteration 35070: loss: 0.2173595428466797\n",
      "iteration 35071: loss: 0.21735939383506775\n",
      "iteration 35072: loss: 0.2173592746257782\n",
      "iteration 35073: loss: 0.21735909581184387\n",
      "iteration 35074: loss: 0.2173590362071991\n",
      "iteration 35075: loss: 0.21735882759094238\n",
      "iteration 35076: loss: 0.21735867857933044\n",
      "iteration 35077: loss: 0.21735858917236328\n",
      "iteration 35078: loss: 0.21735842525959015\n",
      "iteration 35079: loss: 0.21735835075378418\n",
      "iteration 35080: loss: 0.21735823154449463\n",
      "iteration 35081: loss: 0.21735811233520508\n",
      "iteration 35082: loss: 0.21735796332359314\n",
      "iteration 35083: loss: 0.2173577845096588\n",
      "iteration 35084: loss: 0.21735763549804688\n",
      "iteration 35085: loss: 0.2173575460910797\n",
      "iteration 35086: loss: 0.21735739707946777\n",
      "iteration 35087: loss: 0.2173573076725006\n",
      "iteration 35088: loss: 0.21735715866088867\n",
      "iteration 35089: loss: 0.21735700964927673\n",
      "iteration 35090: loss: 0.217356875538826\n",
      "iteration 35091: loss: 0.21735675632953644\n",
      "iteration 35092: loss: 0.21735653281211853\n",
      "iteration 35093: loss: 0.21735644340515137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 35094: loss: 0.2173563539981842\n",
      "iteration 35095: loss: 0.21735617518424988\n",
      "iteration 35096: loss: 0.21735604107379913\n",
      "iteration 35097: loss: 0.21735593676567078\n",
      "iteration 35098: loss: 0.21735577285289764\n",
      "iteration 35099: loss: 0.2173556536436081\n",
      "iteration 35100: loss: 0.21735548973083496\n",
      "iteration 35101: loss: 0.2173553705215454\n",
      "iteration 35102: loss: 0.21735528111457825\n",
      "iteration 35103: loss: 0.21735520660877228\n",
      "iteration 35104: loss: 0.21735504269599915\n",
      "iteration 35105: loss: 0.21735481917858124\n",
      "iteration 35106: loss: 0.21735472977161407\n",
      "iteration 35107: loss: 0.21735461056232452\n",
      "iteration 35108: loss: 0.2173544466495514\n",
      "iteration 35109: loss: 0.21735434234142303\n",
      "iteration 35110: loss: 0.21735422313213348\n",
      "iteration 35111: loss: 0.21735410392284393\n",
      "iteration 35112: loss: 0.21735386550426483\n",
      "iteration 35113: loss: 0.2173537313938141\n",
      "iteration 35114: loss: 0.2173536717891693\n",
      "iteration 35115: loss: 0.21735353767871857\n",
      "iteration 35116: loss: 0.21735338866710663\n",
      "iteration 35117: loss: 0.21735329926013947\n",
      "iteration 35118: loss: 0.21735315024852753\n",
      "iteration 35119: loss: 0.21735301613807678\n",
      "iteration 35120: loss: 0.21735291182994843\n",
      "iteration 35121: loss: 0.2173527181148529\n",
      "iteration 35122: loss: 0.21735265851020813\n",
      "iteration 35123: loss: 0.21735255420207977\n",
      "iteration 35124: loss: 0.21735234558582306\n",
      "iteration 35125: loss: 0.21735218167304993\n",
      "iteration 35126: loss: 0.21735206246376038\n",
      "iteration 35127: loss: 0.21735195815563202\n",
      "iteration 35128: loss: 0.21735182404518127\n",
      "iteration 35129: loss: 0.21735167503356934\n",
      "iteration 35130: loss: 0.2173515111207962\n",
      "iteration 35131: loss: 0.21735143661499023\n",
      "iteration 35132: loss: 0.2173512727022171\n",
      "iteration 35133: loss: 0.21735115349292755\n",
      "iteration 35134: loss: 0.21735107898712158\n",
      "iteration 35135: loss: 0.2173508107662201\n",
      "iteration 35136: loss: 0.21735075116157532\n",
      "iteration 35137: loss: 0.217350572347641\n",
      "iteration 35138: loss: 0.21735045313835144\n",
      "iteration 35139: loss: 0.2173503339290619\n",
      "iteration 35140: loss: 0.21735024452209473\n",
      "iteration 35141: loss: 0.2173500955104828\n",
      "iteration 35142: loss: 0.21734993159770966\n",
      "iteration 35143: loss: 0.2173498570919037\n",
      "iteration 35144: loss: 0.21734969317913055\n",
      "iteration 35145: loss: 0.21734949946403503\n",
      "iteration 35146: loss: 0.21734941005706787\n",
      "iteration 35147: loss: 0.21734926104545593\n",
      "iteration 35148: loss: 0.21734914183616638\n",
      "iteration 35149: loss: 0.21734905242919922\n",
      "iteration 35150: loss: 0.2173488438129425\n",
      "iteration 35151: loss: 0.21734873950481415\n",
      "iteration 35152: loss: 0.2173486053943634\n",
      "iteration 35153: loss: 0.21734848618507385\n",
      "iteration 35154: loss: 0.2173483818769455\n",
      "iteration 35155: loss: 0.21734824776649475\n",
      "iteration 35156: loss: 0.21734805405139923\n",
      "iteration 35157: loss: 0.21734794974327087\n",
      "iteration 35158: loss: 0.21734774112701416\n",
      "iteration 35159: loss: 0.2173476666212082\n",
      "iteration 35160: loss: 0.21734753251075745\n",
      "iteration 35161: loss: 0.2173474282026291\n",
      "iteration 35162: loss: 0.21734723448753357\n",
      "iteration 35163: loss: 0.2173471450805664\n",
      "iteration 35164: loss: 0.21734699606895447\n",
      "iteration 35165: loss: 0.2173469364643097\n",
      "iteration 35166: loss: 0.21734675765037537\n",
      "iteration 35167: loss: 0.21734659373760223\n",
      "iteration 35168: loss: 0.21734651923179626\n",
      "iteration 35169: loss: 0.21734635531902313\n",
      "iteration 35170: loss: 0.2173462212085724\n",
      "iteration 35171: loss: 0.21734611690044403\n",
      "iteration 35172: loss: 0.2173459529876709\n",
      "iteration 35173: loss: 0.21734580397605896\n",
      "iteration 35174: loss: 0.21734566986560822\n",
      "iteration 35175: loss: 0.21734556555747986\n",
      "iteration 35176: loss: 0.2173454761505127\n",
      "iteration 35177: loss: 0.2173452377319336\n",
      "iteration 35178: loss: 0.21734514832496643\n",
      "iteration 35179: loss: 0.21734504401683807\n",
      "iteration 35180: loss: 0.21734485030174255\n",
      "iteration 35181: loss: 0.21734480559825897\n",
      "iteration 35182: loss: 0.21734456717967987\n",
      "iteration 35183: loss: 0.21734455227851868\n",
      "iteration 35184: loss: 0.21734432876110077\n",
      "iteration 35185: loss: 0.2173442840576172\n",
      "iteration 35186: loss: 0.21734412014484406\n",
      "iteration 35187: loss: 0.2173440009355545\n",
      "iteration 35188: loss: 0.21734389662742615\n",
      "iteration 35189: loss: 0.21734373271465302\n",
      "iteration 35190: loss: 0.21734361350536346\n",
      "iteration 35191: loss: 0.21734340488910675\n",
      "iteration 35192: loss: 0.2173432558774948\n",
      "iteration 35193: loss: 0.2173432558774948\n",
      "iteration 35194: loss: 0.2173430174589157\n",
      "iteration 35195: loss: 0.21734288334846497\n",
      "iteration 35196: loss: 0.21734276413917542\n",
      "iteration 35197: loss: 0.21734261512756348\n",
      "iteration 35198: loss: 0.2173425406217575\n",
      "iteration 35199: loss: 0.217342346906662\n",
      "iteration 35200: loss: 0.21734225749969482\n",
      "iteration 35201: loss: 0.21734210848808289\n",
      "iteration 35202: loss: 0.21734198927879333\n",
      "iteration 35203: loss: 0.2173418551683426\n",
      "iteration 35204: loss: 0.21734170615673065\n",
      "iteration 35205: loss: 0.21734151244163513\n",
      "iteration 35206: loss: 0.21734146773815155\n",
      "iteration 35207: loss: 0.21734127402305603\n",
      "iteration 35208: loss: 0.21734113991260529\n",
      "iteration 35209: loss: 0.2173410952091217\n",
      "iteration 35210: loss: 0.21734094619750977\n",
      "iteration 35211: loss: 0.21734078228473663\n",
      "iteration 35212: loss: 0.21734067797660828\n",
      "iteration 35213: loss: 0.21734051406383514\n",
      "iteration 35214: loss: 0.21734042465686798\n",
      "iteration 35215: loss: 0.21734023094177246\n",
      "iteration 35216: loss: 0.2173401564359665\n",
      "iteration 35217: loss: 0.21733999252319336\n",
      "iteration 35218: loss: 0.2173398733139038\n",
      "iteration 35219: loss: 0.21733975410461426\n",
      "iteration 35220: loss: 0.21733959019184113\n",
      "iteration 35221: loss: 0.21733947098255157\n",
      "iteration 35222: loss: 0.21733935177326202\n",
      "iteration 35223: loss: 0.21733920276165009\n",
      "iteration 35224: loss: 0.21733908355236053\n",
      "iteration 35225: loss: 0.2173389196395874\n",
      "iteration 35226: loss: 0.21733884513378143\n",
      "iteration 35227: loss: 0.2173386812210083\n",
      "iteration 35228: loss: 0.21733853220939636\n",
      "iteration 35229: loss: 0.2173384726047516\n",
      "iteration 35230: loss: 0.21733829379081726\n",
      "iteration 35231: loss: 0.21733805537223816\n",
      "iteration 35232: loss: 0.21733799576759338\n",
      "iteration 35233: loss: 0.21733787655830383\n",
      "iteration 35234: loss: 0.21733777225017548\n",
      "iteration 35235: loss: 0.21733760833740234\n",
      "iteration 35236: loss: 0.2173374891281128\n",
      "iteration 35237: loss: 0.21733736991882324\n",
      "iteration 35238: loss: 0.2173372209072113\n",
      "iteration 35239: loss: 0.21733710169792175\n",
      "iteration 35240: loss: 0.21733692288398743\n",
      "iteration 35241: loss: 0.21733680367469788\n",
      "iteration 35242: loss: 0.21733665466308594\n",
      "iteration 35243: loss: 0.21733658015727997\n",
      "iteration 35244: loss: 0.21733641624450684\n",
      "iteration 35245: loss: 0.2173362523317337\n",
      "iteration 35246: loss: 0.21733617782592773\n",
      "iteration 35247: loss: 0.2173360288143158\n",
      "iteration 35248: loss: 0.21733590960502625\n",
      "iteration 35249: loss: 0.21733585000038147\n",
      "iteration 35250: loss: 0.2173355519771576\n",
      "iteration 35251: loss: 0.2173355370759964\n",
      "iteration 35252: loss: 0.21733537316322327\n",
      "iteration 35253: loss: 0.2173352688550949\n",
      "iteration 35254: loss: 0.21733513474464417\n",
      "iteration 35255: loss: 0.21733494102954865\n",
      "iteration 35256: loss: 0.21733489632606506\n",
      "iteration 35257: loss: 0.21733470261096954\n",
      "iteration 35258: loss: 0.2173345983028412\n",
      "iteration 35259: loss: 0.21733446419239044\n",
      "iteration 35260: loss: 0.2173343151807785\n",
      "iteration 35261: loss: 0.21733419597148895\n",
      "iteration 35262: loss: 0.2173341065645218\n",
      "iteration 35263: loss: 0.21733394265174866\n",
      "iteration 35264: loss: 0.21733379364013672\n",
      "iteration 35265: loss: 0.21733370423316956\n",
      "iteration 35266: loss: 0.21733346581459045\n",
      "iteration 35267: loss: 0.21733339130878448\n",
      "iteration 35268: loss: 0.21733327209949493\n",
      "iteration 35269: loss: 0.21733315289020538\n",
      "iteration 35270: loss: 0.21733298897743225\n",
      "iteration 35271: loss: 0.21733291447162628\n",
      "iteration 35272: loss: 0.21733276546001434\n",
      "iteration 35273: loss: 0.2173326313495636\n",
      "iteration 35274: loss: 0.21733251214027405\n",
      "iteration 35275: loss: 0.21733231842517853\n",
      "iteration 35276: loss: 0.21733224391937256\n",
      "iteration 35277: loss: 0.21733209490776062\n",
      "iteration 35278: loss: 0.2173319309949875\n",
      "iteration 35279: loss: 0.21733179688453674\n",
      "iteration 35280: loss: 0.2173316776752472\n",
      "iteration 35281: loss: 0.21733161807060242\n",
      "iteration 35282: loss: 0.21733149886131287\n",
      "iteration 35283: loss: 0.21733126044273376\n",
      "iteration 35284: loss: 0.21733112633228302\n",
      "iteration 35285: loss: 0.2173309624195099\n",
      "iteration 35286: loss: 0.2173309624195099\n",
      "iteration 35287: loss: 0.21733073890209198\n",
      "iteration 35288: loss: 0.21733061969280243\n",
      "iteration 35289: loss: 0.21733054518699646\n",
      "iteration 35290: loss: 0.21733033657073975\n",
      "iteration 35291: loss: 0.21733029186725616\n",
      "iteration 35292: loss: 0.21733012795448303\n",
      "iteration 35293: loss: 0.2173299789428711\n",
      "iteration 35294: loss: 0.21732985973358154\n",
      "iteration 35295: loss: 0.21732966601848602\n",
      "iteration 35296: loss: 0.21732959151268005\n",
      "iteration 35297: loss: 0.2173294574022293\n",
      "iteration 35298: loss: 0.21732930839061737\n",
      "iteration 35299: loss: 0.21732917428016663\n",
      "iteration 35300: loss: 0.21732906997203827\n",
      "iteration 35301: loss: 0.21732893586158752\n",
      "iteration 35302: loss: 0.21732881665229797\n",
      "iteration 35303: loss: 0.21732866764068604\n",
      "iteration 35304: loss: 0.21732857823371887\n",
      "iteration 35305: loss: 0.21732838451862335\n",
      "iteration 35306: loss: 0.2173282653093338\n",
      "iteration 35307: loss: 0.21732814610004425\n",
      "iteration 35308: loss: 0.2173280268907547\n",
      "iteration 35309: loss: 0.21732787787914276\n",
      "iteration 35310: loss: 0.2173277586698532\n",
      "iteration 35311: loss: 0.21732762455940247\n",
      "iteration 35312: loss: 0.21732750535011292\n",
      "iteration 35313: loss: 0.21732740104198456\n",
      "iteration 35314: loss: 0.21732720732688904\n",
      "iteration 35315: loss: 0.2173270732164383\n",
      "iteration 35316: loss: 0.21732687950134277\n",
      "iteration 35317: loss: 0.2173267900943756\n",
      "iteration 35318: loss: 0.21732668578624725\n",
      "iteration 35319: loss: 0.2173265963792801\n",
      "iteration 35320: loss: 0.21732643246650696\n",
      "iteration 35321: loss: 0.21732628345489502\n",
      "iteration 35322: loss: 0.21732613444328308\n",
      "iteration 35323: loss: 0.21732601523399353\n",
      "iteration 35324: loss: 0.21732588112354279\n",
      "iteration 35325: loss: 0.21732577681541443\n",
      "iteration 35326: loss: 0.21732564270496368\n",
      "iteration 35327: loss: 0.21732553839683533\n",
      "iteration 35328: loss: 0.21732544898986816\n",
      "iteration 35329: loss: 0.21732529997825623\n",
      "iteration 35330: loss: 0.21732516586780548\n",
      "iteration 35331: loss: 0.21732494235038757\n",
      "iteration 35332: loss: 0.21732492744922638\n",
      "iteration 35333: loss: 0.21732473373413086\n",
      "iteration 35334: loss: 0.2173246592283249\n",
      "iteration 35335: loss: 0.21732452511787415\n",
      "iteration 35336: loss: 0.21732430160045624\n",
      "iteration 35337: loss: 0.2173241823911667\n",
      "iteration 35338: loss: 0.21732410788536072\n",
      "iteration 35339: loss: 0.21732397377490997\n",
      "iteration 35340: loss: 0.21732382476329803\n",
      "iteration 35341: loss: 0.2173236906528473\n",
      "iteration 35342: loss: 0.21732358634471893\n",
      "iteration 35343: loss: 0.21732337772846222\n",
      "iteration 35344: loss: 0.21732333302497864\n",
      "iteration 35345: loss: 0.2173231542110443\n",
      "iteration 35346: loss: 0.21732299029827118\n",
      "iteration 35347: loss: 0.2173229455947876\n",
      "iteration 35348: loss: 0.21732273697853088\n",
      "iteration 35349: loss: 0.21732266247272491\n",
      "iteration 35350: loss: 0.21732251346111298\n",
      "iteration 35351: loss: 0.21732239425182343\n",
      "iteration 35352: loss: 0.21732226014137268\n",
      "iteration 35353: loss: 0.21732208132743835\n",
      "iteration 35354: loss: 0.21732191741466522\n",
      "iteration 35355: loss: 0.21732184290885925\n",
      "iteration 35356: loss: 0.2173217087984085\n",
      "iteration 35357: loss: 0.21732155978679657\n",
      "iteration 35358: loss: 0.21732144057750702\n",
      "iteration 35359: loss: 0.21732136607170105\n",
      "iteration 35360: loss: 0.2173212319612503\n",
      "iteration 35361: loss: 0.2173210084438324\n",
      "iteration 35362: loss: 0.21732088923454285\n",
      "iteration 35363: loss: 0.2173207700252533\n",
      "iteration 35364: loss: 0.21732063591480255\n",
      "iteration 35365: loss: 0.21732059121131897\n",
      "iteration 35366: loss: 0.21732044219970703\n",
      "iteration 35367: loss: 0.2173202931880951\n",
      "iteration 35368: loss: 0.21732015907764435\n",
      "iteration 35369: loss: 0.2173200100660324\n",
      "iteration 35370: loss: 0.21731987595558167\n",
      "iteration 35371: loss: 0.2173197716474533\n",
      "iteration 35372: loss: 0.21731963753700256\n",
      "iteration 35373: loss: 0.217319518327713\n",
      "iteration 35374: loss: 0.2173193395137787\n",
      "iteration 35375: loss: 0.2173192799091339\n",
      "iteration 35376: loss: 0.2173190861940384\n",
      "iteration 35377: loss: 0.21731892228126526\n",
      "iteration 35378: loss: 0.21731886267662048\n",
      "iteration 35379: loss: 0.21731869876384735\n",
      "iteration 35380: loss: 0.2173185795545578\n",
      "iteration 35381: loss: 0.21731850504875183\n",
      "iteration 35382: loss: 0.2173183262348175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 35383: loss: 0.21731820702552795\n",
      "iteration 35384: loss: 0.2173180878162384\n",
      "iteration 35385: loss: 0.21731790900230408\n",
      "iteration 35386: loss: 0.21731781959533691\n",
      "iteration 35387: loss: 0.21731765568256378\n",
      "iteration 35388: loss: 0.21731750667095184\n",
      "iteration 35389: loss: 0.21731741726398468\n",
      "iteration 35390: loss: 0.21731722354888916\n",
      "iteration 35391: loss: 0.217317134141922\n",
      "iteration 35392: loss: 0.21731702983379364\n",
      "iteration 35393: loss: 0.2173168957233429\n",
      "iteration 35394: loss: 0.21731674671173096\n",
      "iteration 35395: loss: 0.2173166573047638\n",
      "iteration 35396: loss: 0.21731650829315186\n",
      "iteration 35397: loss: 0.21731631457805634\n",
      "iteration 35398: loss: 0.21731624007225037\n",
      "iteration 35399: loss: 0.21731610596179962\n",
      "iteration 35400: loss: 0.21731595695018768\n",
      "iteration 35401: loss: 0.21731586754322052\n",
      "iteration 35402: loss: 0.2173157036304474\n",
      "iteration 35403: loss: 0.21731559932231903\n",
      "iteration 35404: loss: 0.2173154056072235\n",
      "iteration 35405: loss: 0.21731536090373993\n",
      "iteration 35406: loss: 0.2173151671886444\n",
      "iteration 35407: loss: 0.21731507778167725\n",
      "iteration 35408: loss: 0.2173149287700653\n",
      "iteration 35409: loss: 0.21731483936309814\n",
      "iteration 35410: loss: 0.21731464564800262\n",
      "iteration 35411: loss: 0.21731460094451904\n",
      "iteration 35412: loss: 0.2173144370317459\n",
      "iteration 35413: loss: 0.21731427311897278\n",
      "iteration 35414: loss: 0.21731409430503845\n",
      "iteration 35415: loss: 0.21731404960155487\n",
      "iteration 35416: loss: 0.21731393039226532\n",
      "iteration 35417: loss: 0.2173137664794922\n",
      "iteration 35418: loss: 0.21731367707252502\n",
      "iteration 35419: loss: 0.2173134982585907\n",
      "iteration 35420: loss: 0.21731336414813995\n",
      "iteration 35421: loss: 0.2173132449388504\n",
      "iteration 35422: loss: 0.21731309592723846\n",
      "iteration 35423: loss: 0.21731296181678772\n",
      "iteration 35424: loss: 0.21731281280517578\n",
      "iteration 35425: loss: 0.21731266379356384\n",
      "iteration 35426: loss: 0.21731261909008026\n",
      "iteration 35427: loss: 0.21731242537498474\n",
      "iteration 35428: loss: 0.21731236577033997\n",
      "iteration 35429: loss: 0.21731224656105042\n",
      "iteration 35430: loss: 0.2173120528459549\n",
      "iteration 35431: loss: 0.21731194853782654\n",
      "iteration 35432: loss: 0.2173118144273758\n",
      "iteration 35433: loss: 0.21731169521808624\n",
      "iteration 35434: loss: 0.2173115313053131\n",
      "iteration 35435: loss: 0.21731142699718475\n",
      "iteration 35436: loss: 0.217311292886734\n",
      "iteration 35437: loss: 0.21731114387512207\n",
      "iteration 35438: loss: 0.2173110544681549\n",
      "iteration 35439: loss: 0.21731095016002655\n",
      "iteration 35440: loss: 0.21731075644493103\n",
      "iteration 35441: loss: 0.2173105776309967\n",
      "iteration 35442: loss: 0.2173105776309967\n",
      "iteration 35443: loss: 0.2173103541135788\n",
      "iteration 35444: loss: 0.21731023490428925\n",
      "iteration 35445: loss: 0.2173100709915161\n",
      "iteration 35446: loss: 0.21731004118919373\n",
      "iteration 35447: loss: 0.21730978786945343\n",
      "iteration 35448: loss: 0.21730980277061462\n",
      "iteration 35449: loss: 0.21730954945087433\n",
      "iteration 35450: loss: 0.21730944514274597\n",
      "iteration 35451: loss: 0.21730928122997284\n",
      "iteration 35452: loss: 0.21730920672416687\n",
      "iteration 35453: loss: 0.21730904281139374\n",
      "iteration 35454: loss: 0.21730896830558777\n",
      "iteration 35455: loss: 0.21730880439281464\n",
      "iteration 35456: loss: 0.21730868518352509\n",
      "iteration 35457: loss: 0.21730847656726837\n",
      "iteration 35458: loss: 0.21730847656726837\n",
      "iteration 35459: loss: 0.21730828285217285\n",
      "iteration 35460: loss: 0.21730820834636688\n",
      "iteration 35461: loss: 0.21730804443359375\n",
      "iteration 35462: loss: 0.2173078954219818\n",
      "iteration 35463: loss: 0.21730777621269226\n",
      "iteration 35464: loss: 0.21730761229991913\n",
      "iteration 35465: loss: 0.21730752289295197\n",
      "iteration 35466: loss: 0.21730737388134003\n",
      "iteration 35467: loss: 0.2173071801662445\n",
      "iteration 35468: loss: 0.21730712056159973\n",
      "iteration 35469: loss: 0.21730700135231018\n",
      "iteration 35470: loss: 0.2173069268465042\n",
      "iteration 35471: loss: 0.21730676293373108\n",
      "iteration 35472: loss: 0.21730664372444153\n",
      "iteration 35473: loss: 0.2173064649105072\n",
      "iteration 35474: loss: 0.21730630099773407\n",
      "iteration 35475: loss: 0.2173062562942505\n",
      "iteration 35476: loss: 0.21730609238147736\n",
      "iteration 35477: loss: 0.217305988073349\n",
      "iteration 35478: loss: 0.21730580925941467\n",
      "iteration 35479: loss: 0.21730570495128632\n",
      "iteration 35480: loss: 0.21730561554431915\n",
      "iteration 35481: loss: 0.21730542182922363\n",
      "iteration 35482: loss: 0.21730533242225647\n",
      "iteration 35483: loss: 0.21730521321296692\n",
      "iteration 35484: loss: 0.2173050343990326\n",
      "iteration 35485: loss: 0.21730494499206543\n",
      "iteration 35486: loss: 0.2173047959804535\n",
      "iteration 35487: loss: 0.21730466187000275\n",
      "iteration 35488: loss: 0.2173045575618744\n",
      "iteration 35489: loss: 0.21730443835258484\n",
      "iteration 35490: loss: 0.2173042744398117\n",
      "iteration 35491: loss: 0.21730415523052216\n",
      "iteration 35492: loss: 0.21730396151542664\n",
      "iteration 35493: loss: 0.21730391681194305\n",
      "iteration 35494: loss: 0.2173037976026535\n",
      "iteration 35495: loss: 0.21730363368988037\n",
      "iteration 35496: loss: 0.21730346977710724\n",
      "iteration 35497: loss: 0.21730342507362366\n",
      "iteration 35498: loss: 0.21730324625968933\n",
      "iteration 35499: loss: 0.21730312705039978\n",
      "iteration 35500: loss: 0.21730299293994904\n",
      "iteration 35501: loss: 0.21730287373065948\n",
      "iteration 35502: loss: 0.21730276942253113\n",
      "iteration 35503: loss: 0.21730256080627441\n",
      "iteration 35504: loss: 0.21730244159698486\n",
      "iteration 35505: loss: 0.21730241179466248\n",
      "iteration 35506: loss: 0.21730223298072815\n",
      "iteration 35507: loss: 0.2173020839691162\n",
      "iteration 35508: loss: 0.21730196475982666\n",
      "iteration 35509: loss: 0.2173018455505371\n",
      "iteration 35510: loss: 0.21730169653892517\n",
      "iteration 35511: loss: 0.217301607131958\n",
      "iteration 35512: loss: 0.2173013985157013\n",
      "iteration 35513: loss: 0.21730132400989532\n",
      "iteration 35514: loss: 0.2173011302947998\n",
      "iteration 35515: loss: 0.21730101108551025\n",
      "iteration 35516: loss: 0.2173008918762207\n",
      "iteration 35517: loss: 0.21730080246925354\n",
      "iteration 35518: loss: 0.21730072796344757\n",
      "iteration 35519: loss: 0.21730056405067444\n",
      "iteration 35520: loss: 0.2173003852367401\n",
      "iteration 35521: loss: 0.21730026602745056\n",
      "iteration 35522: loss: 0.21730022132396698\n",
      "iteration 35523: loss: 0.21730008721351624\n",
      "iteration 35524: loss: 0.21729986369609833\n",
      "iteration 35525: loss: 0.21729977428913116\n",
      "iteration 35526: loss: 0.21729962527751923\n",
      "iteration 35527: loss: 0.21729950606822968\n",
      "iteration 35528: loss: 0.21729934215545654\n",
      "iteration 35529: loss: 0.21729926764965057\n",
      "iteration 35530: loss: 0.21729914844036102\n",
      "iteration 35531: loss: 0.21729901432991028\n",
      "iteration 35532: loss: 0.21729882061481476\n",
      "iteration 35533: loss: 0.2172987461090088\n",
      "iteration 35534: loss: 0.21729865670204163\n",
      "iteration 35535: loss: 0.2172985076904297\n",
      "iteration 35536: loss: 0.21729835867881775\n",
      "iteration 35537: loss: 0.21729819476604462\n",
      "iteration 35538: loss: 0.21729807555675507\n",
      "iteration 35539: loss: 0.21729794144630432\n",
      "iteration 35540: loss: 0.21729779243469238\n",
      "iteration 35541: loss: 0.21729764342308044\n",
      "iteration 35542: loss: 0.21729759871959686\n",
      "iteration 35543: loss: 0.21729740500450134\n",
      "iteration 35544: loss: 0.2172972857952118\n",
      "iteration 35545: loss: 0.21729719638824463\n",
      "iteration 35546: loss: 0.2172970324754715\n",
      "iteration 35547: loss: 0.21729695796966553\n",
      "iteration 35548: loss: 0.2172967493534088\n",
      "iteration 35549: loss: 0.21729667484760284\n",
      "iteration 35550: loss: 0.21729663014411926\n",
      "iteration 35551: loss: 0.21729643642902374\n",
      "iteration 35552: loss: 0.2172963172197342\n",
      "iteration 35553: loss: 0.21729616820812225\n",
      "iteration 35554: loss: 0.21729592978954315\n",
      "iteration 35555: loss: 0.21729588508605957\n",
      "iteration 35556: loss: 0.2172957956790924\n",
      "iteration 35557: loss: 0.21729564666748047\n",
      "iteration 35558: loss: 0.21729545295238495\n",
      "iteration 35559: loss: 0.21729545295238495\n",
      "iteration 35560: loss: 0.21729525923728943\n",
      "iteration 35561: loss: 0.2172950804233551\n",
      "iteration 35562: loss: 0.21729497611522675\n",
      "iteration 35563: loss: 0.21729488670825958\n",
      "iteration 35564: loss: 0.21729473769664764\n",
      "iteration 35565: loss: 0.2172946035861969\n",
      "iteration 35566: loss: 0.21729448437690735\n",
      "iteration 35567: loss: 0.2172943651676178\n",
      "iteration 35568: loss: 0.21729424595832825\n",
      "iteration 35569: loss: 0.2172940969467163\n",
      "iteration 35570: loss: 0.21729397773742676\n",
      "iteration 35571: loss: 0.21729382872581482\n",
      "iteration 35572: loss: 0.2172936648130417\n",
      "iteration 35573: loss: 0.2172936201095581\n",
      "iteration 35574: loss: 0.21729345619678497\n",
      "iteration 35575: loss: 0.21729329228401184\n",
      "iteration 35576: loss: 0.2172931730747223\n",
      "iteration 35577: loss: 0.21729306876659393\n",
      "iteration 35578: loss: 0.2172929346561432\n",
      "iteration 35579: loss: 0.21729278564453125\n",
      "iteration 35580: loss: 0.2172926366329193\n",
      "iteration 35581: loss: 0.21729251742362976\n",
      "iteration 35582: loss: 0.21729247272014618\n",
      "iteration 35583: loss: 0.21729226410388947\n",
      "iteration 35584: loss: 0.2172921895980835\n",
      "iteration 35585: loss: 0.21729202568531036\n",
      "iteration 35586: loss: 0.2172919511795044\n",
      "iteration 35587: loss: 0.21729175746440887\n",
      "iteration 35588: loss: 0.2172916680574417\n",
      "iteration 35589: loss: 0.21729154884815216\n",
      "iteration 35590: loss: 0.21729139983654022\n",
      "iteration 35591: loss: 0.2172912061214447\n",
      "iteration 35592: loss: 0.21729111671447754\n",
      "iteration 35593: loss: 0.21729102730751038\n",
      "iteration 35594: loss: 0.21729087829589844\n",
      "iteration 35595: loss: 0.2172907292842865\n",
      "iteration 35596: loss: 0.21729066967964172\n",
      "iteration 35597: loss: 0.21729044616222382\n",
      "iteration 35598: loss: 0.21729035675525665\n",
      "iteration 35599: loss: 0.21729020774364471\n",
      "iteration 35600: loss: 0.21729008853435516\n",
      "iteration 35601: loss: 0.217289999127388\n",
      "iteration 35602: loss: 0.21728980541229248\n",
      "iteration 35603: loss: 0.21728971600532532\n",
      "iteration 35604: loss: 0.21728961169719696\n",
      "iteration 35605: loss: 0.21728944778442383\n",
      "iteration 35606: loss: 0.2172892987728119\n",
      "iteration 35607: loss: 0.21728917956352234\n",
      "iteration 35608: loss: 0.2172890603542328\n",
      "iteration 35609: loss: 0.21728894114494324\n",
      "iteration 35610: loss: 0.2172887772321701\n",
      "iteration 35611: loss: 0.21728870272636414\n",
      "iteration 35612: loss: 0.217288538813591\n",
      "iteration 35613: loss: 0.21728846430778503\n",
      "iteration 35614: loss: 0.2172883301973343\n",
      "iteration 35615: loss: 0.21728821098804474\n",
      "iteration 35616: loss: 0.21728801727294922\n",
      "iteration 35617: loss: 0.21728797256946564\n",
      "iteration 35618: loss: 0.2172878086566925\n",
      "iteration 35619: loss: 0.21728768944740295\n",
      "iteration 35620: loss: 0.2172875702381134\n",
      "iteration 35621: loss: 0.21728737652301788\n",
      "iteration 35622: loss: 0.21728727221488953\n",
      "iteration 35623: loss: 0.21728715300559998\n",
      "iteration 35624: loss: 0.21728701889514923\n",
      "iteration 35625: loss: 0.21728691458702087\n",
      "iteration 35626: loss: 0.2172868251800537\n",
      "iteration 35627: loss: 0.21728666126728058\n",
      "iteration 35628: loss: 0.21728654205799103\n",
      "iteration 35629: loss: 0.2172863781452179\n",
      "iteration 35630: loss: 0.21728627383708954\n",
      "iteration 35631: loss: 0.2172861099243164\n",
      "iteration 35632: loss: 0.21728602051734924\n",
      "iteration 35633: loss: 0.2172859013080597\n",
      "iteration 35634: loss: 0.21728579699993134\n",
      "iteration 35635: loss: 0.21728558838367462\n",
      "iteration 35636: loss: 0.21728543937206268\n",
      "iteration 35637: loss: 0.2172853648662567\n",
      "iteration 35638: loss: 0.21728530526161194\n",
      "iteration 35639: loss: 0.21728511154651642\n",
      "iteration 35640: loss: 0.2172849476337433\n",
      "iteration 35641: loss: 0.21728484332561493\n",
      "iteration 35642: loss: 0.21728475391864777\n",
      "iteration 35643: loss: 0.21728456020355225\n",
      "iteration 35644: loss: 0.21728448569774628\n",
      "iteration 35645: loss: 0.21728429198265076\n",
      "iteration 35646: loss: 0.2172841578722\n",
      "iteration 35647: loss: 0.21728408336639404\n",
      "iteration 35648: loss: 0.2172839641571045\n",
      "iteration 35649: loss: 0.21728384494781494\n",
      "iteration 35650: loss: 0.21728365123271942\n",
      "iteration 35651: loss: 0.21728353202342987\n",
      "iteration 35652: loss: 0.2172834426164627\n",
      "iteration 35653: loss: 0.21728332340717316\n",
      "iteration 35654: loss: 0.2172832041978836\n",
      "iteration 35655: loss: 0.21728308498859406\n",
      "iteration 35656: loss: 0.2172829657793045\n",
      "iteration 35657: loss: 0.21728281676769257\n",
      "iteration 35658: loss: 0.2172827273607254\n",
      "iteration 35659: loss: 0.2172825038433075\n",
      "iteration 35660: loss: 0.21728238463401794\n",
      "iteration 35661: loss: 0.2172822654247284\n",
      "iteration 35662: loss: 0.21728213131427765\n",
      "iteration 35663: loss: 0.21728208661079407\n",
      "iteration 35664: loss: 0.21728186309337616\n",
      "iteration 35665: loss: 0.2172817438840866\n",
      "iteration 35666: loss: 0.21728166937828064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 35667: loss: 0.21728146076202393\n",
      "iteration 35668: loss: 0.21728143095970154\n",
      "iteration 35669: loss: 0.217281311750412\n",
      "iteration 35670: loss: 0.21728117763996124\n",
      "iteration 35671: loss: 0.21728098392486572\n",
      "iteration 35672: loss: 0.21728086471557617\n",
      "iteration 35673: loss: 0.217280775308609\n",
      "iteration 35674: loss: 0.21728062629699707\n",
      "iteration 35675: loss: 0.21728050708770752\n",
      "iteration 35676: loss: 0.2172803431749344\n",
      "iteration 35677: loss: 0.21728023886680603\n",
      "iteration 35678: loss: 0.21728011965751648\n",
      "iteration 35679: loss: 0.21727995574474335\n",
      "iteration 35680: loss: 0.21727986633777618\n",
      "iteration 35681: loss: 0.21727971732616425\n",
      "iteration 35682: loss: 0.2172795534133911\n",
      "iteration 35683: loss: 0.21727946400642395\n",
      "iteration 35684: loss: 0.2172793447971344\n",
      "iteration 35685: loss: 0.21727928519248962\n",
      "iteration 35686: loss: 0.2172791063785553\n",
      "iteration 35687: loss: 0.21727898716926575\n",
      "iteration 35688: loss: 0.2172788679599762\n",
      "iteration 35689: loss: 0.21727874875068665\n",
      "iteration 35690: loss: 0.2172786295413971\n",
      "iteration 35691: loss: 0.21727843582630157\n",
      "iteration 35692: loss: 0.21727831661701202\n",
      "iteration 35693: loss: 0.21727819740772247\n",
      "iteration 35694: loss: 0.21727807819843292\n",
      "iteration 35695: loss: 0.21727797389030457\n",
      "iteration 35696: loss: 0.21727785468101501\n",
      "iteration 35697: loss: 0.21727769076824188\n",
      "iteration 35698: loss: 0.21727755665779114\n",
      "iteration 35699: loss: 0.21727736294269562\n",
      "iteration 35700: loss: 0.2172773778438568\n",
      "iteration 35701: loss: 0.21727721393108368\n",
      "iteration 35702: loss: 0.2172771394252777\n",
      "iteration 35703: loss: 0.2172769010066986\n",
      "iteration 35704: loss: 0.21727676689624786\n",
      "iteration 35705: loss: 0.2172766923904419\n",
      "iteration 35706: loss: 0.21727657318115234\n",
      "iteration 35707: loss: 0.2172764241695404\n",
      "iteration 35708: loss: 0.21727633476257324\n",
      "iteration 35709: loss: 0.2172761857509613\n",
      "iteration 35710: loss: 0.21727605164051056\n",
      "iteration 35711: loss: 0.21727590262889862\n",
      "iteration 35712: loss: 0.21727578341960907\n",
      "iteration 35713: loss: 0.21727564930915833\n",
      "iteration 35714: loss: 0.2172755002975464\n",
      "iteration 35715: loss: 0.21727538108825684\n",
      "iteration 35716: loss: 0.21727529168128967\n",
      "iteration 35717: loss: 0.21727514266967773\n",
      "iteration 35718: loss: 0.2172749936580658\n",
      "iteration 35719: loss: 0.21727485954761505\n",
      "iteration 35720: loss: 0.2172747552394867\n",
      "iteration 35721: loss: 0.21727462112903595\n",
      "iteration 35722: loss: 0.2172745019197464\n",
      "iteration 35723: loss: 0.21727438271045685\n",
      "iteration 35724: loss: 0.21727421879768372\n",
      "iteration 35725: loss: 0.21727411448955536\n",
      "iteration 35726: loss: 0.21727398037910461\n",
      "iteration 35727: loss: 0.21727392077445984\n",
      "iteration 35728: loss: 0.21727371215820312\n",
      "iteration 35729: loss: 0.21727362275123596\n",
      "iteration 35730: loss: 0.21727347373962402\n",
      "iteration 35731: loss: 0.21727338433265686\n",
      "iteration 35732: loss: 0.21727320551872253\n",
      "iteration 35733: loss: 0.21727308630943298\n",
      "iteration 35734: loss: 0.21727296710014343\n",
      "iteration 35735: loss: 0.21727287769317627\n",
      "iteration 35736: loss: 0.21727271378040314\n",
      "iteration 35737: loss: 0.21727254986763\n",
      "iteration 35738: loss: 0.21727249026298523\n",
      "iteration 35739: loss: 0.21727235615253448\n",
      "iteration 35740: loss: 0.21727220714092255\n",
      "iteration 35741: loss: 0.2172720730304718\n",
      "iteration 35742: loss: 0.21727195382118225\n",
      "iteration 35743: loss: 0.2172718048095703\n",
      "iteration 35744: loss: 0.21727177500724792\n",
      "iteration 35745: loss: 0.2172716110944748\n",
      "iteration 35746: loss: 0.21727147698402405\n",
      "iteration 35747: loss: 0.21727129817008972\n",
      "iteration 35748: loss: 0.21727117896080017\n",
      "iteration 35749: loss: 0.217271089553833\n",
      "iteration 35750: loss: 0.21727100014686584\n",
      "iteration 35751: loss: 0.21727082133293152\n",
      "iteration 35752: loss: 0.2172706574201584\n",
      "iteration 35753: loss: 0.21727053821086884\n",
      "iteration 35754: loss: 0.21727041900157928\n",
      "iteration 35755: loss: 0.21727028489112854\n",
      "iteration 35756: loss: 0.21727021038532257\n",
      "iteration 35757: loss: 0.21727009117603302\n",
      "iteration 35758: loss: 0.2172699272632599\n",
      "iteration 35759: loss: 0.21726977825164795\n",
      "iteration 35760: loss: 0.217269629240036\n",
      "iteration 35761: loss: 0.21726951003074646\n",
      "iteration 35762: loss: 0.21726937592029572\n",
      "iteration 35763: loss: 0.21726930141448975\n",
      "iteration 35764: loss: 0.2172691822052002\n",
      "iteration 35765: loss: 0.21726903319358826\n",
      "iteration 35766: loss: 0.21726886928081512\n",
      "iteration 35767: loss: 0.21726875007152557\n",
      "iteration 35768: loss: 0.2172686606645584\n",
      "iteration 35769: loss: 0.21726851165294647\n",
      "iteration 35770: loss: 0.21726837754249573\n",
      "iteration 35771: loss: 0.21726834774017334\n",
      "iteration 35772: loss: 0.21726807951927185\n",
      "iteration 35773: loss: 0.2172679901123047\n",
      "iteration 35774: loss: 0.21726791560649872\n",
      "iteration 35775: loss: 0.21726778149604797\n",
      "iteration 35776: loss: 0.21726760268211365\n",
      "iteration 35777: loss: 0.21726751327514648\n",
      "iteration 35778: loss: 0.21726743876934052\n",
      "iteration 35779: loss: 0.21726727485656738\n",
      "iteration 35780: loss: 0.21726715564727783\n",
      "iteration 35781: loss: 0.2172669917345047\n",
      "iteration 35782: loss: 0.21726688742637634\n",
      "iteration 35783: loss: 0.2172667235136032\n",
      "iteration 35784: loss: 0.21726664900779724\n",
      "iteration 35785: loss: 0.21726658940315247\n",
      "iteration 35786: loss: 0.21726635098457336\n",
      "iteration 35787: loss: 0.2172662764787674\n",
      "iteration 35788: loss: 0.21726615726947784\n",
      "iteration 35789: loss: 0.2172660380601883\n",
      "iteration 35790: loss: 0.21726584434509277\n",
      "iteration 35791: loss: 0.21726572513580322\n",
      "iteration 35792: loss: 0.21726565062999725\n",
      "iteration 35793: loss: 0.21726545691490173\n",
      "iteration 35794: loss: 0.2172652930021286\n",
      "iteration 35795: loss: 0.2172652781009674\n",
      "iteration 35796: loss: 0.21726512908935547\n",
      "iteration 35797: loss: 0.2172650396823883\n",
      "iteration 35798: loss: 0.21726486086845398\n",
      "iteration 35799: loss: 0.21726474165916443\n",
      "iteration 35800: loss: 0.21726462244987488\n",
      "iteration 35801: loss: 0.21726445853710175\n",
      "iteration 35802: loss: 0.21726438403129578\n",
      "iteration 35803: loss: 0.21726420521736145\n",
      "iteration 35804: loss: 0.2172640860080719\n",
      "iteration 35805: loss: 0.21726396679878235\n",
      "iteration 35806: loss: 0.21726378798484802\n",
      "iteration 35807: loss: 0.21726369857788086\n",
      "iteration 35808: loss: 0.2172635793685913\n",
      "iteration 35809: loss: 0.21726346015930176\n",
      "iteration 35810: loss: 0.2172633707523346\n",
      "iteration 35811: loss: 0.21726322174072266\n",
      "iteration 35812: loss: 0.2172631025314331\n",
      "iteration 35813: loss: 0.21726295351982117\n",
      "iteration 35814: loss: 0.2172628939151764\n",
      "iteration 35815: loss: 0.21726274490356445\n",
      "iteration 35816: loss: 0.21726259589195251\n",
      "iteration 35817: loss: 0.21726243197917938\n",
      "iteration 35818: loss: 0.21726231276988983\n",
      "iteration 35819: loss: 0.21726223826408386\n",
      "iteration 35820: loss: 0.21726205945014954\n",
      "iteration 35821: loss: 0.21726198494434357\n",
      "iteration 35822: loss: 0.21726183593273163\n",
      "iteration 35823: loss: 0.2172616422176361\n",
      "iteration 35824: loss: 0.21726159751415253\n",
      "iteration 35825: loss: 0.21726147830486298\n",
      "iteration 35826: loss: 0.21726135909557343\n",
      "iteration 35827: loss: 0.2172611951828003\n",
      "iteration 35828: loss: 0.21726104617118835\n",
      "iteration 35829: loss: 0.2172609567642212\n",
      "iteration 35830: loss: 0.21726080775260925\n",
      "iteration 35831: loss: 0.2172607183456421\n",
      "iteration 35832: loss: 0.21726052463054657\n",
      "iteration 35833: loss: 0.2172604501247406\n",
      "iteration 35834: loss: 0.21726031601428986\n",
      "iteration 35835: loss: 0.2172602117061615\n",
      "iteration 35836: loss: 0.21726004779338837\n",
      "iteration 35837: loss: 0.2172599583864212\n",
      "iteration 35838: loss: 0.21725983917713165\n",
      "iteration 35839: loss: 0.21725969016551971\n",
      "iteration 35840: loss: 0.21725955605506897\n",
      "iteration 35841: loss: 0.21725943684577942\n",
      "iteration 35842: loss: 0.21725931763648987\n",
      "iteration 35843: loss: 0.21725913882255554\n",
      "iteration 35844: loss: 0.217259019613266\n",
      "iteration 35845: loss: 0.21725884079933167\n",
      "iteration 35846: loss: 0.21725881099700928\n",
      "iteration 35847: loss: 0.21725866198539734\n",
      "iteration 35848: loss: 0.2172585278749466\n",
      "iteration 35849: loss: 0.21725842356681824\n",
      "iteration 35850: loss: 0.2172582447528839\n",
      "iteration 35851: loss: 0.21725818514823914\n",
      "iteration 35852: loss: 0.21725809574127197\n",
      "iteration 35853: loss: 0.21725793182849884\n",
      "iteration 35854: loss: 0.2172577828168869\n",
      "iteration 35855: loss: 0.21725764870643616\n",
      "iteration 35856: loss: 0.2172575742006302\n",
      "iteration 35857: loss: 0.2172573357820511\n",
      "iteration 35858: loss: 0.2172573059797287\n",
      "iteration 35859: loss: 0.21725714206695557\n",
      "iteration 35860: loss: 0.21725702285766602\n",
      "iteration 35861: loss: 0.21725690364837646\n",
      "iteration 35862: loss: 0.21725673973560333\n",
      "iteration 35863: loss: 0.21725666522979736\n",
      "iteration 35864: loss: 0.2172565460205078\n",
      "iteration 35865: loss: 0.21725638210773468\n",
      "iteration 35866: loss: 0.21725621819496155\n",
      "iteration 35867: loss: 0.21725615859031677\n",
      "iteration 35868: loss: 0.21725602447986603\n",
      "iteration 35869: loss: 0.21725590527057648\n",
      "iteration 35870: loss: 0.21725575625896454\n",
      "iteration 35871: loss: 0.21725566685199738\n",
      "iteration 35872: loss: 0.21725550293922424\n",
      "iteration 35873: loss: 0.2172553837299347\n",
      "iteration 35874: loss: 0.21725527942180634\n",
      "iteration 35875: loss: 0.2172551453113556\n",
      "iteration 35876: loss: 0.21725504100322723\n",
      "iteration 35877: loss: 0.2172548770904541\n",
      "iteration 35878: loss: 0.21725478768348694\n",
      "iteration 35879: loss: 0.217254638671875\n",
      "iteration 35880: loss: 0.21725454926490784\n",
      "iteration 35881: loss: 0.2172544002532959\n",
      "iteration 35882: loss: 0.21725423634052277\n",
      "iteration 35883: loss: 0.21725411713123322\n",
      "iteration 35884: loss: 0.21725404262542725\n",
      "iteration 35885: loss: 0.2172538787126541\n",
      "iteration 35886: loss: 0.21725375950336456\n",
      "iteration 35887: loss: 0.2172536551952362\n",
      "iteration 35888: loss: 0.21725352108478546\n",
      "iteration 35889: loss: 0.2172534018754959\n",
      "iteration 35890: loss: 0.2172532081604004\n",
      "iteration 35891: loss: 0.21725308895111084\n",
      "iteration 35892: loss: 0.21725299954414368\n",
      "iteration 35893: loss: 0.21725285053253174\n",
      "iteration 35894: loss: 0.2172527313232422\n",
      "iteration 35895: loss: 0.21725258231163025\n",
      "iteration 35896: loss: 0.21725249290466309\n",
      "iteration 35897: loss: 0.21725240349769592\n",
      "iteration 35898: loss: 0.2172522097826004\n",
      "iteration 35899: loss: 0.21725210547447205\n",
      "iteration 35900: loss: 0.21725206077098846\n",
      "iteration 35901: loss: 0.21725192666053772\n",
      "iteration 35902: loss: 0.2172517329454422\n",
      "iteration 35903: loss: 0.21725162863731384\n",
      "iteration 35904: loss: 0.2172514945268631\n",
      "iteration 35905: loss: 0.21725130081176758\n",
      "iteration 35906: loss: 0.2172512263059616\n",
      "iteration 35907: loss: 0.21725115180015564\n",
      "iteration 35908: loss: 0.2172509878873825\n",
      "iteration 35909: loss: 0.21725086867809296\n",
      "iteration 35910: loss: 0.2172507345676422\n",
      "iteration 35911: loss: 0.21725061535835266\n",
      "iteration 35912: loss: 0.2172504961490631\n",
      "iteration 35913: loss: 0.21725039184093475\n",
      "iteration 35914: loss: 0.217250257730484\n",
      "iteration 35915: loss: 0.21725010871887207\n",
      "iteration 35916: loss: 0.21724995970726013\n",
      "iteration 35917: loss: 0.21724987030029297\n",
      "iteration 35918: loss: 0.21724972128868103\n",
      "iteration 35919: loss: 0.21724970638751984\n",
      "iteration 35920: loss: 0.21724946796894073\n",
      "iteration 35921: loss: 0.21724936366081238\n",
      "iteration 35922: loss: 0.21724924445152283\n",
      "iteration 35923: loss: 0.21724911034107208\n",
      "iteration 35924: loss: 0.21724896132946014\n",
      "iteration 35925: loss: 0.2172488421201706\n",
      "iteration 35926: loss: 0.21724876761436462\n",
      "iteration 35927: loss: 0.21724851429462433\n",
      "iteration 35928: loss: 0.21724846959114075\n",
      "iteration 35929: loss: 0.21724839508533478\n",
      "iteration 35930: loss: 0.21724823117256165\n",
      "iteration 35931: loss: 0.2172481119632721\n",
      "iteration 35932: loss: 0.21724800765514374\n",
      "iteration 35933: loss: 0.2172478437423706\n",
      "iteration 35934: loss: 0.21724767982959747\n",
      "iteration 35935: loss: 0.21724757552146912\n",
      "iteration 35936: loss: 0.21724751591682434\n",
      "iteration 35937: loss: 0.21724732220172882\n",
      "iteration 35938: loss: 0.21724724769592285\n",
      "iteration 35939: loss: 0.21724705398082733\n",
      "iteration 35940: loss: 0.21724700927734375\n",
      "iteration 35941: loss: 0.2172468602657318\n",
      "iteration 35942: loss: 0.21724672615528107\n",
      "iteration 35943: loss: 0.21724657714366913\n",
      "iteration 35944: loss: 0.21724648773670197\n",
      "iteration 35945: loss: 0.21724624931812286\n",
      "iteration 35946: loss: 0.21724620461463928\n",
      "iteration 35947: loss: 0.21724610030651093\n",
      "iteration 35948: loss: 0.21724602580070496\n",
      "iteration 35949: loss: 0.2172458916902542\n",
      "iteration 35950: loss: 0.2172456979751587\n",
      "iteration 35951: loss: 0.21724562346935272\n",
      "iteration 35952: loss: 0.2172454595565796\n",
      "iteration 35953: loss: 0.21724534034729004\n",
      "iteration 35954: loss: 0.2172451913356781\n",
      "iteration 35955: loss: 0.21724510192871094\n",
      "iteration 35956: loss: 0.21724501252174377\n",
      "iteration 35957: loss: 0.21724483370780945\n",
      "iteration 35958: loss: 0.2172447144985199\n",
      "iteration 35959: loss: 0.21724465489387512\n",
      "iteration 35960: loss: 0.2172444611787796\n",
      "iteration 35961: loss: 0.21724435687065125\n",
      "iteration 35962: loss: 0.2172441929578781\n",
      "iteration 35963: loss: 0.21724407374858856\n",
      "iteration 35964: loss: 0.2172439843416214\n",
      "iteration 35965: loss: 0.21724386513233185\n",
      "iteration 35966: loss: 0.21724364161491394\n",
      "iteration 35967: loss: 0.21724362671375275\n",
      "iteration 35968: loss: 0.21724343299865723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 35969: loss: 0.21724331378936768\n",
      "iteration 35970: loss: 0.2172432243824005\n",
      "iteration 35971: loss: 0.21724307537078857\n",
      "iteration 35972: loss: 0.21724292635917664\n",
      "iteration 35973: loss: 0.21724280714988708\n",
      "iteration 35974: loss: 0.2172427475452423\n",
      "iteration 35975: loss: 0.21724259853363037\n",
      "iteration 35976: loss: 0.2172425091266632\n",
      "iteration 35977: loss: 0.21724233031272888\n",
      "iteration 35978: loss: 0.2172422856092453\n",
      "iteration 35979: loss: 0.21724209189414978\n",
      "iteration 35980: loss: 0.21724191308021545\n",
      "iteration 35981: loss: 0.2172418087720871\n",
      "iteration 35982: loss: 0.21724173426628113\n",
      "iteration 35983: loss: 0.2172415554523468\n",
      "iteration 35984: loss: 0.21724148094654083\n",
      "iteration 35985: loss: 0.2172413319349289\n",
      "iteration 35986: loss: 0.21724124252796173\n",
      "iteration 35987: loss: 0.2172410935163498\n",
      "iteration 35988: loss: 0.21724097430706024\n",
      "iteration 35989: loss: 0.2172408103942871\n",
      "iteration 35990: loss: 0.21724069118499756\n",
      "iteration 35991: loss: 0.21724054217338562\n",
      "iteration 35992: loss: 0.21724045276641846\n",
      "iteration 35993: loss: 0.2172403335571289\n",
      "iteration 35994: loss: 0.21724016964435577\n",
      "iteration 35995: loss: 0.2172400951385498\n",
      "iteration 35996: loss: 0.21723997592926025\n",
      "iteration 35997: loss: 0.2172398567199707\n",
      "iteration 35998: loss: 0.21723966300487518\n",
      "iteration 35999: loss: 0.21723966300487518\n",
      "iteration 36000: loss: 0.21723942458629608\n",
      "iteration 36001: loss: 0.21723933517932892\n",
      "iteration 36002: loss: 0.21723921597003937\n",
      "iteration 36003: loss: 0.21723905205726624\n",
      "iteration 36004: loss: 0.21723899245262146\n",
      "iteration 36005: loss: 0.21723885834217072\n",
      "iteration 36006: loss: 0.21723875403404236\n",
      "iteration 36007: loss: 0.21723859012126923\n",
      "iteration 36008: loss: 0.21723850071430206\n",
      "iteration 36009: loss: 0.21723833680152893\n",
      "iteration 36010: loss: 0.21723821759223938\n",
      "iteration 36011: loss: 0.2172381430864334\n",
      "iteration 36012: loss: 0.21723797917366028\n",
      "iteration 36013: loss: 0.21723783016204834\n",
      "iteration 36014: loss: 0.21723774075508118\n",
      "iteration 36015: loss: 0.21723763644695282\n",
      "iteration 36016: loss: 0.2172374725341797\n",
      "iteration 36017: loss: 0.21723732352256775\n",
      "iteration 36018: loss: 0.217237189412117\n",
      "iteration 36019: loss: 0.21723704040050507\n",
      "iteration 36020: loss: 0.2172369509935379\n",
      "iteration 36021: loss: 0.21723680198192596\n",
      "iteration 36022: loss: 0.21723678708076477\n",
      "iteration 36023: loss: 0.21723656356334686\n",
      "iteration 36024: loss: 0.2172364890575409\n",
      "iteration 36025: loss: 0.21723635494709015\n",
      "iteration 36026: loss: 0.2172362059354782\n",
      "iteration 36027: loss: 0.21723608672618866\n",
      "iteration 36028: loss: 0.2172359675168991\n",
      "iteration 36029: loss: 0.21723584830760956\n",
      "iteration 36030: loss: 0.2172357589006424\n",
      "iteration 36031: loss: 0.21723556518554688\n",
      "iteration 36032: loss: 0.21723544597625732\n",
      "iteration 36033: loss: 0.21723532676696777\n",
      "iteration 36034: loss: 0.2172352522611618\n",
      "iteration 36035: loss: 0.21723511815071106\n",
      "iteration 36036: loss: 0.2172349989414215\n",
      "iteration 36037: loss: 0.21723482012748718\n",
      "iteration 36038: loss: 0.2172347754240036\n",
      "iteration 36039: loss: 0.2172345668077469\n",
      "iteration 36040: loss: 0.21723446249961853\n",
      "iteration 36041: loss: 0.21723437309265137\n",
      "iteration 36042: loss: 0.21723417937755585\n",
      "iteration 36043: loss: 0.21723410487174988\n",
      "iteration 36044: loss: 0.21723401546478271\n",
      "iteration 36045: loss: 0.21723386645317078\n",
      "iteration 36046: loss: 0.21723374724388123\n",
      "iteration 36047: loss: 0.21723361313343048\n",
      "iteration 36048: loss: 0.21723346412181854\n",
      "iteration 36049: loss: 0.217233344912529\n",
      "iteration 36050: loss: 0.21723325550556183\n",
      "iteration 36051: loss: 0.2172331064939499\n",
      "iteration 36052: loss: 0.21723294258117676\n",
      "iteration 36053: loss: 0.2172328680753708\n",
      "iteration 36054: loss: 0.21723274886608124\n",
      "iteration 36055: loss: 0.2172326296567917\n",
      "iteration 36056: loss: 0.21723251044750214\n",
      "iteration 36057: loss: 0.2172323763370514\n",
      "iteration 36058: loss: 0.21723219752311707\n",
      "iteration 36059: loss: 0.21723207831382751\n",
      "iteration 36060: loss: 0.21723195910453796\n",
      "iteration 36061: loss: 0.21723191440105438\n",
      "iteration 36062: loss: 0.21723179519176483\n",
      "iteration 36063: loss: 0.2172316014766693\n",
      "iteration 36064: loss: 0.21723143756389618\n",
      "iteration 36065: loss: 0.2172313928604126\n",
      "iteration 36066: loss: 0.21723122894763947\n",
      "iteration 36067: loss: 0.21723103523254395\n",
      "iteration 36068: loss: 0.21723100543022156\n",
      "iteration 36069: loss: 0.21723084151744843\n",
      "iteration 36070: loss: 0.21723075211048126\n",
      "iteration 36071: loss: 0.2172306478023529\n",
      "iteration 36072: loss: 0.21723051369190216\n",
      "iteration 36073: loss: 0.21723034977912903\n",
      "iteration 36074: loss: 0.21723024547100067\n",
      "iteration 36075: loss: 0.2172301560640335\n",
      "iteration 36076: loss: 0.21723003685474396\n",
      "iteration 36077: loss: 0.21722984313964844\n",
      "iteration 36078: loss: 0.21722975373268127\n",
      "iteration 36079: loss: 0.21722960472106934\n",
      "iteration 36080: loss: 0.21722951531410217\n",
      "iteration 36081: loss: 0.21722936630249023\n",
      "iteration 36082: loss: 0.21722924709320068\n",
      "iteration 36083: loss: 0.21722912788391113\n",
      "iteration 36084: loss: 0.21722900867462158\n",
      "iteration 36085: loss: 0.21722888946533203\n",
      "iteration 36086: loss: 0.2172287404537201\n",
      "iteration 36087: loss: 0.21722860634326935\n",
      "iteration 36088: loss: 0.217228502035141\n",
      "iteration 36089: loss: 0.21722841262817383\n",
      "iteration 36090: loss: 0.2172282636165619\n",
      "iteration 36091: loss: 0.21722817420959473\n",
      "iteration 36092: loss: 0.2172280251979828\n",
      "iteration 36093: loss: 0.21722789108753204\n",
      "iteration 36094: loss: 0.2172277718782425\n",
      "iteration 36095: loss: 0.21722762286663055\n",
      "iteration 36096: loss: 0.21722745895385742\n",
      "iteration 36097: loss: 0.21722736954689026\n",
      "iteration 36098: loss: 0.2172272503376007\n",
      "iteration 36099: loss: 0.21722717583179474\n",
      "iteration 36100: loss: 0.2172270566225052\n",
      "iteration 36101: loss: 0.21722686290740967\n",
      "iteration 36102: loss: 0.21722674369812012\n",
      "iteration 36103: loss: 0.21722662448883057\n",
      "iteration 36104: loss: 0.2172265350818634\n",
      "iteration 36105: loss: 0.21722643077373505\n",
      "iteration 36106: loss: 0.21722623705863953\n",
      "iteration 36107: loss: 0.21722622215747833\n",
      "iteration 36108: loss: 0.2172260731458664\n",
      "iteration 36109: loss: 0.21722593903541565\n",
      "iteration 36110: loss: 0.21722576022148132\n",
      "iteration 36111: loss: 0.21722570061683655\n",
      "iteration 36112: loss: 0.217225581407547\n",
      "iteration 36113: loss: 0.21722538769245148\n",
      "iteration 36114: loss: 0.21722526848316193\n",
      "iteration 36115: loss: 0.21722516417503357\n",
      "iteration 36116: loss: 0.21722498536109924\n",
      "iteration 36117: loss: 0.21722495555877686\n",
      "iteration 36118: loss: 0.21722479164600372\n",
      "iteration 36119: loss: 0.21722464263439178\n",
      "iteration 36120: loss: 0.21722450852394104\n",
      "iteration 36121: loss: 0.2172243595123291\n",
      "iteration 36122: loss: 0.21722431480884552\n",
      "iteration 36123: loss: 0.21722421050071716\n",
      "iteration 36124: loss: 0.21722400188446045\n",
      "iteration 36125: loss: 0.2172239124774933\n",
      "iteration 36126: loss: 0.21722379326820374\n",
      "iteration 36127: loss: 0.2172236442565918\n",
      "iteration 36128: loss: 0.21722355484962463\n",
      "iteration 36129: loss: 0.21722345054149628\n",
      "iteration 36130: loss: 0.21722328662872314\n",
      "iteration 36131: loss: 0.2172231674194336\n",
      "iteration 36132: loss: 0.21722304821014404\n",
      "iteration 36133: loss: 0.21722295880317688\n",
      "iteration 36134: loss: 0.21722280979156494\n",
      "iteration 36135: loss: 0.21722272038459778\n",
      "iteration 36136: loss: 0.21722257137298584\n",
      "iteration 36137: loss: 0.2172224074602127\n",
      "iteration 36138: loss: 0.21722230315208435\n",
      "iteration 36139: loss: 0.2172221839427948\n",
      "iteration 36140: loss: 0.21722206473350525\n",
      "iteration 36141: loss: 0.2172219306230545\n",
      "iteration 36142: loss: 0.21722181141376495\n",
      "iteration 36143: loss: 0.21722164750099182\n",
      "iteration 36144: loss: 0.21722154319286346\n",
      "iteration 36145: loss: 0.21722149848937988\n",
      "iteration 36146: loss: 0.21722133457660675\n",
      "iteration 36147: loss: 0.2172212153673172\n",
      "iteration 36148: loss: 0.21722106635570526\n",
      "iteration 36149: loss: 0.21722090244293213\n",
      "iteration 36150: loss: 0.21722082793712616\n",
      "iteration 36151: loss: 0.21722069382667542\n",
      "iteration 36152: loss: 0.21722054481506348\n",
      "iteration 36153: loss: 0.21722039580345154\n",
      "iteration 36154: loss: 0.217220276594162\n",
      "iteration 36155: loss: 0.21722018718719482\n",
      "iteration 36156: loss: 0.21722003817558289\n",
      "iteration 36157: loss: 0.21721991896629333\n",
      "iteration 36158: loss: 0.21721982955932617\n",
      "iteration 36159: loss: 0.21721963584423065\n",
      "iteration 36160: loss: 0.21721956133842468\n",
      "iteration 36161: loss: 0.21721947193145752\n",
      "iteration 36162: loss: 0.21721932291984558\n",
      "iteration 36163: loss: 0.21721918880939484\n",
      "iteration 36164: loss: 0.21721915900707245\n",
      "iteration 36165: loss: 0.21721892058849335\n",
      "iteration 36166: loss: 0.21721887588500977\n",
      "iteration 36167: loss: 0.21721875667572021\n",
      "iteration 36168: loss: 0.21721863746643066\n",
      "iteration 36169: loss: 0.21721844375133514\n",
      "iteration 36170: loss: 0.21721835434436798\n",
      "iteration 36171: loss: 0.21721819043159485\n",
      "iteration 36172: loss: 0.2172180414199829\n",
      "iteration 36173: loss: 0.21721799671649933\n",
      "iteration 36174: loss: 0.21721787750720978\n",
      "iteration 36175: loss: 0.2172178030014038\n",
      "iteration 36176: loss: 0.2172175645828247\n",
      "iteration 36177: loss: 0.21721749007701874\n",
      "iteration 36178: loss: 0.21721740067005157\n",
      "iteration 36179: loss: 0.21721725165843964\n",
      "iteration 36180: loss: 0.2172171175479889\n",
      "iteration 36181: loss: 0.21721696853637695\n",
      "iteration 36182: loss: 0.2172168791294098\n",
      "iteration 36183: loss: 0.21721668541431427\n",
      "iteration 36184: loss: 0.2172166407108307\n",
      "iteration 36185: loss: 0.21721644699573517\n",
      "iteration 36186: loss: 0.2172164022922516\n",
      "iteration 36187: loss: 0.21721622347831726\n",
      "iteration 36188: loss: 0.21721616387367249\n",
      "iteration 36189: loss: 0.21721604466438293\n",
      "iteration 36190: loss: 0.217215895652771\n",
      "iteration 36191: loss: 0.21721577644348145\n",
      "iteration 36192: loss: 0.2172156125307083\n",
      "iteration 36193: loss: 0.21721549332141876\n",
      "iteration 36194: loss: 0.2172153890132904\n",
      "iteration 36195: loss: 0.21721522510051727\n",
      "iteration 36196: loss: 0.2172151356935501\n",
      "iteration 36197: loss: 0.21721498668193817\n",
      "iteration 36198: loss: 0.217214897274971\n",
      "iteration 36199: loss: 0.21721474826335907\n",
      "iteration 36200: loss: 0.2172146588563919\n",
      "iteration 36201: loss: 0.21721453964710236\n",
      "iteration 36202: loss: 0.21721434593200684\n",
      "iteration 36203: loss: 0.21721431612968445\n",
      "iteration 36204: loss: 0.2172141820192337\n",
      "iteration 36205: loss: 0.21721403300762177\n",
      "iteration 36206: loss: 0.21721391379833221\n",
      "iteration 36207: loss: 0.21721382439136505\n",
      "iteration 36208: loss: 0.21721358597278595\n",
      "iteration 36209: loss: 0.21721355617046356\n",
      "iteration 36210: loss: 0.21721342206001282\n",
      "iteration 36211: loss: 0.21721330285072327\n",
      "iteration 36212: loss: 0.21721315383911133\n",
      "iteration 36213: loss: 0.21721303462982178\n",
      "iteration 36214: loss: 0.21721291542053223\n",
      "iteration 36215: loss: 0.2172127664089203\n",
      "iteration 36216: loss: 0.21721263229846954\n",
      "iteration 36217: loss: 0.21721251308918\n",
      "iteration 36218: loss: 0.2172124832868576\n",
      "iteration 36219: loss: 0.21721228957176208\n",
      "iteration 36220: loss: 0.21721220016479492\n",
      "iteration 36221: loss: 0.2172120362520218\n",
      "iteration 36222: loss: 0.2172120064496994\n",
      "iteration 36223: loss: 0.2172117978334427\n",
      "iteration 36224: loss: 0.21721172332763672\n",
      "iteration 36225: loss: 0.2172115594148636\n",
      "iteration 36226: loss: 0.21721144020557404\n",
      "iteration 36227: loss: 0.21721132099628448\n",
      "iteration 36228: loss: 0.21721120178699493\n",
      "iteration 36229: loss: 0.2172110378742218\n",
      "iteration 36230: loss: 0.21721093356609344\n",
      "iteration 36231: loss: 0.2172108143568039\n",
      "iteration 36232: loss: 0.21721072494983673\n",
      "iteration 36233: loss: 0.21721060574054718\n",
      "iteration 36234: loss: 0.21721044182777405\n",
      "iteration 36235: loss: 0.2172103226184845\n",
      "iteration 36236: loss: 0.21721017360687256\n",
      "iteration 36237: loss: 0.217210054397583\n",
      "iteration 36238: loss: 0.21720996499061584\n",
      "iteration 36239: loss: 0.2172098159790039\n",
      "iteration 36240: loss: 0.21720972657203674\n",
      "iteration 36241: loss: 0.2172096073627472\n",
      "iteration 36242: loss: 0.21720948815345764\n",
      "iteration 36243: loss: 0.2172093391418457\n",
      "iteration 36244: loss: 0.21720921993255615\n",
      "iteration 36245: loss: 0.2172091007232666\n",
      "iteration 36246: loss: 0.21720898151397705\n",
      "iteration 36247: loss: 0.2172088325023651\n",
      "iteration 36248: loss: 0.21720874309539795\n",
      "iteration 36249: loss: 0.2172086238861084\n",
      "iteration 36250: loss: 0.21720847487449646\n",
      "iteration 36251: loss: 0.2172083556652069\n",
      "iteration 36252: loss: 0.21720823645591736\n",
      "iteration 36253: loss: 0.2172081470489502\n",
      "iteration 36254: loss: 0.21720795333385468\n",
      "iteration 36255: loss: 0.21720795333385468\n",
      "iteration 36256: loss: 0.21720775961875916\n",
      "iteration 36257: loss: 0.2172076255083084\n",
      "iteration 36258: loss: 0.21720758080482483\n",
      "iteration 36259: loss: 0.2172074019908905\n",
      "iteration 36260: loss: 0.21720726788043976\n",
      "iteration 36261: loss: 0.2172071933746338\n",
      "iteration 36262: loss: 0.21720698475837708\n",
      "iteration 36263: loss: 0.21720688045024872\n",
      "iteration 36264: loss: 0.21720674633979797\n",
      "iteration 36265: loss: 0.21720662713050842\n",
      "iteration 36266: loss: 0.21720656752586365\n",
      "iteration 36267: loss: 0.2172064483165741\n",
      "iteration 36268: loss: 0.21720626950263977\n",
      "iteration 36269: loss: 0.2172061949968338\n",
      "iteration 36270: loss: 0.21720603108406067\n",
      "iteration 36271: loss: 0.2172059267759323\n",
      "iteration 36272: loss: 0.21720579266548157\n",
      "iteration 36273: loss: 0.21720564365386963\n",
      "iteration 36274: loss: 0.21720559895038605\n",
      "iteration 36275: loss: 0.21720540523529053\n",
      "iteration 36276: loss: 0.217205211520195\n",
      "iteration 36277: loss: 0.2172051966190338\n",
      "iteration 36278: loss: 0.21720504760742188\n",
      "iteration 36279: loss: 0.21720489859580994\n",
      "iteration 36280: loss: 0.21720483899116516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 36281: loss: 0.2172047346830368\n",
      "iteration 36282: loss: 0.2172045260667801\n",
      "iteration 36283: loss: 0.21720440685749054\n",
      "iteration 36284: loss: 0.21720433235168457\n",
      "iteration 36285: loss: 0.21720413863658905\n",
      "iteration 36286: loss: 0.21720409393310547\n",
      "iteration 36287: loss: 0.21720397472381592\n",
      "iteration 36288: loss: 0.21720385551452637\n",
      "iteration 36289: loss: 0.21720370650291443\n",
      "iteration 36290: loss: 0.21720361709594727\n",
      "iteration 36291: loss: 0.21720345318317413\n",
      "iteration 36292: loss: 0.21720334887504578\n",
      "iteration 36293: loss: 0.21720322966575623\n",
      "iteration 36294: loss: 0.21720311045646667\n",
      "iteration 36295: loss: 0.21720297634601593\n",
      "iteration 36296: loss: 0.21720285713672638\n",
      "iteration 36297: loss: 0.21720269322395325\n",
      "iteration 36298: loss: 0.21720261871814728\n",
      "iteration 36299: loss: 0.21720249950885773\n",
      "iteration 36300: loss: 0.21720239520072937\n",
      "iteration 36301: loss: 0.21720223128795624\n",
      "iteration 36302: loss: 0.2172020971775055\n",
      "iteration 36303: loss: 0.21720197796821594\n",
      "iteration 36304: loss: 0.21720190346240997\n",
      "iteration 36305: loss: 0.21720170974731445\n",
      "iteration 36306: loss: 0.2172016203403473\n",
      "iteration 36307: loss: 0.21720150113105774\n",
      "iteration 36308: loss: 0.21720142662525177\n",
      "iteration 36309: loss: 0.21720123291015625\n",
      "iteration 36310: loss: 0.2172011137008667\n",
      "iteration 36311: loss: 0.21720099449157715\n",
      "iteration 36312: loss: 0.21720094978809357\n",
      "iteration 36313: loss: 0.21720072627067566\n",
      "iteration 36314: loss: 0.2172006070613861\n",
      "iteration 36315: loss: 0.21720056235790253\n",
      "iteration 36316: loss: 0.217200368642807\n",
      "iteration 36317: loss: 0.21720023453235626\n",
      "iteration 36318: loss: 0.2172001302242279\n",
      "iteration 36319: loss: 0.21720001101493835\n",
      "iteration 36320: loss: 0.2171998918056488\n",
      "iteration 36321: loss: 0.21719977259635925\n",
      "iteration 36322: loss: 0.2171996533870697\n",
      "iteration 36323: loss: 0.21719951927661896\n",
      "iteration 36324: loss: 0.2171994149684906\n",
      "iteration 36325: loss: 0.21719928085803986\n",
      "iteration 36326: loss: 0.2171991765499115\n",
      "iteration 36327: loss: 0.21719904243946075\n",
      "iteration 36328: loss: 0.21719889342784882\n",
      "iteration 36329: loss: 0.21719880402088165\n",
      "iteration 36330: loss: 0.21719872951507568\n",
      "iteration 36331: loss: 0.21719853579998016\n",
      "iteration 36332: loss: 0.217198446393013\n",
      "iteration 36333: loss: 0.21719828248023987\n",
      "iteration 36334: loss: 0.21719825267791748\n",
      "iteration 36335: loss: 0.21719805896282196\n",
      "iteration 36336: loss: 0.2171979397535324\n",
      "iteration 36337: loss: 0.21719785034656525\n",
      "iteration 36338: loss: 0.21719765663146973\n",
      "iteration 36339: loss: 0.21719761192798615\n",
      "iteration 36340: loss: 0.217197448015213\n",
      "iteration 36341: loss: 0.21719737350940704\n",
      "iteration 36342: loss: 0.2171972244977951\n",
      "iteration 36343: loss: 0.21719703078269958\n",
      "iteration 36344: loss: 0.21719691157341003\n",
      "iteration 36345: loss: 0.21719685196876526\n",
      "iteration 36346: loss: 0.2171967476606369\n",
      "iteration 36347: loss: 0.21719658374786377\n",
      "iteration 36348: loss: 0.2171964943408966\n",
      "iteration 36349: loss: 0.21719631552696228\n",
      "iteration 36350: loss: 0.21719622611999512\n",
      "iteration 36351: loss: 0.21719615161418915\n",
      "iteration 36352: loss: 0.21719594299793243\n",
      "iteration 36353: loss: 0.21719589829444885\n",
      "iteration 36354: loss: 0.2171957790851593\n",
      "iteration 36355: loss: 0.21719558537006378\n",
      "iteration 36356: loss: 0.2171955406665802\n",
      "iteration 36357: loss: 0.21719534695148468\n",
      "iteration 36358: loss: 0.2171952724456787\n",
      "iteration 36359: loss: 0.21719512343406677\n",
      "iteration 36360: loss: 0.21719495952129364\n",
      "iteration 36361: loss: 0.21719494462013245\n",
      "iteration 36362: loss: 0.2171948254108429\n",
      "iteration 36363: loss: 0.217194601893425\n",
      "iteration 36364: loss: 0.21719446778297424\n",
      "iteration 36365: loss: 0.21719446778297424\n",
      "iteration 36366: loss: 0.21719427406787872\n",
      "iteration 36367: loss: 0.21719415485858917\n",
      "iteration 36368: loss: 0.2171940803527832\n",
      "iteration 36369: loss: 0.21719393134117126\n",
      "iteration 36370: loss: 0.2171938717365265\n",
      "iteration 36371: loss: 0.21719364821910858\n",
      "iteration 36372: loss: 0.2171935737133026\n",
      "iteration 36373: loss: 0.21719345450401306\n",
      "iteration 36374: loss: 0.21719332039356232\n",
      "iteration 36375: loss: 0.21719317138195038\n",
      "iteration 36376: loss: 0.21719308197498322\n",
      "iteration 36377: loss: 0.21719296276569366\n",
      "iteration 36378: loss: 0.2171928882598877\n",
      "iteration 36379: loss: 0.21719267964363098\n",
      "iteration 36380: loss: 0.217192605137825\n",
      "iteration 36381: loss: 0.21719244122505188\n",
      "iteration 36382: loss: 0.2171923816204071\n",
      "iteration 36383: loss: 0.217192143201828\n",
      "iteration 36384: loss: 0.2171921730041504\n",
      "iteration 36385: loss: 0.21719197928905487\n",
      "iteration 36386: loss: 0.21719184517860413\n",
      "iteration 36387: loss: 0.21719174087047577\n",
      "iteration 36388: loss: 0.21719162166118622\n",
      "iteration 36389: loss: 0.2171914279460907\n",
      "iteration 36390: loss: 0.21719136834144592\n",
      "iteration 36391: loss: 0.21719124913215637\n",
      "iteration 36392: loss: 0.21719112992286682\n",
      "iteration 36393: loss: 0.21719098091125488\n",
      "iteration 36394: loss: 0.2171909064054489\n",
      "iteration 36395: loss: 0.2171907126903534\n",
      "iteration 36396: loss: 0.21719054877758026\n",
      "iteration 36397: loss: 0.2171904742717743\n",
      "iteration 36398: loss: 0.2171904295682907\n",
      "iteration 36399: loss: 0.2171901911497116\n",
      "iteration 36400: loss: 0.21719010174274445\n",
      "iteration 36401: loss: 0.21719002723693848\n",
      "iteration 36402: loss: 0.21718986332416534\n",
      "iteration 36403: loss: 0.21718978881835938\n",
      "iteration 36404: loss: 0.2171896994113922\n",
      "iteration 36405: loss: 0.2171895056962967\n",
      "iteration 36406: loss: 0.2171894609928131\n",
      "iteration 36407: loss: 0.2171892672777176\n",
      "iteration 36408: loss: 0.21718919277191162\n",
      "iteration 36409: loss: 0.21718907356262207\n",
      "iteration 36410: loss: 0.21718892455101013\n",
      "iteration 36411: loss: 0.21718886494636536\n",
      "iteration 36412: loss: 0.21718862652778625\n",
      "iteration 36413: loss: 0.21718856692314148\n",
      "iteration 36414: loss: 0.21718844771385193\n",
      "iteration 36415: loss: 0.2171882838010788\n",
      "iteration 36416: loss: 0.21718820929527283\n",
      "iteration 36417: loss: 0.2171880304813385\n",
      "iteration 36418: loss: 0.21718795597553253\n",
      "iteration 36419: loss: 0.2171877920627594\n",
      "iteration 36420: loss: 0.21718771755695343\n",
      "iteration 36421: loss: 0.21718759834766388\n",
      "iteration 36422: loss: 0.21718744933605194\n",
      "iteration 36423: loss: 0.21718735992908478\n",
      "iteration 36424: loss: 0.21718725562095642\n",
      "iteration 36425: loss: 0.2171870917081833\n",
      "iteration 36426: loss: 0.21718701720237732\n",
      "iteration 36427: loss: 0.2171868085861206\n",
      "iteration 36428: loss: 0.21718673408031464\n",
      "iteration 36429: loss: 0.21718664467334747\n",
      "iteration 36430: loss: 0.21718649566173553\n",
      "iteration 36431: loss: 0.21718637645244598\n",
      "iteration 36432: loss: 0.21718625724315643\n",
      "iteration 36433: loss: 0.21718613803386688\n",
      "iteration 36434: loss: 0.21718601882457733\n",
      "iteration 36435: loss: 0.21718589961528778\n",
      "iteration 36436: loss: 0.21718576550483704\n",
      "iteration 36437: loss: 0.2171855866909027\n",
      "iteration 36438: loss: 0.21718546748161316\n",
      "iteration 36439: loss: 0.21718542277812958\n",
      "iteration 36440: loss: 0.21718522906303406\n",
      "iteration 36441: loss: 0.21718516945838928\n",
      "iteration 36442: loss: 0.21718502044677734\n",
      "iteration 36443: loss: 0.21718497574329376\n",
      "iteration 36444: loss: 0.21718481183052063\n",
      "iteration 36445: loss: 0.21718469262123108\n",
      "iteration 36446: loss: 0.21718449890613556\n",
      "iteration 36447: loss: 0.2171844244003296\n",
      "iteration 36448: loss: 0.21718427538871765\n",
      "iteration 36449: loss: 0.2171841561794281\n",
      "iteration 36450: loss: 0.21718406677246094\n",
      "iteration 36451: loss: 0.21718397736549377\n",
      "iteration 36452: loss: 0.21718382835388184\n",
      "iteration 36453: loss: 0.21718373894691467\n",
      "iteration 36454: loss: 0.21718354523181915\n",
      "iteration 36455: loss: 0.2171834409236908\n",
      "iteration 36456: loss: 0.21718330681324005\n",
      "iteration 36457: loss: 0.21718323230743408\n",
      "iteration 36458: loss: 0.21718306839466095\n",
      "iteration 36459: loss: 0.2171829640865326\n",
      "iteration 36460: loss: 0.21718282997608185\n",
      "iteration 36461: loss: 0.2171826809644699\n",
      "iteration 36462: loss: 0.21718263626098633\n",
      "iteration 36463: loss: 0.2171824425458908\n",
      "iteration 36464: loss: 0.21718239784240723\n",
      "iteration 36465: loss: 0.2171822041273117\n",
      "iteration 36466: loss: 0.21718211472034454\n",
      "iteration 36467: loss: 0.217181995511055\n",
      "iteration 36468: loss: 0.21718187630176544\n",
      "iteration 36469: loss: 0.2171817272901535\n",
      "iteration 36470: loss: 0.21718165278434753\n",
      "iteration 36471: loss: 0.21718153357505798\n",
      "iteration 36472: loss: 0.21718136966228485\n",
      "iteration 36473: loss: 0.2171812504529953\n",
      "iteration 36474: loss: 0.21718113124370575\n",
      "iteration 36475: loss: 0.2171810120344162\n",
      "iteration 36476: loss: 0.21718092262744904\n",
      "iteration 36477: loss: 0.21718080341815948\n",
      "iteration 36478: loss: 0.21718060970306396\n",
      "iteration 36479: loss: 0.21718057990074158\n",
      "iteration 36480: loss: 0.21718040108680725\n",
      "iteration 36481: loss: 0.2171802967786789\n",
      "iteration 36482: loss: 0.21718016266822815\n",
      "iteration 36483: loss: 0.2171800434589386\n",
      "iteration 36484: loss: 0.21717992424964905\n",
      "iteration 36485: loss: 0.2171798199415207\n",
      "iteration 36486: loss: 0.21717968583106995\n",
      "iteration 36487: loss: 0.2171795666217804\n",
      "iteration 36488: loss: 0.21717946231365204\n",
      "iteration 36489: loss: 0.2171792984008789\n",
      "iteration 36490: loss: 0.21717920899391174\n",
      "iteration 36491: loss: 0.2171790599822998\n",
      "iteration 36492: loss: 0.21717898547649384\n",
      "iteration 36493: loss: 0.21717886626720428\n",
      "iteration 36494: loss: 0.21717873215675354\n",
      "iteration 36495: loss: 0.2171785533428192\n",
      "iteration 36496: loss: 0.21717853844165802\n",
      "iteration 36497: loss: 0.2171783447265625\n",
      "iteration 36498: loss: 0.21717822551727295\n",
      "iteration 36499: loss: 0.2171781361103058\n",
      "iteration 36500: loss: 0.21717798709869385\n",
      "iteration 36501: loss: 0.2171778678894043\n",
      "iteration 36502: loss: 0.21717771887779236\n",
      "iteration 36503: loss: 0.2171776294708252\n",
      "iteration 36504: loss: 0.21717755496501923\n",
      "iteration 36505: loss: 0.21717743575572968\n",
      "iteration 36506: loss: 0.21717722713947296\n",
      "iteration 36507: loss: 0.2171771079301834\n",
      "iteration 36508: loss: 0.21717700362205505\n",
      "iteration 36509: loss: 0.2171768695116043\n",
      "iteration 36510: loss: 0.21717675030231476\n",
      "iteration 36511: loss: 0.2171766310930252\n",
      "iteration 36512: loss: 0.21717651188373566\n",
      "iteration 36513: loss: 0.21717646718025208\n",
      "iteration 36514: loss: 0.21717624366283417\n",
      "iteration 36515: loss: 0.217176154255867\n",
      "iteration 36516: loss: 0.21717604994773865\n",
      "iteration 36517: loss: 0.21717596054077148\n",
      "iteration 36518: loss: 0.21717579662799835\n",
      "iteration 36519: loss: 0.21717572212219238\n",
      "iteration 36520: loss: 0.21717555820941925\n",
      "iteration 36521: loss: 0.2171754539012909\n",
      "iteration 36522: loss: 0.21717531979084015\n",
      "iteration 36523: loss: 0.21717524528503418\n",
      "iteration 36524: loss: 0.21717509627342224\n",
      "iteration 36525: loss: 0.2171749770641327\n",
      "iteration 36526: loss: 0.21717485785484314\n",
      "iteration 36527: loss: 0.2171747237443924\n",
      "iteration 36528: loss: 0.21717461943626404\n",
      "iteration 36529: loss: 0.21717457473278046\n",
      "iteration 36530: loss: 0.21717433631420135\n",
      "iteration 36531: loss: 0.2171742469072342\n",
      "iteration 36532: loss: 0.21717414259910583\n",
      "iteration 36533: loss: 0.2171739637851715\n",
      "iteration 36534: loss: 0.21717385947704315\n",
      "iteration 36535: loss: 0.2171737253665924\n",
      "iteration 36536: loss: 0.21717360615730286\n",
      "iteration 36537: loss: 0.2171735316514969\n",
      "iteration 36538: loss: 0.21717338263988495\n",
      "iteration 36539: loss: 0.21717329323291779\n",
      "iteration 36540: loss: 0.21717314422130585\n",
      "iteration 36541: loss: 0.21717305481433868\n",
      "iteration 36542: loss: 0.21717290580272675\n",
      "iteration 36543: loss: 0.21717286109924316\n",
      "iteration 36544: loss: 0.21717266738414764\n",
      "iteration 36545: loss: 0.21717257797718048\n",
      "iteration 36546: loss: 0.21717241406440735\n",
      "iteration 36547: loss: 0.217172309756279\n",
      "iteration 36548: loss: 0.21717214584350586\n",
      "iteration 36549: loss: 0.2171720564365387\n",
      "iteration 36550: loss: 0.21717199683189392\n",
      "iteration 36551: loss: 0.2171718180179596\n",
      "iteration 36552: loss: 0.21717175841331482\n",
      "iteration 36553: loss: 0.2171715795993805\n",
      "iteration 36554: loss: 0.21717146039009094\n",
      "iteration 36555: loss: 0.21717128157615662\n",
      "iteration 36556: loss: 0.21717119216918945\n",
      "iteration 36557: loss: 0.2171710729598999\n",
      "iteration 36558: loss: 0.21717095375061035\n",
      "iteration 36559: loss: 0.2171708643436432\n",
      "iteration 36560: loss: 0.21717076003551483\n",
      "iteration 36561: loss: 0.2171705961227417\n",
      "iteration 36562: loss: 0.21717050671577454\n",
      "iteration 36563: loss: 0.21717040240764618\n",
      "iteration 36564: loss: 0.21717020869255066\n",
      "iteration 36565: loss: 0.21717007458209991\n",
      "iteration 36566: loss: 0.21717004477977753\n",
      "iteration 36567: loss: 0.21716980636119843\n",
      "iteration 36568: loss: 0.21716979146003723\n",
      "iteration 36569: loss: 0.21716971695423126\n",
      "iteration 36570: loss: 0.21716956794261932\n",
      "iteration 36571: loss: 0.21716943383216858\n",
      "iteration 36572: loss: 0.21716931462287903\n",
      "iteration 36573: loss: 0.2171691358089447\n",
      "iteration 36574: loss: 0.21716901659965515\n",
      "iteration 36575: loss: 0.217168927192688\n",
      "iteration 36576: loss: 0.21716876327991486\n",
      "iteration 36577: loss: 0.2171686589717865\n",
      "iteration 36578: loss: 0.21716856956481934\n",
      "iteration 36579: loss: 0.21716848015785217\n",
      "iteration 36580: loss: 0.21716828644275665\n",
      "iteration 36581: loss: 0.21716824173927307\n",
      "iteration 36582: loss: 0.21716804802417755\n",
      "iteration 36583: loss: 0.21716800332069397\n",
      "iteration 36584: loss: 0.21716780960559845\n",
      "iteration 36585: loss: 0.2171676605939865\n",
      "iteration 36586: loss: 0.2171676605939865\n",
      "iteration 36587: loss: 0.21716740727424622\n",
      "iteration 36588: loss: 0.2171674221754074\n",
      "iteration 36589: loss: 0.2171672284603119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 36590: loss: 0.21716709434986115\n",
      "iteration 36591: loss: 0.21716704964637756\n",
      "iteration 36592: loss: 0.21716687083244324\n",
      "iteration 36593: loss: 0.21716678142547607\n",
      "iteration 36594: loss: 0.21716666221618652\n",
      "iteration 36595: loss: 0.2171664983034134\n",
      "iteration 36596: loss: 0.21716639399528503\n",
      "iteration 36597: loss: 0.2171662598848343\n",
      "iteration 36598: loss: 0.21716609597206116\n",
      "iteration 36599: loss: 0.2171660214662552\n",
      "iteration 36600: loss: 0.21716585755348206\n",
      "iteration 36601: loss: 0.21716579794883728\n",
      "iteration 36602: loss: 0.21716570854187012\n",
      "iteration 36603: loss: 0.21716554462909698\n",
      "iteration 36604: loss: 0.21716542541980743\n",
      "iteration 36605: loss: 0.21716530621051788\n",
      "iteration 36606: loss: 0.21716518700122833\n",
      "iteration 36607: loss: 0.21716508269309998\n",
      "iteration 36608: loss: 0.21716491878032684\n",
      "iteration 36609: loss: 0.21716487407684326\n",
      "iteration 36610: loss: 0.21716471016407013\n",
      "iteration 36611: loss: 0.21716466546058655\n",
      "iteration 36612: loss: 0.2171645164489746\n",
      "iteration 36613: loss: 0.2171643078327179\n",
      "iteration 36614: loss: 0.21716423332691193\n",
      "iteration 36615: loss: 0.21716412901878357\n",
      "iteration 36616: loss: 0.2171640396118164\n",
      "iteration 36617: loss: 0.2171638309955597\n",
      "iteration 36618: loss: 0.21716371178627014\n",
      "iteration 36619: loss: 0.21716363728046417\n",
      "iteration 36620: loss: 0.21716347336769104\n",
      "iteration 36621: loss: 0.2171633541584015\n",
      "iteration 36622: loss: 0.21716323494911194\n",
      "iteration 36623: loss: 0.21716313064098358\n",
      "iteration 36624: loss: 0.21716304123401642\n",
      "iteration 36625: loss: 0.21716293692588806\n",
      "iteration 36626: loss: 0.21716280281543732\n",
      "iteration 36627: loss: 0.21716263890266418\n",
      "iteration 36628: loss: 0.21716256439685822\n",
      "iteration 36629: loss: 0.21716240048408508\n",
      "iteration 36630: loss: 0.2171623259782791\n",
      "iteration 36631: loss: 0.21716220676898956\n",
      "iteration 36632: loss: 0.21716204285621643\n",
      "iteration 36633: loss: 0.21716196835041046\n",
      "iteration 36634: loss: 0.21716180443763733\n",
      "iteration 36635: loss: 0.21716170012950897\n",
      "iteration 36636: loss: 0.2171616107225418\n",
      "iteration 36637: loss: 0.21716146171092987\n",
      "iteration 36638: loss: 0.2171613872051239\n",
      "iteration 36639: loss: 0.2171611487865448\n",
      "iteration 36640: loss: 0.21716110408306122\n",
      "iteration 36641: loss: 0.21716098487377167\n",
      "iteration 36642: loss: 0.21716082096099854\n",
      "iteration 36643: loss: 0.21716077625751495\n",
      "iteration 36644: loss: 0.2171606570482254\n",
      "iteration 36645: loss: 0.21716058254241943\n",
      "iteration 36646: loss: 0.21716037392616272\n",
      "iteration 36647: loss: 0.21716029942035675\n",
      "iteration 36648: loss: 0.21716013550758362\n",
      "iteration 36649: loss: 0.21716003119945526\n",
      "iteration 36650: loss: 0.21715989708900452\n",
      "iteration 36651: loss: 0.21715979278087616\n",
      "iteration 36652: loss: 0.217159703373909\n",
      "iteration 36653: loss: 0.21715958416461945\n",
      "iteration 36654: loss: 0.2171594649553299\n",
      "iteration 36655: loss: 0.21715931594371796\n",
      "iteration 36656: loss: 0.2171591967344284\n",
      "iteration 36657: loss: 0.21715903282165527\n",
      "iteration 36658: loss: 0.2171589881181717\n",
      "iteration 36659: loss: 0.21715879440307617\n",
      "iteration 36660: loss: 0.217158704996109\n",
      "iteration 36661: loss: 0.21715855598449707\n",
      "iteration 36662: loss: 0.2171584814786911\n",
      "iteration 36663: loss: 0.21715834736824036\n",
      "iteration 36664: loss: 0.2171582281589508\n",
      "iteration 36665: loss: 0.21715812385082245\n",
      "iteration 36666: loss: 0.2171580046415329\n",
      "iteration 36667: loss: 0.21715788543224335\n",
      "iteration 36668: loss: 0.2171577513217926\n",
      "iteration 36669: loss: 0.21715763211250305\n",
      "iteration 36670: loss: 0.2171574831008911\n",
      "iteration 36671: loss: 0.21715740859508514\n",
      "iteration 36672: loss: 0.217157244682312\n",
      "iteration 36673: loss: 0.21715712547302246\n",
      "iteration 36674: loss: 0.21715708076953888\n",
      "iteration 36675: loss: 0.21715685725212097\n",
      "iteration 36676: loss: 0.2171568125486374\n",
      "iteration 36677: loss: 0.21715669333934784\n",
      "iteration 36678: loss: 0.2171565592288971\n",
      "iteration 36679: loss: 0.21715641021728516\n",
      "iteration 36680: loss: 0.21715626120567322\n",
      "iteration 36681: loss: 0.21715617179870605\n",
      "iteration 36682: loss: 0.21715609729290009\n",
      "iteration 36683: loss: 0.21715593338012695\n",
      "iteration 36684: loss: 0.2171558439731598\n",
      "iteration 36685: loss: 0.21715565025806427\n",
      "iteration 36686: loss: 0.21715553104877472\n",
      "iteration 36687: loss: 0.21715545654296875\n",
      "iteration 36688: loss: 0.2171553671360016\n",
      "iteration 36689: loss: 0.21715524792671204\n",
      "iteration 36690: loss: 0.21715514361858368\n",
      "iteration 36691: loss: 0.21715497970581055\n",
      "iteration 36692: loss: 0.217154860496521\n",
      "iteration 36693: loss: 0.21715474128723145\n",
      "iteration 36694: loss: 0.2171546220779419\n",
      "iteration 36695: loss: 0.21715447306632996\n",
      "iteration 36696: loss: 0.2171543538570404\n",
      "iteration 36697: loss: 0.21715426445007324\n",
      "iteration 36698: loss: 0.21715417504310608\n",
      "iteration 36699: loss: 0.21715399622917175\n",
      "iteration 36700: loss: 0.21715393662452698\n",
      "iteration 36701: loss: 0.21715378761291504\n",
      "iteration 36702: loss: 0.21715369820594788\n",
      "iteration 36703: loss: 0.21715357899665833\n",
      "iteration 36704: loss: 0.21715345978736877\n",
      "iteration 36705: loss: 0.21715326607227325\n",
      "iteration 36706: loss: 0.2171531617641449\n",
      "iteration 36707: loss: 0.21715304255485535\n",
      "iteration 36708: loss: 0.2171529233455658\n",
      "iteration 36709: loss: 0.21715286374092102\n",
      "iteration 36710: loss: 0.2171526700258255\n",
      "iteration 36711: loss: 0.21715256571769714\n",
      "iteration 36712: loss: 0.21715250611305237\n",
      "iteration 36713: loss: 0.21715235710144043\n",
      "iteration 36714: loss: 0.21715214848518372\n",
      "iteration 36715: loss: 0.21715211868286133\n",
      "iteration 36716: loss: 0.21715199947357178\n",
      "iteration 36717: loss: 0.21715185046195984\n",
      "iteration 36718: loss: 0.2171517163515091\n",
      "iteration 36719: loss: 0.2171516716480255\n",
      "iteration 36720: loss: 0.2171514481306076\n",
      "iteration 36721: loss: 0.21715131402015686\n",
      "iteration 36722: loss: 0.2171512395143509\n",
      "iteration 36723: loss: 0.21715116500854492\n",
      "iteration 36724: loss: 0.21715101599693298\n",
      "iteration 36725: loss: 0.21715088188648224\n",
      "iteration 36726: loss: 0.21715080738067627\n",
      "iteration 36727: loss: 0.21715068817138672\n",
      "iteration 36728: loss: 0.21715056896209717\n",
      "iteration 36729: loss: 0.21715037524700165\n",
      "iteration 36730: loss: 0.21715030074119568\n",
      "iteration 36731: loss: 0.21715018153190613\n",
      "iteration 36732: loss: 0.21715012192726135\n",
      "iteration 36733: loss: 0.2171500027179718\n",
      "iteration 36734: loss: 0.21714989840984344\n",
      "iteration 36735: loss: 0.2171497642993927\n",
      "iteration 36736: loss: 0.21714961528778076\n",
      "iteration 36737: loss: 0.2171494960784912\n",
      "iteration 36738: loss: 0.21714933216571808\n",
      "iteration 36739: loss: 0.21714921295642853\n",
      "iteration 36740: loss: 0.21714913845062256\n",
      "iteration 36741: loss: 0.2171490490436554\n",
      "iteration 36742: loss: 0.21714887022972107\n",
      "iteration 36743: loss: 0.21714875102043152\n",
      "iteration 36744: loss: 0.2171485722064972\n",
      "iteration 36745: loss: 0.2171485424041748\n",
      "iteration 36746: loss: 0.21714842319488525\n",
      "iteration 36747: loss: 0.21714834868907928\n",
      "iteration 36748: loss: 0.21714821457862854\n",
      "iteration 36749: loss: 0.2171480655670166\n",
      "iteration 36750: loss: 0.21714799106121063\n",
      "iteration 36751: loss: 0.21714778244495392\n",
      "iteration 36752: loss: 0.21714767813682556\n",
      "iteration 36753: loss: 0.21714754402637482\n",
      "iteration 36754: loss: 0.21714751422405243\n",
      "iteration 36755: loss: 0.2171473205089569\n",
      "iteration 36756: loss: 0.21714723110198975\n",
      "iteration 36757: loss: 0.2171470820903778\n",
      "iteration 36758: loss: 0.21714694797992706\n",
      "iteration 36759: loss: 0.21714690327644348\n",
      "iteration 36760: loss: 0.21714672446250916\n",
      "iteration 36761: loss: 0.217146635055542\n",
      "iteration 36762: loss: 0.21714651584625244\n",
      "iteration 36763: loss: 0.2171463668346405\n",
      "iteration 36764: loss: 0.21714630722999573\n",
      "iteration 36765: loss: 0.2171461582183838\n",
      "iteration 36766: loss: 0.21714599430561066\n",
      "iteration 36767: loss: 0.2171458750963211\n",
      "iteration 36768: loss: 0.21714577078819275\n",
      "iteration 36769: loss: 0.21714568138122559\n",
      "iteration 36770: loss: 0.21714551746845245\n",
      "iteration 36771: loss: 0.2171454131603241\n",
      "iteration 36772: loss: 0.21714529395103455\n",
      "iteration 36773: loss: 0.217145174741745\n",
      "iteration 36774: loss: 0.21714501082897186\n",
      "iteration 36775: loss: 0.21714499592781067\n",
      "iteration 36776: loss: 0.21714480221271515\n",
      "iteration 36777: loss: 0.21714472770690918\n",
      "iteration 36778: loss: 0.21714451909065247\n",
      "iteration 36779: loss: 0.2171444147825241\n",
      "iteration 36780: loss: 0.21714439988136292\n",
      "iteration 36781: loss: 0.21714429557323456\n",
      "iteration 36782: loss: 0.21714408695697784\n",
      "iteration 36783: loss: 0.2171439677476883\n",
      "iteration 36784: loss: 0.2171439230442047\n",
      "iteration 36785: loss: 0.21714377403259277\n",
      "iteration 36786: loss: 0.21714361011981964\n",
      "iteration 36787: loss: 0.21714350581169128\n",
      "iteration 36788: loss: 0.21714337170124054\n",
      "iteration 36789: loss: 0.21714329719543457\n",
      "iteration 36790: loss: 0.21714313328266144\n",
      "iteration 36791: loss: 0.2171430140733719\n",
      "iteration 36792: loss: 0.21714290976524353\n",
      "iteration 36793: loss: 0.21714277565479279\n",
      "iteration 36794: loss: 0.21714262664318085\n",
      "iteration 36795: loss: 0.21714261174201965\n",
      "iteration 36796: loss: 0.21714243292808533\n",
      "iteration 36797: loss: 0.21714229881763458\n",
      "iteration 36798: loss: 0.21714219450950623\n",
      "iteration 36799: loss: 0.21714206039905548\n",
      "iteration 36800: loss: 0.21714194118976593\n",
      "iteration 36801: loss: 0.21714186668395996\n",
      "iteration 36802: loss: 0.21714170277118683\n",
      "iteration 36803: loss: 0.21714159846305847\n",
      "iteration 36804: loss: 0.21714147925376892\n",
      "iteration 36805: loss: 0.21714136004447937\n",
      "iteration 36806: loss: 0.21714124083518982\n",
      "iteration 36807: loss: 0.2171410769224167\n",
      "iteration 36808: loss: 0.21714098751544952\n",
      "iteration 36809: loss: 0.21714086830615997\n",
      "iteration 36810: loss: 0.21714076399803162\n",
      "iteration 36811: loss: 0.21714062988758087\n",
      "iteration 36812: loss: 0.2171405553817749\n",
      "iteration 36813: loss: 0.21714046597480774\n",
      "iteration 36814: loss: 0.2171403169631958\n",
      "iteration 36815: loss: 0.21714019775390625\n",
      "iteration 36816: loss: 0.21714003384113312\n",
      "iteration 36817: loss: 0.21713992953300476\n",
      "iteration 36818: loss: 0.21713979542255402\n",
      "iteration 36819: loss: 0.21713972091674805\n",
      "iteration 36820: loss: 0.2171395719051361\n",
      "iteration 36821: loss: 0.21713943779468536\n",
      "iteration 36822: loss: 0.2171393632888794\n",
      "iteration 36823: loss: 0.21713924407958984\n",
      "iteration 36824: loss: 0.2171390801668167\n",
      "iteration 36825: loss: 0.21713900566101074\n",
      "iteration 36826: loss: 0.21713881194591522\n",
      "iteration 36827: loss: 0.21713873744010925\n",
      "iteration 36828: loss: 0.2171386182308197\n",
      "iteration 36829: loss: 0.21713852882385254\n",
      "iteration 36830: loss: 0.217138409614563\n",
      "iteration 36831: loss: 0.21713824570178986\n",
      "iteration 36832: loss: 0.2171381264925003\n",
      "iteration 36833: loss: 0.21713800728321075\n",
      "iteration 36834: loss: 0.21713785827159882\n",
      "iteration 36835: loss: 0.21713784337043762\n",
      "iteration 36836: loss: 0.21713769435882568\n",
      "iteration 36837: loss: 0.21713753044605255\n",
      "iteration 36838: loss: 0.217137411236763\n",
      "iteration 36839: loss: 0.21713730692863464\n",
      "iteration 36840: loss: 0.2171371877193451\n",
      "iteration 36841: loss: 0.21713706851005554\n",
      "iteration 36842: loss: 0.217136949300766\n",
      "iteration 36843: loss: 0.21713685989379883\n",
      "iteration 36844: loss: 0.21713674068450928\n",
      "iteration 36845: loss: 0.21713662147521973\n",
      "iteration 36846: loss: 0.21713650226593018\n",
      "iteration 36847: loss: 0.21713638305664062\n",
      "iteration 36848: loss: 0.2171362340450287\n",
      "iteration 36849: loss: 0.21713614463806152\n",
      "iteration 36850: loss: 0.21713599562644958\n",
      "iteration 36851: loss: 0.21713583171367645\n",
      "iteration 36852: loss: 0.21713578701019287\n",
      "iteration 36853: loss: 0.21713563799858093\n",
      "iteration 36854: loss: 0.21713557839393616\n",
      "iteration 36855: loss: 0.21713539958000183\n",
      "iteration 36856: loss: 0.21713528037071228\n",
      "iteration 36857: loss: 0.21713519096374512\n",
      "iteration 36858: loss: 0.21713510155677795\n",
      "iteration 36859: loss: 0.2171349823474884\n",
      "iteration 36860: loss: 0.21713480353355408\n",
      "iteration 36861: loss: 0.21713468432426453\n",
      "iteration 36862: loss: 0.21713456511497498\n",
      "iteration 36863: loss: 0.21713440120220184\n",
      "iteration 36864: loss: 0.21713438630104065\n",
      "iteration 36865: loss: 0.2171342372894287\n",
      "iteration 36866: loss: 0.21713414788246155\n",
      "iteration 36867: loss: 0.21713396906852722\n",
      "iteration 36868: loss: 0.21713390946388245\n",
      "iteration 36869: loss: 0.21713373064994812\n",
      "iteration 36870: loss: 0.21713367104530334\n",
      "iteration 36871: loss: 0.2171335518360138\n",
      "iteration 36872: loss: 0.21713335812091827\n",
      "iteration 36873: loss: 0.2171333134174347\n",
      "iteration 36874: loss: 0.21713313460350037\n",
      "iteration 36875: loss: 0.21713308990001678\n",
      "iteration 36876: loss: 0.21713295578956604\n",
      "iteration 36877: loss: 0.2171327769756317\n",
      "iteration 36878: loss: 0.21713268756866455\n",
      "iteration 36879: loss: 0.21713252365589142\n",
      "iteration 36880: loss: 0.21713247895240784\n",
      "iteration 36881: loss: 0.2171323597431183\n",
      "iteration 36882: loss: 0.21713221073150635\n",
      "iteration 36883: loss: 0.2171320617198944\n",
      "iteration 36884: loss: 0.21713194251060486\n",
      "iteration 36885: loss: 0.21713188290596008\n",
      "iteration 36886: loss: 0.21713176369667053\n",
      "iteration 36887: loss: 0.2171316146850586\n",
      "iteration 36888: loss: 0.217131569981575\n",
      "iteration 36889: loss: 0.2171313762664795\n",
      "iteration 36890: loss: 0.21713128685951233\n",
      "iteration 36891: loss: 0.2171311378479004\n",
      "iteration 36892: loss: 0.21713106334209442\n",
      "iteration 36893: loss: 0.21713092923164368\n",
      "iteration 36894: loss: 0.21713075041770935\n",
      "iteration 36895: loss: 0.21713073551654816\n",
      "iteration 36896: loss: 0.2171306163072586\n",
      "iteration 36897: loss: 0.21713045239448547\n",
      "iteration 36898: loss: 0.21713027358055115\n",
      "iteration 36899: loss: 0.21713018417358398\n",
      "iteration 36900: loss: 0.21713006496429443\n",
      "iteration 36901: loss: 0.21712994575500488\n",
      "iteration 36902: loss: 0.21712985634803772\n",
      "iteration 36903: loss: 0.21712970733642578\n",
      "iteration 36904: loss: 0.21712958812713623\n",
      "iteration 36905: loss: 0.21712949872016907\n",
      "iteration 36906: loss: 0.2171294242143631\n",
      "iteration 36907: loss: 0.21712927520275116\n",
      "iteration 36908: loss: 0.21712911128997803\n",
      "iteration 36909: loss: 0.21712902188301086\n",
      "iteration 36910: loss: 0.2171289026737213\n",
      "iteration 36911: loss: 0.21712879836559296\n",
      "iteration 36912: loss: 0.2171286642551422\n",
      "iteration 36913: loss: 0.21712855994701385\n",
      "iteration 36914: loss: 0.2171284407377243\n",
      "iteration 36915: loss: 0.21712832152843475\n",
      "iteration 36916: loss: 0.2171282023191452\n",
      "iteration 36917: loss: 0.21712803840637207\n",
      "iteration 36918: loss: 0.2171279489994049\n",
      "iteration 36919: loss: 0.21712779998779297\n",
      "iteration 36920: loss: 0.2171277105808258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 36921: loss: 0.21712756156921387\n",
      "iteration 36922: loss: 0.2171274721622467\n",
      "iteration 36923: loss: 0.21712739765644073\n",
      "iteration 36924: loss: 0.21712717413902283\n",
      "iteration 36925: loss: 0.21712705492973328\n",
      "iteration 36926: loss: 0.2171269655227661\n",
      "iteration 36927: loss: 0.21712684631347656\n",
      "iteration 36928: loss: 0.21712669730186462\n",
      "iteration 36929: loss: 0.21712660789489746\n",
      "iteration 36930: loss: 0.21712656319141388\n",
      "iteration 36931: loss: 0.21712641417980194\n",
      "iteration 36932: loss: 0.2171262800693512\n",
      "iteration 36933: loss: 0.21712616086006165\n",
      "iteration 36934: loss: 0.21712610125541687\n",
      "iteration 36935: loss: 0.21712593734264374\n",
      "iteration 36936: loss: 0.2171258181333542\n",
      "iteration 36937: loss: 0.21712574362754822\n",
      "iteration 36938: loss: 0.21712562441825867\n",
      "iteration 36939: loss: 0.21712546050548553\n",
      "iteration 36940: loss: 0.2171253263950348\n",
      "iteration 36941: loss: 0.21712526679039001\n",
      "iteration 36942: loss: 0.21712513267993927\n",
      "iteration 36943: loss: 0.21712498366832733\n",
      "iteration 36944: loss: 0.21712489426136017\n",
      "iteration 36945: loss: 0.21712474524974823\n",
      "iteration 36946: loss: 0.21712465584278107\n",
      "iteration 36947: loss: 0.21712449193000793\n",
      "iteration 36948: loss: 0.21712437272071838\n",
      "iteration 36949: loss: 0.21712426841259003\n",
      "iteration 36950: loss: 0.21712419390678406\n",
      "iteration 36951: loss: 0.21712398529052734\n",
      "iteration 36952: loss: 0.2171238660812378\n",
      "iteration 36953: loss: 0.21712377667427063\n",
      "iteration 36954: loss: 0.21712367236614227\n",
      "iteration 36955: loss: 0.2171235829591751\n",
      "iteration 36956: loss: 0.21712343394756317\n",
      "iteration 36957: loss: 0.21712331473827362\n",
      "iteration 36958: loss: 0.21712322533130646\n",
      "iteration 36959: loss: 0.21712306141853333\n",
      "iteration 36960: loss: 0.2171229124069214\n",
      "iteration 36961: loss: 0.21712283790111542\n",
      "iteration 36962: loss: 0.21712270379066467\n",
      "iteration 36963: loss: 0.2171226292848587\n",
      "iteration 36964: loss: 0.21712246537208557\n",
      "iteration 36965: loss: 0.21712234616279602\n",
      "iteration 36966: loss: 0.21712222695350647\n",
      "iteration 36967: loss: 0.2171221226453781\n",
      "iteration 36968: loss: 0.21712198853492737\n",
      "iteration 36969: loss: 0.2171219140291214\n",
      "iteration 36970: loss: 0.21712175011634827\n",
      "iteration 36971: loss: 0.2171216905117035\n",
      "iteration 36972: loss: 0.21712160110473633\n",
      "iteration 36973: loss: 0.2171214520931244\n",
      "iteration 36974: loss: 0.2171212136745453\n",
      "iteration 36975: loss: 0.2171212136745453\n",
      "iteration 36976: loss: 0.21712107956409454\n",
      "iteration 36977: loss: 0.2171209305524826\n",
      "iteration 36978: loss: 0.21712085604667664\n",
      "iteration 36979: loss: 0.21712079644203186\n",
      "iteration 36980: loss: 0.21712057292461395\n",
      "iteration 36981: loss: 0.2171204388141632\n",
      "iteration 36982: loss: 0.21712036430835724\n",
      "iteration 36983: loss: 0.21712026000022888\n",
      "iteration 36984: loss: 0.21712012588977814\n",
      "iteration 36985: loss: 0.21712005138397217\n",
      "iteration 36986: loss: 0.21711990237236023\n",
      "iteration 36987: loss: 0.21711981296539307\n",
      "iteration 36988: loss: 0.21711961925029755\n",
      "iteration 36989: loss: 0.21711960434913635\n",
      "iteration 36990: loss: 0.21711942553520203\n",
      "iteration 36991: loss: 0.21711929142475128\n",
      "iteration 36992: loss: 0.21711917221546173\n",
      "iteration 36993: loss: 0.21711905300617218\n",
      "iteration 36994: loss: 0.21711894869804382\n",
      "iteration 36995: loss: 0.21711890399456024\n",
      "iteration 36996: loss: 0.2171187400817871\n",
      "iteration 36997: loss: 0.21711862087249756\n",
      "iteration 36998: loss: 0.21711845695972443\n",
      "iteration 36999: loss: 0.21711838245391846\n",
      "iteration 37000: loss: 0.2171182930469513\n",
      "iteration 37001: loss: 0.21711814403533936\n",
      "iteration 37002: loss: 0.2171180248260498\n",
      "iteration 37003: loss: 0.21711786091327667\n",
      "iteration 37004: loss: 0.21711786091327667\n",
      "iteration 37005: loss: 0.21711769700050354\n",
      "iteration 37006: loss: 0.21711750328540802\n",
      "iteration 37007: loss: 0.21711745858192444\n",
      "iteration 37008: loss: 0.2171173393726349\n",
      "iteration 37009: loss: 0.21711726486682892\n",
      "iteration 37010: loss: 0.2171170711517334\n",
      "iteration 37011: loss: 0.21711695194244385\n",
      "iteration 37012: loss: 0.21711686253547668\n",
      "iteration 37013: loss: 0.21711674332618713\n",
      "iteration 37014: loss: 0.2171165496110916\n",
      "iteration 37015: loss: 0.21711644530296326\n",
      "iteration 37016: loss: 0.21711643040180206\n",
      "iteration 37017: loss: 0.21711628139019012\n",
      "iteration 37018: loss: 0.21711614727973938\n",
      "iteration 37019: loss: 0.21711604297161102\n",
      "iteration 37020: loss: 0.21711592376232147\n",
      "iteration 37021: loss: 0.21711575984954834\n",
      "iteration 37022: loss: 0.21711567044258118\n",
      "iteration 37023: loss: 0.21711555123329163\n",
      "iteration 37024: loss: 0.21711547672748566\n",
      "iteration 37025: loss: 0.2171154022216797\n",
      "iteration 37026: loss: 0.21711520850658417\n",
      "iteration 37027: loss: 0.21711507439613342\n",
      "iteration 37028: loss: 0.21711495518684387\n",
      "iteration 37029: loss: 0.21711485087871552\n",
      "iteration 37030: loss: 0.21711480617523193\n",
      "iteration 37031: loss: 0.21711459755897522\n",
      "iteration 37032: loss: 0.21711452305316925\n",
      "iteration 37033: loss: 0.21711432933807373\n",
      "iteration 37034: loss: 0.21711421012878418\n",
      "iteration 37035: loss: 0.21711412072181702\n",
      "iteration 37036: loss: 0.21711404621601105\n",
      "iteration 37037: loss: 0.2171139419078827\n",
      "iteration 37038: loss: 0.21711382269859314\n",
      "iteration 37039: loss: 0.21711365878582\n",
      "iteration 37040: loss: 0.21711358428001404\n",
      "iteration 37041: loss: 0.2171134501695633\n",
      "iteration 37042: loss: 0.21711330115795135\n",
      "iteration 37043: loss: 0.2171132117509842\n",
      "iteration 37044: loss: 0.21711309254169464\n",
      "iteration 37045: loss: 0.21711301803588867\n",
      "iteration 37046: loss: 0.21711277961730957\n",
      "iteration 37047: loss: 0.217112734913826\n",
      "iteration 37048: loss: 0.21711261570453644\n",
      "iteration 37049: loss: 0.21711254119873047\n",
      "iteration 37050: loss: 0.21711237728595734\n",
      "iteration 37051: loss: 0.21711225807666779\n",
      "iteration 37052: loss: 0.21711213886737823\n",
      "iteration 37053: loss: 0.21711203455924988\n",
      "iteration 37054: loss: 0.21711190044879913\n",
      "iteration 37055: loss: 0.21711178123950958\n",
      "iteration 37056: loss: 0.21711163222789764\n",
      "iteration 37057: loss: 0.21711155772209167\n",
      "iteration 37058: loss: 0.21711143851280212\n",
      "iteration 37059: loss: 0.21711131930351257\n",
      "iteration 37060: loss: 0.21711118519306183\n",
      "iteration 37061: loss: 0.2171110212802887\n",
      "iteration 37062: loss: 0.2171110361814499\n",
      "iteration 37063: loss: 0.21711082756519318\n",
      "iteration 37064: loss: 0.21711072325706482\n",
      "iteration 37065: loss: 0.21711063385009766\n",
      "iteration 37066: loss: 0.21711048483848572\n",
      "iteration 37067: loss: 0.21711036562919617\n",
      "iteration 37068: loss: 0.21711024641990662\n",
      "iteration 37069: loss: 0.21711012721061707\n",
      "iteration 37070: loss: 0.2171100378036499\n",
      "iteration 37071: loss: 0.21710988879203796\n",
      "iteration 37072: loss: 0.2171098291873932\n",
      "iteration 37073: loss: 0.21710963547229767\n",
      "iteration 37074: loss: 0.21710948646068573\n",
      "iteration 37075: loss: 0.21710944175720215\n",
      "iteration 37076: loss: 0.21710927784442902\n",
      "iteration 37077: loss: 0.21710917353630066\n",
      "iteration 37078: loss: 0.2171090543270111\n",
      "iteration 37079: loss: 0.21710896492004395\n",
      "iteration 37080: loss: 0.21710887551307678\n",
      "iteration 37081: loss: 0.21710872650146484\n",
      "iteration 37082: loss: 0.2171085774898529\n",
      "iteration 37083: loss: 0.21710853278636932\n",
      "iteration 37084: loss: 0.2171083688735962\n",
      "iteration 37085: loss: 0.21710824966430664\n",
      "iteration 37086: loss: 0.2171081304550171\n",
      "iteration 37087: loss: 0.21710804104804993\n",
      "iteration 37088: loss: 0.21710792183876038\n",
      "iteration 37089: loss: 0.21710777282714844\n",
      "iteration 37090: loss: 0.2171076238155365\n",
      "iteration 37091: loss: 0.21710753440856934\n",
      "iteration 37092: loss: 0.21710744500160217\n",
      "iteration 37093: loss: 0.21710729598999023\n",
      "iteration 37094: loss: 0.21710717678070068\n",
      "iteration 37095: loss: 0.21710710227489471\n",
      "iteration 37096: loss: 0.2171069085597992\n",
      "iteration 37097: loss: 0.2171069085597992\n",
      "iteration 37098: loss: 0.21710678935050964\n",
      "iteration 37099: loss: 0.21710661053657532\n",
      "iteration 37100: loss: 0.21710649132728577\n",
      "iteration 37101: loss: 0.217106431722641\n",
      "iteration 37102: loss: 0.21710629761219025\n",
      "iteration 37103: loss: 0.21710613369941711\n",
      "iteration 37104: loss: 0.21710602939128876\n",
      "iteration 37105: loss: 0.21710586547851562\n",
      "iteration 37106: loss: 0.2171057164669037\n",
      "iteration 37107: loss: 0.2171056568622589\n",
      "iteration 37108: loss: 0.21710553765296936\n",
      "iteration 37109: loss: 0.2171054184436798\n",
      "iteration 37110: loss: 0.21710531413555145\n",
      "iteration 37111: loss: 0.2171051949262619\n",
      "iteration 37112: loss: 0.21710506081581116\n",
      "iteration 37113: loss: 0.2171049416065216\n",
      "iteration 37114: loss: 0.21710488200187683\n",
      "iteration 37115: loss: 0.2171047180891037\n",
      "iteration 37116: loss: 0.21710458397865295\n",
      "iteration 37117: loss: 0.21710450947284698\n",
      "iteration 37118: loss: 0.21710431575775146\n",
      "iteration 37119: loss: 0.2171042412519455\n",
      "iteration 37120: loss: 0.21710416674613953\n",
      "iteration 37121: loss: 0.2171040028333664\n",
      "iteration 37122: loss: 0.21710391342639923\n",
      "iteration 37123: loss: 0.2171037644147873\n",
      "iteration 37124: loss: 0.21710367500782013\n",
      "iteration 37125: loss: 0.217103511095047\n",
      "iteration 37126: loss: 0.21710340678691864\n",
      "iteration 37127: loss: 0.2171032726764679\n",
      "iteration 37128: loss: 0.21710319817066193\n",
      "iteration 37129: loss: 0.21710307896137238\n",
      "iteration 37130: loss: 0.21710292994976044\n",
      "iteration 37131: loss: 0.21710285544395447\n",
      "iteration 37132: loss: 0.21710273623466492\n",
      "iteration 37133: loss: 0.21710267663002014\n",
      "iteration 37134: loss: 0.2171025574207306\n",
      "iteration 37135: loss: 0.21710243821144104\n",
      "iteration 37136: loss: 0.21710224449634552\n",
      "iteration 37137: loss: 0.21710209548473358\n",
      "iteration 37138: loss: 0.21710205078125\n",
      "iteration 37139: loss: 0.21710196137428284\n",
      "iteration 37140: loss: 0.2171017825603485\n",
      "iteration 37141: loss: 0.21710172295570374\n",
      "iteration 37142: loss: 0.21710160374641418\n",
      "iteration 37143: loss: 0.21710141003131866\n",
      "iteration 37144: loss: 0.2171013355255127\n",
      "iteration 37145: loss: 0.21710124611854553\n",
      "iteration 37146: loss: 0.21710105240345\n",
      "iteration 37147: loss: 0.21710100769996643\n",
      "iteration 37148: loss: 0.2171008139848709\n",
      "iteration 37149: loss: 0.21710070967674255\n",
      "iteration 37150: loss: 0.2171005755662918\n",
      "iteration 37151: loss: 0.21710045635700226\n",
      "iteration 37152: loss: 0.2171003818511963\n",
      "iteration 37153: loss: 0.21710026264190674\n",
      "iteration 37154: loss: 0.2171001434326172\n",
      "iteration 37155: loss: 0.21710005402565002\n",
      "iteration 37156: loss: 0.21709993481636047\n",
      "iteration 37157: loss: 0.2170998603105545\n",
      "iteration 37158: loss: 0.21709974110126495\n",
      "iteration 37159: loss: 0.21709951758384705\n",
      "iteration 37160: loss: 0.21709947288036346\n",
      "iteration 37161: loss: 0.2170993834733963\n",
      "iteration 37162: loss: 0.21709918975830078\n",
      "iteration 37163: loss: 0.21709907054901123\n",
      "iteration 37164: loss: 0.21709899604320526\n",
      "iteration 37165: loss: 0.2170988768339157\n",
      "iteration 37166: loss: 0.21709883213043213\n",
      "iteration 37167: loss: 0.217098668217659\n",
      "iteration 37168: loss: 0.21709856390953064\n",
      "iteration 37169: loss: 0.21709832549095154\n",
      "iteration 37170: loss: 0.21709828078746796\n",
      "iteration 37171: loss: 0.217098206281662\n",
      "iteration 37172: loss: 0.21709802746772766\n",
      "iteration 37173: loss: 0.21709796786308289\n",
      "iteration 37174: loss: 0.21709783375263214\n",
      "iteration 37175: loss: 0.21709764003753662\n",
      "iteration 37176: loss: 0.21709755063056946\n",
      "iteration 37177: loss: 0.2170974463224411\n",
      "iteration 37178: loss: 0.21709728240966797\n",
      "iteration 37179: loss: 0.2170972377061844\n",
      "iteration 37180: loss: 0.21709708869457245\n",
      "iteration 37181: loss: 0.2170969694852829\n",
      "iteration 37182: loss: 0.21709685027599335\n",
      "iteration 37183: loss: 0.21709680557250977\n",
      "iteration 37184: loss: 0.21709659695625305\n",
      "iteration 37185: loss: 0.21709652245044708\n",
      "iteration 37186: loss: 0.21709640324115753\n",
      "iteration 37187: loss: 0.21709629893302917\n",
      "iteration 37188: loss: 0.21709616482257843\n",
      "iteration 37189: loss: 0.21709604561328888\n",
      "iteration 37190: loss: 0.21709594130516052\n",
      "iteration 37191: loss: 0.21709585189819336\n",
      "iteration 37192: loss: 0.21709570288658142\n",
      "iteration 37193: loss: 0.21709561347961426\n",
      "iteration 37194: loss: 0.21709546446800232\n",
      "iteration 37195: loss: 0.21709537506103516\n",
      "iteration 37196: loss: 0.21709521114826202\n",
      "iteration 37197: loss: 0.21709510684013367\n",
      "iteration 37198: loss: 0.21709498763084412\n",
      "iteration 37199: loss: 0.21709492802619934\n",
      "iteration 37200: loss: 0.2170947790145874\n",
      "iteration 37201: loss: 0.21709463000297546\n",
      "iteration 37202: loss: 0.21709446609020233\n",
      "iteration 37203: loss: 0.21709442138671875\n",
      "iteration 37204: loss: 0.21709439158439636\n",
      "iteration 37205: loss: 0.21709421277046204\n",
      "iteration 37206: loss: 0.21709409356117249\n",
      "iteration 37207: loss: 0.21709398925304413\n",
      "iteration 37208: loss: 0.21709387004375458\n",
      "iteration 37209: loss: 0.21709370613098145\n",
      "iteration 37210: loss: 0.2170935571193695\n",
      "iteration 37211: loss: 0.21709351241588593\n",
      "iteration 37212: loss: 0.21709337830543518\n",
      "iteration 37213: loss: 0.21709327399730682\n",
      "iteration 37214: loss: 0.21709313988685608\n",
      "iteration 37215: loss: 0.21709303557872772\n",
      "iteration 37216: loss: 0.2170928716659546\n",
      "iteration 37217: loss: 0.2170928418636322\n",
      "iteration 37218: loss: 0.21709270775318146\n",
      "iteration 37219: loss: 0.21709255874156952\n",
      "iteration 37220: loss: 0.21709242463111877\n",
      "iteration 37221: loss: 0.21709232032299042\n",
      "iteration 37222: loss: 0.21709223091602325\n",
      "iteration 37223: loss: 0.21709208190441132\n",
      "iteration 37224: loss: 0.21709194779396057\n",
      "iteration 37225: loss: 0.2170918732881546\n",
      "iteration 37226: loss: 0.21709175407886505\n",
      "iteration 37227: loss: 0.21709167957305908\n",
      "iteration 37228: loss: 0.21709153056144714\n",
      "iteration 37229: loss: 0.2170914113521576\n",
      "iteration 37230: loss: 0.21709127724170685\n",
      "iteration 37231: loss: 0.2170911580324173\n",
      "iteration 37232: loss: 0.21709099411964417\n",
      "iteration 37233: loss: 0.2170909196138382\n",
      "iteration 37234: loss: 0.21709081530570984\n",
      "iteration 37235: loss: 0.2170906364917755\n",
      "iteration 37236: loss: 0.21709057688713074\n",
      "iteration 37237: loss: 0.21709048748016357\n",
      "iteration 37238: loss: 0.21709032356739044\n",
      "iteration 37239: loss: 0.21709027886390686\n",
      "iteration 37240: loss: 0.2170901745557785\n",
      "iteration 37241: loss: 0.2170899659395218\n",
      "iteration 37242: loss: 0.21708986163139343\n",
      "iteration 37243: loss: 0.21708980202674866\n",
      "iteration 37244: loss: 0.21708965301513672\n",
      "iteration 37245: loss: 0.21708953380584717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 37246: loss: 0.21708941459655762\n",
      "iteration 37247: loss: 0.21708936989307404\n",
      "iteration 37248: loss: 0.21708917617797852\n",
      "iteration 37249: loss: 0.21708905696868896\n",
      "iteration 37250: loss: 0.21708893775939941\n",
      "iteration 37251: loss: 0.21708884835243225\n",
      "iteration 37252: loss: 0.21708866953849792\n",
      "iteration 37253: loss: 0.21708855032920837\n",
      "iteration 37254: loss: 0.2170885056257248\n",
      "iteration 37255: loss: 0.21708834171295166\n",
      "iteration 37256: loss: 0.21708819270133972\n",
      "iteration 37257: loss: 0.21708814799785614\n",
      "iteration 37258: loss: 0.217087984085083\n",
      "iteration 37259: loss: 0.21708795428276062\n",
      "iteration 37260: loss: 0.2170877456665039\n",
      "iteration 37261: loss: 0.21708759665489197\n",
      "iteration 37262: loss: 0.2170875370502472\n",
      "iteration 37263: loss: 0.21708743274211884\n",
      "iteration 37264: loss: 0.21708731353282928\n",
      "iteration 37265: loss: 0.21708719432353973\n",
      "iteration 37266: loss: 0.21708707511425018\n",
      "iteration 37267: loss: 0.21708691120147705\n",
      "iteration 37268: loss: 0.21708683669567108\n",
      "iteration 37269: loss: 0.21708671748638153\n",
      "iteration 37270: loss: 0.21708667278289795\n",
      "iteration 37271: loss: 0.21708650887012482\n",
      "iteration 37272: loss: 0.21708638966083527\n",
      "iteration 37273: loss: 0.21708627045154572\n",
      "iteration 37274: loss: 0.21708615124225616\n",
      "iteration 37275: loss: 0.21708598732948303\n",
      "iteration 37276: loss: 0.21708588302135468\n",
      "iteration 37277: loss: 0.2170858085155487\n",
      "iteration 37278: loss: 0.21708568930625916\n",
      "iteration 37279: loss: 0.2170855551958084\n",
      "iteration 37280: loss: 0.21708545088768005\n",
      "iteration 37281: loss: 0.2170853167772293\n",
      "iteration 37282: loss: 0.21708524227142334\n",
      "iteration 37283: loss: 0.2170850783586502\n",
      "iteration 37284: loss: 0.21708497405052185\n",
      "iteration 37285: loss: 0.21708491444587708\n",
      "iteration 37286: loss: 0.21708479523658752\n",
      "iteration 37287: loss: 0.21708469092845917\n",
      "iteration 37288: loss: 0.21708449721336365\n",
      "iteration 37289: loss: 0.21708445250988007\n",
      "iteration 37290: loss: 0.21708431839942932\n",
      "iteration 37291: loss: 0.21708416938781738\n",
      "iteration 37292: loss: 0.21708405017852783\n",
      "iteration 37293: loss: 0.2170838862657547\n",
      "iteration 37294: loss: 0.21708378195762634\n",
      "iteration 37295: loss: 0.21708373725414276\n",
      "iteration 37296: loss: 0.21708360314369202\n",
      "iteration 37297: loss: 0.21708348393440247\n",
      "iteration 37298: loss: 0.2170833796262741\n",
      "iteration 37299: loss: 0.21708324551582336\n",
      "iteration 37300: loss: 0.2170831263065338\n",
      "iteration 37301: loss: 0.21708302199840546\n",
      "iteration 37302: loss: 0.21708285808563232\n",
      "iteration 37303: loss: 0.21708273887634277\n",
      "iteration 37304: loss: 0.2170826941728592\n",
      "iteration 37305: loss: 0.21708253026008606\n",
      "iteration 37306: loss: 0.2170824110507965\n",
      "iteration 37307: loss: 0.21708230674266815\n",
      "iteration 37308: loss: 0.2170821875333786\n",
      "iteration 37309: loss: 0.21708206832408905\n",
      "iteration 37310: loss: 0.2170819789171219\n",
      "iteration 37311: loss: 0.21708182990550995\n",
      "iteration 37312: loss: 0.21708174049854279\n",
      "iteration 37313: loss: 0.21708162128925323\n",
      "iteration 37314: loss: 0.21708151698112488\n",
      "iteration 37315: loss: 0.21708135306835175\n",
      "iteration 37316: loss: 0.2170812338590622\n",
      "iteration 37317: loss: 0.21708114445209503\n",
      "iteration 37318: loss: 0.21708102524280548\n",
      "iteration 37319: loss: 0.21708092093467712\n",
      "iteration 37320: loss: 0.217080757021904\n",
      "iteration 37321: loss: 0.21708068251609802\n",
      "iteration 37322: loss: 0.2170805186033249\n",
      "iteration 37323: loss: 0.21708044409751892\n",
      "iteration 37324: loss: 0.21708030998706818\n",
      "iteration 37325: loss: 0.21708016097545624\n",
      "iteration 37326: loss: 0.21708008646965027\n",
      "iteration 37327: loss: 0.2170799970626831\n",
      "iteration 37328: loss: 0.21707984805107117\n",
      "iteration 37329: loss: 0.217079758644104\n",
      "iteration 37330: loss: 0.21707966923713684\n",
      "iteration 37331: loss: 0.2170795202255249\n",
      "iteration 37332: loss: 0.21707944571971893\n",
      "iteration 37333: loss: 0.21707923710346222\n",
      "iteration 37334: loss: 0.21707913279533386\n",
      "iteration 37335: loss: 0.2170790731906891\n",
      "iteration 37336: loss: 0.21707892417907715\n",
      "iteration 37337: loss: 0.21707883477210999\n",
      "iteration 37338: loss: 0.21707873046398163\n",
      "iteration 37339: loss: 0.21707868576049805\n",
      "iteration 37340: loss: 0.21707847714424133\n",
      "iteration 37341: loss: 0.21707837283611298\n",
      "iteration 37342: loss: 0.2170782834291458\n",
      "iteration 37343: loss: 0.21707811951637268\n",
      "iteration 37344: loss: 0.2170780450105667\n",
      "iteration 37345: loss: 0.21707788109779358\n",
      "iteration 37346: loss: 0.21707776188850403\n",
      "iteration 37347: loss: 0.21707764267921448\n",
      "iteration 37348: loss: 0.21707752346992493\n",
      "iteration 37349: loss: 0.21707744896411896\n",
      "iteration 37350: loss: 0.2170773446559906\n",
      "iteration 37351: loss: 0.21707718074321747\n",
      "iteration 37352: loss: 0.2170770913362503\n",
      "iteration 37353: loss: 0.21707694232463837\n",
      "iteration 37354: loss: 0.21707689762115479\n",
      "iteration 37355: loss: 0.21707673370838165\n",
      "iteration 37356: loss: 0.2170766144990921\n",
      "iteration 37357: loss: 0.21707645058631897\n",
      "iteration 37358: loss: 0.217076376080513\n",
      "iteration 37359: loss: 0.21707627177238464\n",
      "iteration 37360: loss: 0.2170761525630951\n",
      "iteration 37361: loss: 0.21707603335380554\n",
      "iteration 37362: loss: 0.21707594394683838\n",
      "iteration 37363: loss: 0.21707582473754883\n",
      "iteration 37364: loss: 0.2170756757259369\n",
      "iteration 37365: loss: 0.21707561612129211\n",
      "iteration 37366: loss: 0.2170754373073578\n",
      "iteration 37367: loss: 0.21707531809806824\n",
      "iteration 37368: loss: 0.21707522869110107\n",
      "iteration 37369: loss: 0.21707507967948914\n",
      "iteration 37370: loss: 0.21707496047019958\n",
      "iteration 37371: loss: 0.21707487106323242\n",
      "iteration 37372: loss: 0.21707472205162048\n",
      "iteration 37373: loss: 0.21707463264465332\n",
      "iteration 37374: loss: 0.21707454323768616\n",
      "iteration 37375: loss: 0.21707439422607422\n",
      "iteration 37376: loss: 0.21707427501678467\n",
      "iteration 37377: loss: 0.2170742303133011\n",
      "iteration 37378: loss: 0.21707408130168915\n",
      "iteration 37379: loss: 0.21707391738891602\n",
      "iteration 37380: loss: 0.21707384288311005\n",
      "iteration 37381: loss: 0.21707367897033691\n",
      "iteration 37382: loss: 0.21707358956336975\n",
      "iteration 37383: loss: 0.2170734852552414\n",
      "iteration 37384: loss: 0.21707336604595184\n",
      "iteration 37385: loss: 0.2170732021331787\n",
      "iteration 37386: loss: 0.21707317233085632\n",
      "iteration 37387: loss: 0.217072993516922\n",
      "iteration 37388: loss: 0.21707287430763245\n",
      "iteration 37389: loss: 0.21707281470298767\n",
      "iteration 37390: loss: 0.21707268059253693\n",
      "iteration 37391: loss: 0.217072531580925\n",
      "iteration 37392: loss: 0.21707244217395782\n",
      "iteration 37393: loss: 0.21707233786582947\n",
      "iteration 37394: loss: 0.21707220375537872\n",
      "iteration 37395: loss: 0.21707205474376678\n",
      "iteration 37396: loss: 0.21707198023796082\n",
      "iteration 37397: loss: 0.21707186102867126\n",
      "iteration 37398: loss: 0.21707169711589813\n",
      "iteration 37399: loss: 0.21707157790660858\n",
      "iteration 37400: loss: 0.21707157790660858\n",
      "iteration 37401: loss: 0.21707136929035187\n",
      "iteration 37402: loss: 0.21707125008106232\n",
      "iteration 37403: loss: 0.21707113087177277\n",
      "iteration 37404: loss: 0.2170710563659668\n",
      "iteration 37405: loss: 0.21707093715667725\n",
      "iteration 37406: loss: 0.2170708179473877\n",
      "iteration 37407: loss: 0.21707066893577576\n",
      "iteration 37408: loss: 0.217070534825325\n",
      "iteration 37409: loss: 0.217070534825325\n",
      "iteration 37410: loss: 0.21707037091255188\n",
      "iteration 37411: loss: 0.2170702964067459\n",
      "iteration 37412: loss: 0.21707013249397278\n",
      "iteration 37413: loss: 0.21707001328468323\n",
      "iteration 37414: loss: 0.21706989407539368\n",
      "iteration 37415: loss: 0.21706974506378174\n",
      "iteration 37416: loss: 0.21706967055797577\n",
      "iteration 37417: loss: 0.2170695811510086\n",
      "iteration 37418: loss: 0.21706938743591309\n",
      "iteration 37419: loss: 0.2170693427324295\n",
      "iteration 37420: loss: 0.21706922352313995\n",
      "iteration 37421: loss: 0.21706907451152802\n",
      "iteration 37422: loss: 0.21706895530223846\n",
      "iteration 37423: loss: 0.2170688658952713\n",
      "iteration 37424: loss: 0.21706874668598175\n",
      "iteration 37425: loss: 0.2170686423778534\n",
      "iteration 37426: loss: 0.21706850826740265\n",
      "iteration 37427: loss: 0.2170683890581131\n",
      "iteration 37428: loss: 0.21706828474998474\n",
      "iteration 37429: loss: 0.21706819534301758\n",
      "iteration 37430: loss: 0.21706804633140564\n",
      "iteration 37431: loss: 0.21706795692443848\n",
      "iteration 37432: loss: 0.2170678675174713\n",
      "iteration 37433: loss: 0.217067688703537\n",
      "iteration 37434: loss: 0.21706755459308624\n",
      "iteration 37435: loss: 0.21706745028495789\n",
      "iteration 37436: loss: 0.21706728637218475\n",
      "iteration 37437: loss: 0.2170671969652176\n",
      "iteration 37438: loss: 0.21706712245941162\n",
      "iteration 37439: loss: 0.21706700325012207\n",
      "iteration 37440: loss: 0.2170669287443161\n",
      "iteration 37441: loss: 0.21706676483154297\n",
      "iteration 37442: loss: 0.21706664562225342\n",
      "iteration 37443: loss: 0.21706649661064148\n",
      "iteration 37444: loss: 0.21706640720367432\n",
      "iteration 37445: loss: 0.21706628799438477\n",
      "iteration 37446: loss: 0.2170662134885788\n",
      "iteration 37447: loss: 0.21706612408161163\n",
      "iteration 37448: loss: 0.2170659601688385\n",
      "iteration 37449: loss: 0.21706584095954895\n",
      "iteration 37450: loss: 0.2170657217502594\n",
      "iteration 37451: loss: 0.21706564724445343\n",
      "iteration 37452: loss: 0.21706552803516388\n",
      "iteration 37453: loss: 0.21706537902355194\n",
      "iteration 37454: loss: 0.21706530451774597\n",
      "iteration 37455: loss: 0.21706514060497284\n",
      "iteration 37456: loss: 0.2170650064945221\n",
      "iteration 37457: loss: 0.21706482768058777\n",
      "iteration 37458: loss: 0.2170647829771042\n",
      "iteration 37459: loss: 0.21706469357013702\n",
      "iteration 37460: loss: 0.21706458926200867\n",
      "iteration 37461: loss: 0.2170645296573639\n",
      "iteration 37462: loss: 0.21706438064575195\n",
      "iteration 37463: loss: 0.21706421673297882\n",
      "iteration 37464: loss: 0.21706414222717285\n",
      "iteration 37465: loss: 0.21706394851207733\n",
      "iteration 37466: loss: 0.21706385910511017\n",
      "iteration 37467: loss: 0.2170637845993042\n",
      "iteration 37468: loss: 0.21706366539001465\n",
      "iteration 37469: loss: 0.2170635461807251\n",
      "iteration 37470: loss: 0.21706338226795197\n",
      "iteration 37471: loss: 0.21706333756446838\n",
      "iteration 37472: loss: 0.21706314384937286\n",
      "iteration 37473: loss: 0.2170630395412445\n",
      "iteration 37474: loss: 0.21706292033195496\n",
      "iteration 37475: loss: 0.21706286072731018\n",
      "iteration 37476: loss: 0.21706275641918182\n",
      "iteration 37477: loss: 0.21706263720989227\n",
      "iteration 37478: loss: 0.21706247329711914\n",
      "iteration 37479: loss: 0.2170623242855072\n",
      "iteration 37480: loss: 0.21706220507621765\n",
      "iteration 37481: loss: 0.21706214547157288\n",
      "iteration 37482: loss: 0.2170620709657669\n",
      "iteration 37483: loss: 0.21706192195415497\n",
      "iteration 37484: loss: 0.2170618325471878\n",
      "iteration 37485: loss: 0.21706168353557587\n",
      "iteration 37486: loss: 0.21706156432628632\n",
      "iteration 37487: loss: 0.21706143021583557\n",
      "iteration 37488: loss: 0.21706132590770721\n",
      "iteration 37489: loss: 0.21706120669841766\n",
      "iteration 37490: loss: 0.2170610874891281\n",
      "iteration 37491: loss: 0.21706099808216095\n",
      "iteration 37492: loss: 0.2170608937740326\n",
      "iteration 37493: loss: 0.21706080436706543\n",
      "iteration 37494: loss: 0.2170606404542923\n",
      "iteration 37495: loss: 0.21706052124500275\n",
      "iteration 37496: loss: 0.2170604169368744\n",
      "iteration 37497: loss: 0.21706032752990723\n",
      "iteration 37498: loss: 0.2170601636171341\n",
      "iteration 37499: loss: 0.21706004440784454\n",
      "iteration 37500: loss: 0.2170599400997162\n",
      "iteration 37501: loss: 0.21705985069274902\n",
      "iteration 37502: loss: 0.21705973148345947\n",
      "iteration 37503: loss: 0.21705961227416992\n",
      "iteration 37504: loss: 0.21705946326255798\n",
      "iteration 37505: loss: 0.2170594036579132\n",
      "iteration 37506: loss: 0.21705925464630127\n",
      "iteration 37507: loss: 0.2170591801404953\n",
      "iteration 37508: loss: 0.21705901622772217\n",
      "iteration 37509: loss: 0.21705898642539978\n",
      "iteration 37510: loss: 0.21705874800682068\n",
      "iteration 37511: loss: 0.2170586884021759\n",
      "iteration 37512: loss: 0.21705858409404755\n",
      "iteration 37513: loss: 0.217058464884758\n",
      "iteration 37514: loss: 0.21705833077430725\n",
      "iteration 37515: loss: 0.21705827116966248\n",
      "iteration 37516: loss: 0.21705806255340576\n",
      "iteration 37517: loss: 0.21705801784992218\n",
      "iteration 37518: loss: 0.21705779433250427\n",
      "iteration 37519: loss: 0.2170577496290207\n",
      "iteration 37520: loss: 0.21705766022205353\n",
      "iteration 37521: loss: 0.2170574963092804\n",
      "iteration 37522: loss: 0.21705743670463562\n",
      "iteration 37523: loss: 0.2170572578907013\n",
      "iteration 37524: loss: 0.21705719828605652\n",
      "iteration 37525: loss: 0.21705707907676697\n",
      "iteration 37526: loss: 0.2170569896697998\n",
      "iteration 37527: loss: 0.21705682575702667\n",
      "iteration 37528: loss: 0.2170567512512207\n",
      "iteration 37529: loss: 0.21705655753612518\n",
      "iteration 37530: loss: 0.2170564830303192\n",
      "iteration 37531: loss: 0.21705634891986847\n",
      "iteration 37532: loss: 0.2170563042163849\n",
      "iteration 37533: loss: 0.21705612540245056\n",
      "iteration 37534: loss: 0.2170560657978058\n",
      "iteration 37535: loss: 0.21705588698387146\n",
      "iteration 37536: loss: 0.2170557975769043\n",
      "iteration 37537: loss: 0.21705570816993713\n",
      "iteration 37538: loss: 0.21705558896064758\n",
      "iteration 37539: loss: 0.21705541014671326\n",
      "iteration 37540: loss: 0.21705536544322968\n",
      "iteration 37541: loss: 0.21705524623394012\n",
      "iteration 37542: loss: 0.21705515682697296\n",
      "iteration 37543: loss: 0.21705499291419983\n",
      "iteration 37544: loss: 0.21705488860607147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 37545: loss: 0.21705475449562073\n",
      "iteration 37546: loss: 0.21705467998981476\n",
      "iteration 37547: loss: 0.2170545607805252\n",
      "iteration 37548: loss: 0.21705448627471924\n",
      "iteration 37549: loss: 0.21705427765846252\n",
      "iteration 37550: loss: 0.21705415844917297\n",
      "iteration 37551: loss: 0.21705412864685059\n",
      "iteration 37552: loss: 0.21705392003059387\n",
      "iteration 37553: loss: 0.21705392003059387\n",
      "iteration 37554: loss: 0.21705369651317596\n",
      "iteration 37555: loss: 0.2170535773038864\n",
      "iteration 37556: loss: 0.21705350279808044\n",
      "iteration 37557: loss: 0.21705341339111328\n",
      "iteration 37558: loss: 0.21705326437950134\n",
      "iteration 37559: loss: 0.2170531451702118\n",
      "iteration 37560: loss: 0.2170531004667282\n",
      "iteration 37561: loss: 0.21705293655395508\n",
      "iteration 37562: loss: 0.21705281734466553\n",
      "iteration 37563: loss: 0.21705269813537598\n",
      "iteration 37564: loss: 0.21705254912376404\n",
      "iteration 37565: loss: 0.21705245971679688\n",
      "iteration 37566: loss: 0.21705234050750732\n",
      "iteration 37567: loss: 0.21705226600170135\n",
      "iteration 37568: loss: 0.21705210208892822\n",
      "iteration 37569: loss: 0.21705207228660583\n",
      "iteration 37570: loss: 0.21705183386802673\n",
      "iteration 37571: loss: 0.21705178916454315\n",
      "iteration 37572: loss: 0.21705174446105957\n",
      "iteration 37573: loss: 0.21705150604248047\n",
      "iteration 37574: loss: 0.2170514315366745\n",
      "iteration 37575: loss: 0.21705129742622375\n",
      "iteration 37576: loss: 0.2170511931180954\n",
      "iteration 37577: loss: 0.21705102920532227\n",
      "iteration 37578: loss: 0.2170509397983551\n",
      "iteration 37579: loss: 0.21705083549022675\n",
      "iteration 37580: loss: 0.2170507162809372\n",
      "iteration 37581: loss: 0.21705058217048645\n",
      "iteration 37582: loss: 0.21705050766468048\n",
      "iteration 37583: loss: 0.21705038845539093\n",
      "iteration 37584: loss: 0.21705026924610138\n",
      "iteration 37585: loss: 0.2170501947402954\n",
      "iteration 37586: loss: 0.21705003082752228\n",
      "iteration 37587: loss: 0.21704992651939392\n",
      "iteration 37588: loss: 0.21704983711242676\n",
      "iteration 37589: loss: 0.21704967319965363\n",
      "iteration 37590: loss: 0.21704959869384766\n",
      "iteration 37591: loss: 0.2170494794845581\n",
      "iteration 37592: loss: 0.21704939007759094\n",
      "iteration 37593: loss: 0.2170492708683014\n",
      "iteration 37594: loss: 0.21704909205436707\n",
      "iteration 37595: loss: 0.2170490324497223\n",
      "iteration 37596: loss: 0.21704897284507751\n",
      "iteration 37597: loss: 0.21704880893230438\n",
      "iteration 37598: loss: 0.21704861521720886\n",
      "iteration 37599: loss: 0.21704860031604767\n",
      "iteration 37600: loss: 0.21704837679862976\n",
      "iteration 37601: loss: 0.21704833209514618\n",
      "iteration 37602: loss: 0.21704819798469543\n",
      "iteration 37603: loss: 0.21704809367656708\n",
      "iteration 37604: loss: 0.21704792976379395\n",
      "iteration 37605: loss: 0.21704784035682678\n",
      "iteration 37606: loss: 0.21704772114753723\n",
      "iteration 37607: loss: 0.21704760193824768\n",
      "iteration 37608: loss: 0.21704749763011932\n",
      "iteration 37609: loss: 0.21704742312431335\n",
      "iteration 37610: loss: 0.21704736351966858\n",
      "iteration 37611: loss: 0.21704718470573425\n",
      "iteration 37612: loss: 0.2170470505952835\n",
      "iteration 37613: loss: 0.21704688668251038\n",
      "iteration 37614: loss: 0.21704688668251038\n",
      "iteration 37615: loss: 0.21704669296741486\n",
      "iteration 37616: loss: 0.2170466184616089\n",
      "iteration 37617: loss: 0.21704646944999695\n",
      "iteration 37618: loss: 0.21704630553722382\n",
      "iteration 37619: loss: 0.21704630553722382\n",
      "iteration 37620: loss: 0.21704617142677307\n",
      "iteration 37621: loss: 0.21704605221748352\n",
      "iteration 37622: loss: 0.21704590320587158\n",
      "iteration 37623: loss: 0.21704581379890442\n",
      "iteration 37624: loss: 0.21704566478729248\n",
      "iteration 37625: loss: 0.2170456200838089\n",
      "iteration 37626: loss: 0.21704550087451935\n",
      "iteration 37627: loss: 0.21704530715942383\n",
      "iteration 37628: loss: 0.21704521775245667\n",
      "iteration 37629: loss: 0.2170451134443283\n",
      "iteration 37630: loss: 0.21704502403736115\n",
      "iteration 37631: loss: 0.2170448750257492\n",
      "iteration 37632: loss: 0.21704478561878204\n",
      "iteration 37633: loss: 0.2170446664094925\n",
      "iteration 37634: loss: 0.21704451739788055\n",
      "iteration 37635: loss: 0.217044398188591\n",
      "iteration 37636: loss: 0.21704430878162384\n",
      "iteration 37637: loss: 0.2170441448688507\n",
      "iteration 37638: loss: 0.21704411506652832\n",
      "iteration 37639: loss: 0.2170439511537552\n",
      "iteration 37640: loss: 0.21704380214214325\n",
      "iteration 37641: loss: 0.21704372763633728\n",
      "iteration 37642: loss: 0.21704359352588654\n",
      "iteration 37643: loss: 0.21704354882240295\n",
      "iteration 37644: loss: 0.21704337000846863\n",
      "iteration 37645: loss: 0.21704323589801788\n",
      "iteration 37646: loss: 0.2170432060956955\n",
      "iteration 37647: loss: 0.21704307198524475\n",
      "iteration 37648: loss: 0.2170429527759552\n",
      "iteration 37649: loss: 0.2170427292585373\n",
      "iteration 37650: loss: 0.2170427292585373\n",
      "iteration 37651: loss: 0.21704253554344177\n",
      "iteration 37652: loss: 0.21704240143299103\n",
      "iteration 37653: loss: 0.21704237163066864\n",
      "iteration 37654: loss: 0.2170422375202179\n",
      "iteration 37655: loss: 0.21704211831092834\n",
      "iteration 37656: loss: 0.21704205870628357\n",
      "iteration 37657: loss: 0.21704189479351044\n",
      "iteration 37658: loss: 0.21704180538654327\n",
      "iteration 37659: loss: 0.21704164147377014\n",
      "iteration 37660: loss: 0.21704156696796417\n",
      "iteration 37661: loss: 0.21704144775867462\n",
      "iteration 37662: loss: 0.21704132854938507\n",
      "iteration 37663: loss: 0.2170412242412567\n",
      "iteration 37664: loss: 0.21704106032848358\n",
      "iteration 37665: loss: 0.21704097092151642\n",
      "iteration 37666: loss: 0.21704086661338806\n",
      "iteration 37667: loss: 0.2170407772064209\n",
      "iteration 37668: loss: 0.21704062819480896\n",
      "iteration 37669: loss: 0.2170405387878418\n",
      "iteration 37670: loss: 0.21704037487506866\n",
      "iteration 37671: loss: 0.21704034507274628\n",
      "iteration 37672: loss: 0.21704021096229553\n",
      "iteration 37673: loss: 0.2170400321483612\n",
      "iteration 37674: loss: 0.21703991293907166\n",
      "iteration 37675: loss: 0.21703985333442688\n",
      "iteration 37676: loss: 0.21703970432281494\n",
      "iteration 37677: loss: 0.217039555311203\n",
      "iteration 37678: loss: 0.21703951060771942\n",
      "iteration 37679: loss: 0.21703937649726868\n",
      "iteration 37680: loss: 0.21703919768333435\n",
      "iteration 37681: loss: 0.21703915297985077\n",
      "iteration 37682: loss: 0.21703898906707764\n",
      "iteration 37683: loss: 0.21703891456127167\n",
      "iteration 37684: loss: 0.21703878045082092\n",
      "iteration 37685: loss: 0.21703866124153137\n",
      "iteration 37686: loss: 0.2170385867357254\n",
      "iteration 37687: loss: 0.21703843772411346\n",
      "iteration 37688: loss: 0.21703839302062988\n",
      "iteration 37689: loss: 0.21703822910785675\n",
      "iteration 37690: loss: 0.2170381247997284\n",
      "iteration 37691: loss: 0.21703800559043884\n",
      "iteration 37692: loss: 0.2170378863811493\n",
      "iteration 37693: loss: 0.21703776717185974\n",
      "iteration 37694: loss: 0.21703770756721497\n",
      "iteration 37695: loss: 0.21703752875328064\n",
      "iteration 37696: loss: 0.2170374095439911\n",
      "iteration 37697: loss: 0.21703727543354034\n",
      "iteration 37698: loss: 0.21703723073005676\n",
      "iteration 37699: loss: 0.21703708171844482\n",
      "iteration 37700: loss: 0.21703699231147766\n",
      "iteration 37701: loss: 0.21703684329986572\n",
      "iteration 37702: loss: 0.21703672409057617\n",
      "iteration 37703: loss: 0.217036634683609\n",
      "iteration 37704: loss: 0.21703653037548065\n",
      "iteration 37705: loss: 0.2170364111661911\n",
      "iteration 37706: loss: 0.21703629195690155\n",
      "iteration 37707: loss: 0.2170361578464508\n",
      "iteration 37708: loss: 0.21703605353832245\n",
      "iteration 37709: loss: 0.2170359343290329\n",
      "iteration 37710: loss: 0.21703581511974335\n",
      "iteration 37711: loss: 0.21703574061393738\n",
      "iteration 37712: loss: 0.21703562140464783\n",
      "iteration 37713: loss: 0.2170354574918747\n",
      "iteration 37714: loss: 0.21703526377677917\n",
      "iteration 37715: loss: 0.21703526377677917\n",
      "iteration 37716: loss: 0.2170352041721344\n",
      "iteration 37717: loss: 0.21703505516052246\n",
      "iteration 37718: loss: 0.21703489124774933\n",
      "iteration 37719: loss: 0.21703477203845978\n",
      "iteration 37720: loss: 0.21703465282917023\n",
      "iteration 37721: loss: 0.21703454852104187\n",
      "iteration 37722: loss: 0.21703441441059113\n",
      "iteration 37723: loss: 0.21703438460826874\n",
      "iteration 37724: loss: 0.21703419089317322\n",
      "iteration 37725: loss: 0.21703413128852844\n",
      "iteration 37726: loss: 0.2170339822769165\n",
      "iteration 37727: loss: 0.21703395247459412\n",
      "iteration 37728: loss: 0.21703381836414337\n",
      "iteration 37729: loss: 0.21703366935253143\n",
      "iteration 37730: loss: 0.21703355014324188\n",
      "iteration 37731: loss: 0.21703341603279114\n",
      "iteration 37732: loss: 0.2170332968235016\n",
      "iteration 37733: loss: 0.21703317761421204\n",
      "iteration 37734: loss: 0.21703310310840607\n",
      "iteration 37735: loss: 0.2170329988002777\n",
      "iteration 37736: loss: 0.21703286468982697\n",
      "iteration 37737: loss: 0.21703274548053741\n",
      "iteration 37738: loss: 0.21703259646892548\n",
      "iteration 37739: loss: 0.2170325517654419\n",
      "iteration 37740: loss: 0.21703238785266876\n",
      "iteration 37741: loss: 0.2170322686433792\n",
      "iteration 37742: loss: 0.21703214943408966\n",
      "iteration 37743: loss: 0.2170320749282837\n",
      "iteration 37744: loss: 0.21703198552131653\n",
      "iteration 37745: loss: 0.2170318067073822\n",
      "iteration 37746: loss: 0.21703171730041504\n",
      "iteration 37747: loss: 0.2170315682888031\n",
      "iteration 37748: loss: 0.21703147888183594\n",
      "iteration 37749: loss: 0.21703138947486877\n",
      "iteration 37750: loss: 0.21703128516674042\n",
      "iteration 37751: loss: 0.21703116595745087\n",
      "iteration 37752: loss: 0.21703103184700012\n",
      "iteration 37753: loss: 0.21703091263771057\n",
      "iteration 37754: loss: 0.2170308381319046\n",
      "iteration 37755: loss: 0.21703071892261505\n",
      "iteration 37756: loss: 0.2170305699110031\n",
      "iteration 37757: loss: 0.21703049540519714\n",
      "iteration 37758: loss: 0.217030331492424\n",
      "iteration 37759: loss: 0.21703021228313446\n",
      "iteration 37760: loss: 0.21703001856803894\n",
      "iteration 37761: loss: 0.21703000366687775\n",
      "iteration 37762: loss: 0.2170298993587494\n",
      "iteration 37763: loss: 0.21702980995178223\n",
      "iteration 37764: loss: 0.2170296162366867\n",
      "iteration 37765: loss: 0.21702954173088074\n",
      "iteration 37766: loss: 0.21702940762043\n",
      "iteration 37767: loss: 0.21702928841114044\n",
      "iteration 37768: loss: 0.21702921390533447\n",
      "iteration 37769: loss: 0.21702909469604492\n",
      "iteration 37770: loss: 0.21702897548675537\n",
      "iteration 37771: loss: 0.2170288860797882\n",
      "iteration 37772: loss: 0.21702882647514343\n",
      "iteration 37773: loss: 0.2170286476612091\n",
      "iteration 37774: loss: 0.21702852845191956\n",
      "iteration 37775: loss: 0.2170284539461136\n",
      "iteration 37776: loss: 0.21702830493450165\n",
      "iteration 37777: loss: 0.21702811121940613\n",
      "iteration 37778: loss: 0.21702805161476135\n",
      "iteration 37779: loss: 0.21702790260314941\n",
      "iteration 37780: loss: 0.21702778339385986\n",
      "iteration 37781: loss: 0.21702775359153748\n",
      "iteration 37782: loss: 0.21702754497528076\n",
      "iteration 37783: loss: 0.2170274555683136\n",
      "iteration 37784: loss: 0.21702738106250763\n",
      "iteration 37785: loss: 0.21702730655670166\n",
      "iteration 37786: loss: 0.21702709794044495\n",
      "iteration 37787: loss: 0.2170269936323166\n",
      "iteration 37788: loss: 0.217026948928833\n",
      "iteration 37789: loss: 0.21702685952186584\n",
      "iteration 37790: loss: 0.21702666580677032\n",
      "iteration 37791: loss: 0.2170264720916748\n",
      "iteration 37792: loss: 0.2170264720916748\n",
      "iteration 37793: loss: 0.21702635288238525\n",
      "iteration 37794: loss: 0.21702618896961212\n",
      "iteration 37795: loss: 0.21702614426612854\n",
      "iteration 37796: loss: 0.21702603995800018\n",
      "iteration 37797: loss: 0.21702584624290466\n",
      "iteration 37798: loss: 0.21702571213245392\n",
      "iteration 37799: loss: 0.21702566742897034\n",
      "iteration 37800: loss: 0.21702560782432556\n",
      "iteration 37801: loss: 0.21702539920806885\n",
      "iteration 37802: loss: 0.21702532470226288\n",
      "iteration 37803: loss: 0.21702519059181213\n",
      "iteration 37804: loss: 0.21702511608600616\n",
      "iteration 37805: loss: 0.2170249968767166\n",
      "iteration 37806: loss: 0.21702487766742706\n",
      "iteration 37807: loss: 0.21702472865581512\n",
      "iteration 37808: loss: 0.21702465415000916\n",
      "iteration 37809: loss: 0.2170245200395584\n",
      "iteration 37810: loss: 0.21702440083026886\n",
      "iteration 37811: loss: 0.21702425181865692\n",
      "iteration 37812: loss: 0.21702416241168976\n",
      "iteration 37813: loss: 0.21702401340007782\n",
      "iteration 37814: loss: 0.21702389419078827\n",
      "iteration 37815: loss: 0.21702373027801514\n",
      "iteration 37816: loss: 0.21702370047569275\n",
      "iteration 37817: loss: 0.21702361106872559\n",
      "iteration 37818: loss: 0.21702341735363007\n",
      "iteration 37819: loss: 0.21702337265014648\n",
      "iteration 37820: loss: 0.21702322363853455\n",
      "iteration 37821: loss: 0.21702313423156738\n",
      "iteration 37822: loss: 0.21702301502227783\n",
      "iteration 37823: loss: 0.21702294051647186\n",
      "iteration 37824: loss: 0.2170228213071823\n",
      "iteration 37825: loss: 0.21702270209789276\n",
      "iteration 37826: loss: 0.2170225828886032\n",
      "iteration 37827: loss: 0.21702246367931366\n",
      "iteration 37828: loss: 0.2170223444700241\n",
      "iteration 37829: loss: 0.21702225506305695\n",
      "iteration 37830: loss: 0.2170220911502838\n",
      "iteration 37831: loss: 0.21702201664447784\n",
      "iteration 37832: loss: 0.2170218676328659\n",
      "iteration 37833: loss: 0.21702179312705994\n",
      "iteration 37834: loss: 0.21702167391777039\n",
      "iteration 37835: loss: 0.2170216143131256\n",
      "iteration 37836: loss: 0.2170213907957077\n",
      "iteration 37837: loss: 0.21702131628990173\n",
      "iteration 37838: loss: 0.2170211523771286\n",
      "iteration 37839: loss: 0.21702110767364502\n",
      "iteration 37840: loss: 0.21702098846435547\n",
      "iteration 37841: loss: 0.21702082455158234\n",
      "iteration 37842: loss: 0.21702075004577637\n",
      "iteration 37843: loss: 0.2170206755399704\n",
      "iteration 37844: loss: 0.21702055633068085\n",
      "iteration 37845: loss: 0.2170204371213913\n",
      "iteration 37846: loss: 0.21702022850513458\n",
      "iteration 37847: loss: 0.2170201539993286\n",
      "iteration 37848: loss: 0.21702007949352264\n",
      "iteration 37849: loss: 0.2170199453830719\n",
      "iteration 37850: loss: 0.21701984107494354\n",
      "iteration 37851: loss: 0.2170197069644928\n",
      "iteration 37852: loss: 0.21701955795288086\n",
      "iteration 37853: loss: 0.21701951324939728\n",
      "iteration 37854: loss: 0.21701936423778534\n",
      "iteration 37855: loss: 0.21701927483081818\n",
      "iteration 37856: loss: 0.21701917052268982\n",
      "iteration 37857: loss: 0.21701905131340027\n",
      "iteration 37858: loss: 0.21701891720294952\n",
      "iteration 37859: loss: 0.21701879799365997\n",
      "iteration 37860: loss: 0.21701869368553162\n",
      "iteration 37861: loss: 0.21701864898204803\n",
      "iteration 37862: loss: 0.21701844036579132\n",
      "iteration 37863: loss: 0.21701833605766296\n",
      "iteration 37864: loss: 0.2170182466506958\n",
      "iteration 37865: loss: 0.21701812744140625\n",
      "iteration 37866: loss: 0.2170179784297943\n",
      "iteration 37867: loss: 0.21701788902282715\n",
      "iteration 37868: loss: 0.21701781451702118\n",
      "iteration 37869: loss: 0.21701769530773163\n",
      "iteration 37870: loss: 0.21701756119728088\n",
      "iteration 37871: loss: 0.21701745688915253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 37872: loss: 0.217017263174057\n",
      "iteration 37873: loss: 0.21701717376708984\n",
      "iteration 37874: loss: 0.21701712906360626\n",
      "iteration 37875: loss: 0.21701698005199432\n",
      "iteration 37876: loss: 0.21701689064502716\n",
      "iteration 37877: loss: 0.2170167714357376\n",
      "iteration 37878: loss: 0.21701665222644806\n",
      "iteration 37879: loss: 0.2170165479183197\n",
      "iteration 37880: loss: 0.21701645851135254\n",
      "iteration 37881: loss: 0.2170162945985794\n",
      "iteration 37882: loss: 0.21701619029045105\n",
      "iteration 37883: loss: 0.2170161008834839\n",
      "iteration 37884: loss: 0.21701598167419434\n",
      "iteration 37885: loss: 0.21701589226722717\n",
      "iteration 37886: loss: 0.21701574325561523\n",
      "iteration 37887: loss: 0.21701569855213165\n",
      "iteration 37888: loss: 0.21701547503471375\n",
      "iteration 37889: loss: 0.21701541543006897\n",
      "iteration 37890: loss: 0.21701522171497345\n",
      "iteration 37891: loss: 0.21701522171497345\n",
      "iteration 37892: loss: 0.21701499819755554\n",
      "iteration 37893: loss: 0.21701499819755554\n",
      "iteration 37894: loss: 0.2170148342847824\n",
      "iteration 37895: loss: 0.21701470017433167\n",
      "iteration 37896: loss: 0.21701458096504211\n",
      "iteration 37897: loss: 0.21701450645923615\n",
      "iteration 37898: loss: 0.2170144021511078\n",
      "iteration 37899: loss: 0.21701419353485107\n",
      "iteration 37900: loss: 0.21701419353485107\n",
      "iteration 37901: loss: 0.21701404452323914\n",
      "iteration 37902: loss: 0.21701395511627197\n",
      "iteration 37903: loss: 0.217013880610466\n",
      "iteration 37904: loss: 0.21701376140117645\n",
      "iteration 37905: loss: 0.21701359748840332\n",
      "iteration 37906: loss: 0.21701347827911377\n",
      "iteration 37907: loss: 0.21701335906982422\n",
      "iteration 37908: loss: 0.21701326966285706\n",
      "iteration 37909: loss: 0.21701307594776154\n",
      "iteration 37910: loss: 0.21701304614543915\n",
      "iteration 37911: loss: 0.21701285243034363\n",
      "iteration 37912: loss: 0.21701283752918243\n",
      "iteration 37913: loss: 0.2170126885175705\n",
      "iteration 37914: loss: 0.21701255440711975\n",
      "iteration 37915: loss: 0.21701247990131378\n",
      "iteration 37916: loss: 0.21701237559318542\n",
      "iteration 37917: loss: 0.21701224148273468\n",
      "iteration 37918: loss: 0.21701212227344513\n",
      "iteration 37919: loss: 0.21701201796531677\n",
      "iteration 37920: loss: 0.21701183915138245\n",
      "iteration 37921: loss: 0.21701177954673767\n",
      "iteration 37922: loss: 0.21701164543628693\n",
      "iteration 37923: loss: 0.217011496424675\n",
      "iteration 37924: loss: 0.21701140701770782\n",
      "iteration 37925: loss: 0.2170112431049347\n",
      "iteration 37926: loss: 0.2170112133026123\n",
      "iteration 37927: loss: 0.21701104938983917\n",
      "iteration 37928: loss: 0.21701094508171082\n",
      "iteration 37929: loss: 0.21701085567474365\n",
      "iteration 37930: loss: 0.2170107662677765\n",
      "iteration 37931: loss: 0.21701064705848694\n",
      "iteration 37932: loss: 0.2170105278491974\n",
      "iteration 37933: loss: 0.21701034903526306\n",
      "iteration 37934: loss: 0.2170102894306183\n",
      "iteration 37935: loss: 0.21701017022132874\n",
      "iteration 37936: loss: 0.21701005101203918\n",
      "iteration 37937: loss: 0.21700994670391083\n",
      "iteration 37938: loss: 0.21700982749462128\n",
      "iteration 37939: loss: 0.2170097380876541\n",
      "iteration 37940: loss: 0.21700957417488098\n",
      "iteration 37941: loss: 0.21700946986675262\n",
      "iteration 37942: loss: 0.21700939536094666\n",
      "iteration 37943: loss: 0.2170092761516571\n",
      "iteration 37944: loss: 0.21700911223888397\n",
      "iteration 37945: loss: 0.21700897812843323\n",
      "iteration 37946: loss: 0.21700891852378845\n",
      "iteration 37947: loss: 0.2170087844133377\n",
      "iteration 37948: loss: 0.21700866520404816\n",
      "iteration 37949: loss: 0.21700862050056458\n",
      "iteration 37950: loss: 0.21700850129127502\n",
      "iteration 37951: loss: 0.21700838208198547\n",
      "iteration 37952: loss: 0.21700815856456757\n",
      "iteration 37953: loss: 0.2170080840587616\n",
      "iteration 37954: loss: 0.21700796484947205\n",
      "iteration 37955: loss: 0.21700787544250488\n",
      "iteration 37956: loss: 0.21700778603553772\n",
      "iteration 37957: loss: 0.2170076072216034\n",
      "iteration 37958: loss: 0.21700754761695862\n",
      "iteration 37959: loss: 0.21700744330883026\n",
      "iteration 37960: loss: 0.2170073688030243\n",
      "iteration 37961: loss: 0.2170071303844452\n",
      "iteration 37962: loss: 0.217007115483284\n",
      "iteration 37963: loss: 0.21700701117515564\n",
      "iteration 37964: loss: 0.2170068770647049\n",
      "iteration 37965: loss: 0.21700675785541534\n",
      "iteration 37966: loss: 0.2170066386461258\n",
      "iteration 37967: loss: 0.21700653433799744\n",
      "iteration 37968: loss: 0.21700641512870789\n",
      "iteration 37969: loss: 0.21700629591941833\n",
      "iteration 37970: loss: 0.21700617671012878\n",
      "iteration 37971: loss: 0.2170061320066452\n",
      "iteration 37972: loss: 0.21700599789619446\n",
      "iteration 37973: loss: 0.21700584888458252\n",
      "iteration 37974: loss: 0.21700572967529297\n",
      "iteration 37975: loss: 0.2170056402683258\n",
      "iteration 37976: loss: 0.21700546145439148\n",
      "iteration 37977: loss: 0.2170054018497467\n",
      "iteration 37978: loss: 0.21700532734394073\n",
      "iteration 37979: loss: 0.21700520813465118\n",
      "iteration 37980: loss: 0.21700508892536163\n",
      "iteration 37981: loss: 0.21700501441955566\n",
      "iteration 37982: loss: 0.21700482070446014\n",
      "iteration 37983: loss: 0.2170046865940094\n",
      "iteration 37984: loss: 0.21700458228588104\n",
      "iteration 37985: loss: 0.21700449287891388\n",
      "iteration 37986: loss: 0.21700434386730194\n",
      "iteration 37987: loss: 0.21700429916381836\n",
      "iteration 37988: loss: 0.21700410544872284\n",
      "iteration 37989: loss: 0.21700403094291687\n",
      "iteration 37990: loss: 0.21700391173362732\n",
      "iteration 37991: loss: 0.21700377762317657\n",
      "iteration 37992: loss: 0.21700365841388702\n",
      "iteration 37993: loss: 0.21700358390808105\n",
      "iteration 37994: loss: 0.2170034945011139\n",
      "iteration 37995: loss: 0.21700334548950195\n",
      "iteration 37996: loss: 0.2170032262802124\n",
      "iteration 37997: loss: 0.21700303256511688\n",
      "iteration 37998: loss: 0.2170030176639557\n",
      "iteration 37999: loss: 0.21700291335582733\n",
      "iteration 38000: loss: 0.21700279414653778\n",
      "iteration 38001: loss: 0.21700266003608704\n",
      "iteration 38002: loss: 0.21700255572795868\n",
      "iteration 38003: loss: 0.21700254082679749\n",
      "iteration 38004: loss: 0.21700230240821838\n",
      "iteration 38005: loss: 0.21700222790241241\n",
      "iteration 38006: loss: 0.21700207889080048\n",
      "iteration 38007: loss: 0.2170019894838333\n",
      "iteration 38008: loss: 0.21700191497802734\n",
      "iteration 38009: loss: 0.2170017659664154\n",
      "iteration 38010: loss: 0.21700164675712585\n",
      "iteration 38011: loss: 0.21700160205364227\n",
      "iteration 38012: loss: 0.21700146794319153\n",
      "iteration 38013: loss: 0.21700134873390198\n",
      "iteration 38014: loss: 0.21700119972229004\n",
      "iteration 38015: loss: 0.2170010358095169\n",
      "iteration 38016: loss: 0.21700091660022736\n",
      "iteration 38017: loss: 0.21700091660022736\n",
      "iteration 38018: loss: 0.21700072288513184\n",
      "iteration 38019: loss: 0.21700060367584229\n",
      "iteration 38020: loss: 0.21700052917003632\n",
      "iteration 38021: loss: 0.21700043976306915\n",
      "iteration 38022: loss: 0.2170003354549408\n",
      "iteration 38023: loss: 0.21700021624565125\n",
      "iteration 38024: loss: 0.2170000821352005\n",
      "iteration 38025: loss: 0.21699996292591095\n",
      "iteration 38026: loss: 0.2169998437166214\n",
      "iteration 38027: loss: 0.21699972450733185\n",
      "iteration 38028: loss: 0.21699965000152588\n",
      "iteration 38029: loss: 0.21699950098991394\n",
      "iteration 38030: loss: 0.21699941158294678\n",
      "iteration 38031: loss: 0.21699929237365723\n",
      "iteration 38032: loss: 0.2169990986585617\n",
      "iteration 38033: loss: 0.21699905395507812\n",
      "iteration 38034: loss: 0.216998890042305\n",
      "iteration 38035: loss: 0.21699878573417664\n",
      "iteration 38036: loss: 0.21699869632720947\n",
      "iteration 38037: loss: 0.21699854731559753\n",
      "iteration 38038: loss: 0.21699845790863037\n",
      "iteration 38039: loss: 0.2169984132051468\n",
      "iteration 38040: loss: 0.21699829399585724\n",
      "iteration 38041: loss: 0.2169981449842453\n",
      "iteration 38042: loss: 0.21699795126914978\n",
      "iteration 38043: loss: 0.2169979065656662\n",
      "iteration 38044: loss: 0.21699786186218262\n",
      "iteration 38045: loss: 0.21699771285057068\n",
      "iteration 38046: loss: 0.21699750423431396\n",
      "iteration 38047: loss: 0.21699747443199158\n",
      "iteration 38048: loss: 0.21699734032154083\n",
      "iteration 38049: loss: 0.2169971913099289\n",
      "iteration 38050: loss: 0.2169971466064453\n",
      "iteration 38051: loss: 0.21699698269367218\n",
      "iteration 38052: loss: 0.2169969379901886\n",
      "iteration 38053: loss: 0.21699675917625427\n",
      "iteration 38054: loss: 0.2169966995716095\n",
      "iteration 38055: loss: 0.21699652075767517\n",
      "iteration 38056: loss: 0.2169964760541916\n",
      "iteration 38057: loss: 0.21699634194374084\n",
      "iteration 38058: loss: 0.2169962376356125\n",
      "iteration 38059: loss: 0.21699604392051697\n",
      "iteration 38060: loss: 0.2169959992170334\n",
      "iteration 38061: loss: 0.21699586510658264\n",
      "iteration 38062: loss: 0.21699580550193787\n",
      "iteration 38063: loss: 0.21699562668800354\n",
      "iteration 38064: loss: 0.21699556708335876\n",
      "iteration 38065: loss: 0.21699538826942444\n",
      "iteration 38066: loss: 0.21699532866477966\n",
      "iteration 38067: loss: 0.2169952392578125\n",
      "iteration 38068: loss: 0.21699504554271698\n",
      "iteration 38069: loss: 0.216994971036911\n",
      "iteration 38070: loss: 0.21699480712413788\n",
      "iteration 38071: loss: 0.2169947326183319\n",
      "iteration 38072: loss: 0.21699459850788116\n",
      "iteration 38073: loss: 0.2169944792985916\n",
      "iteration 38074: loss: 0.21699437499046326\n",
      "iteration 38075: loss: 0.2169942557811737\n",
      "iteration 38076: loss: 0.21699421107769012\n",
      "iteration 38077: loss: 0.21699409186840057\n",
      "iteration 38078: loss: 0.21699395775794983\n",
      "iteration 38079: loss: 0.21699385344982147\n",
      "iteration 38080: loss: 0.21699373424053192\n",
      "iteration 38081: loss: 0.21699364483356476\n",
      "iteration 38082: loss: 0.21699342131614685\n",
      "iteration 38083: loss: 0.21699342131614685\n",
      "iteration 38084: loss: 0.21699324250221252\n",
      "iteration 38085: loss: 0.21699313819408417\n",
      "iteration 38086: loss: 0.216993048787117\n",
      "iteration 38087: loss: 0.21699294447898865\n",
      "iteration 38088: loss: 0.21699276566505432\n",
      "iteration 38089: loss: 0.21699270606040955\n",
      "iteration 38090: loss: 0.21699258685112\n",
      "iteration 38091: loss: 0.21699252724647522\n",
      "iteration 38092: loss: 0.21699237823486328\n",
      "iteration 38093: loss: 0.21699222922325134\n",
      "iteration 38094: loss: 0.21699216961860657\n",
      "iteration 38095: loss: 0.21699205040931702\n",
      "iteration 38096: loss: 0.21699190139770508\n",
      "iteration 38097: loss: 0.21699181199073792\n",
      "iteration 38098: loss: 0.21699170768260956\n",
      "iteration 38099: loss: 0.21699158847332\n",
      "iteration 38100: loss: 0.21699146926403046\n",
      "iteration 38101: loss: 0.2169913798570633\n",
      "iteration 38102: loss: 0.21699126064777374\n",
      "iteration 38103: loss: 0.2169910967350006\n",
      "iteration 38104: loss: 0.21699103713035583\n",
      "iteration 38105: loss: 0.2169909030199051\n",
      "iteration 38106: loss: 0.21699075400829315\n",
      "iteration 38107: loss: 0.21699067950248718\n",
      "iteration 38108: loss: 0.21699059009552002\n",
      "iteration 38109: loss: 0.21699047088623047\n",
      "iteration 38110: loss: 0.21699032187461853\n",
      "iteration 38111: loss: 0.21699023246765137\n",
      "iteration 38112: loss: 0.2169901430606842\n",
      "iteration 38113: loss: 0.21699002385139465\n",
      "iteration 38114: loss: 0.2169899195432663\n",
      "iteration 38115: loss: 0.21698983013629913\n",
      "iteration 38116: loss: 0.2169896811246872\n",
      "iteration 38117: loss: 0.21698956191539764\n",
      "iteration 38118: loss: 0.2169894278049469\n",
      "iteration 38119: loss: 0.21698932349681854\n",
      "iteration 38120: loss: 0.216989204287529\n",
      "iteration 38121: loss: 0.21698911488056183\n",
      "iteration 38122: loss: 0.21698899567127228\n",
      "iteration 38123: loss: 0.21698884665966034\n",
      "iteration 38124: loss: 0.21698875725269318\n",
      "iteration 38125: loss: 0.21698865294456482\n",
      "iteration 38126: loss: 0.2169884890317917\n",
      "iteration 38127: loss: 0.21698836982250214\n",
      "iteration 38128: loss: 0.21698828041553497\n",
      "iteration 38129: loss: 0.21698813140392303\n",
      "iteration 38130: loss: 0.21698811650276184\n",
      "iteration 38131: loss: 0.2169879674911499\n",
      "iteration 38132: loss: 0.21698780357837677\n",
      "iteration 38133: loss: 0.2169877588748932\n",
      "iteration 38134: loss: 0.21698765456676483\n",
      "iteration 38135: loss: 0.21698753535747528\n",
      "iteration 38136: loss: 0.21698741614818573\n",
      "iteration 38137: loss: 0.21698729693889618\n",
      "iteration 38138: loss: 0.21698720753192902\n",
      "iteration 38139: loss: 0.21698705852031708\n",
      "iteration 38140: loss: 0.2169869840145111\n",
      "iteration 38141: loss: 0.21698680520057678\n",
      "iteration 38142: loss: 0.216986745595932\n",
      "iteration 38143: loss: 0.21698661148548126\n",
      "iteration 38144: loss: 0.2169865071773529\n",
      "iteration 38145: loss: 0.21698641777038574\n",
      "iteration 38146: loss: 0.21698632836341858\n",
      "iteration 38147: loss: 0.21698622405529022\n",
      "iteration 38148: loss: 0.2169860154390335\n",
      "iteration 38149: loss: 0.21698589622974396\n",
      "iteration 38150: loss: 0.2169857919216156\n",
      "iteration 38151: loss: 0.21698570251464844\n",
      "iteration 38152: loss: 0.2169855535030365\n",
      "iteration 38153: loss: 0.21698546409606934\n",
      "iteration 38154: loss: 0.21698534488677979\n",
      "iteration 38155: loss: 0.21698527038097382\n",
      "iteration 38156: loss: 0.21698515117168427\n",
      "iteration 38157: loss: 0.21698503196239471\n",
      "iteration 38158: loss: 0.21698494255542755\n",
      "iteration 38159: loss: 0.21698474884033203\n",
      "iteration 38160: loss: 0.21698470413684845\n",
      "iteration 38161: loss: 0.2169845849275589\n",
      "iteration 38162: loss: 0.21698443591594696\n",
      "iteration 38163: loss: 0.21698439121246338\n",
      "iteration 38164: loss: 0.21698419749736786\n",
      "iteration 38165: loss: 0.2169841080904007\n",
      "iteration 38166: loss: 0.21698403358459473\n",
      "iteration 38167: loss: 0.21698394417762756\n",
      "iteration 38168: loss: 0.216983824968338\n",
      "iteration 38169: loss: 0.21698370575904846\n",
      "iteration 38170: loss: 0.21698355674743652\n",
      "iteration 38171: loss: 0.21698346734046936\n",
      "iteration 38172: loss: 0.2169833481311798\n",
      "iteration 38173: loss: 0.21698322892189026\n",
      "iteration 38174: loss: 0.2169831544160843\n",
      "iteration 38175: loss: 0.21698299050331116\n",
      "iteration 38176: loss: 0.2169828712940216\n",
      "iteration 38177: loss: 0.21698284149169922\n",
      "iteration 38178: loss: 0.21698275208473206\n",
      "iteration 38179: loss: 0.21698260307312012\n",
      "iteration 38180: loss: 0.21698243916034698\n",
      "iteration 38181: loss: 0.21698229014873505\n",
      "iteration 38182: loss: 0.21698227524757385\n",
      "iteration 38183: loss: 0.21698212623596191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 38184: loss: 0.21698197722434998\n",
      "iteration 38185: loss: 0.21698181331157684\n",
      "iteration 38186: loss: 0.21698173880577087\n",
      "iteration 38187: loss: 0.2169816792011261\n",
      "iteration 38188: loss: 0.2169814556837082\n",
      "iteration 38189: loss: 0.2169814109802246\n",
      "iteration 38190: loss: 0.21698132157325745\n",
      "iteration 38191: loss: 0.2169812172651291\n",
      "iteration 38192: loss: 0.21698108315467834\n",
      "iteration 38193: loss: 0.21698100864887238\n",
      "iteration 38194: loss: 0.21698085963726044\n",
      "iteration 38195: loss: 0.2169807404279709\n",
      "iteration 38196: loss: 0.21698054671287537\n",
      "iteration 38197: loss: 0.21698057651519775\n",
      "iteration 38198: loss: 0.21698041260242462\n",
      "iteration 38199: loss: 0.21698030829429626\n",
      "iteration 38200: loss: 0.21698017418384552\n",
      "iteration 38201: loss: 0.21698005497455597\n",
      "iteration 38202: loss: 0.21697993576526642\n",
      "iteration 38203: loss: 0.21697983145713806\n",
      "iteration 38204: loss: 0.2169797420501709\n",
      "iteration 38205: loss: 0.21697957813739777\n",
      "iteration 38206: loss: 0.2169795036315918\n",
      "iteration 38207: loss: 0.21697941422462463\n",
      "iteration 38208: loss: 0.2169792652130127\n",
      "iteration 38209: loss: 0.2169792205095291\n",
      "iteration 38210: loss: 0.2169790267944336\n",
      "iteration 38211: loss: 0.21697893738746643\n",
      "iteration 38212: loss: 0.2169787883758545\n",
      "iteration 38213: loss: 0.2169787436723709\n",
      "iteration 38214: loss: 0.2169785499572754\n",
      "iteration 38215: loss: 0.2169785052537918\n",
      "iteration 38216: loss: 0.21697838604450226\n",
      "iteration 38217: loss: 0.2169782668352127\n",
      "iteration 38218: loss: 0.21697816252708435\n",
      "iteration 38219: loss: 0.2169780731201172\n",
      "iteration 38220: loss: 0.21697792410850525\n",
      "iteration 38221: loss: 0.21697783470153809\n",
      "iteration 38222: loss: 0.21697771549224854\n",
      "iteration 38223: loss: 0.2169775664806366\n",
      "iteration 38224: loss: 0.21697747707366943\n",
      "iteration 38225: loss: 0.21697740256786346\n",
      "iteration 38226: loss: 0.21697716414928436\n",
      "iteration 38227: loss: 0.21697711944580078\n",
      "iteration 38228: loss: 0.2169770449399948\n",
      "iteration 38229: loss: 0.21697692573070526\n",
      "iteration 38230: loss: 0.2169768065214157\n",
      "iteration 38231: loss: 0.21697664260864258\n",
      "iteration 38232: loss: 0.2169765681028366\n",
      "iteration 38233: loss: 0.21697647869586945\n",
      "iteration 38234: loss: 0.2169763296842575\n",
      "iteration 38235: loss: 0.21697625517845154\n",
      "iteration 38236: loss: 0.2169760912656784\n",
      "iteration 38237: loss: 0.21697597205638885\n",
      "iteration 38238: loss: 0.21697589755058289\n",
      "iteration 38239: loss: 0.21697580814361572\n",
      "iteration 38240: loss: 0.21697565913200378\n",
      "iteration 38241: loss: 0.216975599527359\n",
      "iteration 38242: loss: 0.21697542071342468\n",
      "iteration 38243: loss: 0.2169753611087799\n",
      "iteration 38244: loss: 0.21697521209716797\n",
      "iteration 38245: loss: 0.2169751226902008\n",
      "iteration 38246: loss: 0.21697504818439484\n",
      "iteration 38247: loss: 0.2169748842716217\n",
      "iteration 38248: loss: 0.21697476506233215\n",
      "iteration 38249: loss: 0.2169746458530426\n",
      "iteration 38250: loss: 0.21697457134723663\n",
      "iteration 38251: loss: 0.2169744223356247\n",
      "iteration 38252: loss: 0.21697434782981873\n",
      "iteration 38253: loss: 0.21697422862052917\n",
      "iteration 38254: loss: 0.21697402000427246\n",
      "iteration 38255: loss: 0.21697397530078888\n",
      "iteration 38256: loss: 0.21697387099266052\n",
      "iteration 38257: loss: 0.21697378158569336\n",
      "iteration 38258: loss: 0.21697363257408142\n",
      "iteration 38259: loss: 0.21697357296943665\n",
      "iteration 38260: loss: 0.2169734537601471\n",
      "iteration 38261: loss: 0.21697333455085754\n",
      "iteration 38262: loss: 0.2169732302427292\n",
      "iteration 38263: loss: 0.21697311103343964\n",
      "iteration 38264: loss: 0.2169729471206665\n",
      "iteration 38265: loss: 0.21697285771369934\n",
      "iteration 38266: loss: 0.2169727385044098\n",
      "iteration 38267: loss: 0.21697263419628143\n",
      "iteration 38268: loss: 0.21697251498699188\n",
      "iteration 38269: loss: 0.2169724404811859\n",
      "iteration 38270: loss: 0.21697230637073517\n",
      "iteration 38271: loss: 0.21697218716144562\n",
      "iteration 38272: loss: 0.21697211265563965\n",
      "iteration 38273: loss: 0.2169719636440277\n",
      "iteration 38274: loss: 0.21697187423706055\n",
      "iteration 38275: loss: 0.216971755027771\n",
      "iteration 38276: loss: 0.21697159111499786\n",
      "iteration 38277: loss: 0.21697154641151428\n",
      "iteration 38278: loss: 0.21697139739990234\n",
      "iteration 38279: loss: 0.2169712781906128\n",
      "iteration 38280: loss: 0.21697115898132324\n",
      "iteration 38281: loss: 0.2169710099697113\n",
      "iteration 38282: loss: 0.21697095036506653\n",
      "iteration 38283: loss: 0.21697084605693817\n",
      "iteration 38284: loss: 0.21697071194648743\n",
      "iteration 38285: loss: 0.21697060763835907\n",
      "iteration 38286: loss: 0.21697048842906952\n",
      "iteration 38287: loss: 0.21697036921977997\n",
      "iteration 38288: loss: 0.2169702798128128\n",
      "iteration 38289: loss: 0.21697020530700684\n",
      "iteration 38290: loss: 0.21697013080120087\n",
      "iteration 38291: loss: 0.21696993708610535\n",
      "iteration 38292: loss: 0.2169698178768158\n",
      "iteration 38293: loss: 0.21696972846984863\n",
      "iteration 38294: loss: 0.2169695794582367\n",
      "iteration 38295: loss: 0.21696946024894714\n",
      "iteration 38296: loss: 0.21696941554546356\n",
      "iteration 38297: loss: 0.216969296336174\n",
      "iteration 38298: loss: 0.2169691026210785\n",
      "iteration 38299: loss: 0.2169690579175949\n",
      "iteration 38300: loss: 0.21696889400482178\n",
      "iteration 38301: loss: 0.2169688194990158\n",
      "iteration 38302: loss: 0.21696880459785461\n",
      "iteration 38303: loss: 0.2169685810804367\n",
      "iteration 38304: loss: 0.21696846187114716\n",
      "iteration 38305: loss: 0.21696829795837402\n",
      "iteration 38306: loss: 0.21696829795837402\n",
      "iteration 38307: loss: 0.21696817874908447\n",
      "iteration 38308: loss: 0.21696802973747253\n",
      "iteration 38309: loss: 0.21696791052818298\n",
      "iteration 38310: loss: 0.21696777641773224\n",
      "iteration 38311: loss: 0.21696767210960388\n",
      "iteration 38312: loss: 0.21696758270263672\n",
      "iteration 38313: loss: 0.21696743369102478\n",
      "iteration 38314: loss: 0.21696734428405762\n",
      "iteration 38315: loss: 0.21696722507476807\n",
      "iteration 38316: loss: 0.21696710586547852\n",
      "iteration 38317: loss: 0.21696703135967255\n",
      "iteration 38318: loss: 0.21696686744689941\n",
      "iteration 38319: loss: 0.21696677803993225\n",
      "iteration 38320: loss: 0.21696670353412628\n",
      "iteration 38321: loss: 0.21696658432483673\n",
      "iteration 38322: loss: 0.2169664353132248\n",
      "iteration 38323: loss: 0.21696636080741882\n",
      "iteration 38324: loss: 0.2169661968946457\n",
      "iteration 38325: loss: 0.21696612238883972\n",
      "iteration 38326: loss: 0.21696600317955017\n",
      "iteration 38327: loss: 0.216965913772583\n",
      "iteration 38328: loss: 0.21696582436561584\n",
      "iteration 38329: loss: 0.21696564555168152\n",
      "iteration 38330: loss: 0.21696558594703674\n",
      "iteration 38331: loss: 0.2169654667377472\n",
      "iteration 38332: loss: 0.21696528792381287\n",
      "iteration 38333: loss: 0.21696524322032928\n",
      "iteration 38334: loss: 0.21696512401103973\n",
      "iteration 38335: loss: 0.2169649600982666\n",
      "iteration 38336: loss: 0.21696488559246063\n",
      "iteration 38337: loss: 0.2169647216796875\n",
      "iteration 38338: loss: 0.21696467697620392\n",
      "iteration 38339: loss: 0.21696452796459198\n",
      "iteration 38340: loss: 0.21696440875530243\n",
      "iteration 38341: loss: 0.21696431934833527\n",
      "iteration 38342: loss: 0.21696420013904572\n",
      "iteration 38343: loss: 0.21696409583091736\n",
      "iteration 38344: loss: 0.21696403622627258\n",
      "iteration 38345: loss: 0.21696385741233826\n",
      "iteration 38346: loss: 0.2169637680053711\n",
      "iteration 38347: loss: 0.21696364879608154\n",
      "iteration 38348: loss: 0.2169634997844696\n",
      "iteration 38349: loss: 0.2169634848833084\n",
      "iteration 38350: loss: 0.21696336567401886\n",
      "iteration 38351: loss: 0.2169632464647293\n",
      "iteration 38352: loss: 0.21696308255195618\n",
      "iteration 38353: loss: 0.21696297824382782\n",
      "iteration 38354: loss: 0.21696290373802185\n",
      "iteration 38355: loss: 0.21696273982524872\n",
      "iteration 38356: loss: 0.21696265041828156\n",
      "iteration 38357: loss: 0.216962531208992\n",
      "iteration 38358: loss: 0.21696241199970245\n",
      "iteration 38359: loss: 0.2169622927904129\n",
      "iteration 38360: loss: 0.21696217358112335\n",
      "iteration 38361: loss: 0.2169620543718338\n",
      "iteration 38362: loss: 0.21696193516254425\n",
      "iteration 38363: loss: 0.2169618159532547\n",
      "iteration 38364: loss: 0.21696171164512634\n",
      "iteration 38365: loss: 0.2169615775346756\n",
      "iteration 38366: loss: 0.2169615775346756\n",
      "iteration 38367: loss: 0.21696138381958008\n",
      "iteration 38368: loss: 0.21696129441261292\n",
      "iteration 38369: loss: 0.21696117520332336\n",
      "iteration 38370: loss: 0.21696102619171143\n",
      "iteration 38371: loss: 0.21696093678474426\n",
      "iteration 38372: loss: 0.2169608324766159\n",
      "iteration 38373: loss: 0.21696078777313232\n",
      "iteration 38374: loss: 0.2169606238603592\n",
      "iteration 38375: loss: 0.21696051955223083\n",
      "iteration 38376: loss: 0.21696040034294128\n",
      "iteration 38377: loss: 0.21696031093597412\n",
      "iteration 38378: loss: 0.216960147023201\n",
      "iteration 38379: loss: 0.21696007251739502\n",
      "iteration 38380: loss: 0.21695999801158905\n",
      "iteration 38381: loss: 0.2169598639011383\n",
      "iteration 38382: loss: 0.21695967018604279\n",
      "iteration 38383: loss: 0.21695955097675323\n",
      "iteration 38384: loss: 0.21695952117443085\n",
      "iteration 38385: loss: 0.2169594019651413\n",
      "iteration 38386: loss: 0.21695928275585175\n",
      "iteration 38387: loss: 0.21695920825004578\n",
      "iteration 38388: loss: 0.21695902943611145\n",
      "iteration 38389: loss: 0.2169589102268219\n",
      "iteration 38390: loss: 0.21695883572101593\n",
      "iteration 38391: loss: 0.21695871651172638\n",
      "iteration 38392: loss: 0.21695856750011444\n",
      "iteration 38393: loss: 0.21695847809314728\n",
      "iteration 38394: loss: 0.21695837378501892\n",
      "iteration 38395: loss: 0.21695831418037415\n",
      "iteration 38396: loss: 0.21695812046527863\n",
      "iteration 38397: loss: 0.21695801615715027\n",
      "iteration 38398: loss: 0.2169579565525055\n",
      "iteration 38399: loss: 0.21695776283740997\n",
      "iteration 38400: loss: 0.216957688331604\n",
      "iteration 38401: loss: 0.21695759892463684\n",
      "iteration 38402: loss: 0.21695749461650848\n",
      "iteration 38403: loss: 0.21695736050605774\n",
      "iteration 38404: loss: 0.21695730090141296\n",
      "iteration 38405: loss: 0.21695709228515625\n",
      "iteration 38406: loss: 0.21695701777935028\n",
      "iteration 38407: loss: 0.21695688366889954\n",
      "iteration 38408: loss: 0.21695685386657715\n",
      "iteration 38409: loss: 0.21695668995380402\n",
      "iteration 38410: loss: 0.21695658564567566\n",
      "iteration 38411: loss: 0.2169564664363861\n",
      "iteration 38412: loss: 0.21695637702941895\n",
      "iteration 38413: loss: 0.2169562578201294\n",
      "iteration 38414: loss: 0.21695616841316223\n",
      "iteration 38415: loss: 0.21695604920387268\n",
      "iteration 38416: loss: 0.21695590019226074\n",
      "iteration 38417: loss: 0.2169557809829712\n",
      "iteration 38418: loss: 0.2169557362794876\n",
      "iteration 38419: loss: 0.21695561707019806\n",
      "iteration 38420: loss: 0.21695542335510254\n",
      "iteration 38421: loss: 0.21695542335510254\n",
      "iteration 38422: loss: 0.21695522964000702\n",
      "iteration 38423: loss: 0.21695514023303986\n",
      "iteration 38424: loss: 0.2169550210237503\n",
      "iteration 38425: loss: 0.21695490181446075\n",
      "iteration 38426: loss: 0.2169547975063324\n",
      "iteration 38427: loss: 0.21695461869239807\n",
      "iteration 38428: loss: 0.21695461869239807\n",
      "iteration 38429: loss: 0.21695446968078613\n",
      "iteration 38430: loss: 0.2169543206691742\n",
      "iteration 38431: loss: 0.21695426106452942\n",
      "iteration 38432: loss: 0.21695415675640106\n",
      "iteration 38433: loss: 0.2169540375471115\n",
      "iteration 38434: loss: 0.21695390343666077\n",
      "iteration 38435: loss: 0.2169537991285324\n",
      "iteration 38436: loss: 0.21695370972156525\n",
      "iteration 38437: loss: 0.2169535905122757\n",
      "iteration 38438: loss: 0.21695347130298615\n",
      "iteration 38439: loss: 0.21695327758789062\n",
      "iteration 38440: loss: 0.21695324778556824\n",
      "iteration 38441: loss: 0.2169531285762787\n",
      "iteration 38442: loss: 0.21695300936698914\n",
      "iteration 38443: loss: 0.21695291996002197\n",
      "iteration 38444: loss: 0.2169528305530548\n",
      "iteration 38445: loss: 0.2169526070356369\n",
      "iteration 38446: loss: 0.21695256233215332\n",
      "iteration 38447: loss: 0.2169523984193802\n",
      "iteration 38448: loss: 0.2169523686170578\n",
      "iteration 38449: loss: 0.21695224940776825\n",
      "iteration 38450: loss: 0.2169521301984787\n",
      "iteration 38451: loss: 0.21695199608802795\n",
      "iteration 38452: loss: 0.2169518768787384\n",
      "iteration 38453: loss: 0.21695177257061005\n",
      "iteration 38454: loss: 0.2169516533613205\n",
      "iteration 38455: loss: 0.21695153415203094\n",
      "iteration 38456: loss: 0.21695145964622498\n",
      "iteration 38457: loss: 0.21695132553577423\n",
      "iteration 38458: loss: 0.21695120632648468\n",
      "iteration 38459: loss: 0.21695108711719513\n",
      "iteration 38460: loss: 0.21695098280906677\n",
      "iteration 38461: loss: 0.21695084869861603\n",
      "iteration 38462: loss: 0.21695080399513245\n",
      "iteration 38463: loss: 0.21695062518119812\n",
      "iteration 38464: loss: 0.21695058047771454\n",
      "iteration 38465: loss: 0.2169504463672638\n",
      "iteration 38466: loss: 0.21695034205913544\n",
      "iteration 38467: loss: 0.21695025265216827\n",
      "iteration 38468: loss: 0.21695005893707275\n",
      "iteration 38469: loss: 0.21694998443126678\n",
      "iteration 38470: loss: 0.21694982051849365\n",
      "iteration 38471: loss: 0.21694977581501007\n",
      "iteration 38472: loss: 0.21694962680339813\n",
      "iteration 38473: loss: 0.21694955229759216\n",
      "iteration 38474: loss: 0.21694934368133545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 38475: loss: 0.21694931387901306\n",
      "iteration 38476: loss: 0.21694917976856232\n",
      "iteration 38477: loss: 0.21694907546043396\n",
      "iteration 38478: loss: 0.2169489860534668\n",
      "iteration 38479: loss: 0.21694883704185486\n",
      "iteration 38480: loss: 0.21694879233837128\n",
      "iteration 38481: loss: 0.21694865822792053\n",
      "iteration 38482: loss: 0.21694853901863098\n",
      "iteration 38483: loss: 0.216948464512825\n",
      "iteration 38484: loss: 0.2169482707977295\n",
      "iteration 38485: loss: 0.21694815158843994\n",
      "iteration 38486: loss: 0.21694810688495636\n",
      "iteration 38487: loss: 0.2169479876756668\n",
      "iteration 38488: loss: 0.21694794297218323\n",
      "iteration 38489: loss: 0.21694771945476532\n",
      "iteration 38490: loss: 0.21694763004779816\n",
      "iteration 38491: loss: 0.2169475108385086\n",
      "iteration 38492: loss: 0.21694734692573547\n",
      "iteration 38493: loss: 0.21694722771644592\n",
      "iteration 38494: loss: 0.21694716811180115\n",
      "iteration 38495: loss: 0.2169470489025116\n",
      "iteration 38496: loss: 0.21694698929786682\n",
      "iteration 38497: loss: 0.21694688498973846\n",
      "iteration 38498: loss: 0.21694672107696533\n",
      "iteration 38499: loss: 0.21694663166999817\n",
      "iteration 38500: loss: 0.2169465273618698\n",
      "iteration 38501: loss: 0.21694640815258026\n",
      "iteration 38502: loss: 0.2169462889432907\n",
      "iteration 38503: loss: 0.21694616973400116\n",
      "iteration 38504: loss: 0.216946080327034\n",
      "iteration 38505: loss: 0.21694596111774445\n",
      "iteration 38506: loss: 0.2169458568096161\n",
      "iteration 38507: loss: 0.21694576740264893\n",
      "iteration 38508: loss: 0.2169455736875534\n",
      "iteration 38509: loss: 0.2169455736875534\n",
      "iteration 38510: loss: 0.2169453650712967\n",
      "iteration 38511: loss: 0.21694524586200714\n",
      "iteration 38512: loss: 0.21694520115852356\n",
      "iteration 38513: loss: 0.21694500744342804\n",
      "iteration 38514: loss: 0.21694502234458923\n",
      "iteration 38515: loss: 0.2169448435306549\n",
      "iteration 38516: loss: 0.21694469451904297\n",
      "iteration 38517: loss: 0.2169446051120758\n",
      "iteration 38518: loss: 0.21694450080394745\n",
      "iteration 38519: loss: 0.2169443666934967\n",
      "iteration 38520: loss: 0.21694426238536835\n",
      "iteration 38521: loss: 0.2169441431760788\n",
      "iteration 38522: loss: 0.21694405376911163\n",
      "iteration 38523: loss: 0.21694397926330566\n",
      "iteration 38524: loss: 0.21694378554821014\n",
      "iteration 38525: loss: 0.21694377064704895\n",
      "iteration 38526: loss: 0.216943621635437\n",
      "iteration 38527: loss: 0.21694353222846985\n",
      "iteration 38528: loss: 0.2169434130191803\n",
      "iteration 38529: loss: 0.21694326400756836\n",
      "iteration 38530: loss: 0.2169431447982788\n",
      "iteration 38531: loss: 0.21694302558898926\n",
      "iteration 38532: loss: 0.21694286167621613\n",
      "iteration 38533: loss: 0.21694278717041016\n",
      "iteration 38534: loss: 0.216942697763443\n",
      "iteration 38535: loss: 0.21694263815879822\n",
      "iteration 38536: loss: 0.2169424295425415\n",
      "iteration 38537: loss: 0.2169424593448639\n",
      "iteration 38538: loss: 0.21694226562976837\n",
      "iteration 38539: loss: 0.21694214642047882\n",
      "iteration 38540: loss: 0.21694199740886688\n",
      "iteration 38541: loss: 0.2169419229030609\n",
      "iteration 38542: loss: 0.21694180369377136\n",
      "iteration 38543: loss: 0.2169416844844818\n",
      "iteration 38544: loss: 0.21694159507751465\n",
      "iteration 38545: loss: 0.2169414758682251\n",
      "iteration 38546: loss: 0.21694135665893555\n",
      "iteration 38547: loss: 0.216941237449646\n",
      "iteration 38548: loss: 0.21694107353687286\n",
      "iteration 38549: loss: 0.21694102883338928\n",
      "iteration 38550: loss: 0.21694087982177734\n",
      "iteration 38551: loss: 0.21694079041481018\n",
      "iteration 38552: loss: 0.21694068610668182\n",
      "iteration 38553: loss: 0.21694055199623108\n",
      "iteration 38554: loss: 0.2169404923915863\n",
      "iteration 38555: loss: 0.21694037318229675\n",
      "iteration 38556: loss: 0.21694031357765198\n",
      "iteration 38557: loss: 0.21694013476371765\n",
      "iteration 38558: loss: 0.2169400155544281\n",
      "iteration 38559: loss: 0.21693989634513855\n",
      "iteration 38560: loss: 0.21693983674049377\n",
      "iteration 38561: loss: 0.21693968772888184\n",
      "iteration 38562: loss: 0.21693959832191467\n",
      "iteration 38563: loss: 0.2169395387172699\n",
      "iteration 38564: loss: 0.21693937480449677\n",
      "iteration 38565: loss: 0.21693924069404602\n",
      "iteration 38566: loss: 0.21693912148475647\n",
      "iteration 38567: loss: 0.2169390469789505\n",
      "iteration 38568: loss: 0.21693892776966095\n",
      "iteration 38569: loss: 0.21693876385688782\n",
      "iteration 38570: loss: 0.21693861484527588\n",
      "iteration 38571: loss: 0.2169385701417923\n",
      "iteration 38572: loss: 0.21693840622901917\n",
      "iteration 38573: loss: 0.2169383019208908\n",
      "iteration 38574: loss: 0.21693822741508484\n",
      "iteration 38575: loss: 0.2169381082057953\n",
      "iteration 38576: loss: 0.21693801879882812\n",
      "iteration 38577: loss: 0.21693792939186096\n",
      "iteration 38578: loss: 0.2169378250837326\n",
      "iteration 38579: loss: 0.21693766117095947\n",
      "iteration 38580: loss: 0.2169375866651535\n",
      "iteration 38581: loss: 0.21693745255470276\n",
      "iteration 38582: loss: 0.21693739295005798\n",
      "iteration 38583: loss: 0.21693725883960724\n",
      "iteration 38584: loss: 0.2169370949268341\n",
      "iteration 38585: loss: 0.21693703532218933\n",
      "iteration 38586: loss: 0.21693702042102814\n",
      "iteration 38587: loss: 0.216936856508255\n",
      "iteration 38588: loss: 0.21693666279315948\n",
      "iteration 38589: loss: 0.2169366180896759\n",
      "iteration 38590: loss: 0.21693642437458038\n",
      "iteration 38591: loss: 0.21693632006645203\n",
      "iteration 38592: loss: 0.21693626046180725\n",
      "iteration 38593: loss: 0.2169361412525177\n",
      "iteration 38594: loss: 0.21693599224090576\n",
      "iteration 38595: loss: 0.2169359177350998\n",
      "iteration 38596: loss: 0.21693575382232666\n",
      "iteration 38597: loss: 0.2169356644153595\n",
      "iteration 38598: loss: 0.21693556010723114\n",
      "iteration 38599: loss: 0.21693547070026398\n",
      "iteration 38600: loss: 0.21693535149097443\n",
      "iteration 38601: loss: 0.21693524718284607\n",
      "iteration 38602: loss: 0.21693511307239532\n",
      "iteration 38603: loss: 0.21693500876426697\n",
      "iteration 38604: loss: 0.2169349193572998\n",
      "iteration 38605: loss: 0.21693475544452667\n",
      "iteration 38606: loss: 0.21693472564220428\n",
      "iteration 38607: loss: 0.21693459153175354\n",
      "iteration 38608: loss: 0.2169344425201416\n",
      "iteration 38609: loss: 0.21693436801433563\n",
      "iteration 38610: loss: 0.21693415939807892\n",
      "iteration 38611: loss: 0.21693411469459534\n",
      "iteration 38612: loss: 0.2169339656829834\n",
      "iteration 38613: loss: 0.216933935880661\n",
      "iteration 38614: loss: 0.21693381667137146\n",
      "iteration 38615: loss: 0.21693365275859833\n",
      "iteration 38616: loss: 0.21693351864814758\n",
      "iteration 38617: loss: 0.2169334441423416\n",
      "iteration 38618: loss: 0.21693332493305206\n",
      "iteration 38619: loss: 0.2169332504272461\n",
      "iteration 38620: loss: 0.21693308651447296\n",
      "iteration 38621: loss: 0.2169329673051834\n",
      "iteration 38622: loss: 0.21693293750286102\n",
      "iteration 38623: loss: 0.21693269908428192\n",
      "iteration 38624: loss: 0.21693268418312073\n",
      "iteration 38625: loss: 0.2169325053691864\n",
      "iteration 38626: loss: 0.21693244576454163\n",
      "iteration 38627: loss: 0.2169322967529297\n",
      "iteration 38628: loss: 0.21693220734596252\n",
      "iteration 38629: loss: 0.21693214774131775\n",
      "iteration 38630: loss: 0.21693198382854462\n",
      "iteration 38631: loss: 0.21693189442157745\n",
      "iteration 38632: loss: 0.2169317752122879\n",
      "iteration 38633: loss: 0.21693161129951477\n",
      "iteration 38634: loss: 0.21693155169487\n",
      "iteration 38635: loss: 0.21693141758441925\n",
      "iteration 38636: loss: 0.21693137288093567\n",
      "iteration 38637: loss: 0.2169312685728073\n",
      "iteration 38638: loss: 0.21693110466003418\n",
      "iteration 38639: loss: 0.2169310301542282\n",
      "iteration 38640: loss: 0.21693086624145508\n",
      "iteration 38641: loss: 0.2169308215379715\n",
      "iteration 38642: loss: 0.21693067252635956\n",
      "iteration 38643: loss: 0.2169305831193924\n",
      "iteration 38644: loss: 0.21693043410778046\n",
      "iteration 38645: loss: 0.2169303148984909\n",
      "iteration 38646: loss: 0.21693019568920135\n",
      "iteration 38647: loss: 0.21693003177642822\n",
      "iteration 38648: loss: 0.21693000197410583\n",
      "iteration 38649: loss: 0.21692988276481628\n",
      "iteration 38650: loss: 0.21692974865436554\n",
      "iteration 38651: loss: 0.21692967414855957\n",
      "iteration 38652: loss: 0.21692955493927002\n",
      "iteration 38653: loss: 0.21692940592765808\n",
      "iteration 38654: loss: 0.21692931652069092\n",
      "iteration 38655: loss: 0.21692915260791779\n",
      "iteration 38656: loss: 0.2169291079044342\n",
      "iteration 38657: loss: 0.21692900359630585\n",
      "iteration 38658: loss: 0.2169288694858551\n",
      "iteration 38659: loss: 0.21692869067192078\n",
      "iteration 38660: loss: 0.21692867577075958\n",
      "iteration 38661: loss: 0.21692851185798645\n",
      "iteration 38662: loss: 0.2169283926486969\n",
      "iteration 38663: loss: 0.21692827343940735\n",
      "iteration 38664: loss: 0.216928169131279\n",
      "iteration 38665: loss: 0.21692803502082825\n",
      "iteration 38666: loss: 0.21692800521850586\n",
      "iteration 38667: loss: 0.2169279158115387\n",
      "iteration 38668: loss: 0.21692776679992676\n",
      "iteration 38669: loss: 0.21692761778831482\n",
      "iteration 38670: loss: 0.21692752838134766\n",
      "iteration 38671: loss: 0.21692737936973572\n",
      "iteration 38672: loss: 0.21692726016044617\n",
      "iteration 38673: loss: 0.2169272005558014\n",
      "iteration 38674: loss: 0.21692709624767303\n",
      "iteration 38675: loss: 0.21692700684070587\n",
      "iteration 38676: loss: 0.21692685782909393\n",
      "iteration 38677: loss: 0.21692678332328796\n",
      "iteration 38678: loss: 0.21692661941051483\n",
      "iteration 38679: loss: 0.21692657470703125\n",
      "iteration 38680: loss: 0.21692638099193573\n",
      "iteration 38681: loss: 0.21692624688148499\n",
      "iteration 38682: loss: 0.2169262170791626\n",
      "iteration 38683: loss: 0.21692605316638947\n",
      "iteration 38684: loss: 0.21692593395709991\n",
      "iteration 38685: loss: 0.21692585945129395\n",
      "iteration 38686: loss: 0.2169257402420044\n",
      "iteration 38687: loss: 0.21692565083503723\n",
      "iteration 38688: loss: 0.21692553162574768\n",
      "iteration 38689: loss: 0.21692535281181335\n",
      "iteration 38690: loss: 0.21692529320716858\n",
      "iteration 38691: loss: 0.21692518889904022\n",
      "iteration 38692: loss: 0.21692511439323425\n",
      "iteration 38693: loss: 0.2169249802827835\n",
      "iteration 38694: loss: 0.21692486107349396\n",
      "iteration 38695: loss: 0.2169247567653656\n",
      "iteration 38696: loss: 0.21692459285259247\n",
      "iteration 38697: loss: 0.2169245481491089\n",
      "iteration 38698: loss: 0.21692439913749695\n",
      "iteration 38699: loss: 0.21692433953285217\n",
      "iteration 38700: loss: 0.21692419052124023\n",
      "iteration 38701: loss: 0.21692414581775665\n",
      "iteration 38702: loss: 0.21692399680614471\n",
      "iteration 38703: loss: 0.21692386269569397\n",
      "iteration 38704: loss: 0.21692368388175964\n",
      "iteration 38705: loss: 0.21692362427711487\n",
      "iteration 38706: loss: 0.2169235646724701\n",
      "iteration 38707: loss: 0.21692343056201935\n",
      "iteration 38708: loss: 0.2169232815504074\n",
      "iteration 38709: loss: 0.21692319214344025\n",
      "iteration 38710: loss: 0.2169230729341507\n",
      "iteration 38711: loss: 0.21692302823066711\n",
      "iteration 38712: loss: 0.2169228494167328\n",
      "iteration 38713: loss: 0.21692268550395966\n",
      "iteration 38714: loss: 0.21692264080047607\n",
      "iteration 38715: loss: 0.21692247688770294\n",
      "iteration 38716: loss: 0.21692243218421936\n",
      "iteration 38717: loss: 0.21692225337028503\n",
      "iteration 38718: loss: 0.21692219376564026\n",
      "iteration 38719: loss: 0.21692204475402832\n",
      "iteration 38720: loss: 0.21692195534706116\n",
      "iteration 38721: loss: 0.2169218510389328\n",
      "iteration 38722: loss: 0.21692176163196564\n",
      "iteration 38723: loss: 0.2169216424226761\n",
      "iteration 38724: loss: 0.21692153811454773\n",
      "iteration 38725: loss: 0.2169213742017746\n",
      "iteration 38726: loss: 0.21692128479480743\n",
      "iteration 38727: loss: 0.21692116558551788\n",
      "iteration 38728: loss: 0.21692109107971191\n",
      "iteration 38729: loss: 0.21692100167274475\n",
      "iteration 38730: loss: 0.2169208526611328\n",
      "iteration 38731: loss: 0.21692076325416565\n",
      "iteration 38732: loss: 0.2169206440448761\n",
      "iteration 38733: loss: 0.21692052483558655\n",
      "iteration 38734: loss: 0.216920405626297\n",
      "iteration 38735: loss: 0.21692028641700745\n",
      "iteration 38736: loss: 0.21692021191120148\n",
      "iteration 38737: loss: 0.21692004799842834\n",
      "iteration 38738: loss: 0.2169199436903\n",
      "iteration 38739: loss: 0.21691985428333282\n",
      "iteration 38740: loss: 0.2169197052717209\n",
      "iteration 38741: loss: 0.21691961586475372\n",
      "iteration 38742: loss: 0.21691949665546417\n",
      "iteration 38743: loss: 0.2169194221496582\n",
      "iteration 38744: loss: 0.21691930294036865\n",
      "iteration 38745: loss: 0.21691910922527313\n",
      "iteration 38746: loss: 0.21691903471946716\n",
      "iteration 38747: loss: 0.21691899001598358\n",
      "iteration 38748: loss: 0.21691890060901642\n",
      "iteration 38749: loss: 0.2169187068939209\n",
      "iteration 38750: loss: 0.21691855788230896\n",
      "iteration 38751: loss: 0.21691855788230896\n",
      "iteration 38752: loss: 0.21691839396953583\n",
      "iteration 38753: loss: 0.21691831946372986\n",
      "iteration 38754: loss: 0.21691815555095673\n",
      "iteration 38755: loss: 0.2169179618358612\n",
      "iteration 38756: loss: 0.2169179469347\n",
      "iteration 38757: loss: 0.21691790223121643\n",
      "iteration 38758: loss: 0.2169177532196045\n",
      "iteration 38759: loss: 0.21691766381263733\n",
      "iteration 38760: loss: 0.21691754460334778\n",
      "iteration 38761: loss: 0.21691736578941345\n",
      "iteration 38762: loss: 0.21691732108592987\n",
      "iteration 38763: loss: 0.21691718697547913\n",
      "iteration 38764: loss: 0.21691708266735077\n",
      "iteration 38765: loss: 0.21691691875457764\n",
      "iteration 38766: loss: 0.21691684424877167\n",
      "iteration 38767: loss: 0.2169167548418045\n",
      "iteration 38768: loss: 0.21691660583019257\n",
      "iteration 38769: loss: 0.21691648662090302\n",
      "iteration 38770: loss: 0.21691636741161346\n",
      "iteration 38771: loss: 0.21691632270812988\n",
      "iteration 38772: loss: 0.21691612899303436\n",
      "iteration 38773: loss: 0.2169160544872284\n",
      "iteration 38774: loss: 0.21691596508026123\n",
      "iteration 38775: loss: 0.2169158160686493\n",
      "iteration 38776: loss: 0.21691575646400452\n",
      "iteration 38777: loss: 0.21691560745239258\n",
      "iteration 38778: loss: 0.21691551804542542\n",
      "iteration 38779: loss: 0.21691536903381348\n",
      "iteration 38780: loss: 0.2169152945280075\n",
      "iteration 38781: loss: 0.21691516041755676\n",
      "iteration 38782: loss: 0.21691513061523438\n",
      "iteration 38783: loss: 0.21691496670246124\n",
      "iteration 38784: loss: 0.21691486239433289\n",
      "iteration 38785: loss: 0.21691468358039856\n",
      "iteration 38786: loss: 0.21691465377807617\n",
      "iteration 38787: loss: 0.21691453456878662\n",
      "iteration 38788: loss: 0.21691441535949707\n",
      "iteration 38789: loss: 0.21691426634788513\n",
      "iteration 38790: loss: 0.21691417694091797\n",
      "iteration 38791: loss: 0.216914102435112\n",
      "iteration 38792: loss: 0.21691398322582245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 38793: loss: 0.2169138640165329\n",
      "iteration 38794: loss: 0.21691374480724335\n",
      "iteration 38795: loss: 0.21691358089447021\n",
      "iteration 38796: loss: 0.21691350638866425\n",
      "iteration 38797: loss: 0.21691341698169708\n",
      "iteration 38798: loss: 0.21691329777240753\n",
      "iteration 38799: loss: 0.2169131338596344\n",
      "iteration 38800: loss: 0.21691302955150604\n",
      "iteration 38801: loss: 0.21691294014453888\n",
      "iteration 38802: loss: 0.2169128954410553\n",
      "iteration 38803: loss: 0.21691274642944336\n",
      "iteration 38804: loss: 0.2169126272201538\n",
      "iteration 38805: loss: 0.21691250801086426\n",
      "iteration 38806: loss: 0.21691229939460754\n",
      "iteration 38807: loss: 0.21691229939460754\n",
      "iteration 38808: loss: 0.2169121950864792\n",
      "iteration 38809: loss: 0.21691206097602844\n",
      "iteration 38810: loss: 0.21691195666790009\n",
      "iteration 38811: loss: 0.21691186726093292\n",
      "iteration 38812: loss: 0.21691174805164337\n",
      "iteration 38813: loss: 0.21691155433654785\n",
      "iteration 38814: loss: 0.21691150963306427\n",
      "iteration 38815: loss: 0.21691136062145233\n",
      "iteration 38816: loss: 0.2169112265110016\n",
      "iteration 38817: loss: 0.2169112265110016\n",
      "iteration 38818: loss: 0.21691112220287323\n",
      "iteration 38819: loss: 0.21691098809242249\n",
      "iteration 38820: loss: 0.21691083908081055\n",
      "iteration 38821: loss: 0.21691074967384338\n",
      "iteration 38822: loss: 0.21691064536571503\n",
      "iteration 38823: loss: 0.2169104516506195\n",
      "iteration 38824: loss: 0.21691040694713593\n",
      "iteration 38825: loss: 0.2169102430343628\n",
      "iteration 38826: loss: 0.21691015362739563\n",
      "iteration 38827: loss: 0.21691007912158966\n",
      "iteration 38828: loss: 0.21690991520881653\n",
      "iteration 38829: loss: 0.21690981090068817\n",
      "iteration 38830: loss: 0.21690969169139862\n",
      "iteration 38831: loss: 0.21690960228443146\n",
      "iteration 38832: loss: 0.21690957248210907\n",
      "iteration 38833: loss: 0.21690937876701355\n",
      "iteration 38834: loss: 0.2169092893600464\n",
      "iteration 38835: loss: 0.21690919995307922\n",
      "iteration 38836: loss: 0.21690905094146729\n",
      "iteration 38837: loss: 0.21690897643566132\n",
      "iteration 38838: loss: 0.21690884232521057\n",
      "iteration 38839: loss: 0.2169087827205658\n",
      "iteration 38840: loss: 0.21690860390663147\n",
      "iteration 38841: loss: 0.2169085294008255\n",
      "iteration 38842: loss: 0.21690841019153595\n",
      "iteration 38843: loss: 0.21690833568572998\n",
      "iteration 38844: loss: 0.21690817177295685\n",
      "iteration 38845: loss: 0.2169080525636673\n",
      "iteration 38846: loss: 0.21690794825553894\n",
      "iteration 38847: loss: 0.2169078290462494\n",
      "iteration 38848: loss: 0.21690770983695984\n",
      "iteration 38849: loss: 0.21690765023231506\n",
      "iteration 38850: loss: 0.2169075459241867\n",
      "iteration 38851: loss: 0.21690741181373596\n",
      "iteration 38852: loss: 0.2169073075056076\n",
      "iteration 38853: loss: 0.21690723299980164\n",
      "iteration 38854: loss: 0.21690699458122253\n",
      "iteration 38855: loss: 0.21690693497657776\n",
      "iteration 38856: loss: 0.21690687537193298\n",
      "iteration 38857: loss: 0.21690666675567627\n",
      "iteration 38858: loss: 0.2169065773487091\n",
      "iteration 38859: loss: 0.21690647304058075\n",
      "iteration 38860: loss: 0.21690639853477478\n",
      "iteration 38861: loss: 0.21690630912780762\n",
      "iteration 38862: loss: 0.21690618991851807\n",
      "iteration 38863: loss: 0.21690607070922852\n",
      "iteration 38864: loss: 0.21690595149993896\n",
      "iteration 38865: loss: 0.21690583229064941\n",
      "iteration 38866: loss: 0.21690568327903748\n",
      "iteration 38867: loss: 0.21690556406974792\n",
      "iteration 38868: loss: 0.21690551936626434\n",
      "iteration 38869: loss: 0.2169053554534912\n",
      "iteration 38870: loss: 0.21690531075000763\n",
      "iteration 38871: loss: 0.2169051617383957\n",
      "iteration 38872: loss: 0.2169051170349121\n",
      "iteration 38873: loss: 0.21690495312213898\n",
      "iteration 38874: loss: 0.21690480411052704\n",
      "iteration 38875: loss: 0.21690472960472107\n",
      "iteration 38876: loss: 0.2169046401977539\n",
      "iteration 38877: loss: 0.21690455079078674\n",
      "iteration 38878: loss: 0.2169044017791748\n",
      "iteration 38879: loss: 0.21690428256988525\n",
      "iteration 38880: loss: 0.2169041633605957\n",
      "iteration 38881: loss: 0.21690401434898376\n",
      "iteration 38882: loss: 0.216903954744339\n",
      "iteration 38883: loss: 0.21690383553504944\n",
      "iteration 38884: loss: 0.2169036865234375\n",
      "iteration 38885: loss: 0.2169036865234375\n",
      "iteration 38886: loss: 0.21690353751182556\n",
      "iteration 38887: loss: 0.21690337359905243\n",
      "iteration 38888: loss: 0.21690328419208527\n",
      "iteration 38889: loss: 0.21690312027931213\n",
      "iteration 38890: loss: 0.21690306067466736\n",
      "iteration 38891: loss: 0.21690289676189423\n",
      "iteration 38892: loss: 0.21690285205841064\n",
      "iteration 38893: loss: 0.2169026881456375\n",
      "iteration 38894: loss: 0.21690264344215393\n",
      "iteration 38895: loss: 0.216902494430542\n",
      "iteration 38896: loss: 0.2169024497270584\n",
      "iteration 38897: loss: 0.21690233051776886\n",
      "iteration 38898: loss: 0.21690213680267334\n",
      "iteration 38899: loss: 0.21690213680267334\n",
      "iteration 38900: loss: 0.2169019877910614\n",
      "iteration 38901: loss: 0.21690182387828827\n",
      "iteration 38902: loss: 0.2169017493724823\n",
      "iteration 38903: loss: 0.21690163016319275\n",
      "iteration 38904: loss: 0.21690154075622559\n",
      "iteration 38905: loss: 0.21690139174461365\n",
      "iteration 38906: loss: 0.2169012576341629\n",
      "iteration 38907: loss: 0.21690115332603455\n",
      "iteration 38908: loss: 0.2169010192155838\n",
      "iteration 38909: loss: 0.21690087020397186\n",
      "iteration 38910: loss: 0.21690085530281067\n",
      "iteration 38911: loss: 0.2169007509946823\n",
      "iteration 38912: loss: 0.21690063178539276\n",
      "iteration 38913: loss: 0.21690049767494202\n",
      "iteration 38914: loss: 0.21690034866333008\n",
      "iteration 38915: loss: 0.21690025925636292\n",
      "iteration 38916: loss: 0.21690015494823456\n",
      "iteration 38917: loss: 0.2169000655412674\n",
      "iteration 38918: loss: 0.21689991652965546\n",
      "iteration 38919: loss: 0.21689987182617188\n",
      "iteration 38920: loss: 0.21689970791339874\n",
      "iteration 38921: loss: 0.21689963340759277\n",
      "iteration 38922: loss: 0.21689948439598083\n",
      "iteration 38923: loss: 0.21689939498901367\n",
      "iteration 38924: loss: 0.21689923107624054\n",
      "iteration 38925: loss: 0.21689918637275696\n",
      "iteration 38926: loss: 0.2168990671634674\n",
      "iteration 38927: loss: 0.21689894795417786\n",
      "iteration 38928: loss: 0.2168988436460495\n",
      "iteration 38929: loss: 0.21689875423908234\n",
      "iteration 38930: loss: 0.21689863502979279\n",
      "iteration 38931: loss: 0.21689848601818085\n",
      "iteration 38932: loss: 0.21689841151237488\n",
      "iteration 38933: loss: 0.21689829230308533\n",
      "iteration 38934: loss: 0.21689820289611816\n",
      "iteration 38935: loss: 0.2168980836868286\n",
      "iteration 38936: loss: 0.21689803898334503\n",
      "iteration 38937: loss: 0.2168978899717331\n",
      "iteration 38938: loss: 0.21689777076244354\n",
      "iteration 38939: loss: 0.2168976366519928\n",
      "iteration 38940: loss: 0.21689748764038086\n",
      "iteration 38941: loss: 0.2168973982334137\n",
      "iteration 38942: loss: 0.21689729392528534\n",
      "iteration 38943: loss: 0.2168971598148346\n",
      "iteration 38944: loss: 0.21689705550670624\n",
      "iteration 38945: loss: 0.21689701080322266\n",
      "iteration 38946: loss: 0.21689680218696594\n",
      "iteration 38947: loss: 0.21689669787883759\n",
      "iteration 38948: loss: 0.216896653175354\n",
      "iteration 38949: loss: 0.21689650416374207\n",
      "iteration 38950: loss: 0.21689637005329132\n",
      "iteration 38951: loss: 0.21689632534980774\n",
      "iteration 38952: loss: 0.2168961465358734\n",
      "iteration 38953: loss: 0.21689602732658386\n",
      "iteration 38954: loss: 0.21689598262310028\n",
      "iteration 38955: loss: 0.21689589321613312\n",
      "iteration 38956: loss: 0.2168956995010376\n",
      "iteration 38957: loss: 0.21689565479755402\n",
      "iteration 38958: loss: 0.21689555048942566\n",
      "iteration 38959: loss: 0.2168954312801361\n",
      "iteration 38960: loss: 0.21689531207084656\n",
      "iteration 38961: loss: 0.2168951779603958\n",
      "iteration 38962: loss: 0.21689507365226746\n",
      "iteration 38963: loss: 0.21689501404762268\n",
      "iteration 38964: loss: 0.21689483523368835\n",
      "iteration 38965: loss: 0.2168947011232376\n",
      "iteration 38966: loss: 0.21689465641975403\n",
      "iteration 38967: loss: 0.2168945074081421\n",
      "iteration 38968: loss: 0.21689435839653015\n",
      "iteration 38969: loss: 0.216894268989563\n",
      "iteration 38970: loss: 0.21689414978027344\n",
      "iteration 38971: loss: 0.21689407527446747\n",
      "iteration 38972: loss: 0.21689394116401672\n",
      "iteration 38973: loss: 0.21689382195472717\n",
      "iteration 38974: loss: 0.21689371764659882\n",
      "iteration 38975: loss: 0.21689367294311523\n",
      "iteration 38976: loss: 0.2168935090303421\n",
      "iteration 38977: loss: 0.21689340472221375\n",
      "iteration 38978: loss: 0.2168932408094406\n",
      "iteration 38979: loss: 0.21689316630363464\n",
      "iteration 38980: loss: 0.2168930321931839\n",
      "iteration 38981: loss: 0.21689298748970032\n",
      "iteration 38982: loss: 0.216892808675766\n",
      "iteration 38983: loss: 0.21689274907112122\n",
      "iteration 38984: loss: 0.21689262986183167\n",
      "iteration 38985: loss: 0.2168925255537033\n",
      "iteration 38986: loss: 0.21689245104789734\n",
      "iteration 38987: loss: 0.2168922871351242\n",
      "iteration 38988: loss: 0.21689221262931824\n",
      "iteration 38989: loss: 0.2168920487165451\n",
      "iteration 38990: loss: 0.21689195930957794\n",
      "iteration 38991: loss: 0.2168917953968048\n",
      "iteration 38992: loss: 0.21689172089099884\n",
      "iteration 38993: loss: 0.21689161658287048\n",
      "iteration 38994: loss: 0.21689148247241974\n",
      "iteration 38995: loss: 0.21689137816429138\n",
      "iteration 38996: loss: 0.2168913185596466\n",
      "iteration 38997: loss: 0.21689121425151825\n",
      "iteration 38998: loss: 0.2168910950422287\n",
      "iteration 38999: loss: 0.21689090132713318\n",
      "iteration 39000: loss: 0.21689088642597198\n",
      "iteration 39001: loss: 0.21689076721668243\n",
      "iteration 39002: loss: 0.21689066290855408\n",
      "iteration 39003: loss: 0.21689057350158691\n",
      "iteration 39004: loss: 0.2168903797864914\n",
      "iteration 39005: loss: 0.21689030528068542\n",
      "iteration 39006: loss: 0.21689018607139587\n",
      "iteration 39007: loss: 0.21689005196094513\n",
      "iteration 39008: loss: 0.21688997745513916\n",
      "iteration 39009: loss: 0.21688981354236603\n",
      "iteration 39010: loss: 0.21688970923423767\n",
      "iteration 39011: loss: 0.2168896496295929\n",
      "iteration 39012: loss: 0.21688953042030334\n",
      "iteration 39013: loss: 0.2168893814086914\n",
      "iteration 39014: loss: 0.21688933670520782\n",
      "iteration 39015: loss: 0.2168891876935959\n",
      "iteration 39016: loss: 0.21688902378082275\n",
      "iteration 39017: loss: 0.21688897907733917\n",
      "iteration 39018: loss: 0.21688885986804962\n",
      "iteration 39019: loss: 0.21688875555992126\n",
      "iteration 39020: loss: 0.2168886363506317\n",
      "iteration 39021: loss: 0.21688850224018097\n",
      "iteration 39022: loss: 0.21688838303089142\n",
      "iteration 39023: loss: 0.21688823401927948\n",
      "iteration 39024: loss: 0.2168882191181183\n",
      "iteration 39025: loss: 0.21688802540302277\n",
      "iteration 39026: loss: 0.2168879508972168\n",
      "iteration 39027: loss: 0.21688787639141083\n",
      "iteration 39028: loss: 0.21688780188560486\n",
      "iteration 39029: loss: 0.21688762307167053\n",
      "iteration 39030: loss: 0.21688751876354218\n",
      "iteration 39031: loss: 0.216887429356575\n",
      "iteration 39032: loss: 0.21688726544380188\n",
      "iteration 39033: loss: 0.21688716113567352\n",
      "iteration 39034: loss: 0.21688714623451233\n",
      "iteration 39035: loss: 0.2168869525194168\n",
      "iteration 39036: loss: 0.21688684821128845\n",
      "iteration 39037: loss: 0.21688680350780487\n",
      "iteration 39038: loss: 0.21688660979270935\n",
      "iteration 39039: loss: 0.2168865203857422\n",
      "iteration 39040: loss: 0.21688643097877502\n",
      "iteration 39041: loss: 0.21688628196716309\n",
      "iteration 39042: loss: 0.21688620746135712\n",
      "iteration 39043: loss: 0.21688611805438995\n",
      "iteration 39044: loss: 0.21688596904277802\n",
      "iteration 39045: loss: 0.21688584983348846\n",
      "iteration 39046: loss: 0.2168857604265213\n",
      "iteration 39047: loss: 0.21688561141490936\n",
      "iteration 39048: loss: 0.2168855369091034\n",
      "iteration 39049: loss: 0.21688544750213623\n",
      "iteration 39050: loss: 0.21688537299633026\n",
      "iteration 39051: loss: 0.21688520908355713\n",
      "iteration 39052: loss: 0.21688508987426758\n",
      "iteration 39053: loss: 0.216885045170784\n",
      "iteration 39054: loss: 0.21688488125801086\n",
      "iteration 39055: loss: 0.2168847620487213\n",
      "iteration 39056: loss: 0.21688468754291534\n",
      "iteration 39057: loss: 0.216884583234787\n",
      "iteration 39058: loss: 0.21688446402549744\n",
      "iteration 39059: loss: 0.2168843001127243\n",
      "iteration 39060: loss: 0.21688418090343475\n",
      "iteration 39061: loss: 0.2168840616941452\n",
      "iteration 39062: loss: 0.21688401699066162\n",
      "iteration 39063: loss: 0.2168838530778885\n",
      "iteration 39064: loss: 0.21688374876976013\n",
      "iteration 39065: loss: 0.21688365936279297\n",
      "iteration 39066: loss: 0.21688351035118103\n",
      "iteration 39067: loss: 0.21688342094421387\n",
      "iteration 39068: loss: 0.21688325703144073\n",
      "iteration 39069: loss: 0.21688321232795715\n",
      "iteration 39070: loss: 0.2168831080198288\n",
      "iteration 39071: loss: 0.21688298881053925\n",
      "iteration 39072: loss: 0.21688289940357208\n",
      "iteration 39073: loss: 0.21688279509544373\n",
      "iteration 39074: loss: 0.21688266098499298\n",
      "iteration 39075: loss: 0.21688254177570343\n",
      "iteration 39076: loss: 0.21688242256641388\n",
      "iteration 39077: loss: 0.21688227355480194\n",
      "iteration 39078: loss: 0.21688218414783478\n",
      "iteration 39079: loss: 0.21688207983970642\n",
      "iteration 39080: loss: 0.21688194572925568\n",
      "iteration 39081: loss: 0.21688182651996613\n",
      "iteration 39082: loss: 0.21688175201416016\n",
      "iteration 39083: loss: 0.21688160300254822\n",
      "iteration 39084: loss: 0.21688158810138702\n",
      "iteration 39085: loss: 0.2168814241886139\n",
      "iteration 39086: loss: 0.21688130497932434\n",
      "iteration 39087: loss: 0.21688120067119598\n",
      "iteration 39088: loss: 0.21688108146190643\n",
      "iteration 39089: loss: 0.21688100695610046\n",
      "iteration 39090: loss: 0.21688084304332733\n",
      "iteration 39091: loss: 0.21688076853752136\n",
      "iteration 39092: loss: 0.21688063442707062\n",
      "iteration 39093: loss: 0.21688053011894226\n",
      "iteration 39094: loss: 0.2168804407119751\n",
      "iteration 39095: loss: 0.21688029170036316\n",
      "iteration 39096: loss: 0.2168801724910736\n",
      "iteration 39097: loss: 0.21688012778759003\n",
      "iteration 39098: loss: 0.21688000857830048\n",
      "iteration 39099: loss: 0.2168799191713333\n",
      "iteration 39100: loss: 0.21687975525856018\n",
      "iteration 39101: loss: 0.21687965095043182\n",
      "iteration 39102: loss: 0.21687956154346466\n",
      "iteration 39103: loss: 0.21687941253185272\n",
      "iteration 39104: loss: 0.21687932312488556\n",
      "iteration 39105: loss: 0.2168792188167572\n",
      "iteration 39106: loss: 0.21687908470630646\n",
      "iteration 39107: loss: 0.21687905490398407\n",
      "iteration 39108: loss: 0.21687893569469452\n",
      "iteration 39109: loss: 0.2168787717819214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 39110: loss: 0.21687868237495422\n",
      "iteration 39111: loss: 0.21687860786914825\n",
      "iteration 39112: loss: 0.21687836945056915\n",
      "iteration 39113: loss: 0.21687829494476318\n",
      "iteration 39114: loss: 0.2168782651424408\n",
      "iteration 39115: loss: 0.21687810122966766\n",
      "iteration 39116: loss: 0.2168779820203781\n",
      "iteration 39117: loss: 0.21687790751457214\n",
      "iteration 39118: loss: 0.21687784790992737\n",
      "iteration 39119: loss: 0.21687760949134827\n",
      "iteration 39120: loss: 0.2168775498867035\n",
      "iteration 39121: loss: 0.21687746047973633\n",
      "iteration 39122: loss: 0.2168773114681244\n",
      "iteration 39123: loss: 0.21687717735767365\n",
      "iteration 39124: loss: 0.21687713265419006\n",
      "iteration 39125: loss: 0.21687695384025574\n",
      "iteration 39126: loss: 0.2168768346309662\n",
      "iteration 39127: loss: 0.2168767899274826\n",
      "iteration 39128: loss: 0.21687670052051544\n",
      "iteration 39129: loss: 0.21687659621238708\n",
      "iteration 39130: loss: 0.21687643229961395\n",
      "iteration 39131: loss: 0.2168763130903244\n",
      "iteration 39132: loss: 0.21687622368335724\n",
      "iteration 39133: loss: 0.21687617897987366\n",
      "iteration 39134: loss: 0.2168760597705841\n",
      "iteration 39135: loss: 0.21687588095664978\n",
      "iteration 39136: loss: 0.21687579154968262\n",
      "iteration 39137: loss: 0.21687571704387665\n",
      "iteration 39138: loss: 0.2168755978345871\n",
      "iteration 39139: loss: 0.21687547862529755\n",
      "iteration 39140: loss: 0.2168753445148468\n",
      "iteration 39141: loss: 0.21687527000904083\n",
      "iteration 39142: loss: 0.21687515079975128\n",
      "iteration 39143: loss: 0.21687495708465576\n",
      "iteration 39144: loss: 0.21687492728233337\n",
      "iteration 39145: loss: 0.21687479317188263\n",
      "iteration 39146: loss: 0.21687467396259308\n",
      "iteration 39147: loss: 0.21687456965446472\n",
      "iteration 39148: loss: 0.21687445044517517\n",
      "iteration 39149: loss: 0.21687431633472443\n",
      "iteration 39150: loss: 0.21687427163124084\n",
      "iteration 39151: loss: 0.2168741226196289\n",
      "iteration 39152: loss: 0.21687403321266174\n",
      "iteration 39153: loss: 0.2168739289045334\n",
      "iteration 39154: loss: 0.21687379479408264\n",
      "iteration 39155: loss: 0.21687369048595428\n",
      "iteration 39156: loss: 0.21687360107898712\n",
      "iteration 39157: loss: 0.21687349677085876\n",
      "iteration 39158: loss: 0.21687333285808563\n",
      "iteration 39159: loss: 0.21687328815460205\n",
      "iteration 39160: loss: 0.2168731689453125\n",
      "iteration 39161: loss: 0.21687300503253937\n",
      "iteration 39162: loss: 0.21687285602092743\n",
      "iteration 39163: loss: 0.21687284111976624\n",
      "iteration 39164: loss: 0.2168726921081543\n",
      "iteration 39165: loss: 0.21687257289886475\n",
      "iteration 39166: loss: 0.21687249839305878\n",
      "iteration 39167: loss: 0.21687233448028564\n",
      "iteration 39168: loss: 0.21687225997447968\n",
      "iteration 39169: loss: 0.21687212586402893\n",
      "iteration 39170: loss: 0.21687205135822296\n",
      "iteration 39171: loss: 0.2168719321489334\n",
      "iteration 39172: loss: 0.21687185764312744\n",
      "iteration 39173: loss: 0.21687164902687073\n",
      "iteration 39174: loss: 0.21687157452106476\n",
      "iteration 39175: loss: 0.2168715000152588\n",
      "iteration 39176: loss: 0.21687135100364685\n",
      "iteration 39177: loss: 0.2168712317943573\n",
      "iteration 39178: loss: 0.21687114238739014\n",
      "iteration 39179: loss: 0.21687105298042297\n",
      "iteration 39180: loss: 0.21687087416648865\n",
      "iteration 39181: loss: 0.21687078475952148\n",
      "iteration 39182: loss: 0.21687063574790955\n",
      "iteration 39183: loss: 0.21687063574790955\n",
      "iteration 39184: loss: 0.21687054634094238\n",
      "iteration 39185: loss: 0.21687035262584686\n",
      "iteration 39186: loss: 0.2168702632188797\n",
      "iteration 39187: loss: 0.21687014400959015\n",
      "iteration 39188: loss: 0.2168699949979782\n",
      "iteration 39189: loss: 0.21686995029449463\n",
      "iteration 39190: loss: 0.21686987578868866\n",
      "iteration 39191: loss: 0.21686974167823792\n",
      "iteration 39192: loss: 0.2168695479631424\n",
      "iteration 39193: loss: 0.2168695032596588\n",
      "iteration 39194: loss: 0.21686939895153046\n",
      "iteration 39195: loss: 0.2168692648410797\n",
      "iteration 39196: loss: 0.21686919033527374\n",
      "iteration 39197: loss: 0.2168690264225006\n",
      "iteration 39198: loss: 0.21686892211437225\n",
      "iteration 39199: loss: 0.21686884760856628\n",
      "iteration 39200: loss: 0.21686871349811554\n",
      "iteration 39201: loss: 0.21686860918998718\n",
      "iteration 39202: loss: 0.21686851978302002\n",
      "iteration 39203: loss: 0.21686837077140808\n",
      "iteration 39204: loss: 0.2168683558702469\n",
      "iteration 39205: loss: 0.21686813235282898\n",
      "iteration 39206: loss: 0.21686804294586182\n",
      "iteration 39207: loss: 0.21686796844005585\n",
      "iteration 39208: loss: 0.2168678343296051\n",
      "iteration 39209: loss: 0.21686768531799316\n",
      "iteration 39210: loss: 0.2168675661087036\n",
      "iteration 39211: loss: 0.21686752140522003\n",
      "iteration 39212: loss: 0.21686740219593048\n",
      "iteration 39213: loss: 0.21686725318431854\n",
      "iteration 39214: loss: 0.21686720848083496\n",
      "iteration 39215: loss: 0.21686705946922302\n",
      "iteration 39216: loss: 0.21686692535877228\n",
      "iteration 39217: loss: 0.21686680614948273\n",
      "iteration 39218: loss: 0.21686670184135437\n",
      "iteration 39219: loss: 0.2168666422367096\n",
      "iteration 39220: loss: 0.21686646342277527\n",
      "iteration 39221: loss: 0.2168664038181305\n",
      "iteration 39222: loss: 0.21686625480651855\n",
      "iteration 39223: loss: 0.216866135597229\n",
      "iteration 39224: loss: 0.21686609089374542\n",
      "iteration 39225: loss: 0.21686598658561707\n",
      "iteration 39226: loss: 0.21686582267284393\n",
      "iteration 39227: loss: 0.21686573326587677\n",
      "iteration 39228: loss: 0.2168656289577484\n",
      "iteration 39229: loss: 0.21686553955078125\n",
      "iteration 39230: loss: 0.21686534583568573\n",
      "iteration 39231: loss: 0.21686534583568573\n",
      "iteration 39232: loss: 0.2168651521205902\n",
      "iteration 39233: loss: 0.21686506271362305\n",
      "iteration 39234: loss: 0.2168649137020111\n",
      "iteration 39235: loss: 0.21686482429504395\n",
      "iteration 39236: loss: 0.21686473488807678\n",
      "iteration 39237: loss: 0.21686463057994843\n",
      "iteration 39238: loss: 0.21686454117298126\n",
      "iteration 39239: loss: 0.21686437726020813\n",
      "iteration 39240: loss: 0.21686431765556335\n",
      "iteration 39241: loss: 0.2168641984462738\n",
      "iteration 39242: loss: 0.21686406433582306\n",
      "iteration 39243: loss: 0.2168639451265335\n",
      "iteration 39244: loss: 0.216863751411438\n",
      "iteration 39245: loss: 0.2168637216091156\n",
      "iteration 39246: loss: 0.21686367690563202\n",
      "iteration 39247: loss: 0.21686354279518127\n",
      "iteration 39248: loss: 0.21686343848705292\n",
      "iteration 39249: loss: 0.21686330437660217\n",
      "iteration 39250: loss: 0.21686315536499023\n",
      "iteration 39251: loss: 0.21686306595802307\n",
      "iteration 39252: loss: 0.2168629914522171\n",
      "iteration 39253: loss: 0.2168627679347992\n",
      "iteration 39254: loss: 0.2168627232313156\n",
      "iteration 39255: loss: 0.21686264872550964\n",
      "iteration 39256: loss: 0.21686258912086487\n",
      "iteration 39257: loss: 0.21686244010925293\n",
      "iteration 39258: loss: 0.2168622463941574\n",
      "iteration 39259: loss: 0.21686215698719025\n",
      "iteration 39260: loss: 0.21686199307441711\n",
      "iteration 39261: loss: 0.21686196327209473\n",
      "iteration 39262: loss: 0.21686184406280518\n",
      "iteration 39263: loss: 0.216861754655838\n",
      "iteration 39264: loss: 0.21686165034770966\n",
      "iteration 39265: loss: 0.2168615758419037\n",
      "iteration 39266: loss: 0.21686144173145294\n",
      "iteration 39267: loss: 0.216861292719841\n",
      "iteration 39268: loss: 0.21686120331287384\n",
      "iteration 39269: loss: 0.21686109900474548\n",
      "iteration 39270: loss: 0.21686096489429474\n",
      "iteration 39271: loss: 0.21686092019081116\n",
      "iteration 39272: loss: 0.2168608009815216\n",
      "iteration 39273: loss: 0.2168606072664261\n",
      "iteration 39274: loss: 0.21686053276062012\n",
      "iteration 39275: loss: 0.21686044335365295\n",
      "iteration 39276: loss: 0.21686029434204102\n",
      "iteration 39277: loss: 0.21686021983623505\n",
      "iteration 39278: loss: 0.21686005592346191\n",
      "iteration 39279: loss: 0.21685998141765594\n",
      "iteration 39280: loss: 0.21685990691184998\n",
      "iteration 39281: loss: 0.21685977280139923\n",
      "iteration 39282: loss: 0.21685965359210968\n",
      "iteration 39283: loss: 0.21685954928398132\n",
      "iteration 39284: loss: 0.21685941517353058\n",
      "iteration 39285: loss: 0.2168593406677246\n",
      "iteration 39286: loss: 0.21685925126075745\n",
      "iteration 39287: loss: 0.2168591320514679\n",
      "iteration 39288: loss: 0.21685902774333954\n",
      "iteration 39289: loss: 0.2168588638305664\n",
      "iteration 39290: loss: 0.21685878932476044\n",
      "iteration 39291: loss: 0.2168586254119873\n",
      "iteration 39292: loss: 0.21685853600502014\n",
      "iteration 39293: loss: 0.21685846149921417\n",
      "iteration 39294: loss: 0.21685829758644104\n",
      "iteration 39295: loss: 0.21685822308063507\n",
      "iteration 39296: loss: 0.21685807406902313\n",
      "iteration 39297: loss: 0.21685798466205597\n",
      "iteration 39298: loss: 0.2168578803539276\n",
      "iteration 39299: loss: 0.21685779094696045\n",
      "iteration 39300: loss: 0.2168576717376709\n",
      "iteration 39301: loss: 0.21685752272605896\n",
      "iteration 39302: loss: 0.21685738861560822\n",
      "iteration 39303: loss: 0.21685723960399628\n",
      "iteration 39304: loss: 0.21685723960399628\n",
      "iteration 39305: loss: 0.21685704588890076\n",
      "iteration 39306: loss: 0.2168569564819336\n",
      "iteration 39307: loss: 0.21685686707496643\n",
      "iteration 39308: loss: 0.21685676276683807\n",
      "iteration 39309: loss: 0.2168566733598709\n",
      "iteration 39310: loss: 0.21685655415058136\n",
      "iteration 39311: loss: 0.21685639023780823\n",
      "iteration 39312: loss: 0.21685636043548584\n",
      "iteration 39313: loss: 0.2168562412261963\n",
      "iteration 39314: loss: 0.21685615181922913\n",
      "iteration 39315: loss: 0.2168560028076172\n",
      "iteration 39316: loss: 0.21685592830181122\n",
      "iteration 39317: loss: 0.21685576438903809\n",
      "iteration 39318: loss: 0.21685560047626495\n",
      "iteration 39319: loss: 0.21685552597045898\n",
      "iteration 39320: loss: 0.21685545146465302\n",
      "iteration 39321: loss: 0.2168552577495575\n",
      "iteration 39322: loss: 0.21685531735420227\n",
      "iteration 39323: loss: 0.21685513854026794\n",
      "iteration 39324: loss: 0.2168550193309784\n",
      "iteration 39325: loss: 0.21685490012168884\n",
      "iteration 39326: loss: 0.2168547660112381\n",
      "iteration 39327: loss: 0.2168547362089157\n",
      "iteration 39328: loss: 0.21685457229614258\n",
      "iteration 39329: loss: 0.2168544977903366\n",
      "iteration 39330: loss: 0.21685433387756348\n",
      "iteration 39331: loss: 0.2168542593717575\n",
      "iteration 39332: loss: 0.21685409545898438\n",
      "iteration 39333: loss: 0.21685397624969482\n",
      "iteration 39334: loss: 0.21685388684272766\n",
      "iteration 39335: loss: 0.21685382723808289\n",
      "iteration 39336: loss: 0.21685369312763214\n",
      "iteration 39337: loss: 0.2168535739183426\n",
      "iteration 39338: loss: 0.21685345470905304\n",
      "iteration 39339: loss: 0.21685338020324707\n",
      "iteration 39340: loss: 0.2168532907962799\n",
      "iteration 39341: loss: 0.216853067278862\n",
      "iteration 39342: loss: 0.2168530523777008\n",
      "iteration 39343: loss: 0.21685285866260529\n",
      "iteration 39344: loss: 0.2168528139591217\n",
      "iteration 39345: loss: 0.21685266494750977\n",
      "iteration 39346: loss: 0.21685262024402618\n",
      "iteration 39347: loss: 0.21685247123241425\n",
      "iteration 39348: loss: 0.21685238182544708\n",
      "iteration 39349: loss: 0.21685223281383514\n",
      "iteration 39350: loss: 0.21685218811035156\n",
      "iteration 39351: loss: 0.21685203909873962\n",
      "iteration 39352: loss: 0.21685191988945007\n",
      "iteration 39353: loss: 0.21685180068016052\n",
      "iteration 39354: loss: 0.21685166656970978\n",
      "iteration 39355: loss: 0.21685156226158142\n",
      "iteration 39356: loss: 0.21685147285461426\n",
      "iteration 39357: loss: 0.2168513834476471\n",
      "iteration 39358: loss: 0.21685123443603516\n",
      "iteration 39359: loss: 0.21685118973255157\n",
      "iteration 39360: loss: 0.21685102581977844\n",
      "iteration 39361: loss: 0.2168509066104889\n",
      "iteration 39362: loss: 0.21685075759887695\n",
      "iteration 39363: loss: 0.21685068309307098\n",
      "iteration 39364: loss: 0.21685056388378143\n",
      "iteration 39365: loss: 0.21685048937797546\n",
      "iteration 39366: loss: 0.21685035526752472\n",
      "iteration 39367: loss: 0.21685025095939636\n",
      "iteration 39368: loss: 0.2168501317501068\n",
      "iteration 39369: loss: 0.21685004234313965\n",
      "iteration 39370: loss: 0.2168499231338501\n",
      "iteration 39371: loss: 0.21684983372688293\n",
      "iteration 39372: loss: 0.21684972941875458\n",
      "iteration 39373: loss: 0.2168496549129486\n",
      "iteration 39374: loss: 0.2168494462966919\n",
      "iteration 39375: loss: 0.2168494462966919\n",
      "iteration 39376: loss: 0.21684928238391876\n",
      "iteration 39377: loss: 0.2168491631746292\n",
      "iteration 39378: loss: 0.21684908866882324\n",
      "iteration 39379: loss: 0.21684888005256653\n",
      "iteration 39380: loss: 0.21684882044792175\n",
      "iteration 39381: loss: 0.21684876084327698\n",
      "iteration 39382: loss: 0.21684858202934265\n",
      "iteration 39383: loss: 0.2168484628200531\n",
      "iteration 39384: loss: 0.21684841811656952\n",
      "iteration 39385: loss: 0.21684828400611877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 39386: loss: 0.21684816479682922\n",
      "iteration 39387: loss: 0.21684809029102325\n",
      "iteration 39388: loss: 0.2168479710817337\n",
      "iteration 39389: loss: 0.21684780716896057\n",
      "iteration 39390: loss: 0.21684770286083221\n",
      "iteration 39391: loss: 0.21684768795967102\n",
      "iteration 39392: loss: 0.2168475091457367\n",
      "iteration 39393: loss: 0.21684741973876953\n",
      "iteration 39394: loss: 0.2168472707271576\n",
      "iteration 39395: loss: 0.21684710681438446\n",
      "iteration 39396: loss: 0.21684709191322327\n",
      "iteration 39397: loss: 0.21684694290161133\n",
      "iteration 39398: loss: 0.21684686839580536\n",
      "iteration 39399: loss: 0.21684673428535461\n",
      "iteration 39400: loss: 0.21684661507606506\n",
      "iteration 39401: loss: 0.21684646606445312\n",
      "iteration 39402: loss: 0.21684637665748596\n",
      "iteration 39403: loss: 0.2168462723493576\n",
      "iteration 39404: loss: 0.21684618294239044\n",
      "iteration 39405: loss: 0.2168460339307785\n",
      "iteration 39406: loss: 0.21684591472148895\n",
      "iteration 39407: loss: 0.21684584021568298\n",
      "iteration 39408: loss: 0.21684567630290985\n",
      "iteration 39409: loss: 0.2168455868959427\n",
      "iteration 39410: loss: 0.21684551239013672\n",
      "iteration 39411: loss: 0.21684536337852478\n",
      "iteration 39412: loss: 0.2168453186750412\n",
      "iteration 39413: loss: 0.21684527397155762\n",
      "iteration 39414: loss: 0.21684512495994568\n",
      "iteration 39415: loss: 0.21684496104717255\n",
      "iteration 39416: loss: 0.216844841837883\n",
      "iteration 39417: loss: 0.21684475243091583\n",
      "iteration 39418: loss: 0.2168446034193039\n",
      "iteration 39419: loss: 0.21684446930885315\n",
      "iteration 39420: loss: 0.21684439480304718\n",
      "iteration 39421: loss: 0.21684423089027405\n",
      "iteration 39422: loss: 0.21684417128562927\n",
      "iteration 39423: loss: 0.2168440818786621\n",
      "iteration 39424: loss: 0.21684400737285614\n",
      "iteration 39425: loss: 0.21684379875659943\n",
      "iteration 39426: loss: 0.21684376895427704\n",
      "iteration 39427: loss: 0.2168436348438263\n",
      "iteration 39428: loss: 0.21684351563453674\n",
      "iteration 39429: loss: 0.2168434113264084\n",
      "iteration 39430: loss: 0.21684329211711884\n",
      "iteration 39431: loss: 0.21684321761131287\n",
      "iteration 39432: loss: 0.21684303879737854\n",
      "iteration 39433: loss: 0.21684297919273376\n",
      "iteration 39434: loss: 0.2168428599834442\n",
      "iteration 39435: loss: 0.21684277057647705\n",
      "iteration 39436: loss: 0.2168426513671875\n",
      "iteration 39437: loss: 0.21684250235557556\n",
      "iteration 39438: loss: 0.21684245765209198\n",
      "iteration 39439: loss: 0.21684229373931885\n",
      "iteration 39440: loss: 0.21684220433235168\n",
      "iteration 39441: loss: 0.21684208512306213\n",
      "iteration 39442: loss: 0.21684196591377258\n",
      "iteration 39443: loss: 0.2168418914079666\n",
      "iteration 39444: loss: 0.21684177219867706\n",
      "iteration 39445: loss: 0.21684160828590393\n",
      "iteration 39446: loss: 0.21684157848358154\n",
      "iteration 39447: loss: 0.2168414294719696\n",
      "iteration 39448: loss: 0.21684134006500244\n",
      "iteration 39449: loss: 0.2168411910533905\n",
      "iteration 39450: loss: 0.21684105694293976\n",
      "iteration 39451: loss: 0.21684101223945618\n",
      "iteration 39452: loss: 0.21684086322784424\n",
      "iteration 39453: loss: 0.21684078872203827\n",
      "iteration 39454: loss: 0.21684065461158752\n",
      "iteration 39455: loss: 0.21684058010578156\n",
      "iteration 39456: loss: 0.21684041619300842\n",
      "iteration 39457: loss: 0.21684031188488007\n",
      "iteration 39458: loss: 0.21684017777442932\n",
      "iteration 39459: loss: 0.21684005856513977\n",
      "iteration 39460: loss: 0.2168399542570114\n",
      "iteration 39461: loss: 0.2168399542570114\n",
      "iteration 39462: loss: 0.2168397605419159\n",
      "iteration 39463: loss: 0.21683970093727112\n",
      "iteration 39464: loss: 0.21683955192565918\n",
      "iteration 39465: loss: 0.21683946251869202\n",
      "iteration 39466: loss: 0.21683931350708008\n",
      "iteration 39467: loss: 0.21683922410011292\n",
      "iteration 39468: loss: 0.21683916449546814\n",
      "iteration 39469: loss: 0.2168390303850174\n",
      "iteration 39470: loss: 0.21683888137340546\n",
      "iteration 39471: loss: 0.2168387472629547\n",
      "iteration 39472: loss: 0.21683868765830994\n",
      "iteration 39473: loss: 0.2168385535478592\n",
      "iteration 39474: loss: 0.21683840453624725\n",
      "iteration 39475: loss: 0.21683835983276367\n",
      "iteration 39476: loss: 0.21683821082115173\n",
      "iteration 39477: loss: 0.21683812141418457\n",
      "iteration 39478: loss: 0.21683795750141144\n",
      "iteration 39479: loss: 0.21683792769908905\n",
      "iteration 39480: loss: 0.2168378084897995\n",
      "iteration 39481: loss: 0.21683768928050995\n",
      "iteration 39482: loss: 0.21683759987354279\n",
      "iteration 39483: loss: 0.21683745086193085\n",
      "iteration 39484: loss: 0.21683740615844727\n",
      "iteration 39485: loss: 0.21683724224567413\n",
      "iteration 39486: loss: 0.21683713793754578\n",
      "iteration 39487: loss: 0.216837078332901\n",
      "iteration 39488: loss: 0.21683695912361145\n",
      "iteration 39489: loss: 0.2168368399143219\n",
      "iteration 39490: loss: 0.21683672070503235\n",
      "iteration 39491: loss: 0.216836616396904\n",
      "iteration 39492: loss: 0.21683649718761444\n",
      "iteration 39493: loss: 0.2168363630771637\n",
      "iteration 39494: loss: 0.21683624386787415\n",
      "iteration 39495: loss: 0.21683606505393982\n",
      "iteration 39496: loss: 0.21683606505393982\n",
      "iteration 39497: loss: 0.21683594584465027\n",
      "iteration 39498: loss: 0.21683581173419952\n",
      "iteration 39499: loss: 0.21683573722839355\n",
      "iteration 39500: loss: 0.216835618019104\n",
      "iteration 39501: loss: 0.21683545410633087\n",
      "iteration 39502: loss: 0.2168353796005249\n",
      "iteration 39503: loss: 0.21683526039123535\n",
      "iteration 39504: loss: 0.21683521568775177\n",
      "iteration 39505: loss: 0.21683502197265625\n",
      "iteration 39506: loss: 0.2168349325656891\n",
      "iteration 39507: loss: 0.21683481335639954\n",
      "iteration 39508: loss: 0.21683469414710999\n",
      "iteration 39509: loss: 0.21683451533317566\n",
      "iteration 39510: loss: 0.21683450043201447\n",
      "iteration 39511: loss: 0.21683433651924133\n",
      "iteration 39512: loss: 0.21683427691459656\n",
      "iteration 39513: loss: 0.21683423221111298\n",
      "iteration 39514: loss: 0.21683403849601746\n",
      "iteration 39515: loss: 0.21683387458324432\n",
      "iteration 39516: loss: 0.21683380007743835\n",
      "iteration 39517: loss: 0.2168336659669876\n",
      "iteration 39518: loss: 0.21683359146118164\n",
      "iteration 39519: loss: 0.2168334275484085\n",
      "iteration 39520: loss: 0.21683339774608612\n",
      "iteration 39521: loss: 0.21683327853679657\n",
      "iteration 39522: loss: 0.2168332040309906\n",
      "iteration 39523: loss: 0.21683311462402344\n",
      "iteration 39524: loss: 0.2168329656124115\n",
      "iteration 39525: loss: 0.21683287620544434\n",
      "iteration 39526: loss: 0.2168327122926712\n",
      "iteration 39527: loss: 0.21683260798454285\n",
      "iteration 39528: loss: 0.2168324887752533\n",
      "iteration 39529: loss: 0.21683236956596375\n",
      "iteration 39530: loss: 0.21683228015899658\n",
      "iteration 39531: loss: 0.21683213114738464\n",
      "iteration 39532: loss: 0.21683207154273987\n",
      "iteration 39533: loss: 0.21683192253112793\n",
      "iteration 39534: loss: 0.21683180332183838\n",
      "iteration 39535: loss: 0.21683171391487122\n",
      "iteration 39536: loss: 0.21683163940906525\n",
      "iteration 39537: loss: 0.2168315351009369\n",
      "iteration 39538: loss: 0.21683137118816376\n",
      "iteration 39539: loss: 0.21683120727539062\n",
      "iteration 39540: loss: 0.21683113276958466\n",
      "iteration 39541: loss: 0.2168310433626175\n",
      "iteration 39542: loss: 0.21683089435100555\n",
      "iteration 39543: loss: 0.21683081984519958\n",
      "iteration 39544: loss: 0.21683073043823242\n",
      "iteration 39545: loss: 0.21683065593242645\n",
      "iteration 39546: loss: 0.21683049201965332\n",
      "iteration 39547: loss: 0.21683037281036377\n",
      "iteration 39548: loss: 0.2168302983045578\n",
      "iteration 39549: loss: 0.21683016419410706\n",
      "iteration 39550: loss: 0.2168300598859787\n",
      "iteration 39551: loss: 0.21682992577552795\n",
      "iteration 39552: loss: 0.21682992577552795\n",
      "iteration 39553: loss: 0.21682970225811005\n",
      "iteration 39554: loss: 0.2168295681476593\n",
      "iteration 39555: loss: 0.21682950854301453\n",
      "iteration 39556: loss: 0.21682937443256378\n",
      "iteration 39557: loss: 0.2168292999267578\n",
      "iteration 39558: loss: 0.21682918071746826\n",
      "iteration 39559: loss: 0.2168290615081787\n",
      "iteration 39560: loss: 0.21682897210121155\n",
      "iteration 39561: loss: 0.2168288230895996\n",
      "iteration 39562: loss: 0.21682873368263245\n",
      "iteration 39563: loss: 0.21682855486869812\n",
      "iteration 39564: loss: 0.2168285846710205\n",
      "iteration 39565: loss: 0.216828390955925\n",
      "iteration 39566: loss: 0.2168283462524414\n",
      "iteration 39567: loss: 0.21682819724082947\n",
      "iteration 39568: loss: 0.2168281078338623\n",
      "iteration 39569: loss: 0.21682794392108917\n",
      "iteration 39570: loss: 0.2168278694152832\n",
      "iteration 39571: loss: 0.21682775020599365\n",
      "iteration 39572: loss: 0.2168276309967041\n",
      "iteration 39573: loss: 0.21682751178741455\n",
      "iteration 39574: loss: 0.21682746708393097\n",
      "iteration 39575: loss: 0.21682731807231903\n",
      "iteration 39576: loss: 0.21682719886302948\n",
      "iteration 39577: loss: 0.2168271243572235\n",
      "iteration 39578: loss: 0.21682699024677277\n",
      "iteration 39579: loss: 0.21682682633399963\n",
      "iteration 39580: loss: 0.21682676672935486\n",
      "iteration 39581: loss: 0.2168266475200653\n",
      "iteration 39582: loss: 0.21682655811309814\n",
      "iteration 39583: loss: 0.2168264389038086\n",
      "iteration 39584: loss: 0.21682628989219666\n",
      "iteration 39585: loss: 0.21682627499103546\n",
      "iteration 39586: loss: 0.2168261557817459\n",
      "iteration 39587: loss: 0.21682599186897278\n",
      "iteration 39588: loss: 0.21682588756084442\n",
      "iteration 39589: loss: 0.21682576835155487\n",
      "iteration 39590: loss: 0.2168256789445877\n",
      "iteration 39591: loss: 0.21682555973529816\n",
      "iteration 39592: loss: 0.21682539582252502\n",
      "iteration 39593: loss: 0.21682532131671906\n",
      "iteration 39594: loss: 0.21682517230510712\n",
      "iteration 39595: loss: 0.21682509779930115\n",
      "iteration 39596: loss: 0.21682505309581757\n",
      "iteration 39597: loss: 0.21682488918304443\n",
      "iteration 39598: loss: 0.21682476997375488\n",
      "iteration 39599: loss: 0.21682465076446533\n",
      "iteration 39600: loss: 0.21682457625865936\n",
      "iteration 39601: loss: 0.21682441234588623\n",
      "iteration 39602: loss: 0.21682432293891907\n",
      "iteration 39603: loss: 0.21682420372962952\n",
      "iteration 39604: loss: 0.21682409942150116\n",
      "iteration 39605: loss: 0.2168239802122116\n",
      "iteration 39606: loss: 0.21682389080524445\n",
      "iteration 39607: loss: 0.2168237715959549\n",
      "iteration 39608: loss: 0.2168237268924713\n",
      "iteration 39609: loss: 0.21682360768318176\n",
      "iteration 39610: loss: 0.2168234884738922\n",
      "iteration 39611: loss: 0.21682330965995789\n",
      "iteration 39612: loss: 0.21682322025299072\n",
      "iteration 39613: loss: 0.21682314574718475\n",
      "iteration 39614: loss: 0.216823011636734\n",
      "iteration 39615: loss: 0.21682290732860565\n",
      "iteration 39616: loss: 0.2168227881193161\n",
      "iteration 39617: loss: 0.21682262420654297\n",
      "iteration 39618: loss: 0.2168225795030594\n",
      "iteration 39619: loss: 0.21682247519493103\n",
      "iteration 39620: loss: 0.21682235598564148\n",
      "iteration 39621: loss: 0.21682226657867432\n",
      "iteration 39622: loss: 0.21682210266590118\n",
      "iteration 39623: loss: 0.2168220579624176\n",
      "iteration 39624: loss: 0.21682190895080566\n",
      "iteration 39625: loss: 0.21682175993919373\n",
      "iteration 39626: loss: 0.21682167053222656\n",
      "iteration 39627: loss: 0.2168215811252594\n",
      "iteration 39628: loss: 0.21682146191596985\n",
      "iteration 39629: loss: 0.2168213129043579\n",
      "iteration 39630: loss: 0.21682123839855194\n",
      "iteration 39631: loss: 0.21682114899158478\n",
      "iteration 39632: loss: 0.21682102978229523\n",
      "iteration 39633: loss: 0.21682098507881165\n",
      "iteration 39634: loss: 0.21682076156139374\n",
      "iteration 39635: loss: 0.21682074666023254\n",
      "iteration 39636: loss: 0.21682055294513702\n",
      "iteration 39637: loss: 0.21682052314281464\n",
      "iteration 39638: loss: 0.2168203890323639\n",
      "iteration 39639: loss: 0.21682028472423553\n",
      "iteration 39640: loss: 0.21682016551494598\n",
      "iteration 39641: loss: 0.21682007610797882\n",
      "iteration 39642: loss: 0.2168198823928833\n",
      "iteration 39643: loss: 0.21681980788707733\n",
      "iteration 39644: loss: 0.21681968867778778\n",
      "iteration 39645: loss: 0.21681959927082062\n",
      "iteration 39646: loss: 0.21681945025920868\n",
      "iteration 39647: loss: 0.2168194055557251\n",
      "iteration 39648: loss: 0.21681924164295197\n",
      "iteration 39649: loss: 0.21681907773017883\n",
      "iteration 39650: loss: 0.21681904792785645\n",
      "iteration 39651: loss: 0.21681895852088928\n",
      "iteration 39652: loss: 0.21681876480579376\n",
      "iteration 39653: loss: 0.2168186604976654\n",
      "iteration 39654: loss: 0.21681860089302063\n",
      "iteration 39655: loss: 0.21681848168373108\n",
      "iteration 39656: loss: 0.21681833267211914\n",
      "iteration 39657: loss: 0.21681833267211914\n",
      "iteration 39658: loss: 0.2168181836605072\n",
      "iteration 39659: loss: 0.21681800484657288\n",
      "iteration 39660: loss: 0.2168179750442505\n",
      "iteration 39661: loss: 0.21681782603263855\n",
      "iteration 39662: loss: 0.216817706823349\n",
      "iteration 39663: loss: 0.21681758761405945\n",
      "iteration 39664: loss: 0.2168174684047699\n",
      "iteration 39665: loss: 0.21681737899780273\n",
      "iteration 39666: loss: 0.21681730449199677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 39667: loss: 0.21681718528270721\n",
      "iteration 39668: loss: 0.21681705117225647\n",
      "iteration 39669: loss: 0.21681693196296692\n",
      "iteration 39670: loss: 0.21681682765483856\n",
      "iteration 39671: loss: 0.2168167531490326\n",
      "iteration 39672: loss: 0.21681666374206543\n",
      "iteration 39673: loss: 0.2168164998292923\n",
      "iteration 39674: loss: 0.21681638062000275\n",
      "iteration 39675: loss: 0.2168162614107132\n",
      "iteration 39676: loss: 0.21681618690490723\n",
      "iteration 39677: loss: 0.2168160378932953\n",
      "iteration 39678: loss: 0.21681591868400574\n",
      "iteration 39679: loss: 0.2168157994747162\n",
      "iteration 39680: loss: 0.2168157547712326\n",
      "iteration 39681: loss: 0.21681556105613708\n",
      "iteration 39682: loss: 0.2168155014514923\n",
      "iteration 39683: loss: 0.21681539714336395\n",
      "iteration 39684: loss: 0.2168152779340744\n",
      "iteration 39685: loss: 0.21681520342826843\n",
      "iteration 39686: loss: 0.21681508421897888\n",
      "iteration 39687: loss: 0.21681495010852814\n",
      "iteration 39688: loss: 0.21681484580039978\n",
      "iteration 39689: loss: 0.21681472659111023\n",
      "iteration 39690: loss: 0.21681460738182068\n",
      "iteration 39691: loss: 0.21681451797485352\n",
      "iteration 39692: loss: 0.21681436896324158\n",
      "iteration 39693: loss: 0.216814324259758\n",
      "iteration 39694: loss: 0.21681420505046844\n",
      "iteration 39695: loss: 0.2168140709400177\n",
      "iteration 39696: loss: 0.21681396663188934\n",
      "iteration 39697: loss: 0.2168138474225998\n",
      "iteration 39698: loss: 0.21681377291679382\n",
      "iteration 39699: loss: 0.21681365370750427\n",
      "iteration 39700: loss: 0.21681351959705353\n",
      "iteration 39701: loss: 0.21681341528892517\n",
      "iteration 39702: loss: 0.216813325881958\n",
      "iteration 39703: loss: 0.21681325137615204\n",
      "iteration 39704: loss: 0.2168130874633789\n",
      "iteration 39705: loss: 0.21681293845176697\n",
      "iteration 39706: loss: 0.2168128490447998\n",
      "iteration 39707: loss: 0.21681275963783264\n",
      "iteration 39708: loss: 0.2168126404285431\n",
      "iteration 39709: loss: 0.21681253612041473\n",
      "iteration 39710: loss: 0.216812402009964\n",
      "iteration 39711: loss: 0.21681229770183563\n",
      "iteration 39712: loss: 0.21681217849254608\n",
      "iteration 39713: loss: 0.21681205928325653\n",
      "iteration 39714: loss: 0.21681198477745056\n",
      "iteration 39715: loss: 0.21681185066699982\n",
      "iteration 39716: loss: 0.21681180596351624\n",
      "iteration 39717: loss: 0.21681168675422668\n",
      "iteration 39718: loss: 0.21681149303913116\n",
      "iteration 39719: loss: 0.2168114185333252\n",
      "iteration 39720: loss: 0.21681134402751923\n",
      "iteration 39721: loss: 0.2168111503124237\n",
      "iteration 39722: loss: 0.21681110560894012\n",
      "iteration 39723: loss: 0.21681103110313416\n",
      "iteration 39724: loss: 0.2168108969926834\n",
      "iteration 39725: loss: 0.21681079268455505\n",
      "iteration 39726: loss: 0.2168107032775879\n",
      "iteration 39727: loss: 0.21681055426597595\n",
      "iteration 39728: loss: 0.2168104350566864\n",
      "iteration 39729: loss: 0.21681030094623566\n",
      "iteration 39730: loss: 0.2168102264404297\n",
      "iteration 39731: loss: 0.21681010723114014\n",
      "iteration 39732: loss: 0.21680998802185059\n",
      "iteration 39733: loss: 0.21680989861488342\n",
      "iteration 39734: loss: 0.21680982410907745\n",
      "iteration 39735: loss: 0.21680966019630432\n",
      "iteration 39736: loss: 0.21680955588817596\n",
      "iteration 39737: loss: 0.21680948138237\n",
      "iteration 39738: loss: 0.21680931746959686\n",
      "iteration 39739: loss: 0.21680918335914612\n",
      "iteration 39740: loss: 0.21680912375450134\n",
      "iteration 39741: loss: 0.2168089896440506\n",
      "iteration 39742: loss: 0.21680888533592224\n",
      "iteration 39743: loss: 0.2168087661266327\n",
      "iteration 39744: loss: 0.2168087214231491\n",
      "iteration 39745: loss: 0.21680860221385956\n",
      "iteration 39746: loss: 0.21680840849876404\n",
      "iteration 39747: loss: 0.21680839359760284\n",
      "iteration 39748: loss: 0.2168082296848297\n",
      "iteration 39749: loss: 0.21680812537670135\n",
      "iteration 39750: loss: 0.21680805087089539\n",
      "iteration 39751: loss: 0.21680791676044464\n",
      "iteration 39752: loss: 0.2168077677488327\n",
      "iteration 39753: loss: 0.21680764853954315\n",
      "iteration 39754: loss: 0.21680757403373718\n",
      "iteration 39755: loss: 0.21680745482444763\n",
      "iteration 39756: loss: 0.21680736541748047\n",
      "iteration 39757: loss: 0.21680724620819092\n",
      "iteration 39758: loss: 0.21680712699890137\n",
      "iteration 39759: loss: 0.2168070375919342\n",
      "iteration 39760: loss: 0.21680693328380585\n",
      "iteration 39761: loss: 0.2168067991733551\n",
      "iteration 39762: loss: 0.21680665016174316\n",
      "iteration 39763: loss: 0.21680660545825958\n",
      "iteration 39764: loss: 0.21680645644664764\n",
      "iteration 39765: loss: 0.21680638194084167\n",
      "iteration 39766: loss: 0.21680624783039093\n",
      "iteration 39767: loss: 0.216806098818779\n",
      "iteration 39768: loss: 0.21680602431297302\n",
      "iteration 39769: loss: 0.2168058604001999\n",
      "iteration 39770: loss: 0.2168058156967163\n",
      "iteration 39771: loss: 0.21680569648742676\n",
      "iteration 39772: loss: 0.2168056070804596\n",
      "iteration 39773: loss: 0.21680541336536407\n",
      "iteration 39774: loss: 0.2168053686618805\n",
      "iteration 39775: loss: 0.21680529415607452\n",
      "iteration 39776: loss: 0.21680517494678497\n",
      "iteration 39777: loss: 0.21680507063865662\n",
      "iteration 39778: loss: 0.21680493652820587\n",
      "iteration 39779: loss: 0.21680478751659393\n",
      "iteration 39780: loss: 0.21680474281311035\n",
      "iteration 39781: loss: 0.2168045938014984\n",
      "iteration 39782: loss: 0.21680447459220886\n",
      "iteration 39783: loss: 0.2168043851852417\n",
      "iteration 39784: loss: 0.21680423617362976\n",
      "iteration 39785: loss: 0.21680417656898499\n",
      "iteration 39786: loss: 0.21680407226085663\n",
      "iteration 39787: loss: 0.21680395305156708\n",
      "iteration 39788: loss: 0.2168038785457611\n",
      "iteration 39789: loss: 0.21680369973182678\n",
      "iteration 39790: loss: 0.21680358052253723\n",
      "iteration 39791: loss: 0.21680352091789246\n",
      "iteration 39792: loss: 0.2168033868074417\n",
      "iteration 39793: loss: 0.21680328249931335\n",
      "iteration 39794: loss: 0.2168031483888626\n",
      "iteration 39795: loss: 0.21680302917957306\n",
      "iteration 39796: loss: 0.21680298447608948\n",
      "iteration 39797: loss: 0.21680280566215515\n",
      "iteration 39798: loss: 0.2168026715517044\n",
      "iteration 39799: loss: 0.21680259704589844\n",
      "iteration 39800: loss: 0.21680250763893127\n",
      "iteration 39801: loss: 0.2168024331331253\n",
      "iteration 39802: loss: 0.21680231392383575\n",
      "iteration 39803: loss: 0.2168021947145462\n",
      "iteration 39804: loss: 0.21680209040641785\n",
      "iteration 39805: loss: 0.2168019711971283\n",
      "iteration 39806: loss: 0.21680183708667755\n",
      "iteration 39807: loss: 0.2168017327785492\n",
      "iteration 39808: loss: 0.21680167317390442\n",
      "iteration 39809: loss: 0.21680152416229248\n",
      "iteration 39810: loss: 0.21680140495300293\n",
      "iteration 39811: loss: 0.21680131554603577\n",
      "iteration 39812: loss: 0.2168012112379074\n",
      "iteration 39813: loss: 0.21680109202861786\n",
      "iteration 39814: loss: 0.2168010026216507\n",
      "iteration 39815: loss: 0.21680089831352234\n",
      "iteration 39816: loss: 0.2168007642030716\n",
      "iteration 39817: loss: 0.21680065989494324\n",
      "iteration 39818: loss: 0.2168005406856537\n",
      "iteration 39819: loss: 0.21680040657520294\n",
      "iteration 39820: loss: 0.21680036187171936\n",
      "iteration 39821: loss: 0.21680021286010742\n",
      "iteration 39822: loss: 0.21680006384849548\n",
      "iteration 39823: loss: 0.21679994463920593\n",
      "iteration 39824: loss: 0.21679988503456116\n",
      "iteration 39825: loss: 0.2167997658252716\n",
      "iteration 39826: loss: 0.21679966151714325\n",
      "iteration 39827: loss: 0.21679942309856415\n",
      "iteration 39828: loss: 0.21679940819740295\n",
      "iteration 39829: loss: 0.2167993038892746\n",
      "iteration 39830: loss: 0.21679921448230743\n",
      "iteration 39831: loss: 0.21679911017417908\n",
      "iteration 39832: loss: 0.21679899096488953\n",
      "iteration 39833: loss: 0.21679887175559998\n",
      "iteration 39834: loss: 0.21679869294166565\n",
      "iteration 39835: loss: 0.21679861843585968\n",
      "iteration 39836: loss: 0.21679851412773132\n",
      "iteration 39837: loss: 0.21679845452308655\n",
      "iteration 39838: loss: 0.2167983502149582\n",
      "iteration 39839: loss: 0.21679821610450745\n",
      "iteration 39840: loss: 0.2167981117963791\n",
      "iteration 39841: loss: 0.21679794788360596\n",
      "iteration 39842: loss: 0.2167978286743164\n",
      "iteration 39843: loss: 0.21679770946502686\n",
      "iteration 39844: loss: 0.2167976200580597\n",
      "iteration 39845: loss: 0.21679751574993134\n",
      "iteration 39846: loss: 0.21679744124412537\n",
      "iteration 39847: loss: 0.21679732203483582\n",
      "iteration 39848: loss: 0.21679718792438507\n",
      "iteration 39849: loss: 0.21679703891277313\n",
      "iteration 39850: loss: 0.21679703891277313\n",
      "iteration 39851: loss: 0.21679694950580597\n",
      "iteration 39852: loss: 0.21679678559303284\n",
      "iteration 39853: loss: 0.21679668128490448\n",
      "iteration 39854: loss: 0.21679656207561493\n",
      "iteration 39855: loss: 0.21679644286632538\n",
      "iteration 39856: loss: 0.2167963683605194\n",
      "iteration 39857: loss: 0.21679624915122986\n",
      "iteration 39858: loss: 0.2167961299419403\n",
      "iteration 39859: loss: 0.21679599583148956\n",
      "iteration 39860: loss: 0.2167958766222\n",
      "iteration 39861: loss: 0.21679577231407166\n",
      "iteration 39862: loss: 0.2167956531047821\n",
      "iteration 39863: loss: 0.21679556369781494\n",
      "iteration 39864: loss: 0.21679547429084778\n",
      "iteration 39865: loss: 0.21679535508155823\n",
      "iteration 39866: loss: 0.21679528057575226\n",
      "iteration 39867: loss: 0.2167951762676239\n",
      "iteration 39868: loss: 0.21679499745368958\n",
      "iteration 39869: loss: 0.21679489314556122\n",
      "iteration 39870: loss: 0.21679480373859406\n",
      "iteration 39871: loss: 0.21679472923278809\n",
      "iteration 39872: loss: 0.21679458022117615\n",
      "iteration 39873: loss: 0.21679441630840302\n",
      "iteration 39874: loss: 0.21679432690143585\n",
      "iteration 39875: loss: 0.2167942225933075\n",
      "iteration 39876: loss: 0.21679408848285675\n",
      "iteration 39877: loss: 0.21679404377937317\n",
      "iteration 39878: loss: 0.21679392457008362\n",
      "iteration 39879: loss: 0.2167937457561493\n",
      "iteration 39880: loss: 0.21679368615150452\n",
      "iteration 39881: loss: 0.21679362654685974\n",
      "iteration 39882: loss: 0.2167934626340866\n",
      "iteration 39883: loss: 0.21679341793060303\n",
      "iteration 39884: loss: 0.2167932540178299\n",
      "iteration 39885: loss: 0.21679310500621796\n",
      "iteration 39886: loss: 0.2167930155992508\n",
      "iteration 39887: loss: 0.2167929708957672\n",
      "iteration 39888: loss: 0.2167927473783493\n",
      "iteration 39889: loss: 0.2167927324771881\n",
      "iteration 39890: loss: 0.2167925387620926\n",
      "iteration 39891: loss: 0.21679246425628662\n",
      "iteration 39892: loss: 0.21679238975048065\n",
      "iteration 39893: loss: 0.21679231524467468\n",
      "iteration 39894: loss: 0.21679218113422394\n",
      "iteration 39895: loss: 0.2167920172214508\n",
      "iteration 39896: loss: 0.21679195761680603\n",
      "iteration 39897: loss: 0.21679182350635529\n",
      "iteration 39898: loss: 0.21679171919822693\n",
      "iteration 39899: loss: 0.21679165959358215\n",
      "iteration 39900: loss: 0.21679151058197021\n",
      "iteration 39901: loss: 0.21679136157035828\n",
      "iteration 39902: loss: 0.21679124236106873\n",
      "iteration 39903: loss: 0.21679115295410156\n",
      "iteration 39904: loss: 0.2167910635471344\n",
      "iteration 39905: loss: 0.21679098904132843\n",
      "iteration 39906: loss: 0.21679076552391052\n",
      "iteration 39907: loss: 0.21679070591926575\n",
      "iteration 39908: loss: 0.21679063141345978\n",
      "iteration 39909: loss: 0.21679046750068665\n",
      "iteration 39910: loss: 0.21679039299488068\n",
      "iteration 39911: loss: 0.21679028868675232\n",
      "iteration 39912: loss: 0.2167901247739792\n",
      "iteration 39913: loss: 0.21679000556468964\n",
      "iteration 39914: loss: 0.21678996086120605\n",
      "iteration 39915: loss: 0.2167898416519165\n",
      "iteration 39916: loss: 0.21678967773914337\n",
      "iteration 39917: loss: 0.21678964793682098\n",
      "iteration 39918: loss: 0.21678955852985382\n",
      "iteration 39919: loss: 0.2167893350124359\n",
      "iteration 39920: loss: 0.21678927540779114\n",
      "iteration 39921: loss: 0.21678920090198517\n",
      "iteration 39922: loss: 0.21678905189037323\n",
      "iteration 39923: loss: 0.21678893268108368\n",
      "iteration 39924: loss: 0.21678879857063293\n",
      "iteration 39925: loss: 0.21678869426250458\n",
      "iteration 39926: loss: 0.21678860485553741\n",
      "iteration 39927: loss: 0.21678853034973145\n",
      "iteration 39928: loss: 0.21678844094276428\n",
      "iteration 39929: loss: 0.21678832173347473\n",
      "iteration 39930: loss: 0.2167881727218628\n",
      "iteration 39931: loss: 0.21678805351257324\n",
      "iteration 39932: loss: 0.21678797900676727\n",
      "iteration 39933: loss: 0.21678784489631653\n",
      "iteration 39934: loss: 0.21678772568702698\n",
      "iteration 39935: loss: 0.21678762137889862\n",
      "iteration 39936: loss: 0.21678753197193146\n",
      "iteration 39937: loss: 0.21678738296031952\n",
      "iteration 39938: loss: 0.21678736805915833\n",
      "iteration 39939: loss: 0.2167872190475464\n",
      "iteration 39940: loss: 0.21678707003593445\n",
      "iteration 39941: loss: 0.21678701043128967\n",
      "iteration 39942: loss: 0.21678690612316132\n",
      "iteration 39943: loss: 0.21678677201271057\n",
      "iteration 39944: loss: 0.21678659319877625\n",
      "iteration 39945: loss: 0.21678653359413147\n",
      "iteration 39946: loss: 0.2167864292860031\n",
      "iteration 39947: loss: 0.21678631007671356\n",
      "iteration 39948: loss: 0.2167862206697464\n",
      "iteration 39949: loss: 0.21678602695465088\n",
      "iteration 39950: loss: 0.21678590774536133\n",
      "iteration 39951: loss: 0.21678581833839417\n",
      "iteration 39952: loss: 0.2167857438325882\n",
      "iteration 39953: loss: 0.21678566932678223\n",
      "iteration 39954: loss: 0.2167855203151703\n",
      "iteration 39955: loss: 0.21678540110588074\n",
      "iteration 39956: loss: 0.21678526699543\n",
      "iteration 39957: loss: 0.2167852222919464\n",
      "iteration 39958: loss: 0.21678511798381805\n",
      "iteration 39959: loss: 0.2167849987745285\n",
      "iteration 39960: loss: 0.21678487956523895\n",
      "iteration 39961: loss: 0.2167847603559494\n",
      "iteration 39962: loss: 0.21678471565246582\n",
      "iteration 39963: loss: 0.2167845517396927\n",
      "iteration 39964: loss: 0.21678440272808075\n",
      "iteration 39965: loss: 0.21678438782691956\n",
      "iteration 39966: loss: 0.21678423881530762\n",
      "iteration 39967: loss: 0.21678414940834045\n",
      "iteration 39968: loss: 0.2167840451002121\n",
      "iteration 39969: loss: 0.21678391098976135\n",
      "iteration 39970: loss: 0.2167837917804718\n",
      "iteration 39971: loss: 0.21678364276885986\n",
      "iteration 39972: loss: 0.21678364276885986\n",
      "iteration 39973: loss: 0.21678343415260315\n",
      "iteration 39974: loss: 0.21678337454795837\n",
      "iteration 39975: loss: 0.21678325533866882\n",
      "iteration 39976: loss: 0.21678312122821808\n",
      "iteration 39977: loss: 0.21678301692008972\n",
      "iteration 39978: loss: 0.21678288280963898\n",
      "iteration 39979: loss: 0.21678277850151062\n",
      "iteration 39980: loss: 0.21678265929222107\n",
      "iteration 39981: loss: 0.21678254008293152\n",
      "iteration 39982: loss: 0.21678248047828674\n",
      "iteration 39983: loss: 0.2167823612689972\n",
      "iteration 39984: loss: 0.21678218245506287\n",
      "iteration 39985: loss: 0.2167820930480957\n",
      "iteration 39986: loss: 0.21678189933300018\n",
      "iteration 39987: loss: 0.21678189933300018\n",
      "iteration 39988: loss: 0.2167818248271942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 39989: loss: 0.21678173542022705\n",
      "iteration 39990: loss: 0.21678154170513153\n",
      "iteration 39991: loss: 0.21678146719932556\n",
      "iteration 39992: loss: 0.21678142249584198\n",
      "iteration 39993: loss: 0.21678122878074646\n",
      "iteration 39994: loss: 0.21678116917610168\n",
      "iteration 39995: loss: 0.21678106486797333\n",
      "iteration 39996: loss: 0.2167808562517166\n",
      "iteration 39997: loss: 0.21678081154823303\n",
      "iteration 39998: loss: 0.2167806625366211\n",
      "iteration 39999: loss: 0.21678051352500916\n",
      "iteration 40000: loss: 0.21678049862384796\n",
      "iteration 40001: loss: 0.2167803943157196\n",
      "iteration 40002: loss: 0.21678023040294647\n",
      "iteration 40003: loss: 0.2167801409959793\n",
      "iteration 40004: loss: 0.21678003668785095\n",
      "iteration 40005: loss: 0.21677987277507782\n",
      "iteration 40006: loss: 0.21677978336811066\n",
      "iteration 40007: loss: 0.2167796790599823\n",
      "iteration 40008: loss: 0.21677955985069275\n",
      "iteration 40009: loss: 0.21677951514720917\n",
      "iteration 40010: loss: 0.21677938103675842\n",
      "iteration 40011: loss: 0.21677927672863007\n",
      "iteration 40012: loss: 0.2167792022228241\n",
      "iteration 40013: loss: 0.21677903831005096\n",
      "iteration 40014: loss: 0.2167789191007614\n",
      "iteration 40015: loss: 0.21677887439727783\n",
      "iteration 40016: loss: 0.21677863597869873\n",
      "iteration 40017: loss: 0.21677860617637634\n",
      "iteration 40018: loss: 0.2167784720659256\n",
      "iteration 40019: loss: 0.21677836775779724\n",
      "iteration 40020: loss: 0.21677827835083008\n",
      "iteration 40021: loss: 0.2167782336473465\n",
      "iteration 40022: loss: 0.21677808463573456\n",
      "iteration 40023: loss: 0.21677787601947784\n",
      "iteration 40024: loss: 0.21677787601947784\n",
      "iteration 40025: loss: 0.2167777121067047\n",
      "iteration 40026: loss: 0.21677760779857635\n",
      "iteration 40027: loss: 0.2167774885892868\n",
      "iteration 40028: loss: 0.21677736937999725\n",
      "iteration 40029: loss: 0.21677729487419128\n",
      "iteration 40030: loss: 0.21677713096141815\n",
      "iteration 40031: loss: 0.21677705645561218\n",
      "iteration 40032: loss: 0.21677696704864502\n",
      "iteration 40033: loss: 0.21677681803703308\n",
      "iteration 40034: loss: 0.21677669882774353\n",
      "iteration 40035: loss: 0.21677660942077637\n",
      "iteration 40036: loss: 0.21677649021148682\n",
      "iteration 40037: loss: 0.21677641570568085\n",
      "iteration 40038: loss: 0.2167762815952301\n",
      "iteration 40039: loss: 0.21677613258361816\n",
      "iteration 40040: loss: 0.2167760133743286\n",
      "iteration 40041: loss: 0.21677598357200623\n",
      "iteration 40042: loss: 0.21677586436271667\n",
      "iteration 40043: loss: 0.21677574515342712\n",
      "iteration 40044: loss: 0.21677561104297638\n",
      "iteration 40045: loss: 0.21677549183368683\n",
      "iteration 40046: loss: 0.21677537262439728\n",
      "iteration 40047: loss: 0.2167752981185913\n",
      "iteration 40048: loss: 0.21677513420581818\n",
      "iteration 40049: loss: 0.2167750895023346\n",
      "iteration 40050: loss: 0.21677489578723907\n",
      "iteration 40051: loss: 0.2167748659849167\n",
      "iteration 40052: loss: 0.21677477657794952\n",
      "iteration 40053: loss: 0.2167746126651764\n",
      "iteration 40054: loss: 0.21677449345588684\n",
      "iteration 40055: loss: 0.21677441895008087\n",
      "iteration 40056: loss: 0.21677422523498535\n",
      "iteration 40057: loss: 0.21677422523498535\n",
      "iteration 40058: loss: 0.2167740762233734\n",
      "iteration 40059: loss: 0.21677395701408386\n",
      "iteration 40060: loss: 0.2167738974094391\n",
      "iteration 40061: loss: 0.21677377820014954\n",
      "iteration 40062: loss: 0.21677365899085999\n",
      "iteration 40063: loss: 0.21677350997924805\n",
      "iteration 40064: loss: 0.21677343547344208\n",
      "iteration 40065: loss: 0.21677334606647491\n",
      "iteration 40066: loss: 0.21677319705486298\n",
      "iteration 40067: loss: 0.2167731076478958\n",
      "iteration 40068: loss: 0.21677298843860626\n",
      "iteration 40069: loss: 0.2167728841304779\n",
      "iteration 40070: loss: 0.21677275002002716\n",
      "iteration 40071: loss: 0.21677260100841522\n",
      "iteration 40072: loss: 0.21677251160144806\n",
      "iteration 40073: loss: 0.2167724072933197\n",
      "iteration 40074: loss: 0.21677234768867493\n",
      "iteration 40075: loss: 0.2167721688747406\n",
      "iteration 40076: loss: 0.2167721539735794\n",
      "iteration 40077: loss: 0.21677199006080627\n",
      "iteration 40078: loss: 0.21677184104919434\n",
      "iteration 40079: loss: 0.21677175164222717\n",
      "iteration 40080: loss: 0.2167716771364212\n",
      "iteration 40081: loss: 0.21677152812480927\n",
      "iteration 40082: loss: 0.21677140891551971\n",
      "iteration 40083: loss: 0.21677131950855255\n",
      "iteration 40084: loss: 0.2167711704969406\n",
      "iteration 40085: loss: 0.21677108108997345\n",
      "iteration 40086: loss: 0.21677100658416748\n",
      "iteration 40087: loss: 0.2167709320783615\n",
      "iteration 40088: loss: 0.21677076816558838\n",
      "iteration 40089: loss: 0.21677064895629883\n",
      "iteration 40090: loss: 0.21677055954933167\n",
      "iteration 40091: loss: 0.2167704552412033\n",
      "iteration 40092: loss: 0.21677038073539734\n",
      "iteration 40093: loss: 0.2167702615261078\n",
      "iteration 40094: loss: 0.21677012741565704\n",
      "iteration 40095: loss: 0.2167700082063675\n",
      "iteration 40096: loss: 0.21676988899707794\n",
      "iteration 40097: loss: 0.216769739985466\n",
      "iteration 40098: loss: 0.21676966547966003\n",
      "iteration 40099: loss: 0.21676954627037048\n",
      "iteration 40100: loss: 0.2167694866657257\n",
      "iteration 40101: loss: 0.21676933765411377\n",
      "iteration 40102: loss: 0.2167692482471466\n",
      "iteration 40103: loss: 0.21676914393901825\n",
      "iteration 40104: loss: 0.21676898002624512\n",
      "iteration 40105: loss: 0.21676883101463318\n",
      "iteration 40106: loss: 0.21676874160766602\n",
      "iteration 40107: loss: 0.21676871180534363\n",
      "iteration 40108: loss: 0.21676859259605408\n",
      "iteration 40109: loss: 0.21676845848560333\n",
      "iteration 40110: loss: 0.21676835417747498\n",
      "iteration 40111: loss: 0.2167682647705078\n",
      "iteration 40112: loss: 0.21676811575889587\n",
      "iteration 40113: loss: 0.21676799654960632\n",
      "iteration 40114: loss: 0.21676787734031677\n",
      "iteration 40115: loss: 0.2167677879333496\n",
      "iteration 40116: loss: 0.21676766872406006\n",
      "iteration 40117: loss: 0.2167675942182541\n",
      "iteration 40118: loss: 0.21676746010780334\n",
      "iteration 40119: loss: 0.216767355799675\n",
      "iteration 40120: loss: 0.21676726639270782\n",
      "iteration 40121: loss: 0.2167671024799347\n",
      "iteration 40122: loss: 0.21676699817180634\n",
      "iteration 40123: loss: 0.21676692366600037\n",
      "iteration 40124: loss: 0.2167668640613556\n",
      "iteration 40125: loss: 0.21676668524742126\n",
      "iteration 40126: loss: 0.2167665660381317\n",
      "iteration 40127: loss: 0.21676644682884216\n",
      "iteration 40128: loss: 0.21676640212535858\n",
      "iteration 40129: loss: 0.21676628291606903\n",
      "iteration 40130: loss: 0.2167661488056183\n",
      "iteration 40131: loss: 0.21676604449748993\n",
      "iteration 40132: loss: 0.21676592528820038\n",
      "iteration 40133: loss: 0.2167658805847168\n",
      "iteration 40134: loss: 0.21676573157310486\n",
      "iteration 40135: loss: 0.2167655974626541\n",
      "iteration 40136: loss: 0.21676547825336456\n",
      "iteration 40137: loss: 0.2167653739452362\n",
      "iteration 40138: loss: 0.21676525473594666\n",
      "iteration 40139: loss: 0.21676509082317352\n",
      "iteration 40140: loss: 0.21676497161388397\n",
      "iteration 40141: loss: 0.2167649269104004\n",
      "iteration 40142: loss: 0.21676477789878845\n",
      "iteration 40143: loss: 0.2167646884918213\n",
      "iteration 40144: loss: 0.21676459908485413\n",
      "iteration 40145: loss: 0.21676447987556458\n",
      "iteration 40146: loss: 0.21676437556743622\n",
      "iteration 40147: loss: 0.21676428616046906\n",
      "iteration 40148: loss: 0.2167641669511795\n",
      "iteration 40149: loss: 0.21676409244537354\n",
      "iteration 40150: loss: 0.2167639434337616\n",
      "iteration 40151: loss: 0.21676377952098846\n",
      "iteration 40152: loss: 0.2167637050151825\n",
      "iteration 40153: loss: 0.21676364541053772\n",
      "iteration 40154: loss: 0.21676352620124817\n",
      "iteration 40155: loss: 0.2167634218931198\n",
      "iteration 40156: loss: 0.21676328778266907\n",
      "iteration 40157: loss: 0.21676316857337952\n",
      "iteration 40158: loss: 0.21676313877105713\n",
      "iteration 40159: loss: 0.2167629450559616\n",
      "iteration 40160: loss: 0.21676285564899445\n",
      "iteration 40161: loss: 0.2167627364397049\n",
      "iteration 40162: loss: 0.21676258742809296\n",
      "iteration 40163: loss: 0.2167624980211258\n",
      "iteration 40164: loss: 0.21676234900951385\n",
      "iteration 40165: loss: 0.21676227450370789\n",
      "iteration 40166: loss: 0.2167622148990631\n",
      "iteration 40167: loss: 0.21676206588745117\n",
      "iteration 40168: loss: 0.216761976480484\n",
      "iteration 40169: loss: 0.21676179766654968\n",
      "iteration 40170: loss: 0.2167617529630661\n",
      "iteration 40171: loss: 0.21676166355609894\n",
      "iteration 40172: loss: 0.2167615443468094\n",
      "iteration 40173: loss: 0.21676139533519745\n",
      "iteration 40174: loss: 0.21676132082939148\n",
      "iteration 40175: loss: 0.21676115691661835\n",
      "iteration 40176: loss: 0.21676106750965118\n",
      "iteration 40177: loss: 0.21676096320152283\n",
      "iteration 40178: loss: 0.21676091849803925\n",
      "iteration 40179: loss: 0.2167607545852661\n",
      "iteration 40180: loss: 0.21676063537597656\n",
      "iteration 40181: loss: 0.21676047146320343\n",
      "iteration 40182: loss: 0.21676036715507507\n",
      "iteration 40183: loss: 0.2167603224515915\n",
      "iteration 40184: loss: 0.21676015853881836\n",
      "iteration 40185: loss: 0.2167600691318512\n",
      "iteration 40186: loss: 0.21675996482372284\n",
      "iteration 40187: loss: 0.2167598456144333\n",
      "iteration 40188: loss: 0.21675977110862732\n",
      "iteration 40189: loss: 0.21675963699817657\n",
      "iteration 40190: loss: 0.21675953269004822\n",
      "iteration 40191: loss: 0.21675941348075867\n",
      "iteration 40192: loss: 0.21675929427146912\n",
      "iteration 40193: loss: 0.21675917506217957\n",
      "iteration 40194: loss: 0.2167590856552124\n",
      "iteration 40195: loss: 0.21675896644592285\n",
      "iteration 40196: loss: 0.2167588770389557\n",
      "iteration 40197: loss: 0.21675872802734375\n",
      "iteration 40198: loss: 0.21675865352153778\n",
      "iteration 40199: loss: 0.21675853431224823\n",
      "iteration 40200: loss: 0.21675845980644226\n",
      "iteration 40201: loss: 0.21675825119018555\n",
      "iteration 40202: loss: 0.21675817668437958\n",
      "iteration 40203: loss: 0.21675804257392883\n",
      "iteration 40204: loss: 0.21675796806812286\n",
      "iteration 40205: loss: 0.21675792336463928\n",
      "iteration 40206: loss: 0.21675774455070496\n",
      "iteration 40207: loss: 0.21675768494606018\n",
      "iteration 40208: loss: 0.21675756573677063\n",
      "iteration 40209: loss: 0.21675744652748108\n",
      "iteration 40210: loss: 0.21675729751586914\n",
      "iteration 40211: loss: 0.21675726771354675\n",
      "iteration 40212: loss: 0.21675710380077362\n",
      "iteration 40213: loss: 0.21675696969032288\n",
      "iteration 40214: loss: 0.2167569398880005\n",
      "iteration 40215: loss: 0.21675674617290497\n",
      "iteration 40216: loss: 0.21675662696361542\n",
      "iteration 40217: loss: 0.21675658226013184\n",
      "iteration 40218: loss: 0.2167564183473587\n",
      "iteration 40219: loss: 0.21675634384155273\n",
      "iteration 40220: loss: 0.2167561948299408\n",
      "iteration 40221: loss: 0.21675607562065125\n",
      "iteration 40222: loss: 0.21675601601600647\n",
      "iteration 40223: loss: 0.2167559117078781\n",
      "iteration 40224: loss: 0.21675574779510498\n",
      "iteration 40225: loss: 0.2167557179927826\n",
      "iteration 40226: loss: 0.21675559878349304\n",
      "iteration 40227: loss: 0.2167554348707199\n",
      "iteration 40228: loss: 0.21675530076026917\n",
      "iteration 40229: loss: 0.2167551964521408\n",
      "iteration 40230: loss: 0.21675507724285126\n",
      "iteration 40231: loss: 0.2167550027370453\n",
      "iteration 40232: loss: 0.2167549431324005\n",
      "iteration 40233: loss: 0.216754749417305\n",
      "iteration 40234: loss: 0.21675467491149902\n",
      "iteration 40235: loss: 0.21675458550453186\n",
      "iteration 40236: loss: 0.21675443649291992\n",
      "iteration 40237: loss: 0.21675439178943634\n",
      "iteration 40238: loss: 0.2167542427778244\n",
      "iteration 40239: loss: 0.21675412356853485\n",
      "iteration 40240: loss: 0.21675404906272888\n",
      "iteration 40241: loss: 0.21675391495227814\n",
      "iteration 40242: loss: 0.21675381064414978\n",
      "iteration 40243: loss: 0.21675372123718262\n",
      "iteration 40244: loss: 0.21675363183021545\n",
      "iteration 40245: loss: 0.21675345301628113\n",
      "iteration 40246: loss: 0.21675331890583038\n",
      "iteration 40247: loss: 0.21675321459770203\n",
      "iteration 40248: loss: 0.21675315499305725\n",
      "iteration 40249: loss: 0.2167530357837677\n",
      "iteration 40250: loss: 0.21675291657447815\n",
      "iteration 40251: loss: 0.21675291657447815\n",
      "iteration 40252: loss: 0.21675264835357666\n",
      "iteration 40253: loss: 0.2167525738477707\n",
      "iteration 40254: loss: 0.21675245463848114\n",
      "iteration 40255: loss: 0.21675238013267517\n",
      "iteration 40256: loss: 0.21675226092338562\n",
      "iteration 40257: loss: 0.21675217151641846\n",
      "iteration 40258: loss: 0.21675200760364532\n",
      "iteration 40259: loss: 0.21675193309783936\n",
      "iteration 40260: loss: 0.2167518138885498\n",
      "iteration 40261: loss: 0.21675172448158264\n",
      "iteration 40262: loss: 0.21675154566764832\n",
      "iteration 40263: loss: 0.21675145626068115\n",
      "iteration 40264: loss: 0.2167513370513916\n",
      "iteration 40265: loss: 0.21675124764442444\n",
      "iteration 40266: loss: 0.21675117313861847\n",
      "iteration 40267: loss: 0.2167510688304901\n",
      "iteration 40268: loss: 0.2167508602142334\n",
      "iteration 40269: loss: 0.21675078570842743\n",
      "iteration 40270: loss: 0.21675071120262146\n",
      "iteration 40271: loss: 0.2167506217956543\n",
      "iteration 40272: loss: 0.21675042808055878\n",
      "iteration 40273: loss: 0.2167503386735916\n",
      "iteration 40274: loss: 0.21675023436546326\n",
      "iteration 40275: loss: 0.21675018966197968\n",
      "iteration 40276: loss: 0.21675007045269012\n",
      "iteration 40277: loss: 0.21674995124340057\n",
      "iteration 40278: loss: 0.21674978733062744\n",
      "iteration 40279: loss: 0.21674978733062744\n",
      "iteration 40280: loss: 0.2167496383190155\n",
      "iteration 40281: loss: 0.21674945950508118\n",
      "iteration 40282: loss: 0.2167493999004364\n",
      "iteration 40283: loss: 0.21674931049346924\n",
      "iteration 40284: loss: 0.2167491465806961\n",
      "iteration 40285: loss: 0.21674904227256775\n",
      "iteration 40286: loss: 0.21674895286560059\n",
      "iteration 40287: loss: 0.21674886345863342\n",
      "iteration 40288: loss: 0.21674874424934387\n",
      "iteration 40289: loss: 0.21674859523773193\n",
      "iteration 40290: loss: 0.21674855053424835\n",
      "iteration 40291: loss: 0.21674838662147522\n",
      "iteration 40292: loss: 0.21674828231334686\n",
      "iteration 40293: loss: 0.2167481631040573\n",
      "iteration 40294: loss: 0.21674802899360657\n",
      "iteration 40295: loss: 0.21674799919128418\n",
      "iteration 40296: loss: 0.21674783527851105\n",
      "iteration 40297: loss: 0.2167477160692215\n",
      "iteration 40298: loss: 0.21674764156341553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 40299: loss: 0.21674744784832\n",
      "iteration 40300: loss: 0.2167474329471588\n",
      "iteration 40301: loss: 0.21674728393554688\n",
      "iteration 40302: loss: 0.2167472094297409\n",
      "iteration 40303: loss: 0.21674701571464539\n",
      "iteration 40304: loss: 0.21674692630767822\n",
      "iteration 40305: loss: 0.21674683690071106\n",
      "iteration 40306: loss: 0.2167467623949051\n",
      "iteration 40307: loss: 0.21674656867980957\n",
      "iteration 40308: loss: 0.216746523976326\n",
      "iteration 40309: loss: 0.21674644947052002\n",
      "iteration 40310: loss: 0.21674630045890808\n",
      "iteration 40311: loss: 0.21674618124961853\n",
      "iteration 40312: loss: 0.21674606204032898\n",
      "iteration 40313: loss: 0.2167460173368454\n",
      "iteration 40314: loss: 0.21674580872058868\n",
      "iteration 40315: loss: 0.2167457640171051\n",
      "iteration 40316: loss: 0.21674564480781555\n",
      "iteration 40317: loss: 0.216745525598526\n",
      "iteration 40318: loss: 0.21674546599388123\n",
      "iteration 40319: loss: 0.2167453020811081\n",
      "iteration 40320: loss: 0.21674518287181854\n",
      "iteration 40321: loss: 0.2167450487613678\n",
      "iteration 40322: loss: 0.21674494445323944\n",
      "iteration 40323: loss: 0.21674485504627228\n",
      "iteration 40324: loss: 0.2167448103427887\n",
      "iteration 40325: loss: 0.21674463152885437\n",
      "iteration 40326: loss: 0.21674449741840363\n",
      "iteration 40327: loss: 0.21674445271492004\n",
      "iteration 40328: loss: 0.2167443335056305\n",
      "iteration 40329: loss: 0.21674421429634094\n",
      "iteration 40330: loss: 0.2167440950870514\n",
      "iteration 40331: loss: 0.21674397587776184\n",
      "iteration 40332: loss: 0.21674387156963348\n",
      "iteration 40333: loss: 0.21674375236034393\n",
      "iteration 40334: loss: 0.21674366295337677\n",
      "iteration 40335: loss: 0.21674351394176483\n",
      "iteration 40336: loss: 0.21674339473247528\n",
      "iteration 40337: loss: 0.21674330532550812\n",
      "iteration 40338: loss: 0.21674315631389618\n",
      "iteration 40339: loss: 0.2167430818080902\n",
      "iteration 40340: loss: 0.21674302220344543\n",
      "iteration 40341: loss: 0.2167428433895111\n",
      "iteration 40342: loss: 0.21674272418022156\n",
      "iteration 40343: loss: 0.21674266457557678\n",
      "iteration 40344: loss: 0.21674251556396484\n",
      "iteration 40345: loss: 0.21674248576164246\n",
      "iteration 40346: loss: 0.2167423665523529\n",
      "iteration 40347: loss: 0.21674223244190216\n",
      "iteration 40348: loss: 0.21674208343029022\n",
      "iteration 40349: loss: 0.21674200892448425\n",
      "iteration 40350: loss: 0.21674196422100067\n",
      "iteration 40351: loss: 0.21674180030822754\n",
      "iteration 40352: loss: 0.21674160659313202\n",
      "iteration 40353: loss: 0.21674156188964844\n",
      "iteration 40354: loss: 0.2167414128780365\n",
      "iteration 40355: loss: 0.21674132347106934\n",
      "iteration 40356: loss: 0.21674124896526337\n",
      "iteration 40357: loss: 0.21674108505249023\n",
      "iteration 40358: loss: 0.21674096584320068\n",
      "iteration 40359: loss: 0.21674084663391113\n",
      "iteration 40360: loss: 0.2167406976222992\n",
      "iteration 40361: loss: 0.2167406529188156\n",
      "iteration 40362: loss: 0.21674056351184845\n",
      "iteration 40363: loss: 0.21674048900604248\n",
      "iteration 40364: loss: 0.21674029529094696\n",
      "iteration 40365: loss: 0.21674028038978577\n",
      "iteration 40366: loss: 0.21674013137817383\n",
      "iteration 40367: loss: 0.2167399823665619\n",
      "iteration 40368: loss: 0.21673984825611115\n",
      "iteration 40369: loss: 0.2167397439479828\n",
      "iteration 40370: loss: 0.21673965454101562\n",
      "iteration 40371: loss: 0.2167394906282425\n",
      "iteration 40372: loss: 0.21673941612243652\n",
      "iteration 40373: loss: 0.21673929691314697\n",
      "iteration 40374: loss: 0.216739222407341\n",
      "iteration 40375: loss: 0.21673910319805145\n",
      "iteration 40376: loss: 0.21673902869224548\n",
      "iteration 40377: loss: 0.21673889458179474\n",
      "iteration 40378: loss: 0.21673879027366638\n",
      "iteration 40379: loss: 0.21673867106437683\n",
      "iteration 40380: loss: 0.2167385071516037\n",
      "iteration 40381: loss: 0.21673846244812012\n",
      "iteration 40382: loss: 0.21673831343650818\n",
      "iteration 40383: loss: 0.2167382538318634\n",
      "iteration 40384: loss: 0.21673807501792908\n",
      "iteration 40385: loss: 0.21673798561096191\n",
      "iteration 40386: loss: 0.21673791110515594\n",
      "iteration 40387: loss: 0.2167377918958664\n",
      "iteration 40388: loss: 0.21673770248889923\n",
      "iteration 40389: loss: 0.2167375534772873\n",
      "iteration 40390: loss: 0.21673743426799774\n",
      "iteration 40391: loss: 0.2167373150587082\n",
      "iteration 40392: loss: 0.21673722565174103\n",
      "iteration 40393: loss: 0.21673710644245148\n",
      "iteration 40394: loss: 0.2167370319366455\n",
      "iteration 40395: loss: 0.21673688292503357\n",
      "iteration 40396: loss: 0.21673686802387238\n",
      "iteration 40397: loss: 0.21673670411109924\n",
      "iteration 40398: loss: 0.21673652529716492\n",
      "iteration 40399: loss: 0.21673646569252014\n",
      "iteration 40400: loss: 0.2167363464832306\n",
      "iteration 40401: loss: 0.21673624217510223\n",
      "iteration 40402: loss: 0.21673616766929626\n",
      "iteration 40403: loss: 0.21673600375652313\n",
      "iteration 40404: loss: 0.21673591434955597\n",
      "iteration 40405: loss: 0.21673579514026642\n",
      "iteration 40406: loss: 0.21673569083213806\n",
      "iteration 40407: loss: 0.2167356312274933\n",
      "iteration 40408: loss: 0.21673551201820374\n",
      "iteration 40409: loss: 0.21673539280891418\n",
      "iteration 40410: loss: 0.21673524379730225\n",
      "iteration 40411: loss: 0.2167351245880127\n",
      "iteration 40412: loss: 0.2167350798845291\n",
      "iteration 40413: loss: 0.21673491597175598\n",
      "iteration 40414: loss: 0.21673484146595\n",
      "iteration 40415: loss: 0.21673469245433807\n",
      "iteration 40416: loss: 0.21673455834388733\n",
      "iteration 40417: loss: 0.21673449873924255\n",
      "iteration 40418: loss: 0.2167344093322754\n",
      "iteration 40419: loss: 0.21673424541950226\n",
      "iteration 40420: loss: 0.2167341411113739\n",
      "iteration 40421: loss: 0.21673397719860077\n",
      "iteration 40422: loss: 0.2167339324951172\n",
      "iteration 40423: loss: 0.21673384308815002\n",
      "iteration 40424: loss: 0.21673373878002167\n",
      "iteration 40425: loss: 0.21673361957073212\n",
      "iteration 40426: loss: 0.21673348546028137\n",
      "iteration 40427: loss: 0.21673336625099182\n",
      "iteration 40428: loss: 0.2167331874370575\n",
      "iteration 40429: loss: 0.2167331874370575\n",
      "iteration 40430: loss: 0.21673305332660675\n",
      "iteration 40431: loss: 0.2167329341173172\n",
      "iteration 40432: loss: 0.21673281490802765\n",
      "iteration 40433: loss: 0.2167326956987381\n",
      "iteration 40434: loss: 0.21673257648944855\n",
      "iteration 40435: loss: 0.21673254668712616\n",
      "iteration 40436: loss: 0.21673238277435303\n",
      "iteration 40437: loss: 0.2167321890592575\n",
      "iteration 40438: loss: 0.2167321741580963\n",
      "iteration 40439: loss: 0.21673205494880676\n",
      "iteration 40440: loss: 0.21673190593719482\n",
      "iteration 40441: loss: 0.21673183143138885\n",
      "iteration 40442: loss: 0.2167317420244217\n",
      "iteration 40443: loss: 0.21673162281513214\n",
      "iteration 40444: loss: 0.216731458902359\n",
      "iteration 40445: loss: 0.21673133969306946\n",
      "iteration 40446: loss: 0.2167312651872635\n",
      "iteration 40447: loss: 0.21673114597797394\n",
      "iteration 40448: loss: 0.21673104166984558\n",
      "iteration 40449: loss: 0.21673092246055603\n",
      "iteration 40450: loss: 0.21673083305358887\n",
      "iteration 40451: loss: 0.2167307585477829\n",
      "iteration 40452: loss: 0.21673059463500977\n",
      "iteration 40453: loss: 0.2167305201292038\n",
      "iteration 40454: loss: 0.21673043072223663\n",
      "iteration 40455: loss: 0.2167302370071411\n",
      "iteration 40456: loss: 0.21673019230365753\n",
      "iteration 40457: loss: 0.21673007309436798\n",
      "iteration 40458: loss: 0.21672992408275604\n",
      "iteration 40459: loss: 0.21672987937927246\n",
      "iteration 40460: loss: 0.21672971546649933\n",
      "iteration 40461: loss: 0.21672964096069336\n",
      "iteration 40462: loss: 0.21672949194908142\n",
      "iteration 40463: loss: 0.21672940254211426\n",
      "iteration 40464: loss: 0.2167293131351471\n",
      "iteration 40465: loss: 0.21672920882701874\n",
      "iteration 40466: loss: 0.2167290449142456\n",
      "iteration 40467: loss: 0.21672889590263367\n",
      "iteration 40468: loss: 0.21672877669334412\n",
      "iteration 40469: loss: 0.21672873198986053\n",
      "iteration 40470: loss: 0.21672861278057098\n",
      "iteration 40471: loss: 0.21672853827476501\n",
      "iteration 40472: loss: 0.21672840416431427\n",
      "iteration 40473: loss: 0.21672825515270233\n",
      "iteration 40474: loss: 0.21672821044921875\n",
      "iteration 40475: loss: 0.21672804653644562\n",
      "iteration 40476: loss: 0.21672794222831726\n",
      "iteration 40477: loss: 0.2167278230190277\n",
      "iteration 40478: loss: 0.21672770380973816\n",
      "iteration 40479: loss: 0.21672753989696503\n",
      "iteration 40480: loss: 0.21672753989696503\n",
      "iteration 40481: loss: 0.21672745048999786\n",
      "iteration 40482: loss: 0.21672730147838593\n",
      "iteration 40483: loss: 0.21672718226909637\n",
      "iteration 40484: loss: 0.21672704815864563\n",
      "iteration 40485: loss: 0.21672692894935608\n",
      "iteration 40486: loss: 0.21672680974006653\n",
      "iteration 40487: loss: 0.21672673523426056\n",
      "iteration 40488: loss: 0.2167266309261322\n",
      "iteration 40489: loss: 0.21672649681568146\n",
      "iteration 40490: loss: 0.21672645211219788\n",
      "iteration 40491: loss: 0.21672627329826355\n",
      "iteration 40492: loss: 0.2167261838912964\n",
      "iteration 40493: loss: 0.21672609448432922\n",
      "iteration 40494: loss: 0.21672599017620087\n",
      "iteration 40495: loss: 0.21672587096691132\n",
      "iteration 40496: loss: 0.21672575175762177\n",
      "iteration 40497: loss: 0.21672561764717102\n",
      "iteration 40498: loss: 0.21672551333904266\n",
      "iteration 40499: loss: 0.2167253941297531\n",
      "iteration 40500: loss: 0.21672530472278595\n",
      "iteration 40501: loss: 0.2167252004146576\n",
      "iteration 40502: loss: 0.21672508120536804\n",
      "iteration 40503: loss: 0.21672503650188446\n",
      "iteration 40504: loss: 0.21672487258911133\n",
      "iteration 40505: loss: 0.21672475337982178\n",
      "iteration 40506: loss: 0.21672466397285461\n",
      "iteration 40507: loss: 0.2167244851589203\n",
      "iteration 40508: loss: 0.2167244404554367\n",
      "iteration 40509: loss: 0.21672432124614716\n",
      "iteration 40510: loss: 0.21672423183918\n",
      "iteration 40511: loss: 0.21672411262989044\n",
      "iteration 40512: loss: 0.2167239934206009\n",
      "iteration 40513: loss: 0.21672391891479492\n",
      "iteration 40514: loss: 0.2167237550020218\n",
      "iteration 40515: loss: 0.21672363579273224\n",
      "iteration 40516: loss: 0.21672359108924866\n",
      "iteration 40517: loss: 0.21672336757183075\n",
      "iteration 40518: loss: 0.21672323346138\n",
      "iteration 40519: loss: 0.21672323346138\n",
      "iteration 40520: loss: 0.21672312915325165\n",
      "iteration 40521: loss: 0.2167229950428009\n",
      "iteration 40522: loss: 0.21672287583351135\n",
      "iteration 40523: loss: 0.21672272682189941\n",
      "iteration 40524: loss: 0.21672268211841583\n",
      "iteration 40525: loss: 0.2167225331068039\n",
      "iteration 40526: loss: 0.21672244369983673\n",
      "iteration 40527: loss: 0.21672233939170837\n",
      "iteration 40528: loss: 0.21672222018241882\n",
      "iteration 40529: loss: 0.21672216057777405\n",
      "iteration 40530: loss: 0.21672198176383972\n",
      "iteration 40531: loss: 0.21672189235687256\n",
      "iteration 40532: loss: 0.2167218178510666\n",
      "iteration 40533: loss: 0.21672165393829346\n",
      "iteration 40534: loss: 0.21672150492668152\n",
      "iteration 40535: loss: 0.21672144532203674\n",
      "iteration 40536: loss: 0.2167213261127472\n",
      "iteration 40537: loss: 0.21672125160694122\n",
      "iteration 40538: loss: 0.21672110259532928\n",
      "iteration 40539: loss: 0.21672101318836212\n",
      "iteration 40540: loss: 0.21672086417675018\n",
      "iteration 40541: loss: 0.21672073006629944\n",
      "iteration 40542: loss: 0.21672065556049347\n",
      "iteration 40543: loss: 0.2167205810546875\n",
      "iteration 40544: loss: 0.21672041714191437\n",
      "iteration 40545: loss: 0.21672029793262482\n",
      "iteration 40546: loss: 0.21672022342681885\n",
      "iteration 40547: loss: 0.21672005951404572\n",
      "iteration 40548: loss: 0.21671995520591736\n",
      "iteration 40549: loss: 0.2167198359966278\n",
      "iteration 40550: loss: 0.21671977639198303\n",
      "iteration 40551: loss: 0.21671967208385468\n",
      "iteration 40552: loss: 0.21671955287456512\n",
      "iteration 40553: loss: 0.21671947836875916\n",
      "iteration 40554: loss: 0.21671926975250244\n",
      "iteration 40555: loss: 0.21671918034553528\n",
      "iteration 40556: loss: 0.2167191058397293\n",
      "iteration 40557: loss: 0.21671894192695618\n",
      "iteration 40558: loss: 0.2167189121246338\n",
      "iteration 40559: loss: 0.21671876311302185\n",
      "iteration 40560: loss: 0.2167186737060547\n",
      "iteration 40561: loss: 0.21671855449676514\n",
      "iteration 40562: loss: 0.21671846508979797\n",
      "iteration 40563: loss: 0.21671834588050842\n",
      "iteration 40564: loss: 0.2167181670665741\n",
      "iteration 40565: loss: 0.21671810746192932\n",
      "iteration 40566: loss: 0.21671800315380096\n",
      "iteration 40567: loss: 0.21671786904335022\n",
      "iteration 40568: loss: 0.21671779453754425\n",
      "iteration 40569: loss: 0.2167176902294159\n",
      "iteration 40570: loss: 0.21671752631664276\n",
      "iteration 40571: loss: 0.2167174369096756\n",
      "iteration 40572: loss: 0.21671733260154724\n",
      "iteration 40573: loss: 0.21671728789806366\n",
      "iteration 40574: loss: 0.21671712398529053\n",
      "iteration 40575: loss: 0.21671700477600098\n",
      "iteration 40576: loss: 0.21671688556671143\n",
      "iteration 40577: loss: 0.2167167365550995\n",
      "iteration 40578: loss: 0.2167167216539383\n",
      "iteration 40579: loss: 0.21671664714813232\n",
      "iteration 40580: loss: 0.2167164832353592\n",
      "iteration 40581: loss: 0.21671633422374725\n",
      "iteration 40582: loss: 0.2167162001132965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 40583: loss: 0.21671609580516815\n",
      "iteration 40584: loss: 0.21671602129936218\n",
      "iteration 40585: loss: 0.21671590209007263\n",
      "iteration 40586: loss: 0.2167157679796219\n",
      "iteration 40587: loss: 0.21671566367149353\n",
      "iteration 40588: loss: 0.21671560406684875\n",
      "iteration 40589: loss: 0.2167154997587204\n",
      "iteration 40590: loss: 0.21671538054943085\n",
      "iteration 40591: loss: 0.2167152464389801\n",
      "iteration 40592: loss: 0.21671512722969055\n",
      "iteration 40593: loss: 0.2167150229215622\n",
      "iteration 40594: loss: 0.21671494841575623\n",
      "iteration 40595: loss: 0.21671481430530548\n",
      "iteration 40596: loss: 0.21671469509601593\n",
      "iteration 40597: loss: 0.21671457588672638\n",
      "iteration 40598: loss: 0.21671438217163086\n",
      "iteration 40599: loss: 0.2167143076658249\n",
      "iteration 40600: loss: 0.21671411395072937\n",
      "iteration 40601: loss: 0.21671409904956818\n",
      "iteration 40602: loss: 0.2167140543460846\n",
      "iteration 40603: loss: 0.21671393513679504\n",
      "iteration 40604: loss: 0.2167138159275055\n",
      "iteration 40605: loss: 0.21671369671821594\n",
      "iteration 40606: loss: 0.2167135775089264\n",
      "iteration 40607: loss: 0.21671347320079803\n",
      "iteration 40608: loss: 0.2167133092880249\n",
      "iteration 40609: loss: 0.21671327948570251\n",
      "iteration 40610: loss: 0.21671311557292938\n",
      "iteration 40611: loss: 0.21671302616596222\n",
      "iteration 40612: loss: 0.21671292185783386\n",
      "iteration 40613: loss: 0.21671274304389954\n",
      "iteration 40614: loss: 0.21671268343925476\n",
      "iteration 40615: loss: 0.2167125642299652\n",
      "iteration 40616: loss: 0.21671250462532043\n",
      "iteration 40617: loss: 0.21671243011951447\n",
      "iteration 40618: loss: 0.21671216189861298\n",
      "iteration 40619: loss: 0.2167121171951294\n",
      "iteration 40620: loss: 0.216712087392807\n",
      "iteration 40621: loss: 0.21671192348003387\n",
      "iteration 40622: loss: 0.21671180427074432\n",
      "iteration 40623: loss: 0.21671168506145477\n",
      "iteration 40624: loss: 0.21671147644519806\n",
      "iteration 40625: loss: 0.21671152114868164\n",
      "iteration 40626: loss: 0.21671132743358612\n",
      "iteration 40627: loss: 0.21671123802661896\n",
      "iteration 40628: loss: 0.21671119332313538\n",
      "iteration 40629: loss: 0.21671101450920105\n",
      "iteration 40630: loss: 0.21671096980571747\n",
      "iteration 40631: loss: 0.21671077609062195\n",
      "iteration 40632: loss: 0.2167106568813324\n",
      "iteration 40633: loss: 0.21671059727668762\n",
      "iteration 40634: loss: 0.21671052277088165\n",
      "iteration 40635: loss: 0.21671037375926971\n",
      "iteration 40636: loss: 0.21671023964881897\n",
      "iteration 40637: loss: 0.216710165143013\n",
      "iteration 40638: loss: 0.21671004593372345\n",
      "iteration 40639: loss: 0.2167099267244339\n",
      "iteration 40640: loss: 0.21670980751514435\n",
      "iteration 40641: loss: 0.2167096883058548\n",
      "iteration 40642: loss: 0.21670961380004883\n",
      "iteration 40643: loss: 0.21670952439308167\n",
      "iteration 40644: loss: 0.21670930087566376\n",
      "iteration 40645: loss: 0.21670930087566376\n",
      "iteration 40646: loss: 0.2167092263698578\n",
      "iteration 40647: loss: 0.21670904755592346\n",
      "iteration 40648: loss: 0.2167089730501175\n",
      "iteration 40649: loss: 0.21670885384082794\n",
      "iteration 40650: loss: 0.216708704829216\n",
      "iteration 40651: loss: 0.21670861542224884\n",
      "iteration 40652: loss: 0.2167084515094757\n",
      "iteration 40653: loss: 0.21670837700366974\n",
      "iteration 40654: loss: 0.2167082279920578\n",
      "iteration 40655: loss: 0.21670815348625183\n",
      "iteration 40656: loss: 0.21670806407928467\n",
      "iteration 40657: loss: 0.21670787036418915\n",
      "iteration 40658: loss: 0.21670778095722198\n",
      "iteration 40659: loss: 0.21670770645141602\n",
      "iteration 40660: loss: 0.21670758724212646\n",
      "iteration 40661: loss: 0.21670746803283691\n",
      "iteration 40662: loss: 0.21670737862586975\n",
      "iteration 40663: loss: 0.2167072743177414\n",
      "iteration 40664: loss: 0.21670718491077423\n",
      "iteration 40665: loss: 0.21670706570148468\n",
      "iteration 40666: loss: 0.21670694649219513\n",
      "iteration 40667: loss: 0.21670682728290558\n",
      "iteration 40668: loss: 0.21670672297477722\n",
      "iteration 40669: loss: 0.21670666337013245\n",
      "iteration 40670: loss: 0.21670648455619812\n",
      "iteration 40671: loss: 0.21670636534690857\n",
      "iteration 40672: loss: 0.2167063057422638\n",
      "iteration 40673: loss: 0.21670612692832947\n",
      "iteration 40674: loss: 0.2167060375213623\n",
      "iteration 40675: loss: 0.21670594811439514\n",
      "iteration 40676: loss: 0.2167058289051056\n",
      "iteration 40677: loss: 0.21670575439929962\n",
      "iteration 40678: loss: 0.21670560538768768\n",
      "iteration 40679: loss: 0.21670547127723694\n",
      "iteration 40680: loss: 0.21670544147491455\n",
      "iteration 40681: loss: 0.21670527756214142\n",
      "iteration 40682: loss: 0.21670517325401306\n",
      "iteration 40683: loss: 0.2167050838470459\n",
      "iteration 40684: loss: 0.21670491993427277\n",
      "iteration 40685: loss: 0.2167048454284668\n",
      "iteration 40686: loss: 0.21670469641685486\n",
      "iteration 40687: loss: 0.2167046070098877\n",
      "iteration 40688: loss: 0.21670453250408173\n",
      "iteration 40689: loss: 0.21670441329479218\n",
      "iteration 40690: loss: 0.21670427918434143\n",
      "iteration 40691: loss: 0.21670420467853546\n",
      "iteration 40692: loss: 0.2167040854692459\n",
      "iteration 40693: loss: 0.21670392155647278\n",
      "iteration 40694: loss: 0.2167038470506668\n",
      "iteration 40695: loss: 0.21670374274253845\n",
      "iteration 40696: loss: 0.2167036235332489\n",
      "iteration 40697: loss: 0.21670350432395935\n",
      "iteration 40698: loss: 0.2167034149169922\n",
      "iteration 40699: loss: 0.21670326590538025\n",
      "iteration 40700: loss: 0.2167031466960907\n",
      "iteration 40701: loss: 0.21670305728912354\n",
      "iteration 40702: loss: 0.21670286357402802\n",
      "iteration 40703: loss: 0.21670278906822205\n",
      "iteration 40704: loss: 0.21670274436473846\n",
      "iteration 40705: loss: 0.2167026251554489\n",
      "iteration 40706: loss: 0.21670249104499817\n",
      "iteration 40707: loss: 0.21670237183570862\n",
      "iteration 40708: loss: 0.21670226752758026\n",
      "iteration 40709: loss: 0.2167021781206131\n",
      "iteration 40710: loss: 0.21670202910900116\n",
      "iteration 40711: loss: 0.216701939702034\n",
      "iteration 40712: loss: 0.21670183539390564\n",
      "iteration 40713: loss: 0.21670179069042206\n",
      "iteration 40714: loss: 0.21670159697532654\n",
      "iteration 40715: loss: 0.2167014628648758\n",
      "iteration 40716: loss: 0.21670135855674744\n",
      "iteration 40717: loss: 0.21670134365558624\n",
      "iteration 40718: loss: 0.2167011797428131\n",
      "iteration 40719: loss: 0.21670110523700714\n",
      "iteration 40720: loss: 0.21670100092887878\n",
      "iteration 40721: loss: 0.21670086681842804\n",
      "iteration 40722: loss: 0.2167007029056549\n",
      "iteration 40723: loss: 0.21670064330101013\n",
      "iteration 40724: loss: 0.21670052409172058\n",
      "iteration 40725: loss: 0.21670040488243103\n",
      "iteration 40726: loss: 0.2167002409696579\n",
      "iteration 40727: loss: 0.21670019626617432\n",
      "iteration 40728: loss: 0.21670007705688477\n",
      "iteration 40729: loss: 0.21669992804527283\n",
      "iteration 40730: loss: 0.21669980883598328\n",
      "iteration 40731: loss: 0.2166997641324997\n",
      "iteration 40732: loss: 0.21669957041740417\n",
      "iteration 40733: loss: 0.21669955551624298\n",
      "iteration 40734: loss: 0.21669939160346985\n",
      "iteration 40735: loss: 0.21669921278953552\n",
      "iteration 40736: loss: 0.21669912338256836\n",
      "iteration 40737: loss: 0.21669915318489075\n",
      "iteration 40738: loss: 0.2166990041732788\n",
      "iteration 40739: loss: 0.21669884026050568\n",
      "iteration 40740: loss: 0.21669873595237732\n",
      "iteration 40741: loss: 0.2166985720396042\n",
      "iteration 40742: loss: 0.21669849753379822\n",
      "iteration 40743: loss: 0.21669836342334747\n",
      "iteration 40744: loss: 0.2166982889175415\n",
      "iteration 40745: loss: 0.21669816970825195\n",
      "iteration 40746: loss: 0.2166980803012848\n",
      "iteration 40747: loss: 0.21669797599315643\n",
      "iteration 40748: loss: 0.21669788658618927\n",
      "iteration 40749: loss: 0.21669772267341614\n",
      "iteration 40750: loss: 0.21669761836528778\n",
      "iteration 40751: loss: 0.21669749915599823\n",
      "iteration 40752: loss: 0.21669742465019226\n",
      "iteration 40753: loss: 0.21669729053974152\n",
      "iteration 40754: loss: 0.21669717133045197\n",
      "iteration 40755: loss: 0.21669717133045197\n",
      "iteration 40756: loss: 0.21669700741767883\n",
      "iteration 40757: loss: 0.2166968584060669\n",
      "iteration 40758: loss: 0.21669678390026093\n",
      "iteration 40759: loss: 0.21669664978981018\n",
      "iteration 40760: loss: 0.21669650077819824\n",
      "iteration 40761: loss: 0.21669641137123108\n",
      "iteration 40762: loss: 0.21669629216194153\n",
      "iteration 40763: loss: 0.21669618785381317\n",
      "iteration 40764: loss: 0.216696098446846\n",
      "iteration 40765: loss: 0.21669597923755646\n",
      "iteration 40766: loss: 0.2166958600282669\n",
      "iteration 40767: loss: 0.21669578552246094\n",
      "iteration 40768: loss: 0.2166956663131714\n",
      "iteration 40769: loss: 0.21669557690620422\n",
      "iteration 40770: loss: 0.2166953980922699\n",
      "iteration 40771: loss: 0.21669527888298035\n",
      "iteration 40772: loss: 0.21669530868530273\n",
      "iteration 40773: loss: 0.21669504046440125\n",
      "iteration 40774: loss: 0.21669498085975647\n",
      "iteration 40775: loss: 0.2166948765516281\n",
      "iteration 40776: loss: 0.21669475734233856\n",
      "iteration 40777: loss: 0.2166946828365326\n",
      "iteration 40778: loss: 0.21669450402259827\n",
      "iteration 40779: loss: 0.2166944295167923\n",
      "iteration 40780: loss: 0.21669432520866394\n",
      "iteration 40781: loss: 0.2166941910982132\n",
      "iteration 40782: loss: 0.21669411659240723\n",
      "iteration 40783: loss: 0.2166939675807953\n",
      "iteration 40784: loss: 0.2166939079761505\n",
      "iteration 40785: loss: 0.2166936844587326\n",
      "iteration 40786: loss: 0.21669363975524902\n",
      "iteration 40787: loss: 0.21669359505176544\n",
      "iteration 40788: loss: 0.21669340133666992\n",
      "iteration 40789: loss: 0.21669335663318634\n",
      "iteration 40790: loss: 0.2166931927204132\n",
      "iteration 40791: loss: 0.21669307351112366\n",
      "iteration 40792: loss: 0.2166929990053177\n",
      "iteration 40793: loss: 0.21669283509254456\n",
      "iteration 40794: loss: 0.2166927605867386\n",
      "iteration 40795: loss: 0.21669261157512665\n",
      "iteration 40796: loss: 0.21669256687164307\n",
      "iteration 40797: loss: 0.2166924774646759\n",
      "iteration 40798: loss: 0.216692253947258\n",
      "iteration 40799: loss: 0.2166922390460968\n",
      "iteration 40800: loss: 0.21669211983680725\n",
      "iteration 40801: loss: 0.2166919708251953\n",
      "iteration 40802: loss: 0.21669188141822815\n",
      "iteration 40803: loss: 0.2166917324066162\n",
      "iteration 40804: loss: 0.21669161319732666\n",
      "iteration 40805: loss: 0.2166915237903595\n",
      "iteration 40806: loss: 0.21669144928455353\n",
      "iteration 40807: loss: 0.21669133007526398\n",
      "iteration 40808: loss: 0.21669122576713562\n",
      "iteration 40809: loss: 0.21669116616249084\n",
      "iteration 40810: loss: 0.21669097244739532\n",
      "iteration 40811: loss: 0.21669086813926697\n",
      "iteration 40812: loss: 0.2166908085346222\n",
      "iteration 40813: loss: 0.21669070422649384\n",
      "iteration 40814: loss: 0.2166905701160431\n",
      "iteration 40815: loss: 0.21669045090675354\n",
      "iteration 40816: loss: 0.2166903018951416\n",
      "iteration 40817: loss: 0.21669018268585205\n",
      "iteration 40818: loss: 0.2166900932788849\n",
      "iteration 40819: loss: 0.21668997406959534\n",
      "iteration 40820: loss: 0.21668986976146698\n",
      "iteration 40821: loss: 0.21668975055217743\n",
      "iteration 40822: loss: 0.21668970584869385\n",
      "iteration 40823: loss: 0.21668954193592072\n",
      "iteration 40824: loss: 0.2166893482208252\n",
      "iteration 40825: loss: 0.21668927371501923\n",
      "iteration 40826: loss: 0.21668915450572968\n",
      "iteration 40827: loss: 0.2166890650987625\n",
      "iteration 40828: loss: 0.21668896079063416\n",
      "iteration 40829: loss: 0.21668891608715057\n",
      "iteration 40830: loss: 0.21668872237205505\n",
      "iteration 40831: loss: 0.2166886031627655\n",
      "iteration 40832: loss: 0.21668855845928192\n",
      "iteration 40833: loss: 0.21668843924999237\n",
      "iteration 40834: loss: 0.2166883498430252\n",
      "iteration 40835: loss: 0.21668820083141327\n",
      "iteration 40836: loss: 0.21668806672096252\n",
      "iteration 40837: loss: 0.21668794751167297\n",
      "iteration 40838: loss: 0.216687873005867\n",
      "iteration 40839: loss: 0.21668775379657745\n",
      "iteration 40840: loss: 0.21668760478496552\n",
      "iteration 40841: loss: 0.21668756008148193\n",
      "iteration 40842: loss: 0.21668748557567596\n",
      "iteration 40843: loss: 0.21668729186058044\n",
      "iteration 40844: loss: 0.2166871279478073\n",
      "iteration 40845: loss: 0.21668711304664612\n",
      "iteration 40846: loss: 0.21668699383735657\n",
      "iteration 40847: loss: 0.21668684482574463\n",
      "iteration 40848: loss: 0.21668680012226105\n",
      "iteration 40849: loss: 0.2166866511106491\n",
      "iteration 40850: loss: 0.21668657660484314\n",
      "iteration 40851: loss: 0.2166864573955536\n",
      "iteration 40852: loss: 0.21668629348278046\n",
      "iteration 40853: loss: 0.21668624877929688\n",
      "iteration 40854: loss: 0.21668608486652374\n",
      "iteration 40855: loss: 0.21668598055839539\n",
      "iteration 40856: loss: 0.21668581664562225\n",
      "iteration 40857: loss: 0.2166856825351715\n",
      "iteration 40858: loss: 0.2166856974363327\n",
      "iteration 40859: loss: 0.216685488820076\n",
      "iteration 40860: loss: 0.2166854441165924\n",
      "iteration 40861: loss: 0.21668533980846405\n",
      "iteration 40862: loss: 0.2166852056980133\n",
      "iteration 40863: loss: 0.21668513119220734\n",
      "iteration 40864: loss: 0.2166849821805954\n",
      "iteration 40865: loss: 0.21668484807014465\n",
      "iteration 40866: loss: 0.21668478846549988\n",
      "iteration 40867: loss: 0.21668466925621033\n",
      "iteration 40868: loss: 0.21668460965156555\n",
      "iteration 40869: loss: 0.2166844606399536\n",
      "iteration 40870: loss: 0.21668434143066406\n",
      "iteration 40871: loss: 0.2166842222213745\n",
      "iteration 40872: loss: 0.21668410301208496\n",
      "iteration 40873: loss: 0.2166840136051178\n",
      "iteration 40874: loss: 0.21668386459350586\n",
      "iteration 40875: loss: 0.2166837453842163\n",
      "iteration 40876: loss: 0.21668367087841034\n",
      "iteration 40877: loss: 0.21668358147144318\n",
      "iteration 40878: loss: 0.21668341755867004\n",
      "iteration 40879: loss: 0.2166832983493805\n",
      "iteration 40880: loss: 0.21668319404125214\n",
      "iteration 40881: loss: 0.2166830599308014\n",
      "iteration 40882: loss: 0.21668300032615662\n",
      "iteration 40883: loss: 0.21668286621570587\n",
      "iteration 40884: loss: 0.21668274700641632\n",
      "iteration 40885: loss: 0.21668264269828796\n",
      "iteration 40886: loss: 0.2166825532913208\n",
      "iteration 40887: loss: 0.21668240427970886\n",
      "iteration 40888: loss: 0.2166822850704193\n",
      "iteration 40889: loss: 0.21668219566345215\n",
      "iteration 40890: loss: 0.21668215095996857\n",
      "iteration 40891: loss: 0.21668195724487305\n",
      "iteration 40892: loss: 0.21668188273906708\n",
      "iteration 40893: loss: 0.21668171882629395\n",
      "iteration 40894: loss: 0.21668167412281036\n",
      "iteration 40895: loss: 0.216681569814682\n",
      "iteration 40896: loss: 0.21668139100074768\n",
      "iteration 40897: loss: 0.21668128669261932\n",
      "iteration 40898: loss: 0.21668127179145813\n",
      "iteration 40899: loss: 0.2166811227798462\n",
      "iteration 40900: loss: 0.21668100357055664\n",
      "iteration 40901: loss: 0.21668091416358948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 40902: loss: 0.21668072044849396\n",
      "iteration 40903: loss: 0.21668067574501038\n",
      "iteration 40904: loss: 0.21668052673339844\n",
      "iteration 40905: loss: 0.2166803777217865\n",
      "iteration 40906: loss: 0.21668028831481934\n",
      "iteration 40907: loss: 0.21668019890785217\n",
      "iteration 40908: loss: 0.21668009459972382\n",
      "iteration 40909: loss: 0.21668000519275665\n",
      "iteration 40910: loss: 0.2166798859834671\n",
      "iteration 40911: loss: 0.21667976677417755\n",
      "iteration 40912: loss: 0.2166796177625656\n",
      "iteration 40913: loss: 0.21667952835559845\n",
      "iteration 40914: loss: 0.21667945384979248\n",
      "iteration 40915: loss: 0.21667928993701935\n",
      "iteration 40916: loss: 0.216679185628891\n",
      "iteration 40917: loss: 0.2166791707277298\n",
      "iteration 40918: loss: 0.21667897701263428\n",
      "iteration 40919: loss: 0.21667888760566711\n",
      "iteration 40920: loss: 0.21667876839637756\n",
      "iteration 40921: loss: 0.21667858958244324\n",
      "iteration 40922: loss: 0.21667852997779846\n",
      "iteration 40923: loss: 0.2166784107685089\n",
      "iteration 40924: loss: 0.21667830646038055\n",
      "iteration 40925: loss: 0.216678187251091\n",
      "iteration 40926: loss: 0.21667809784412384\n",
      "iteration 40927: loss: 0.2166779488325119\n",
      "iteration 40928: loss: 0.21667781472206116\n",
      "iteration 40929: loss: 0.2166777402162552\n",
      "iteration 40930: loss: 0.21667766571044922\n",
      "iteration 40931: loss: 0.21667757630348206\n",
      "iteration 40932: loss: 0.2166774719953537\n",
      "iteration 40933: loss: 0.21667730808258057\n",
      "iteration 40934: loss: 0.21667718887329102\n",
      "iteration 40935: loss: 0.21667703986167908\n",
      "iteration 40936: loss: 0.21667703986167908\n",
      "iteration 40937: loss: 0.21667690575122833\n",
      "iteration 40938: loss: 0.2166767418384552\n",
      "iteration 40939: loss: 0.21667662262916565\n",
      "iteration 40940: loss: 0.21667656302452087\n",
      "iteration 40941: loss: 0.21667639911174774\n",
      "iteration 40942: loss: 0.21667638421058655\n",
      "iteration 40943: loss: 0.21667619049549103\n",
      "iteration 40944: loss: 0.21667607128620148\n",
      "iteration 40945: loss: 0.21667595207691193\n",
      "iteration 40946: loss: 0.21667587757110596\n",
      "iteration 40947: loss: 0.21667571365833282\n",
      "iteration 40948: loss: 0.21667566895484924\n",
      "iteration 40949: loss: 0.2166755199432373\n",
      "iteration 40950: loss: 0.21667543053627014\n",
      "iteration 40951: loss: 0.21667535603046417\n",
      "iteration 40952: loss: 0.21667519211769104\n",
      "iteration 40953: loss: 0.21667513251304626\n",
      "iteration 40954: loss: 0.21667499840259552\n",
      "iteration 40955: loss: 0.2166748344898224\n",
      "iteration 40956: loss: 0.21667475998401642\n",
      "iteration 40957: loss: 0.21667461097240448\n",
      "iteration 40958: loss: 0.21667452156543732\n",
      "iteration 40959: loss: 0.21667444705963135\n",
      "iteration 40960: loss: 0.21667428314685822\n",
      "iteration 40961: loss: 0.21667417883872986\n",
      "iteration 40962: loss: 0.2166740447282791\n",
      "iteration 40963: loss: 0.21667400002479553\n",
      "iteration 40964: loss: 0.21667388081550598\n",
      "iteration 40965: loss: 0.2166738063097\n",
      "iteration 40966: loss: 0.21667364239692688\n",
      "iteration 40967: loss: 0.21667352318763733\n",
      "iteration 40968: loss: 0.216673344373703\n",
      "iteration 40969: loss: 0.2166733294725418\n",
      "iteration 40970: loss: 0.21667321026325226\n",
      "iteration 40971: loss: 0.2166731059551239\n",
      "iteration 40972: loss: 0.21667298674583435\n",
      "iteration 40973: loss: 0.2166728526353836\n",
      "iteration 40974: loss: 0.21667274832725525\n",
      "iteration 40975: loss: 0.21667273342609406\n",
      "iteration 40976: loss: 0.21667253971099854\n",
      "iteration 40977: loss: 0.21667234599590302\n",
      "iteration 40978: loss: 0.21667233109474182\n",
      "iteration 40979: loss: 0.21667222678661346\n",
      "iteration 40980: loss: 0.2166721075773239\n",
      "iteration 40981: loss: 0.21667201817035675\n",
      "iteration 40982: loss: 0.2166718691587448\n",
      "iteration 40983: loss: 0.21667174994945526\n",
      "iteration 40984: loss: 0.2166716307401657\n",
      "iteration 40985: loss: 0.21667146682739258\n",
      "iteration 40986: loss: 0.21667146682739258\n",
      "iteration 40987: loss: 0.21667131781578064\n",
      "iteration 40988: loss: 0.2166711986064911\n",
      "iteration 40989: loss: 0.2166711390018463\n",
      "iteration 40990: loss: 0.2166709154844284\n",
      "iteration 40991: loss: 0.21667087078094482\n",
      "iteration 40992: loss: 0.21667079627513885\n",
      "iteration 40993: loss: 0.21667060256004333\n",
      "iteration 40994: loss: 0.21667051315307617\n",
      "iteration 40995: loss: 0.216670423746109\n",
      "iteration 40996: loss: 0.21667031943798065\n",
      "iteration 40997: loss: 0.2166702300310135\n",
      "iteration 40998: loss: 0.21667008101940155\n",
      "iteration 40999: loss: 0.2166699916124344\n",
      "iteration 41000: loss: 0.21666984260082245\n",
      "iteration 41001: loss: 0.21666979789733887\n",
      "iteration 41002: loss: 0.21666967868804932\n",
      "iteration 41003: loss: 0.2166694700717926\n",
      "iteration 41004: loss: 0.21666944026947021\n",
      "iteration 41005: loss: 0.21666927635669708\n",
      "iteration 41006: loss: 0.21666917204856873\n",
      "iteration 41007: loss: 0.21666905283927917\n",
      "iteration 41008: loss: 0.216668963432312\n",
      "iteration 41009: loss: 0.21666888892650604\n",
      "iteration 41010: loss: 0.2166687548160553\n",
      "iteration 41011: loss: 0.21666865050792694\n",
      "iteration 41012: loss: 0.21666860580444336\n",
      "iteration 41013: loss: 0.21666845679283142\n",
      "iteration 41014: loss: 0.2166682928800583\n",
      "iteration 41015: loss: 0.21666817367076874\n",
      "iteration 41016: loss: 0.21666812896728516\n",
      "iteration 41017: loss: 0.21666789054870605\n",
      "iteration 41018: loss: 0.21666792035102844\n",
      "iteration 41019: loss: 0.2166677713394165\n",
      "iteration 41020: loss: 0.21666765213012695\n",
      "iteration 41021: loss: 0.2166675329208374\n",
      "iteration 41022: loss: 0.21666741371154785\n",
      "iteration 41023: loss: 0.2166673243045807\n",
      "iteration 41024: loss: 0.21666717529296875\n",
      "iteration 41025: loss: 0.2166670858860016\n",
      "iteration 41026: loss: 0.21666693687438965\n",
      "iteration 41027: loss: 0.21666684746742249\n",
      "iteration 41028: loss: 0.2166668176651001\n",
      "iteration 41029: loss: 0.21666669845581055\n",
      "iteration 41030: loss: 0.21666648983955383\n",
      "iteration 41031: loss: 0.21666643023490906\n",
      "iteration 41032: loss: 0.2166663110256195\n",
      "iteration 41033: loss: 0.21666626632213593\n",
      "iteration 41034: loss: 0.2166661024093628\n",
      "iteration 41035: loss: 0.21666595339775085\n",
      "iteration 41036: loss: 0.2166658639907837\n",
      "iteration 41037: loss: 0.2166658192873001\n",
      "iteration 41038: loss: 0.21666565537452698\n",
      "iteration 41039: loss: 0.21666555106639862\n",
      "iteration 41040: loss: 0.2166653871536255\n",
      "iteration 41041: loss: 0.2166653573513031\n",
      "iteration 41042: loss: 0.21666523814201355\n",
      "iteration 41043: loss: 0.2166651040315628\n",
      "iteration 41044: loss: 0.21666495501995087\n",
      "iteration 41045: loss: 0.21666483581066132\n",
      "iteration 41046: loss: 0.21666474640369415\n",
      "iteration 41047: loss: 0.2166646420955658\n",
      "iteration 41048: loss: 0.21666450798511505\n",
      "iteration 41049: loss: 0.2166643589735031\n",
      "iteration 41050: loss: 0.21666426956653595\n",
      "iteration 41051: loss: 0.21666419506072998\n",
      "iteration 41052: loss: 0.21666410565376282\n",
      "iteration 41053: loss: 0.21666395664215088\n",
      "iteration 41054: loss: 0.21666379272937775\n",
      "iteration 41055: loss: 0.21666379272937775\n",
      "iteration 41056: loss: 0.21666362881660461\n",
      "iteration 41057: loss: 0.21666355431079865\n",
      "iteration 41058: loss: 0.2166634052991867\n",
      "iteration 41059: loss: 0.21666336059570312\n",
      "iteration 41060: loss: 0.2166632115840912\n",
      "iteration 41061: loss: 0.21666309237480164\n",
      "iteration 41062: loss: 0.2166629582643509\n",
      "iteration 41063: loss: 0.2166629135608673\n",
      "iteration 41064: loss: 0.21666273474693298\n",
      "iteration 41065: loss: 0.21666261553764343\n",
      "iteration 41066: loss: 0.21666249632835388\n",
      "iteration 41067: loss: 0.21666240692138672\n",
      "iteration 41068: loss: 0.21666225790977478\n",
      "iteration 41069: loss: 0.21666219830513\n",
      "iteration 41070: loss: 0.21666207909584045\n",
      "iteration 41071: loss: 0.2166619598865509\n",
      "iteration 41072: loss: 0.21666188538074493\n",
      "iteration 41073: loss: 0.216661736369133\n",
      "iteration 41074: loss: 0.21666161715984344\n",
      "iteration 41075: loss: 0.2166614979505539\n",
      "iteration 41076: loss: 0.21666137874126434\n",
      "iteration 41077: loss: 0.2166612595319748\n",
      "iteration 41078: loss: 0.21666112542152405\n",
      "iteration 41079: loss: 0.21666112542152405\n",
      "iteration 41080: loss: 0.21666093170642853\n",
      "iteration 41081: loss: 0.21666088700294495\n",
      "iteration 41082: loss: 0.21666070818901062\n",
      "iteration 41083: loss: 0.21666064858436584\n",
      "iteration 41084: loss: 0.2166605442762375\n",
      "iteration 41085: loss: 0.21666045486927032\n",
      "iteration 41086: loss: 0.2166602909564972\n",
      "iteration 41087: loss: 0.21666018664836884\n",
      "iteration 41088: loss: 0.21666006743907928\n",
      "iteration 41089: loss: 0.21665994822978973\n",
      "iteration 41090: loss: 0.2166597843170166\n",
      "iteration 41091: loss: 0.2166597545146942\n",
      "iteration 41092: loss: 0.2166595757007599\n",
      "iteration 41093: loss: 0.21665950119495392\n",
      "iteration 41094: loss: 0.21665945649147034\n",
      "iteration 41095: loss: 0.21665921807289124\n",
      "iteration 41096: loss: 0.21665915846824646\n",
      "iteration 41097: loss: 0.21665911376476288\n",
      "iteration 41098: loss: 0.21665892004966736\n",
      "iteration 41099: loss: 0.2166587859392166\n",
      "iteration 41100: loss: 0.21665871143341064\n",
      "iteration 41101: loss: 0.21665862202644348\n",
      "iteration 41102: loss: 0.2166585475206375\n",
      "iteration 41103: loss: 0.216658353805542\n",
      "iteration 41104: loss: 0.21665827929973602\n",
      "iteration 41105: loss: 0.21665816009044647\n",
      "iteration 41106: loss: 0.21665802597999573\n",
      "iteration 41107: loss: 0.21665792167186737\n",
      "iteration 41108: loss: 0.2166578322649002\n",
      "iteration 41109: loss: 0.21665772795677185\n",
      "iteration 41110: loss: 0.21665766835212708\n",
      "iteration 41111: loss: 0.21665748953819275\n",
      "iteration 41112: loss: 0.21665744483470917\n",
      "iteration 41113: loss: 0.21665728092193604\n",
      "iteration 41114: loss: 0.21665720641613007\n",
      "iteration 41115: loss: 0.21665701270103455\n",
      "iteration 41116: loss: 0.21665696799755096\n",
      "iteration 41117: loss: 0.216656893491745\n",
      "iteration 41118: loss: 0.21665671467781067\n",
      "iteration 41119: loss: 0.21665659546852112\n",
      "iteration 41120: loss: 0.21665649116039276\n",
      "iteration 41121: loss: 0.2166563719511032\n",
      "iteration 41122: loss: 0.21665628254413605\n",
      "iteration 41123: loss: 0.2166561633348465\n",
      "iteration 41124: loss: 0.21665605902671814\n",
      "iteration 41125: loss: 0.2166559398174286\n",
      "iteration 41126: loss: 0.21665582060813904\n",
      "iteration 41127: loss: 0.21665573120117188\n",
      "iteration 41128: loss: 0.2166556417942047\n",
      "iteration 41129: loss: 0.21665553748607635\n",
      "iteration 41130: loss: 0.21665537357330322\n",
      "iteration 41131: loss: 0.21665532886981964\n",
      "iteration 41132: loss: 0.2166551649570465\n",
      "iteration 41133: loss: 0.21665509045124054\n",
      "iteration 41134: loss: 0.216654971241951\n",
      "iteration 41135: loss: 0.21665482223033905\n",
      "iteration 41136: loss: 0.2166547328233719\n",
      "iteration 41137: loss: 0.21665462851524353\n",
      "iteration 41138: loss: 0.2166544497013092\n",
      "iteration 41139: loss: 0.21665433049201965\n",
      "iteration 41140: loss: 0.21665430068969727\n",
      "iteration 41141: loss: 0.21665410697460175\n",
      "iteration 41142: loss: 0.21665406227111816\n",
      "iteration 41143: loss: 0.21665386855602264\n",
      "iteration 41144: loss: 0.21665382385253906\n",
      "iteration 41145: loss: 0.2166537344455719\n",
      "iteration 41146: loss: 0.21665354073047638\n",
      "iteration 41147: loss: 0.216653510928154\n",
      "iteration 41148: loss: 0.21665334701538086\n",
      "iteration 41149: loss: 0.2166532278060913\n",
      "iteration 41150: loss: 0.21665313839912415\n",
      "iteration 41151: loss: 0.21665310859680176\n",
      "iteration 41152: loss: 0.21665295958518982\n",
      "iteration 41153: loss: 0.2166527956724167\n",
      "iteration 41154: loss: 0.21665272116661072\n",
      "iteration 41155: loss: 0.21665260195732117\n",
      "iteration 41156: loss: 0.21665248274803162\n",
      "iteration 41157: loss: 0.21665242314338684\n",
      "iteration 41158: loss: 0.21665224432945251\n",
      "iteration 41159: loss: 0.21665212512016296\n",
      "iteration 41160: loss: 0.2166520059108734\n",
      "iteration 41161: loss: 0.21665191650390625\n",
      "iteration 41162: loss: 0.2166518270969391\n",
      "iteration 41163: loss: 0.21665170788764954\n",
      "iteration 41164: loss: 0.21665163338184357\n",
      "iteration 41165: loss: 0.21665146946907043\n",
      "iteration 41166: loss: 0.2166513204574585\n",
      "iteration 41167: loss: 0.2166512906551361\n",
      "iteration 41168: loss: 0.21665112674236298\n",
      "iteration 41169: loss: 0.2166510373353958\n",
      "iteration 41170: loss: 0.21665091812610626\n",
      "iteration 41171: loss: 0.2166508138179779\n",
      "iteration 41172: loss: 0.21665069460868835\n",
      "iteration 41173: loss: 0.2166505753993988\n",
      "iteration 41174: loss: 0.21665045619010925\n",
      "iteration 41175: loss: 0.21665039658546448\n",
      "iteration 41176: loss: 0.21665020287036896\n",
      "iteration 41177: loss: 0.21665017306804657\n",
      "iteration 41178: loss: 0.21665005385875702\n",
      "iteration 41179: loss: 0.21664993464946747\n",
      "iteration 41180: loss: 0.21664981544017792\n",
      "iteration 41181: loss: 0.21664972603321075\n",
      "iteration 41182: loss: 0.2166496217250824\n",
      "iteration 41183: loss: 0.21664948761463165\n",
      "iteration 41184: loss: 0.2166493684053421\n",
      "iteration 41185: loss: 0.21664920449256897\n",
      "iteration 41186: loss: 0.2166491001844406\n",
      "iteration 41187: loss: 0.21664901077747345\n",
      "iteration 41188: loss: 0.21664896607398987\n",
      "iteration 41189: loss: 0.21664881706237793\n",
      "iteration 41190: loss: 0.2166486233472824\n",
      "iteration 41191: loss: 0.21664860844612122\n",
      "iteration 41192: loss: 0.21664850413799286\n",
      "iteration 41193: loss: 0.21664834022521973\n",
      "iteration 41194: loss: 0.21664829552173615\n",
      "iteration 41195: loss: 0.216648131608963\n",
      "iteration 41196: loss: 0.21664805710315704\n",
      "iteration 41197: loss: 0.21664802730083466\n",
      "iteration 41198: loss: 0.21664783358573914\n",
      "iteration 41199: loss: 0.216647669672966\n",
      "iteration 41200: loss: 0.21664758026599884\n",
      "iteration 41201: loss: 0.21664747595787048\n",
      "iteration 41202: loss: 0.21664734184741974\n",
      "iteration 41203: loss: 0.21664723753929138\n",
      "iteration 41204: loss: 0.21664711833000183\n",
      "iteration 41205: loss: 0.21664699912071228\n",
      "iteration 41206: loss: 0.21664687991142273\n",
      "iteration 41207: loss: 0.21664679050445557\n",
      "iteration 41208: loss: 0.2166467159986496\n",
      "iteration 41209: loss: 0.21664658188819885\n",
      "iteration 41210: loss: 0.2166464775800705\n",
      "iteration 41211: loss: 0.21664635837078094\n",
      "iteration 41212: loss: 0.21664631366729736\n",
      "iteration 41213: loss: 0.21664610505104065\n",
      "iteration 41214: loss: 0.2166459858417511\n",
      "iteration 41215: loss: 0.21664588153362274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 41216: loss: 0.21664583683013916\n",
      "iteration 41217: loss: 0.216645747423172\n",
      "iteration 41218: loss: 0.21664555370807648\n",
      "iteration 41219: loss: 0.2166455090045929\n",
      "iteration 41220: loss: 0.216645285487175\n",
      "iteration 41221: loss: 0.216645285487175\n",
      "iteration 41222: loss: 0.21664515137672424\n",
      "iteration 41223: loss: 0.21664495766162872\n",
      "iteration 41224: loss: 0.21664492785930634\n",
      "iteration 41225: loss: 0.2166447639465332\n",
      "iteration 41226: loss: 0.21664461493492126\n",
      "iteration 41227: loss: 0.21664457023143768\n",
      "iteration 41228: loss: 0.21664443612098694\n",
      "iteration 41229: loss: 0.21664433181285858\n",
      "iteration 41230: loss: 0.216644287109375\n",
      "iteration 41231: loss: 0.21664413809776306\n",
      "iteration 41232: loss: 0.21664400398731232\n",
      "iteration 41233: loss: 0.21664389967918396\n",
      "iteration 41234: loss: 0.2166437804698944\n",
      "iteration 41235: loss: 0.21664364635944366\n",
      "iteration 41236: loss: 0.2166435271501541\n",
      "iteration 41237: loss: 0.21664342284202576\n",
      "iteration 41238: loss: 0.2166433036327362\n",
      "iteration 41239: loss: 0.216643288731575\n",
      "iteration 41240: loss: 0.21664313971996307\n",
      "iteration 41241: loss: 0.21664300560951233\n",
      "iteration 41242: loss: 0.21664290130138397\n",
      "iteration 41243: loss: 0.21664278209209442\n",
      "iteration 41244: loss: 0.21664269268512726\n",
      "iteration 41245: loss: 0.21664246916770935\n",
      "iteration 41246: loss: 0.2166423797607422\n",
      "iteration 41247: loss: 0.21664229035377502\n",
      "iteration 41248: loss: 0.21664221584796906\n",
      "iteration 41249: loss: 0.21664218604564667\n",
      "iteration 41250: loss: 0.21664199233055115\n",
      "iteration 41251: loss: 0.21664193272590637\n",
      "iteration 41252: loss: 0.21664175391197205\n",
      "iteration 41253: loss: 0.21664169430732727\n",
      "iteration 41254: loss: 0.21664157509803772\n",
      "iteration 41255: loss: 0.21664147078990936\n",
      "iteration 41256: loss: 0.21664133667945862\n",
      "iteration 41257: loss: 0.21664118766784668\n",
      "iteration 41258: loss: 0.21664109826087952\n",
      "iteration 41259: loss: 0.21664099395275116\n",
      "iteration 41260: loss: 0.216640904545784\n",
      "iteration 41261: loss: 0.21664078533649445\n",
      "iteration 41262: loss: 0.2166406661272049\n",
      "iteration 41263: loss: 0.2166406214237213\n",
      "iteration 41264: loss: 0.2166403979063034\n",
      "iteration 41265: loss: 0.21664026379585266\n",
      "iteration 41266: loss: 0.2166401892900467\n",
      "iteration 41267: loss: 0.21664008498191833\n",
      "iteration 41268: loss: 0.2166399508714676\n",
      "iteration 41269: loss: 0.216639906167984\n",
      "iteration 41270: loss: 0.2166396677494049\n",
      "iteration 41271: loss: 0.2166396826505661\n",
      "iteration 41272: loss: 0.21663951873779297\n",
      "iteration 41273: loss: 0.2166394293308258\n",
      "iteration 41274: loss: 0.21663931012153625\n",
      "iteration 41275: loss: 0.21663925051689148\n",
      "iteration 41276: loss: 0.21663911640644073\n",
      "iteration 41277: loss: 0.21663899719715118\n",
      "iteration 41278: loss: 0.21663887798786163\n",
      "iteration 41279: loss: 0.21663875877857208\n",
      "iteration 41280: loss: 0.21663860976696014\n",
      "iteration 41281: loss: 0.2166384905576706\n",
      "iteration 41282: loss: 0.21663841605186462\n",
      "iteration 41283: loss: 0.21663829684257507\n",
      "iteration 41284: loss: 0.21663817763328552\n",
      "iteration 41285: loss: 0.21663808822631836\n",
      "iteration 41286: loss: 0.2166379988193512\n",
      "iteration 41287: loss: 0.21663782000541687\n",
      "iteration 41288: loss: 0.21663780510425568\n",
      "iteration 41289: loss: 0.21663764119148254\n",
      "iteration 41290: loss: 0.2166374921798706\n",
      "iteration 41291: loss: 0.21663741767406464\n",
      "iteration 41292: loss: 0.2166372835636139\n",
      "iteration 41293: loss: 0.21663720905780792\n",
      "iteration 41294: loss: 0.21663713455200195\n",
      "iteration 41295: loss: 0.21663698554039001\n",
      "iteration 41296: loss: 0.21663686633110046\n",
      "iteration 41297: loss: 0.2166367471218109\n",
      "iteration 41298: loss: 0.21663668751716614\n",
      "iteration 41299: loss: 0.21663646399974823\n",
      "iteration 41300: loss: 0.21663637459278107\n",
      "iteration 41301: loss: 0.21663632988929749\n",
      "iteration 41302: loss: 0.21663615107536316\n",
      "iteration 41303: loss: 0.21663610637187958\n",
      "iteration 41304: loss: 0.21663594245910645\n",
      "iteration 41305: loss: 0.21663586795330048\n",
      "iteration 41306: loss: 0.2166357934474945\n",
      "iteration 41307: loss: 0.21663561463356018\n",
      "iteration 41308: loss: 0.2166355550289154\n",
      "iteration 41309: loss: 0.21663546562194824\n",
      "iteration 41310: loss: 0.2166353017091751\n",
      "iteration 41311: loss: 0.21663518249988556\n",
      "iteration 41312: loss: 0.21663503348827362\n",
      "iteration 41313: loss: 0.21663489937782288\n",
      "iteration 41314: loss: 0.2166348248720169\n",
      "iteration 41315: loss: 0.21663475036621094\n",
      "iteration 41316: loss: 0.216634601354599\n",
      "iteration 41317: loss: 0.21663451194763184\n",
      "iteration 41318: loss: 0.2166343629360199\n",
      "iteration 41319: loss: 0.21663424372673035\n",
      "iteration 41320: loss: 0.21663418412208557\n",
      "iteration 41321: loss: 0.21663403511047363\n",
      "iteration 41322: loss: 0.21663391590118408\n",
      "iteration 41323: loss: 0.2166338413953781\n",
      "iteration 41324: loss: 0.21663372218608856\n",
      "iteration 41325: loss: 0.21663358807563782\n",
      "iteration 41326: loss: 0.21663346886634827\n",
      "iteration 41327: loss: 0.2166334092617035\n",
      "iteration 41328: loss: 0.21663329005241394\n",
      "iteration 41329: loss: 0.2166331261396408\n",
      "iteration 41330: loss: 0.21663303673267365\n",
      "iteration 41331: loss: 0.2166328877210617\n",
      "iteration 41332: loss: 0.21663284301757812\n",
      "iteration 41333: loss: 0.21663272380828857\n",
      "iteration 41334: loss: 0.21663260459899902\n",
      "iteration 41335: loss: 0.21663248538970947\n",
      "iteration 41336: loss: 0.21663236618041992\n",
      "iteration 41337: loss: 0.21663229167461395\n",
      "iteration 41338: loss: 0.21663212776184082\n",
      "iteration 41339: loss: 0.21663203835487366\n",
      "iteration 41340: loss: 0.2166319191455841\n",
      "iteration 41341: loss: 0.21663179993629456\n",
      "iteration 41342: loss: 0.2166317254304886\n",
      "iteration 41343: loss: 0.21663160622119904\n",
      "iteration 41344: loss: 0.2166314423084259\n",
      "iteration 41345: loss: 0.21663132309913635\n",
      "iteration 41346: loss: 0.21663124859333038\n",
      "iteration 41347: loss: 0.21663114428520203\n",
      "iteration 41348: loss: 0.2166309803724289\n",
      "iteration 41349: loss: 0.2166309654712677\n",
      "iteration 41350: loss: 0.21663077175617218\n",
      "iteration 41351: loss: 0.2166306972503662\n",
      "iteration 41352: loss: 0.21663060784339905\n",
      "iteration 41353: loss: 0.21663042902946472\n",
      "iteration 41354: loss: 0.21663036942481995\n",
      "iteration 41355: loss: 0.2166302651166916\n",
      "iteration 41356: loss: 0.21663013100624084\n",
      "iteration 41357: loss: 0.2166300266981125\n",
      "iteration 41358: loss: 0.21662995219230652\n",
      "iteration 41359: loss: 0.21662983298301697\n",
      "iteration 41360: loss: 0.21662969887256622\n",
      "iteration 41361: loss: 0.21662959456443787\n",
      "iteration 41362: loss: 0.21662946045398712\n",
      "iteration 41363: loss: 0.2166292667388916\n",
      "iteration 41364: loss: 0.2166292369365692\n",
      "iteration 41365: loss: 0.21662911772727966\n",
      "iteration 41366: loss: 0.2166290581226349\n",
      "iteration 41367: loss: 0.21662887930870056\n",
      "iteration 41368: loss: 0.21662883460521698\n",
      "iteration 41369: loss: 0.21662867069244385\n",
      "iteration 41370: loss: 0.21662858128547668\n",
      "iteration 41371: loss: 0.21662846207618713\n",
      "iteration 41372: loss: 0.21662834286689758\n",
      "iteration 41373: loss: 0.21662816405296326\n",
      "iteration 41374: loss: 0.21662811934947968\n",
      "iteration 41375: loss: 0.21662798523902893\n",
      "iteration 41376: loss: 0.21662792563438416\n",
      "iteration 41377: loss: 0.2166277915239334\n",
      "iteration 41378: loss: 0.21662764251232147\n",
      "iteration 41379: loss: 0.2166275978088379\n",
      "iteration 41380: loss: 0.21662740409374237\n",
      "iteration 41381: loss: 0.2166273295879364\n",
      "iteration 41382: loss: 0.21662719547748566\n",
      "iteration 41383: loss: 0.2166270911693573\n",
      "iteration 41384: loss: 0.21662695705890656\n",
      "iteration 41385: loss: 0.21662688255310059\n",
      "iteration 41386: loss: 0.21662676334381104\n",
      "iteration 41387: loss: 0.21662668883800507\n",
      "iteration 41388: loss: 0.21662655472755432\n",
      "iteration 41389: loss: 0.21662640571594238\n",
      "iteration 41390: loss: 0.21662637591362\n",
      "iteration 41391: loss: 0.21662616729736328\n",
      "iteration 41392: loss: 0.21662607789039612\n",
      "iteration 41393: loss: 0.21662601828575134\n",
      "iteration 41394: loss: 0.2166258990764618\n",
      "iteration 41395: loss: 0.21662577986717224\n",
      "iteration 41396: loss: 0.2166256606578827\n",
      "iteration 41397: loss: 0.21662554144859314\n",
      "iteration 41398: loss: 0.2166254222393036\n",
      "iteration 41399: loss: 0.21662533283233643\n",
      "iteration 41400: loss: 0.21662521362304688\n",
      "iteration 41401: loss: 0.2166251242160797\n",
      "iteration 41402: loss: 0.21662497520446777\n",
      "iteration 41403: loss: 0.21662482619285583\n",
      "iteration 41404: loss: 0.21662473678588867\n",
      "iteration 41405: loss: 0.2166246473789215\n",
      "iteration 41406: loss: 0.21662457287311554\n",
      "iteration 41407: loss: 0.216624453663826\n",
      "iteration 41408: loss: 0.21662433445453644\n",
      "iteration 41409: loss: 0.2166242152452469\n",
      "iteration 41410: loss: 0.21662411093711853\n",
      "iteration 41411: loss: 0.2166239470243454\n",
      "iteration 41412: loss: 0.21662385761737823\n",
      "iteration 41413: loss: 0.21662378311157227\n",
      "iteration 41414: loss: 0.21662363409996033\n",
      "iteration 41415: loss: 0.21662351489067078\n",
      "iteration 41416: loss: 0.2166234254837036\n",
      "iteration 41417: loss: 0.21662330627441406\n",
      "iteration 41418: loss: 0.2166232168674469\n",
      "iteration 41419: loss: 0.21662311255931854\n",
      "iteration 41420: loss: 0.21662287414073944\n",
      "iteration 41421: loss: 0.21662282943725586\n",
      "iteration 41422: loss: 0.21662278473377228\n",
      "iteration 41423: loss: 0.21662263572216034\n",
      "iteration 41424: loss: 0.21662254631519318\n",
      "iteration 41425: loss: 0.21662242710590363\n",
      "iteration 41426: loss: 0.21662220358848572\n",
      "iteration 41427: loss: 0.21662220358848572\n",
      "iteration 41428: loss: 0.21662206947803497\n",
      "iteration 41429: loss: 0.21662196516990662\n",
      "iteration 41430: loss: 0.21662187576293945\n",
      "iteration 41431: loss: 0.21662172675132751\n",
      "iteration 41432: loss: 0.21662159264087677\n",
      "iteration 41433: loss: 0.21662147343158722\n",
      "iteration 41434: loss: 0.21662139892578125\n",
      "iteration 41435: loss: 0.2166212797164917\n",
      "iteration 41436: loss: 0.21662120521068573\n",
      "iteration 41437: loss: 0.2166210412979126\n",
      "iteration 41438: loss: 0.21662089228630066\n",
      "iteration 41439: loss: 0.21662083268165588\n",
      "iteration 41440: loss: 0.21662063896656036\n",
      "iteration 41441: loss: 0.21662060916423798\n",
      "iteration 41442: loss: 0.21662044525146484\n",
      "iteration 41443: loss: 0.21662041544914246\n",
      "iteration 41444: loss: 0.21662023663520813\n",
      "iteration 41445: loss: 0.21662016212940216\n",
      "iteration 41446: loss: 0.21661996841430664\n",
      "iteration 41447: loss: 0.21661993861198425\n",
      "iteration 41448: loss: 0.2166198492050171\n",
      "iteration 41449: loss: 0.21661964058876038\n",
      "iteration 41450: loss: 0.2166195809841156\n",
      "iteration 41451: loss: 0.21661953628063202\n",
      "iteration 41452: loss: 0.21661941707134247\n",
      "iteration 41453: loss: 0.2166193276643753\n",
      "iteration 41454: loss: 0.21661916375160217\n",
      "iteration 41455: loss: 0.21661905944347382\n",
      "iteration 41456: loss: 0.21661894023418427\n",
      "iteration 41457: loss: 0.21661880612373352\n",
      "iteration 41458: loss: 0.21661868691444397\n",
      "iteration 41459: loss: 0.21661856770515442\n",
      "iteration 41460: loss: 0.21661849319934845\n",
      "iteration 41461: loss: 0.2166183441877365\n",
      "iteration 41462: loss: 0.21661829948425293\n",
      "iteration 41463: loss: 0.21661821007728577\n",
      "iteration 41464: loss: 0.21661806106567383\n",
      "iteration 41465: loss: 0.2166179120540619\n",
      "iteration 41466: loss: 0.21661777794361115\n",
      "iteration 41467: loss: 0.2166176289319992\n",
      "iteration 41468: loss: 0.216617614030838\n",
      "iteration 41469: loss: 0.21661753952503204\n",
      "iteration 41470: loss: 0.2166174203157425\n",
      "iteration 41471: loss: 0.21661722660064697\n",
      "iteration 41472: loss: 0.2166171371936798\n",
      "iteration 41473: loss: 0.21661706268787384\n",
      "iteration 41474: loss: 0.21661686897277832\n",
      "iteration 41475: loss: 0.21661682426929474\n",
      "iteration 41476: loss: 0.2166166603565216\n",
      "iteration 41477: loss: 0.21661655604839325\n",
      "iteration 41478: loss: 0.21661648154258728\n",
      "iteration 41479: loss: 0.21661631762981415\n",
      "iteration 41480: loss: 0.2166161984205246\n",
      "iteration 41481: loss: 0.21661615371704102\n",
      "iteration 41482: loss: 0.21661598980426788\n",
      "iteration 41483: loss: 0.21661591529846191\n",
      "iteration 41484: loss: 0.21661582589149475\n",
      "iteration 41485: loss: 0.2166157066822052\n",
      "iteration 41486: loss: 0.21661555767059326\n",
      "iteration 41487: loss: 0.21661540865898132\n",
      "iteration 41488: loss: 0.21661528944969177\n",
      "iteration 41489: loss: 0.21661528944969177\n",
      "iteration 41490: loss: 0.21661512553691864\n",
      "iteration 41491: loss: 0.21661505103111267\n",
      "iteration 41492: loss: 0.21661488711833954\n",
      "iteration 41493: loss: 0.2166147232055664\n",
      "iteration 41494: loss: 0.21661463379859924\n",
      "iteration 41495: loss: 0.2166145145893097\n",
      "iteration 41496: loss: 0.21661441028118134\n",
      "iteration 41497: loss: 0.2166142761707306\n",
      "iteration 41498: loss: 0.2166142463684082\n",
      "iteration 41499: loss: 0.21661417186260223\n",
      "iteration 41500: loss: 0.2166139781475067\n",
      "iteration 41501: loss: 0.21661385893821716\n",
      "iteration 41502: loss: 0.21661376953125\n",
      "iteration 41503: loss: 0.21661365032196045\n",
      "iteration 41504: loss: 0.2166135013103485\n",
      "iteration 41505: loss: 0.21661338210105896\n",
      "iteration 41506: loss: 0.21661324799060822\n",
      "iteration 41507: loss: 0.21661321818828583\n",
      "iteration 41508: loss: 0.21661312878131866\n",
      "iteration 41509: loss: 0.21661293506622314\n",
      "iteration 41510: loss: 0.2166128158569336\n",
      "iteration 41511: loss: 0.21661269664764404\n",
      "iteration 41512: loss: 0.21661262214183807\n",
      "iteration 41513: loss: 0.2166125327348709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 41514: loss: 0.21661242842674255\n",
      "iteration 41515: loss: 0.2166123390197754\n",
      "iteration 41516: loss: 0.21661219000816345\n",
      "iteration 41517: loss: 0.2166120707988739\n",
      "iteration 41518: loss: 0.21661193668842316\n",
      "iteration 41519: loss: 0.2166118174791336\n",
      "iteration 41520: loss: 0.21661166846752167\n",
      "iteration 41521: loss: 0.21661162376403809\n",
      "iteration 41522: loss: 0.21661154925823212\n",
      "iteration 41523: loss: 0.2166113555431366\n",
      "iteration 41524: loss: 0.21661131083965302\n",
      "iteration 41525: loss: 0.21661114692687988\n",
      "iteration 41526: loss: 0.21661105751991272\n",
      "iteration 41527: loss: 0.21661093831062317\n",
      "iteration 41528: loss: 0.21661081910133362\n",
      "iteration 41529: loss: 0.21661071479320526\n",
      "iteration 41530: loss: 0.2166106253862381\n",
      "iteration 41531: loss: 0.21661047637462616\n",
      "iteration 41532: loss: 0.216610386967659\n",
      "iteration 41533: loss: 0.21661031246185303\n",
      "iteration 41534: loss: 0.21661019325256348\n",
      "iteration 41535: loss: 0.21661002933979034\n",
      "iteration 41536: loss: 0.21660995483398438\n",
      "iteration 41537: loss: 0.2166098803281784\n",
      "iteration 41538: loss: 0.21660971641540527\n",
      "iteration 41539: loss: 0.2166096270084381\n",
      "iteration 41540: loss: 0.21660952270030975\n",
      "iteration 41541: loss: 0.2166094034910202\n",
      "iteration 41542: loss: 0.21660923957824707\n",
      "iteration 41543: loss: 0.2166091948747635\n",
      "iteration 41544: loss: 0.21660900115966797\n",
      "iteration 41545: loss: 0.21660900115966797\n",
      "iteration 41546: loss: 0.21660879254341125\n",
      "iteration 41547: loss: 0.21660873293876648\n",
      "iteration 41548: loss: 0.21660856902599335\n",
      "iteration 41549: loss: 0.21660840511322021\n",
      "iteration 41550: loss: 0.21660833060741425\n",
      "iteration 41551: loss: 0.2166082113981247\n",
      "iteration 41552: loss: 0.21660813689231873\n",
      "iteration 41553: loss: 0.2166079729795456\n",
      "iteration 41554: loss: 0.21660788357257843\n",
      "iteration 41555: loss: 0.21660777926445007\n",
      "iteration 41556: loss: 0.2166076898574829\n",
      "iteration 41557: loss: 0.21660760045051575\n",
      "iteration 41558: loss: 0.21660757064819336\n",
      "iteration 41559: loss: 0.21660733222961426\n",
      "iteration 41560: loss: 0.21660718321800232\n",
      "iteration 41561: loss: 0.21660712361335754\n",
      "iteration 41562: loss: 0.2166070193052292\n",
      "iteration 41563: loss: 0.21660688519477844\n",
      "iteration 41564: loss: 0.21660682559013367\n",
      "iteration 41565: loss: 0.21660669147968292\n",
      "iteration 41566: loss: 0.21660654246807098\n",
      "iteration 41567: loss: 0.21660646796226501\n",
      "iteration 41568: loss: 0.21660634875297546\n",
      "iteration 41569: loss: 0.2166062593460083\n",
      "iteration 41570: loss: 0.21660611033439636\n",
      "iteration 41571: loss: 0.2166059911251068\n",
      "iteration 41572: loss: 0.21660590171813965\n",
      "iteration 41573: loss: 0.2166057527065277\n",
      "iteration 41574: loss: 0.21660566329956055\n",
      "iteration 41575: loss: 0.21660557389259338\n",
      "iteration 41576: loss: 0.21660539507865906\n",
      "iteration 41577: loss: 0.21660535037517548\n",
      "iteration 41578: loss: 0.21660515666007996\n",
      "iteration 41579: loss: 0.2166050374507904\n",
      "iteration 41580: loss: 0.21660499274730682\n",
      "iteration 41581: loss: 0.21660485863685608\n",
      "iteration 41582: loss: 0.2166047990322113\n",
      "iteration 41583: loss: 0.21660462021827698\n",
      "iteration 41584: loss: 0.2166045606136322\n",
      "iteration 41585: loss: 0.21660444140434265\n",
      "iteration 41586: loss: 0.2166043072938919\n",
      "iteration 41587: loss: 0.21660423278808594\n",
      "iteration 41588: loss: 0.2166040688753128\n",
      "iteration 41589: loss: 0.21660402417182922\n",
      "iteration 41590: loss: 0.21660391986370087\n",
      "iteration 41591: loss: 0.21660372614860535\n",
      "iteration 41592: loss: 0.21660366654396057\n",
      "iteration 41593: loss: 0.21660354733467102\n",
      "iteration 41594: loss: 0.21660344302654266\n",
      "iteration 41595: loss: 0.21660327911376953\n",
      "iteration 41596: loss: 0.21660318970680237\n",
      "iteration 41597: loss: 0.2166031301021576\n",
      "iteration 41598: loss: 0.21660296618938446\n",
      "iteration 41599: loss: 0.2166028767824173\n",
      "iteration 41600: loss: 0.21660275757312775\n",
      "iteration 41601: loss: 0.21660256385803223\n",
      "iteration 41602: loss: 0.21660248935222626\n",
      "iteration 41603: loss: 0.2166024148464203\n",
      "iteration 41604: loss: 0.21660225093364716\n",
      "iteration 41605: loss: 0.21660216152668\n",
      "iteration 41606: loss: 0.21660205721855164\n",
      "iteration 41607: loss: 0.21660196781158447\n",
      "iteration 41608: loss: 0.21660181879997253\n",
      "iteration 41609: loss: 0.21660175919532776\n",
      "iteration 41610: loss: 0.21660158038139343\n",
      "iteration 41611: loss: 0.21660152077674866\n",
      "iteration 41612: loss: 0.2166014015674591\n",
      "iteration 41613: loss: 0.21660122275352478\n",
      "iteration 41614: loss: 0.21660116314888\n",
      "iteration 41615: loss: 0.21660098433494568\n",
      "iteration 41616: loss: 0.21660086512565613\n",
      "iteration 41617: loss: 0.21660080552101135\n",
      "iteration 41618: loss: 0.21660077571868896\n",
      "iteration 41619: loss: 0.21660053730010986\n",
      "iteration 41620: loss: 0.2166004627943039\n",
      "iteration 41621: loss: 0.21660037338733673\n",
      "iteration 41622: loss: 0.21660026907920837\n",
      "iteration 41623: loss: 0.21660010516643524\n",
      "iteration 41624: loss: 0.2165999859571457\n",
      "iteration 41625: loss: 0.21659991145133972\n",
      "iteration 41626: loss: 0.21659979224205017\n",
      "iteration 41627: loss: 0.2165997326374054\n",
      "iteration 41628: loss: 0.21659962832927704\n",
      "iteration 41629: loss: 0.21659943461418152\n",
      "iteration 41630: loss: 0.21659934520721436\n",
      "iteration 41631: loss: 0.2165992259979248\n",
      "iteration 41632: loss: 0.21659907698631287\n",
      "iteration 41633: loss: 0.2165989875793457\n",
      "iteration 41634: loss: 0.21659883856773376\n",
      "iteration 41635: loss: 0.21659879386425018\n",
      "iteration 41636: loss: 0.21659862995147705\n",
      "iteration 41637: loss: 0.21659855544567108\n",
      "iteration 41638: loss: 0.2165984809398651\n",
      "iteration 41639: loss: 0.21659836173057556\n",
      "iteration 41640: loss: 0.21659818291664124\n",
      "iteration 41641: loss: 0.21659812331199646\n",
      "iteration 41642: loss: 0.21659806370735168\n",
      "iteration 41643: loss: 0.21659794449806213\n",
      "iteration 41644: loss: 0.2165977656841278\n",
      "iteration 41645: loss: 0.21659760177135468\n",
      "iteration 41646: loss: 0.2165975570678711\n",
      "iteration 41647: loss: 0.21659740805625916\n",
      "iteration 41648: loss: 0.21659724414348602\n",
      "iteration 41649: loss: 0.21659719944000244\n",
      "iteration 41650: loss: 0.21659715473651886\n",
      "iteration 41651: loss: 0.21659700572490692\n",
      "iteration 41652: loss: 0.21659693121910095\n",
      "iteration 41653: loss: 0.21659676730632782\n",
      "iteration 41654: loss: 0.2165966033935547\n",
      "iteration 41655: loss: 0.2165965586900711\n",
      "iteration 41656: loss: 0.21659643948078156\n",
      "iteration 41657: loss: 0.21659639477729797\n",
      "iteration 41658: loss: 0.21659617125988007\n",
      "iteration 41659: loss: 0.2165960967540741\n",
      "iteration 41660: loss: 0.21659600734710693\n",
      "iteration 41661: loss: 0.2165958434343338\n",
      "iteration 41662: loss: 0.21659569442272186\n",
      "iteration 41663: loss: 0.21659567952156067\n",
      "iteration 41664: loss: 0.21659553050994873\n",
      "iteration 41665: loss: 0.21659541130065918\n",
      "iteration 41666: loss: 0.21659526228904724\n",
      "iteration 41667: loss: 0.21659517288208008\n",
      "iteration 41668: loss: 0.21659500896930695\n",
      "iteration 41669: loss: 0.21659496426582336\n",
      "iteration 41670: loss: 0.2165948450565338\n",
      "iteration 41671: loss: 0.21659474074840546\n",
      "iteration 41672: loss: 0.2165946066379547\n",
      "iteration 41673: loss: 0.21659454703330994\n",
      "iteration 41674: loss: 0.2165943682193756\n",
      "iteration 41675: loss: 0.21659430861473083\n",
      "iteration 41676: loss: 0.2165941745042801\n",
      "iteration 41677: loss: 0.21659401059150696\n",
      "iteration 41678: loss: 0.216593936085701\n",
      "iteration 41679: loss: 0.21659383177757263\n",
      "iteration 41680: loss: 0.2165936529636383\n",
      "iteration 41681: loss: 0.21659357845783234\n",
      "iteration 41682: loss: 0.21659345924854279\n",
      "iteration 41683: loss: 0.21659331023693085\n",
      "iteration 41684: loss: 0.21659326553344727\n",
      "iteration 41685: loss: 0.21659311652183533\n",
      "iteration 41686: loss: 0.21659305691719055\n",
      "iteration 41687: loss: 0.216592937707901\n",
      "iteration 41688: loss: 0.21659281849861145\n",
      "iteration 41689: loss: 0.2165927141904831\n",
      "iteration 41690: loss: 0.21659259498119354\n",
      "iteration 41691: loss: 0.2165924608707428\n",
      "iteration 41692: loss: 0.21659238636493683\n",
      "iteration 41693: loss: 0.2165921926498413\n",
      "iteration 41694: loss: 0.21659214794635773\n",
      "iteration 41695: loss: 0.2165919989347458\n",
      "iteration 41696: loss: 0.21659187972545624\n",
      "iteration 41697: loss: 0.2165917158126831\n",
      "iteration 41698: loss: 0.21659164130687714\n",
      "iteration 41699: loss: 0.21659159660339355\n",
      "iteration 41700: loss: 0.21659143269062042\n",
      "iteration 41701: loss: 0.21659128367900848\n",
      "iteration 41702: loss: 0.2165912687778473\n",
      "iteration 41703: loss: 0.21659108996391296\n",
      "iteration 41704: loss: 0.2165909707546234\n",
      "iteration 41705: loss: 0.21659091114997864\n",
      "iteration 41706: loss: 0.21659080684185028\n",
      "iteration 41707: loss: 0.21659061312675476\n",
      "iteration 41708: loss: 0.21659055352210999\n",
      "iteration 41709: loss: 0.21659040451049805\n",
      "iteration 41710: loss: 0.21659031510353088\n",
      "iteration 41711: loss: 0.21659019589424133\n",
      "iteration 41712: loss: 0.21659012138843536\n",
      "iteration 41713: loss: 0.21658997237682343\n",
      "iteration 41714: loss: 0.21658985316753387\n",
      "iteration 41715: loss: 0.2165897786617279\n",
      "iteration 41716: loss: 0.21658968925476074\n",
      "iteration 41717: loss: 0.2165895402431488\n",
      "iteration 41718: loss: 0.21658942103385925\n",
      "iteration 41719: loss: 0.2165893018245697\n",
      "iteration 41720: loss: 0.21658916771411896\n",
      "iteration 41721: loss: 0.2165890634059906\n",
      "iteration 41722: loss: 0.2165890485048294\n",
      "iteration 41723: loss: 0.2165888547897339\n",
      "iteration 41724: loss: 0.21658870577812195\n",
      "iteration 41725: loss: 0.21658854186534882\n",
      "iteration 41726: loss: 0.21658849716186523\n",
      "iteration 41727: loss: 0.21658840775489807\n",
      "iteration 41728: loss: 0.21658830344676971\n",
      "iteration 41729: loss: 0.2165881097316742\n",
      "iteration 41730: loss: 0.216588094830513\n",
      "iteration 41731: loss: 0.21658797562122345\n",
      "iteration 41732: loss: 0.21658781170845032\n",
      "iteration 41733: loss: 0.216587632894516\n",
      "iteration 41734: loss: 0.21658769249916077\n",
      "iteration 41735: loss: 0.21658746898174286\n",
      "iteration 41736: loss: 0.2165873795747757\n",
      "iteration 41737: loss: 0.21658726036548615\n",
      "iteration 41738: loss: 0.21658721566200256\n",
      "iteration 41739: loss: 0.21658706665039062\n",
      "iteration 41740: loss: 0.21658699214458466\n",
      "iteration 41741: loss: 0.21658682823181152\n",
      "iteration 41742: loss: 0.21658667922019958\n",
      "iteration 41743: loss: 0.216586634516716\n",
      "iteration 41744: loss: 0.21658651530742645\n",
      "iteration 41745: loss: 0.21658632159233093\n",
      "iteration 41746: loss: 0.21658620238304138\n",
      "iteration 41747: loss: 0.2165861576795578\n",
      "iteration 41748: loss: 0.21658596396446228\n",
      "iteration 41749: loss: 0.2165859192609787\n",
      "iteration 41750: loss: 0.21658578515052795\n",
      "iteration 41751: loss: 0.21658571064472198\n",
      "iteration 41752: loss: 0.21658560633659363\n",
      "iteration 41753: loss: 0.21658547222614288\n",
      "iteration 41754: loss: 0.21658532321453094\n",
      "iteration 41755: loss: 0.21658524870872498\n",
      "iteration 41756: loss: 0.21658512949943542\n",
      "iteration 41757: loss: 0.21658501029014587\n",
      "iteration 41758: loss: 0.21658489108085632\n",
      "iteration 41759: loss: 0.21658477187156677\n",
      "iteration 41760: loss: 0.2165846824645996\n",
      "iteration 41761: loss: 0.21658453345298767\n",
      "iteration 41762: loss: 0.2165844440460205\n",
      "iteration 41763: loss: 0.21658429503440857\n",
      "iteration 41764: loss: 0.21658417582511902\n",
      "iteration 41765: loss: 0.21658416092395782\n",
      "iteration 41766: loss: 0.2165839970111847\n",
      "iteration 41767: loss: 0.21658392250537872\n",
      "iteration 41768: loss: 0.21658377349376678\n",
      "iteration 41769: loss: 0.21658360958099365\n",
      "iteration 41770: loss: 0.21658353507518768\n",
      "iteration 41771: loss: 0.2165834605693817\n",
      "iteration 41772: loss: 0.21658332645893097\n",
      "iteration 41773: loss: 0.21658317744731903\n",
      "iteration 41774: loss: 0.21658310294151306\n",
      "iteration 41775: loss: 0.21658293902873993\n",
      "iteration 41776: loss: 0.21658286452293396\n",
      "iteration 41777: loss: 0.2165827751159668\n",
      "iteration 41778: loss: 0.21658262610435486\n",
      "iteration 41779: loss: 0.21658246219158173\n",
      "iteration 41780: loss: 0.21658237278461456\n",
      "iteration 41781: loss: 0.216582253575325\n",
      "iteration 41782: loss: 0.21658220887184143\n",
      "iteration 41783: loss: 0.21658208966255188\n",
      "iteration 41784: loss: 0.21658197045326233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 41785: loss: 0.21658185124397278\n",
      "iteration 41786: loss: 0.2165817767381668\n",
      "iteration 41787: loss: 0.21658162772655487\n",
      "iteration 41788: loss: 0.21658150851726532\n",
      "iteration 41789: loss: 0.21658137440681458\n",
      "iteration 41790: loss: 0.2165813148021698\n",
      "iteration 41791: loss: 0.21658119559288025\n",
      "iteration 41792: loss: 0.21658103168010712\n",
      "iteration 41793: loss: 0.21658094227313995\n",
      "iteration 41794: loss: 0.21658079326152802\n",
      "iteration 41795: loss: 0.21658070385456085\n",
      "iteration 41796: loss: 0.2165805995464325\n",
      "iteration 41797: loss: 0.21658048033714294\n",
      "iteration 41798: loss: 0.2165803611278534\n",
      "iteration 41799: loss: 0.21658027172088623\n",
      "iteration 41800: loss: 0.21658018231391907\n",
      "iteration 41801: loss: 0.21658000349998474\n",
      "iteration 41802: loss: 0.21657991409301758\n",
      "iteration 41803: loss: 0.21657975018024445\n",
      "iteration 41804: loss: 0.21657970547676086\n",
      "iteration 41805: loss: 0.21657955646514893\n",
      "iteration 41806: loss: 0.21657943725585938\n",
      "iteration 41807: loss: 0.2165793478488922\n",
      "iteration 41808: loss: 0.21657924354076385\n",
      "iteration 41809: loss: 0.2165791541337967\n",
      "iteration 41810: loss: 0.21657900512218475\n",
      "iteration 41811: loss: 0.2165789157152176\n",
      "iteration 41812: loss: 0.21657875180244446\n",
      "iteration 41813: loss: 0.21657869219779968\n",
      "iteration 41814: loss: 0.21657852828502655\n",
      "iteration 41815: loss: 0.21657845377922058\n",
      "iteration 41816: loss: 0.2165783941745758\n",
      "iteration 41817: loss: 0.21657820045948029\n",
      "iteration 41818: loss: 0.21657808125019073\n",
      "iteration 41819: loss: 0.2165779322385788\n",
      "iteration 41820: loss: 0.21657784283161163\n",
      "iteration 41821: loss: 0.21657773852348328\n",
      "iteration 41822: loss: 0.2165776491165161\n",
      "iteration 41823: loss: 0.2165776491165161\n",
      "iteration 41824: loss: 0.2165774554014206\n",
      "iteration 41825: loss: 0.21657733619213104\n",
      "iteration 41826: loss: 0.2165772020816803\n",
      "iteration 41827: loss: 0.21657709777355194\n",
      "iteration 41828: loss: 0.2165769785642624\n",
      "iteration 41829: loss: 0.21657685935497284\n",
      "iteration 41830: loss: 0.21657676994800568\n",
      "iteration 41831: loss: 0.2165766954421997\n",
      "iteration 41832: loss: 0.216576486825943\n",
      "iteration 41833: loss: 0.21657636761665344\n",
      "iteration 41834: loss: 0.21657629311084747\n",
      "iteration 41835: loss: 0.21657614409923553\n",
      "iteration 41836: loss: 0.21657606959342957\n",
      "iteration 41837: loss: 0.21657586097717285\n",
      "iteration 41838: loss: 0.21657586097717285\n",
      "iteration 41839: loss: 0.21657566726207733\n",
      "iteration 41840: loss: 0.21657562255859375\n",
      "iteration 41841: loss: 0.2165755331516266\n",
      "iteration 41842: loss: 0.21657545864582062\n",
      "iteration 41843: loss: 0.21657533943653107\n",
      "iteration 41844: loss: 0.21657514572143555\n",
      "iteration 41845: loss: 0.216575026512146\n",
      "iteration 41846: loss: 0.21657493710517883\n",
      "iteration 41847: loss: 0.21657486259937286\n",
      "iteration 41848: loss: 0.21657471358776093\n",
      "iteration 41849: loss: 0.21657462418079376\n",
      "iteration 41850: loss: 0.2165745049715042\n",
      "iteration 41851: loss: 0.21657438576221466\n",
      "iteration 41852: loss: 0.2165742665529251\n",
      "iteration 41853: loss: 0.21657414734363556\n",
      "iteration 41854: loss: 0.2165740430355072\n",
      "iteration 41855: loss: 0.21657390892505646\n",
      "iteration 41856: loss: 0.21657375991344452\n",
      "iteration 41857: loss: 0.21657371520996094\n",
      "iteration 41858: loss: 0.21657362580299377\n",
      "iteration 41859: loss: 0.21657347679138184\n",
      "iteration 41860: loss: 0.2165733128786087\n",
      "iteration 41861: loss: 0.21657323837280273\n",
      "iteration 41862: loss: 0.21657316386699677\n",
      "iteration 41863: loss: 0.21657297015190125\n",
      "iteration 41864: loss: 0.21657292544841766\n",
      "iteration 41865: loss: 0.2165728062391281\n",
      "iteration 41866: loss: 0.21657267212867737\n",
      "iteration 41867: loss: 0.2165725976228714\n",
      "iteration 41868: loss: 0.21657244861125946\n",
      "iteration 41869: loss: 0.21657240390777588\n",
      "iteration 41870: loss: 0.21657225489616394\n",
      "iteration 41871: loss: 0.21657207608222961\n",
      "iteration 41872: loss: 0.21657201647758484\n",
      "iteration 41873: loss: 0.21657195687294006\n",
      "iteration 41874: loss: 0.21657171845436096\n",
      "iteration 41875: loss: 0.2165716588497162\n",
      "iteration 41876: loss: 0.21657153964042664\n",
      "iteration 41877: loss: 0.21657142043113708\n",
      "iteration 41878: loss: 0.21657128632068634\n",
      "iteration 41879: loss: 0.21657118201255798\n",
      "iteration 41880: loss: 0.21657109260559082\n",
      "iteration 41881: loss: 0.21657097339630127\n",
      "iteration 41882: loss: 0.21657080948352814\n",
      "iteration 41883: loss: 0.21657070517539978\n",
      "iteration 41884: loss: 0.21657058596611023\n",
      "iteration 41885: loss: 0.21657057106494904\n",
      "iteration 41886: loss: 0.2165704220533371\n",
      "iteration 41887: loss: 0.21657030284404755\n",
      "iteration 41888: loss: 0.21657013893127441\n",
      "iteration 41889: loss: 0.21657004952430725\n",
      "iteration 41890: loss: 0.2165699005126953\n",
      "iteration 41891: loss: 0.21656987071037292\n",
      "iteration 41892: loss: 0.2165696620941162\n",
      "iteration 41893: loss: 0.21656961739063263\n",
      "iteration 41894: loss: 0.21656949818134308\n",
      "iteration 41895: loss: 0.21656937897205353\n",
      "iteration 41896: loss: 0.2165692299604416\n",
      "iteration 41897: loss: 0.21656911075115204\n",
      "iteration 41898: loss: 0.21656903624534607\n",
      "iteration 41899: loss: 0.21656891703605652\n",
      "iteration 41900: loss: 0.21656882762908936\n",
      "iteration 41901: loss: 0.2165687382221222\n",
      "iteration 41902: loss: 0.21656851470470428\n",
      "iteration 41903: loss: 0.2165684700012207\n",
      "iteration 41904: loss: 0.21656835079193115\n",
      "iteration 41905: loss: 0.2165682315826416\n",
      "iteration 41906: loss: 0.21656814217567444\n",
      "iteration 41907: loss: 0.2165680229663849\n",
      "iteration 41908: loss: 0.21656791865825653\n",
      "iteration 41909: loss: 0.21656782925128937\n",
      "iteration 41910: loss: 0.21656768023967743\n",
      "iteration 41911: loss: 0.21656759083271027\n",
      "iteration 41912: loss: 0.21656744182109833\n",
      "iteration 41913: loss: 0.21656732261180878\n",
      "iteration 41914: loss: 0.21656720340251923\n",
      "iteration 41915: loss: 0.21656712889671326\n",
      "iteration 41916: loss: 0.2165670096874237\n",
      "iteration 41917: loss: 0.21656684577465057\n",
      "iteration 41918: loss: 0.2165667712688446\n",
      "iteration 41919: loss: 0.21656663715839386\n",
      "iteration 41920: loss: 0.2165665626525879\n",
      "iteration 41921: loss: 0.21656641364097595\n",
      "iteration 41922: loss: 0.2165663242340088\n",
      "iteration 41923: loss: 0.21656620502471924\n",
      "iteration 41924: loss: 0.21656613051891327\n",
      "iteration 41925: loss: 0.21656596660614014\n",
      "iteration 41926: loss: 0.21656584739685059\n",
      "iteration 41927: loss: 0.2165658175945282\n",
      "iteration 41928: loss: 0.21656569838523865\n",
      "iteration 41929: loss: 0.21656551957130432\n",
      "iteration 41930: loss: 0.21656545996665955\n",
      "iteration 41931: loss: 0.2165653258562088\n",
      "iteration 41932: loss: 0.21656517684459686\n",
      "iteration 41933: loss: 0.21656498312950134\n",
      "iteration 41934: loss: 0.21656496822834015\n",
      "iteration 41935: loss: 0.2165648192167282\n",
      "iteration 41936: loss: 0.21656470000743866\n",
      "iteration 41937: loss: 0.21656468510627747\n",
      "iteration 41938: loss: 0.21656446158885956\n",
      "iteration 41939: loss: 0.2165643721818924\n",
      "iteration 41940: loss: 0.2165643274784088\n",
      "iteration 41941: loss: 0.21656420826911926\n",
      "iteration 41942: loss: 0.21656401455402374\n",
      "iteration 41943: loss: 0.21656394004821777\n",
      "iteration 41944: loss: 0.21656382083892822\n",
      "iteration 41945: loss: 0.21656370162963867\n",
      "iteration 41946: loss: 0.21656355261802673\n",
      "iteration 41947: loss: 0.21656346321105957\n",
      "iteration 41948: loss: 0.21656343340873718\n",
      "iteration 41949: loss: 0.21656322479248047\n",
      "iteration 41950: loss: 0.2165631502866745\n",
      "iteration 41951: loss: 0.21656298637390137\n",
      "iteration 41952: loss: 0.21656283736228943\n",
      "iteration 41953: loss: 0.21656277775764465\n",
      "iteration 41954: loss: 0.21656262874603271\n",
      "iteration 41955: loss: 0.21656255424022675\n",
      "iteration 41956: loss: 0.216562420129776\n",
      "iteration 41957: loss: 0.21656231582164764\n",
      "iteration 41958: loss: 0.21656224131584167\n",
      "iteration 41959: loss: 0.21656207740306854\n",
      "iteration 41960: loss: 0.21656206250190735\n",
      "iteration 41961: loss: 0.21656183898448944\n",
      "iteration 41962: loss: 0.2165617197751999\n",
      "iteration 41963: loss: 0.2165617048740387\n",
      "iteration 41964: loss: 0.21656155586242676\n",
      "iteration 41965: loss: 0.2165614664554596\n",
      "iteration 41966: loss: 0.21656128764152527\n",
      "iteration 41967: loss: 0.2165612280368805\n",
      "iteration 41968: loss: 0.21656107902526855\n",
      "iteration 41969: loss: 0.216560959815979\n",
      "iteration 41970: loss: 0.21656084060668945\n",
      "iteration 41971: loss: 0.2165607511997223\n",
      "iteration 41972: loss: 0.21656063199043274\n",
      "iteration 41973: loss: 0.2165604829788208\n",
      "iteration 41974: loss: 0.21656043827533722\n",
      "iteration 41975: loss: 0.21656028926372528\n",
      "iteration 41976: loss: 0.21656019985675812\n",
      "iteration 41977: loss: 0.21656012535095215\n",
      "iteration 41978: loss: 0.21655996143817902\n",
      "iteration 41979: loss: 0.21655984222888947\n",
      "iteration 41980: loss: 0.2165597379207611\n",
      "iteration 41981: loss: 0.21655964851379395\n",
      "iteration 41982: loss: 0.216559499502182\n",
      "iteration 41983: loss: 0.21655936539173126\n",
      "iteration 41984: loss: 0.2165592908859253\n",
      "iteration 41985: loss: 0.21655912697315216\n",
      "iteration 41986: loss: 0.2165590524673462\n",
      "iteration 41987: loss: 0.21655888855457306\n",
      "iteration 41988: loss: 0.2165587842464447\n",
      "iteration 41989: loss: 0.21655869483947754\n",
      "iteration 41990: loss: 0.2165585458278656\n",
      "iteration 41991: loss: 0.21655850112438202\n",
      "iteration 41992: loss: 0.21655838191509247\n",
      "iteration 41993: loss: 0.21655818819999695\n",
      "iteration 41994: loss: 0.21655812859535217\n",
      "iteration 41995: loss: 0.2165580540895462\n",
      "iteration 41996: loss: 0.21655789017677307\n",
      "iteration 41997: loss: 0.2165578156709671\n",
      "iteration 41998: loss: 0.21655774116516113\n",
      "iteration 41999: loss: 0.216557577252388\n",
      "iteration 42000: loss: 0.21655750274658203\n",
      "iteration 42001: loss: 0.2165573537349701\n",
      "iteration 42002: loss: 0.21655721962451935\n",
      "iteration 42003: loss: 0.21655714511871338\n",
      "iteration 42004: loss: 0.21655699610710144\n",
      "iteration 42005: loss: 0.2165568768978119\n",
      "iteration 42006: loss: 0.21655669808387756\n",
      "iteration 42007: loss: 0.2165566235780716\n",
      "iteration 42008: loss: 0.21655650436878204\n",
      "iteration 42009: loss: 0.2165563851594925\n",
      "iteration 42010: loss: 0.2165563404560089\n",
      "iteration 42011: loss: 0.21655622124671936\n",
      "iteration 42012: loss: 0.216556116938591\n",
      "iteration 42013: loss: 0.21655592322349548\n",
      "iteration 42014: loss: 0.2165558785200119\n",
      "iteration 42015: loss: 0.21655574440956116\n",
      "iteration 42016: loss: 0.21655559539794922\n",
      "iteration 42017: loss: 0.21655559539794922\n",
      "iteration 42018: loss: 0.2165554314851761\n",
      "iteration 42019: loss: 0.21655526757240295\n",
      "iteration 42020: loss: 0.21655519306659698\n",
      "iteration 42021: loss: 0.21655504405498505\n",
      "iteration 42022: loss: 0.2165549248456955\n",
      "iteration 42023: loss: 0.21655483543872833\n",
      "iteration 42024: loss: 0.21655471622943878\n",
      "iteration 42025: loss: 0.21655459702014923\n",
      "iteration 42026: loss: 0.21655447781085968\n",
      "iteration 42027: loss: 0.21655432879924774\n",
      "iteration 42028: loss: 0.21655425429344177\n",
      "iteration 42029: loss: 0.2165542095899582\n",
      "iteration 42030: loss: 0.21655404567718506\n",
      "iteration 42031: loss: 0.21655389666557312\n",
      "iteration 42032: loss: 0.21655377745628357\n",
      "iteration 42033: loss: 0.2165536880493164\n",
      "iteration 42034: loss: 0.21655353903770447\n",
      "iteration 42035: loss: 0.2165534496307373\n",
      "iteration 42036: loss: 0.21655336022377014\n",
      "iteration 42037: loss: 0.21655328571796417\n",
      "iteration 42038: loss: 0.21655309200286865\n",
      "iteration 42039: loss: 0.2165529727935791\n",
      "iteration 42040: loss: 0.21655289828777313\n",
      "iteration 42041: loss: 0.21655277907848358\n",
      "iteration 42042: loss: 0.21655268967151642\n",
      "iteration 42043: loss: 0.21655257046222687\n",
      "iteration 42044: loss: 0.21655242145061493\n",
      "iteration 42045: loss: 0.21655230224132538\n",
      "iteration 42046: loss: 0.21655221283435822\n",
      "iteration 42047: loss: 0.21655210852622986\n",
      "iteration 42048: loss: 0.21655192971229553\n",
      "iteration 42049: loss: 0.21655185520648956\n",
      "iteration 42050: loss: 0.2165517359972\n",
      "iteration 42051: loss: 0.21655169129371643\n",
      "iteration 42052: loss: 0.2165514975786209\n",
      "iteration 42053: loss: 0.21655139327049255\n",
      "iteration 42054: loss: 0.21655134856700897\n",
      "iteration 42055: loss: 0.21655115485191345\n",
      "iteration 42056: loss: 0.21655115485191345\n",
      "iteration 42057: loss: 0.21655090153217316\n",
      "iteration 42058: loss: 0.21655087172985077\n",
      "iteration 42059: loss: 0.21655075252056122\n",
      "iteration 42060: loss: 0.21655061841011047\n",
      "iteration 42061: loss: 0.21655043959617615\n",
      "iteration 42062: loss: 0.21655042469501495\n",
      "iteration 42063: loss: 0.2165503054857254\n",
      "iteration 42064: loss: 0.21655014157295227\n",
      "iteration 42065: loss: 0.2165500670671463\n",
      "iteration 42066: loss: 0.21654996275901794\n",
      "iteration 42067: loss: 0.21654978394508362\n",
      "iteration 42068: loss: 0.21654967963695526\n",
      "iteration 42069: loss: 0.2165495902299881\n",
      "iteration 42070: loss: 0.21654954552650452\n",
      "iteration 42071: loss: 0.216549351811409\n",
      "iteration 42072: loss: 0.21654923260211945\n",
      "iteration 42073: loss: 0.2165491133928299\n",
      "iteration 42074: loss: 0.2165490686893463\n",
      "iteration 42075: loss: 0.216548889875412\n",
      "iteration 42076: loss: 0.21654875576496124\n",
      "iteration 42077: loss: 0.2165486365556717\n",
      "iteration 42078: loss: 0.21654856204986572\n",
      "iteration 42079: loss: 0.21654847264289856\n",
      "iteration 42080: loss: 0.216548353433609\n",
      "iteration 42081: loss: 0.21654820442199707\n",
      "iteration 42082: loss: 0.21654805541038513\n",
      "iteration 42083: loss: 0.21654801070690155\n",
      "iteration 42084: loss: 0.21654793620109558\n",
      "iteration 42085: loss: 0.21654775738716125\n",
      "iteration 42086: loss: 0.21654757857322693\n",
      "iteration 42087: loss: 0.21654751896858215\n",
      "iteration 42088: loss: 0.21654736995697021\n",
      "iteration 42089: loss: 0.21654732525348663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 42090: loss: 0.2165471613407135\n",
      "iteration 42091: loss: 0.21654708683490753\n",
      "iteration 42092: loss: 0.2165469378232956\n",
      "iteration 42093: loss: 0.21654686331748962\n",
      "iteration 42094: loss: 0.21654672920703888\n",
      "iteration 42095: loss: 0.21654662489891052\n",
      "iteration 42096: loss: 0.21654653549194336\n",
      "iteration 42097: loss: 0.21654634177684784\n",
      "iteration 42098: loss: 0.2165462225675583\n",
      "iteration 42099: loss: 0.2165461778640747\n",
      "iteration 42100: loss: 0.21654602885246277\n",
      "iteration 42101: loss: 0.21654590964317322\n",
      "iteration 42102: loss: 0.21654579043388367\n",
      "iteration 42103: loss: 0.21654577553272247\n",
      "iteration 42104: loss: 0.21654558181762695\n",
      "iteration 42105: loss: 0.2165454924106598\n",
      "iteration 42106: loss: 0.21654534339904785\n",
      "iteration 42107: loss: 0.2165452241897583\n",
      "iteration 42108: loss: 0.21654507517814636\n",
      "iteration 42109: loss: 0.2165450155735016\n",
      "iteration 42110: loss: 0.2165449559688568\n",
      "iteration 42111: loss: 0.2165447175502777\n",
      "iteration 42112: loss: 0.21654467284679413\n",
      "iteration 42113: loss: 0.21654453873634338\n",
      "iteration 42114: loss: 0.21654438972473145\n",
      "iteration 42115: loss: 0.21654434502124786\n",
      "iteration 42116: loss: 0.21654418110847473\n",
      "iteration 42117: loss: 0.21654410660266876\n",
      "iteration 42118: loss: 0.2165439873933792\n",
      "iteration 42119: loss: 0.21654382348060608\n",
      "iteration 42120: loss: 0.2165437638759613\n",
      "iteration 42121: loss: 0.21654364466667175\n",
      "iteration 42122: loss: 0.21654348075389862\n",
      "iteration 42123: loss: 0.21654340624809265\n",
      "iteration 42124: loss: 0.2165433168411255\n",
      "iteration 42125: loss: 0.21654319763183594\n",
      "iteration 42126: loss: 0.216543048620224\n",
      "iteration 42127: loss: 0.21654298901557922\n",
      "iteration 42128: loss: 0.2165428102016449\n",
      "iteration 42129: loss: 0.21654275059700012\n",
      "iteration 42130: loss: 0.21654264628887177\n",
      "iteration 42131: loss: 0.21654248237609863\n",
      "iteration 42132: loss: 0.21654243767261505\n",
      "iteration 42133: loss: 0.2165422886610031\n",
      "iteration 42134: loss: 0.21654215455055237\n",
      "iteration 42135: loss: 0.216542050242424\n",
      "iteration 42136: loss: 0.21654196083545685\n",
      "iteration 42137: loss: 0.2165418118238449\n",
      "iteration 42138: loss: 0.21654172241687775\n",
      "iteration 42139: loss: 0.21654155850410461\n",
      "iteration 42140: loss: 0.21654145419597626\n",
      "iteration 42141: loss: 0.2165413200855255\n",
      "iteration 42142: loss: 0.21654124557971954\n",
      "iteration 42143: loss: 0.2165411412715912\n",
      "iteration 42144: loss: 0.21654102206230164\n",
      "iteration 42145: loss: 0.21654090285301208\n",
      "iteration 42146: loss: 0.21654072403907776\n",
      "iteration 42147: loss: 0.21654066443443298\n",
      "iteration 42148: loss: 0.21654057502746582\n",
      "iteration 42149: loss: 0.21654053032398224\n",
      "iteration 42150: loss: 0.21654033660888672\n",
      "iteration 42151: loss: 0.21654024720191956\n",
      "iteration 42152: loss: 0.21654006838798523\n",
      "iteration 42153: loss: 0.21653993427753448\n",
      "iteration 42154: loss: 0.21653982996940613\n",
      "iteration 42155: loss: 0.21653981506824493\n",
      "iteration 42156: loss: 0.21653962135314941\n",
      "iteration 42157: loss: 0.21653953194618225\n",
      "iteration 42158: loss: 0.2165393829345703\n",
      "iteration 42159: loss: 0.21653930842876434\n",
      "iteration 42160: loss: 0.21653921902179718\n",
      "iteration 42161: loss: 0.21653909981250763\n",
      "iteration 42162: loss: 0.2165389358997345\n",
      "iteration 42163: loss: 0.21653886139392853\n",
      "iteration 42164: loss: 0.21653874218463898\n",
      "iteration 42165: loss: 0.21653862297534943\n",
      "iteration 42166: loss: 0.21653850376605988\n",
      "iteration 42167: loss: 0.21653833985328674\n",
      "iteration 42168: loss: 0.21653828024864197\n",
      "iteration 42169: loss: 0.21653816103935242\n",
      "iteration 42170: loss: 0.21653799712657928\n",
      "iteration 42171: loss: 0.21653790771961212\n",
      "iteration 42172: loss: 0.21653780341148376\n",
      "iteration 42173: loss: 0.21653775870800018\n",
      "iteration 42174: loss: 0.21653756499290466\n",
      "iteration 42175: loss: 0.2165374457836151\n",
      "iteration 42176: loss: 0.21653732657432556\n",
      "iteration 42177: loss: 0.2165372371673584\n",
      "iteration 42178: loss: 0.21653708815574646\n",
      "iteration 42179: loss: 0.21653702855110168\n",
      "iteration 42180: loss: 0.2165369689464569\n",
      "iteration 42181: loss: 0.2165367603302002\n",
      "iteration 42182: loss: 0.21653664112091064\n",
      "iteration 42183: loss: 0.21653655171394348\n",
      "iteration 42184: loss: 0.21653640270233154\n",
      "iteration 42185: loss: 0.21653635799884796\n",
      "iteration 42186: loss: 0.2165362387895584\n",
      "iteration 42187: loss: 0.21653613448143005\n",
      "iteration 42188: loss: 0.21653595566749573\n",
      "iteration 42189: loss: 0.21653585135936737\n",
      "iteration 42190: loss: 0.21653571724891663\n",
      "iteration 42191: loss: 0.21653561294078827\n",
      "iteration 42192: loss: 0.2165355235338211\n",
      "iteration 42193: loss: 0.21653541922569275\n",
      "iteration 42194: loss: 0.216535285115242\n",
      "iteration 42195: loss: 0.21653518080711365\n",
      "iteration 42196: loss: 0.2165350615978241\n",
      "iteration 42197: loss: 0.21653500199317932\n",
      "iteration 42198: loss: 0.21653492748737335\n",
      "iteration 42199: loss: 0.2165348082780838\n",
      "iteration 42200: loss: 0.21653464436531067\n",
      "iteration 42201: loss: 0.2165345698595047\n",
      "iteration 42202: loss: 0.2165343463420868\n",
      "iteration 42203: loss: 0.21653428673744202\n",
      "iteration 42204: loss: 0.21653421223163605\n",
      "iteration 42205: loss: 0.21653398871421814\n",
      "iteration 42206: loss: 0.21653392910957336\n",
      "iteration 42207: loss: 0.216533824801445\n",
      "iteration 42208: loss: 0.21653366088867188\n",
      "iteration 42209: loss: 0.2165335714817047\n",
      "iteration 42210: loss: 0.21653349697589874\n",
      "iteration 42211: loss: 0.2165333777666092\n",
      "iteration 42212: loss: 0.21653321385383606\n",
      "iteration 42213: loss: 0.2165331393480301\n",
      "iteration 42214: loss: 0.21653303503990173\n",
      "iteration 42215: loss: 0.2165328562259674\n",
      "iteration 42216: loss: 0.21653279662132263\n",
      "iteration 42217: loss: 0.2165326625108719\n",
      "iteration 42218: loss: 0.2165326178073883\n",
      "iteration 42219: loss: 0.21653243899345398\n",
      "iteration 42220: loss: 0.21653227508068085\n",
      "iteration 42221: loss: 0.21653218567371368\n",
      "iteration 42222: loss: 0.21653211116790771\n",
      "iteration 42223: loss: 0.21653203666210175\n",
      "iteration 42224: loss: 0.21653184294700623\n",
      "iteration 42225: loss: 0.21653175354003906\n",
      "iteration 42226: loss: 0.2165316343307495\n",
      "iteration 42227: loss: 0.21653148531913757\n",
      "iteration 42228: loss: 0.2165314257144928\n",
      "iteration 42229: loss: 0.21653135120868683\n",
      "iteration 42230: loss: 0.2165311872959137\n",
      "iteration 42231: loss: 0.21653103828430176\n",
      "iteration 42232: loss: 0.21653099358081818\n",
      "iteration 42233: loss: 0.21653082966804504\n",
      "iteration 42234: loss: 0.2165307253599167\n",
      "iteration 42235: loss: 0.21653053164482117\n",
      "iteration 42236: loss: 0.2165304720401764\n",
      "iteration 42237: loss: 0.216530442237854\n",
      "iteration 42238: loss: 0.21653024852275848\n",
      "iteration 42239: loss: 0.21653015911579132\n",
      "iteration 42240: loss: 0.21653001010417938\n",
      "iteration 42241: loss: 0.2165299355983734\n",
      "iteration 42242: loss: 0.21652977168560028\n",
      "iteration 42243: loss: 0.2165297567844391\n",
      "iteration 42244: loss: 0.21652951836585999\n",
      "iteration 42245: loss: 0.21652941405773163\n",
      "iteration 42246: loss: 0.21652927994728088\n",
      "iteration 42247: loss: 0.2165292501449585\n",
      "iteration 42248: loss: 0.21652913093566895\n",
      "iteration 42249: loss: 0.21652904152870178\n",
      "iteration 42250: loss: 0.21652884781360626\n",
      "iteration 42251: loss: 0.2165287733078003\n",
      "iteration 42252: loss: 0.21652865409851074\n",
      "iteration 42253: loss: 0.2165285348892212\n",
      "iteration 42254: loss: 0.21652844548225403\n",
      "iteration 42255: loss: 0.21652832627296448\n",
      "iteration 42256: loss: 0.21652822196483612\n",
      "iteration 42257: loss: 0.2165280133485794\n",
      "iteration 42258: loss: 0.21652796864509583\n",
      "iteration 42259: loss: 0.2165278196334839\n",
      "iteration 42260: loss: 0.2165277898311615\n",
      "iteration 42261: loss: 0.21652765572071075\n",
      "iteration 42262: loss: 0.21652749180793762\n",
      "iteration 42263: loss: 0.21652738749980927\n",
      "iteration 42264: loss: 0.21652726829051971\n",
      "iteration 42265: loss: 0.21652714908123016\n",
      "iteration 42266: loss: 0.2165270745754242\n",
      "iteration 42267: loss: 0.21652695536613464\n",
      "iteration 42268: loss: 0.21652691066265106\n",
      "iteration 42269: loss: 0.21652670204639435\n",
      "iteration 42270: loss: 0.2165265828371048\n",
      "iteration 42271: loss: 0.21652641892433167\n",
      "iteration 42272: loss: 0.2165263593196869\n",
      "iteration 42273: loss: 0.21652626991271973\n",
      "iteration 42274: loss: 0.21652615070343018\n",
      "iteration 42275: loss: 0.21652598679065704\n",
      "iteration 42276: loss: 0.21652591228485107\n",
      "iteration 42277: loss: 0.21652574837207794\n",
      "iteration 42278: loss: 0.21652564406394958\n",
      "iteration 42279: loss: 0.21652555465698242\n",
      "iteration 42280: loss: 0.21652548015117645\n",
      "iteration 42281: loss: 0.2165253609418869\n",
      "iteration 42282: loss: 0.21652524173259735\n",
      "iteration 42283: loss: 0.21652503311634064\n",
      "iteration 42284: loss: 0.21652500331401825\n",
      "iteration 42285: loss: 0.2165248841047287\n",
      "iteration 42286: loss: 0.21652472019195557\n",
      "iteration 42287: loss: 0.21652455627918243\n",
      "iteration 42288: loss: 0.21652455627918243\n",
      "iteration 42289: loss: 0.2165243923664093\n",
      "iteration 42290: loss: 0.21652433276176453\n",
      "iteration 42291: loss: 0.2165241688489914\n",
      "iteration 42292: loss: 0.21652403473854065\n",
      "iteration 42293: loss: 0.21652397513389587\n",
      "iteration 42294: loss: 0.21652379631996155\n",
      "iteration 42295: loss: 0.21652373671531677\n",
      "iteration 42296: loss: 0.21652355790138245\n",
      "iteration 42297: loss: 0.2165234535932541\n",
      "iteration 42298: loss: 0.2165234088897705\n",
      "iteration 42299: loss: 0.216523215174675\n",
      "iteration 42300: loss: 0.2165231704711914\n",
      "iteration 42301: loss: 0.21652308106422424\n",
      "iteration 42302: loss: 0.2165229320526123\n",
      "iteration 42303: loss: 0.21652281284332275\n",
      "iteration 42304: loss: 0.21652276813983917\n",
      "iteration 42305: loss: 0.21652260422706604\n",
      "iteration 42306: loss: 0.21652241051197052\n",
      "iteration 42307: loss: 0.21652236580848694\n",
      "iteration 42308: loss: 0.21652217209339142\n",
      "iteration 42309: loss: 0.21652212738990784\n",
      "iteration 42310: loss: 0.21652202308177948\n",
      "iteration 42311: loss: 0.21652190387248993\n",
      "iteration 42312: loss: 0.21652178466320038\n",
      "iteration 42313: loss: 0.21652165055274963\n",
      "iteration 42314: loss: 0.2165215015411377\n",
      "iteration 42315: loss: 0.21652138233184814\n",
      "iteration 42316: loss: 0.21652133762836456\n",
      "iteration 42317: loss: 0.21652118861675262\n",
      "iteration 42318: loss: 0.2165210247039795\n",
      "iteration 42319: loss: 0.21652093529701233\n",
      "iteration 42320: loss: 0.21652087569236755\n",
      "iteration 42321: loss: 0.2165207415819168\n",
      "iteration 42322: loss: 0.21652063727378845\n",
      "iteration 42323: loss: 0.2165205031633377\n",
      "iteration 42324: loss: 0.21652035415172577\n",
      "iteration 42325: loss: 0.21652023494243622\n",
      "iteration 42326: loss: 0.21652016043663025\n",
      "iteration 42327: loss: 0.21652007102966309\n",
      "iteration 42328: loss: 0.21651998162269592\n",
      "iteration 42329: loss: 0.2165198028087616\n",
      "iteration 42330: loss: 0.21651971340179443\n",
      "iteration 42331: loss: 0.2165195196866989\n",
      "iteration 42332: loss: 0.2165195196866989\n",
      "iteration 42333: loss: 0.2165193259716034\n",
      "iteration 42334: loss: 0.2165192812681198\n",
      "iteration 42335: loss: 0.21651911735534668\n",
      "iteration 42336: loss: 0.21651902794837952\n",
      "iteration 42337: loss: 0.21651887893676758\n",
      "iteration 42338: loss: 0.216518834233284\n",
      "iteration 42339: loss: 0.21651864051818848\n",
      "iteration 42340: loss: 0.2165185660123825\n",
      "iteration 42341: loss: 0.21651843190193176\n",
      "iteration 42342: loss: 0.2165183573961258\n",
      "iteration 42343: loss: 0.21651819348335266\n",
      "iteration 42344: loss: 0.21651813387870789\n",
      "iteration 42345: loss: 0.21651796996593475\n",
      "iteration 42346: loss: 0.2165178805589676\n",
      "iteration 42347: loss: 0.21651771664619446\n",
      "iteration 42348: loss: 0.2165176123380661\n",
      "iteration 42349: loss: 0.21651753783226013\n",
      "iteration 42350: loss: 0.2165173590183258\n",
      "iteration 42351: loss: 0.21651725471019745\n",
      "iteration 42352: loss: 0.21651721000671387\n",
      "iteration 42353: loss: 0.21651701629161835\n",
      "iteration 42354: loss: 0.21651692688465118\n",
      "iteration 42355: loss: 0.21651673316955566\n",
      "iteration 42356: loss: 0.21651673316955566\n",
      "iteration 42357: loss: 0.2165166586637497\n",
      "iteration 42358: loss: 0.21651649475097656\n",
      "iteration 42359: loss: 0.21651633083820343\n",
      "iteration 42360: loss: 0.21651621162891388\n",
      "iteration 42361: loss: 0.21651609241962433\n",
      "iteration 42362: loss: 0.21651604771614075\n",
      "iteration 42363: loss: 0.21651586890220642\n",
      "iteration 42364: loss: 0.21651582419872284\n",
      "iteration 42365: loss: 0.21651561558246613\n",
      "iteration 42366: loss: 0.21651554107666016\n",
      "iteration 42367: loss: 0.2165154665708542\n",
      "iteration 42368: loss: 0.21651530265808105\n",
      "iteration 42369: loss: 0.21651522815227509\n",
      "iteration 42370: loss: 0.21651510894298553\n",
      "iteration 42371: loss: 0.2165149450302124\n",
      "iteration 42372: loss: 0.21651490032672882\n",
      "iteration 42373: loss: 0.2165147364139557\n",
      "iteration 42374: loss: 0.2165146768093109\n",
      "iteration 42375: loss: 0.2165144979953766\n",
      "iteration 42376: loss: 0.21651442348957062\n",
      "iteration 42377: loss: 0.21651427447795868\n",
      "iteration 42378: loss: 0.21651418507099152\n",
      "iteration 42379: loss: 0.21651406586170197\n",
      "iteration 42380: loss: 0.21651391685009003\n",
      "iteration 42381: loss: 0.21651382744312286\n",
      "iteration 42382: loss: 0.2165137231349945\n",
      "iteration 42383: loss: 0.21651355922222137\n",
      "iteration 42384: loss: 0.2165134698152542\n",
      "iteration 42385: loss: 0.21651335060596466\n",
      "iteration 42386: loss: 0.2165132462978363\n",
      "iteration 42387: loss: 0.21651315689086914\n",
      "iteration 42388: loss: 0.21651306748390198\n",
      "iteration 42389: loss: 0.21651291847229004\n",
      "iteration 42390: loss: 0.21651282906532288\n",
      "iteration 42391: loss: 0.21651268005371094\n",
      "iteration 42392: loss: 0.2165125161409378\n",
      "iteration 42393: loss: 0.21651239693164825\n",
      "iteration 42394: loss: 0.2165122926235199\n",
      "iteration 42395: loss: 0.21651217341423035\n",
      "iteration 42396: loss: 0.21651211380958557\n",
      "iteration 42397: loss: 0.21651196479797363\n",
      "iteration 42398: loss: 0.21651187539100647\n",
      "iteration 42399: loss: 0.2165117710828781\n",
      "iteration 42400: loss: 0.21651163697242737\n",
      "iteration 42401: loss: 0.21651151776313782\n",
      "iteration 42402: loss: 0.21651141345500946\n",
      "iteration 42403: loss: 0.21651124954223633\n",
      "iteration 42404: loss: 0.21651117503643036\n",
      "iteration 42405: loss: 0.2165111005306244\n",
      "iteration 42406: loss: 0.21651089191436768\n",
      "iteration 42407: loss: 0.2165108025074005\n",
      "iteration 42408: loss: 0.21651069819927216\n",
      "iteration 42409: loss: 0.2165105789899826\n",
      "iteration 42410: loss: 0.21651044487953186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 42411: loss: 0.2165103405714035\n",
      "iteration 42412: loss: 0.21651022136211395\n",
      "iteration 42413: loss: 0.21651020646095276\n",
      "iteration 42414: loss: 0.21651005744934082\n",
      "iteration 42415: loss: 0.21650990843772888\n",
      "iteration 42416: loss: 0.21650977432727814\n",
      "iteration 42417: loss: 0.21650967001914978\n",
      "iteration 42418: loss: 0.21650958061218262\n",
      "iteration 42419: loss: 0.21650949120521545\n",
      "iteration 42420: loss: 0.21650934219360352\n",
      "iteration 42421: loss: 0.21650925278663635\n",
      "iteration 42422: loss: 0.2165091335773468\n",
      "iteration 42423: loss: 0.21650902926921844\n",
      "iteration 42424: loss: 0.21650882065296173\n",
      "iteration 42425: loss: 0.21650874614715576\n",
      "iteration 42426: loss: 0.2165086269378662\n",
      "iteration 42427: loss: 0.21650847792625427\n",
      "iteration 42428: loss: 0.21650835871696472\n",
      "iteration 42429: loss: 0.21650826930999756\n",
      "iteration 42430: loss: 0.2165081799030304\n",
      "iteration 42431: loss: 0.21650806069374084\n",
      "iteration 42432: loss: 0.21650798618793488\n",
      "iteration 42433: loss: 0.21650786697864532\n",
      "iteration 42434: loss: 0.2165077179670334\n",
      "iteration 42435: loss: 0.21650755405426025\n",
      "iteration 42436: loss: 0.21650747954845428\n",
      "iteration 42437: loss: 0.21650736033916473\n",
      "iteration 42438: loss: 0.2165071964263916\n",
      "iteration 42439: loss: 0.2165071666240692\n",
      "iteration 42440: loss: 0.21650700271129608\n",
      "iteration 42441: loss: 0.21650691330432892\n",
      "iteration 42442: loss: 0.21650680899620056\n",
      "iteration 42443: loss: 0.216506689786911\n",
      "iteration 42444: loss: 0.21650657057762146\n",
      "iteration 42445: loss: 0.2165064811706543\n",
      "iteration 42446: loss: 0.21650633215904236\n",
      "iteration 42447: loss: 0.2165062427520752\n",
      "iteration 42448: loss: 0.21650604903697968\n",
      "iteration 42449: loss: 0.2165060043334961\n",
      "iteration 42450: loss: 0.21650588512420654\n",
      "iteration 42451: loss: 0.2165057212114334\n",
      "iteration 42452: loss: 0.21650567650794983\n",
      "iteration 42453: loss: 0.2165054976940155\n",
      "iteration 42454: loss: 0.21650543808937073\n",
      "iteration 42455: loss: 0.21650519967079163\n",
      "iteration 42456: loss: 0.21650519967079163\n",
      "iteration 42457: loss: 0.2165050059556961\n",
      "iteration 42458: loss: 0.21650496125221252\n",
      "iteration 42459: loss: 0.21650481224060059\n",
      "iteration 42460: loss: 0.216504767537117\n",
      "iteration 42461: loss: 0.21650460362434387\n",
      "iteration 42462: loss: 0.21650448441505432\n",
      "iteration 42463: loss: 0.21650438010692596\n",
      "iteration 42464: loss: 0.21650424599647522\n",
      "iteration 42465: loss: 0.21650418639183044\n",
      "iteration 42466: loss: 0.21650400757789612\n",
      "iteration 42467: loss: 0.21650385856628418\n",
      "iteration 42468: loss: 0.2165037840604782\n",
      "iteration 42469: loss: 0.21650366485118866\n",
      "iteration 42470: loss: 0.2165035456418991\n",
      "iteration 42471: loss: 0.21650342643260956\n",
      "iteration 42472: loss: 0.2165032923221588\n",
      "iteration 42473: loss: 0.21650321781635284\n",
      "iteration 42474: loss: 0.2165030986070633\n",
      "iteration 42475: loss: 0.21650293469429016\n",
      "iteration 42476: loss: 0.21650287508964539\n",
      "iteration 42477: loss: 0.21650278568267822\n",
      "iteration 42478: loss: 0.2165025919675827\n",
      "iteration 42479: loss: 0.21650242805480957\n",
      "iteration 42480: loss: 0.21650242805480957\n",
      "iteration 42481: loss: 0.21650227904319763\n",
      "iteration 42482: loss: 0.21650215983390808\n",
      "iteration 42483: loss: 0.21650207042694092\n",
      "iteration 42484: loss: 0.21650190651416779\n",
      "iteration 42485: loss: 0.21650180220603943\n",
      "iteration 42486: loss: 0.21650171279907227\n",
      "iteration 42487: loss: 0.21650156378746033\n",
      "iteration 42488: loss: 0.21650142967700958\n",
      "iteration 42489: loss: 0.2165013551712036\n",
      "iteration 42490: loss: 0.21650126576423645\n",
      "iteration 42491: loss: 0.2165011465549469\n",
      "iteration 42492: loss: 0.21650099754333496\n",
      "iteration 42493: loss: 0.2165009081363678\n",
      "iteration 42494: loss: 0.21650084853172302\n",
      "iteration 42495: loss: 0.2165006697177887\n",
      "iteration 42496: loss: 0.21650052070617676\n",
      "iteration 42497: loss: 0.21650047600269318\n",
      "iteration 42498: loss: 0.21650028228759766\n",
      "iteration 42499: loss: 0.21650023758411407\n",
      "iteration 42500: loss: 0.21650011837482452\n",
      "iteration 42501: loss: 0.21649996936321259\n",
      "iteration 42502: loss: 0.21649989485740662\n",
      "iteration 42503: loss: 0.21649977564811707\n",
      "iteration 42504: loss: 0.2164996862411499\n",
      "iteration 42505: loss: 0.2164994776248932\n",
      "iteration 42506: loss: 0.2164994180202484\n",
      "iteration 42507: loss: 0.21649932861328125\n",
      "iteration 42508: loss: 0.21649916470050812\n",
      "iteration 42509: loss: 0.21649904549121857\n",
      "iteration 42510: loss: 0.21649892628192902\n",
      "iteration 42511: loss: 0.21649882197380066\n",
      "iteration 42512: loss: 0.2164987325668335\n",
      "iteration 42513: loss: 0.21649856865406036\n",
      "iteration 42514: loss: 0.21649852395057678\n",
      "iteration 42515: loss: 0.21649833023548126\n",
      "iteration 42516: loss: 0.2164982110261917\n",
      "iteration 42517: loss: 0.21649813652038574\n",
      "iteration 42518: loss: 0.21649806201457977\n",
      "iteration 42519: loss: 0.21649794280529022\n",
      "iteration 42520: loss: 0.2164977341890335\n",
      "iteration 42521: loss: 0.21649768948554993\n",
      "iteration 42522: loss: 0.2164975106716156\n",
      "iteration 42523: loss: 0.21649739146232605\n",
      "iteration 42524: loss: 0.2164973020553589\n",
      "iteration 42525: loss: 0.21649721264839172\n",
      "iteration 42526: loss: 0.21649713814258575\n",
      "iteration 42527: loss: 0.2164970189332962\n",
      "iteration 42528: loss: 0.21649686992168427\n",
      "iteration 42529: loss: 0.21649673581123352\n",
      "iteration 42530: loss: 0.2164965569972992\n",
      "iteration 42531: loss: 0.2164965122938156\n",
      "iteration 42532: loss: 0.21649643778800964\n",
      "iteration 42533: loss: 0.2164963036775589\n",
      "iteration 42534: loss: 0.21649613976478577\n",
      "iteration 42535: loss: 0.21649599075317383\n",
      "iteration 42536: loss: 0.21649590134620667\n",
      "iteration 42537: loss: 0.2164957970380783\n",
      "iteration 42538: loss: 0.21649572253227234\n",
      "iteration 42539: loss: 0.2164955586194992\n",
      "iteration 42540: loss: 0.21649546921253204\n",
      "iteration 42541: loss: 0.2164953202009201\n",
      "iteration 42542: loss: 0.21649518609046936\n",
      "iteration 42543: loss: 0.21649512648582458\n",
      "iteration 42544: loss: 0.21649499237537384\n",
      "iteration 42545: loss: 0.21649491786956787\n",
      "iteration 42546: loss: 0.2164948433637619\n",
      "iteration 42547: loss: 0.21649464964866638\n",
      "iteration 42548: loss: 0.21649453043937683\n",
      "iteration 42549: loss: 0.21649444103240967\n",
      "iteration 42550: loss: 0.21649427711963654\n",
      "iteration 42551: loss: 0.21649417281150818\n",
      "iteration 42552: loss: 0.21649405360221863\n",
      "iteration 42553: loss: 0.21649393439292908\n",
      "iteration 42554: loss: 0.21649381518363953\n",
      "iteration 42555: loss: 0.21649369597434998\n",
      "iteration 42556: loss: 0.21649357676506042\n",
      "iteration 42557: loss: 0.21649344265460968\n",
      "iteration 42558: loss: 0.21649332344532013\n",
      "iteration 42559: loss: 0.21649327874183655\n",
      "iteration 42560: loss: 0.216493159532547\n",
      "iteration 42561: loss: 0.21649308502674103\n",
      "iteration 42562: loss: 0.2164929360151291\n",
      "iteration 42563: loss: 0.21649277210235596\n",
      "iteration 42564: loss: 0.2164926826953888\n",
      "iteration 42565: loss: 0.21649257838726044\n",
      "iteration 42566: loss: 0.2164924591779709\n",
      "iteration 42567: loss: 0.21649236977100372\n",
      "iteration 42568: loss: 0.2164922058582306\n",
      "iteration 42569: loss: 0.21649213135242462\n",
      "iteration 42570: loss: 0.2164919674396515\n",
      "iteration 42571: loss: 0.21649186313152313\n",
      "iteration 42572: loss: 0.21649178862571716\n",
      "iteration 42573: loss: 0.21649165451526642\n",
      "iteration 42574: loss: 0.21649158000946045\n",
      "iteration 42575: loss: 0.21649137139320374\n",
      "iteration 42576: loss: 0.21649131178855896\n",
      "iteration 42577: loss: 0.21649117767810822\n",
      "iteration 42578: loss: 0.21649107336997986\n",
      "iteration 42579: loss: 0.2164909839630127\n",
      "iteration 42580: loss: 0.21649082005023956\n",
      "iteration 42581: loss: 0.2164907157421112\n",
      "iteration 42582: loss: 0.21649058163166046\n",
      "iteration 42583: loss: 0.2164904624223709\n",
      "iteration 42584: loss: 0.21649031341075897\n",
      "iteration 42585: loss: 0.2164902687072754\n",
      "iteration 42586: loss: 0.21649017930030823\n",
      "iteration 42587: loss: 0.2164899855852127\n",
      "iteration 42588: loss: 0.21648988127708435\n",
      "iteration 42589: loss: 0.2164897918701172\n",
      "iteration 42590: loss: 0.21648964285850525\n",
      "iteration 42591: loss: 0.21648958325386047\n",
      "iteration 42592: loss: 0.2164895087480545\n",
      "iteration 42593: loss: 0.21648934483528137\n",
      "iteration 42594: loss: 0.21648924052715302\n",
      "iteration 42595: loss: 0.21648910641670227\n",
      "iteration 42596: loss: 0.21648892760276794\n",
      "iteration 42597: loss: 0.21648888289928436\n",
      "iteration 42598: loss: 0.2164888083934784\n",
      "iteration 42599: loss: 0.21648864448070526\n",
      "iteration 42600: loss: 0.21648851037025452\n",
      "iteration 42601: loss: 0.21648839116096497\n",
      "iteration 42602: loss: 0.216488316655159\n",
      "iteration 42603: loss: 0.21648819744586945\n",
      "iteration 42604: loss: 0.2164880484342575\n",
      "iteration 42605: loss: 0.21648791432380676\n",
      "iteration 42606: loss: 0.2164878398180008\n",
      "iteration 42607: loss: 0.21648767590522766\n",
      "iteration 42608: loss: 0.2164875715970993\n",
      "iteration 42609: loss: 0.21648749709129333\n",
      "iteration 42610: loss: 0.216487318277359\n",
      "iteration 42611: loss: 0.21648725867271423\n",
      "iteration 42612: loss: 0.2164871245622635\n",
      "iteration 42613: loss: 0.21648700535297394\n",
      "iteration 42614: loss: 0.2164868861436844\n",
      "iteration 42615: loss: 0.2164868414402008\n",
      "iteration 42616: loss: 0.21648672223091125\n",
      "iteration 42617: loss: 0.21648649871349335\n",
      "iteration 42618: loss: 0.21648648381233215\n",
      "iteration 42619: loss: 0.21648626029491425\n",
      "iteration 42620: loss: 0.2164861261844635\n",
      "iteration 42621: loss: 0.2164860963821411\n",
      "iteration 42622: loss: 0.21648594737052917\n",
      "iteration 42623: loss: 0.216485857963562\n",
      "iteration 42624: loss: 0.21648578345775604\n",
      "iteration 42625: loss: 0.21648558974266052\n",
      "iteration 42626: loss: 0.21648553013801575\n",
      "iteration 42627: loss: 0.21648535132408142\n",
      "iteration 42628: loss: 0.21648526191711426\n",
      "iteration 42629: loss: 0.2164851427078247\n",
      "iteration 42630: loss: 0.21648502349853516\n",
      "iteration 42631: loss: 0.2164849042892456\n",
      "iteration 42632: loss: 0.21648478507995605\n",
      "iteration 42633: loss: 0.2164846956729889\n",
      "iteration 42634: loss: 0.21648457646369934\n",
      "iteration 42635: loss: 0.2164844572544098\n",
      "iteration 42636: loss: 0.21648438274860382\n",
      "iteration 42637: loss: 0.21648426353931427\n",
      "iteration 42638: loss: 0.2164841592311859\n",
      "iteration 42639: loss: 0.21648399531841278\n",
      "iteration 42640: loss: 0.21648387610912323\n",
      "iteration 42641: loss: 0.21648374199867249\n",
      "iteration 42642: loss: 0.21648359298706055\n",
      "iteration 42643: loss: 0.216483473777771\n",
      "iteration 42644: loss: 0.21648339927196503\n",
      "iteration 42645: loss: 0.21648326516151428\n",
      "iteration 42646: loss: 0.2164832353591919\n",
      "iteration 42647: loss: 0.21648308634757996\n",
      "iteration 42648: loss: 0.2164829671382904\n",
      "iteration 42649: loss: 0.21648278832435608\n",
      "iteration 42650: loss: 0.21648268401622772\n",
      "iteration 42651: loss: 0.21648259460926056\n",
      "iteration 42652: loss: 0.2164824903011322\n",
      "iteration 42653: loss: 0.21648240089416504\n",
      "iteration 42654: loss: 0.2164822369813919\n",
      "iteration 42655: loss: 0.21648208796977997\n",
      "iteration 42656: loss: 0.2164820432662964\n",
      "iteration 42657: loss: 0.21648189425468445\n",
      "iteration 42658: loss: 0.21648180484771729\n",
      "iteration 42659: loss: 0.21648168563842773\n",
      "iteration 42660: loss: 0.2164815366268158\n",
      "iteration 42661: loss: 0.21648141741752625\n",
      "iteration 42662: loss: 0.21648132801055908\n",
      "iteration 42663: loss: 0.21648120880126953\n",
      "iteration 42664: loss: 0.2164810448884964\n",
      "iteration 42665: loss: 0.21648094058036804\n",
      "iteration 42666: loss: 0.2164808213710785\n",
      "iteration 42667: loss: 0.2164807766675949\n",
      "iteration 42668: loss: 0.21648068726062775\n",
      "iteration 42669: loss: 0.21648046374320984\n",
      "iteration 42670: loss: 0.2164803445339203\n",
      "iteration 42671: loss: 0.2164802998304367\n",
      "iteration 42672: loss: 0.21648016571998596\n",
      "iteration 42673: loss: 0.2164800614118576\n",
      "iteration 42674: loss: 0.21647992730140686\n",
      "iteration 42675: loss: 0.2164798080921173\n",
      "iteration 42676: loss: 0.21647968888282776\n",
      "iteration 42677: loss: 0.21647951006889343\n",
      "iteration 42678: loss: 0.21647942066192627\n",
      "iteration 42679: loss: 0.2164793461561203\n",
      "iteration 42680: loss: 0.21647921204566956\n",
      "iteration 42681: loss: 0.2164791375398636\n",
      "iteration 42682: loss: 0.21647903323173523\n",
      "iteration 42683: loss: 0.21647882461547852\n",
      "iteration 42684: loss: 0.21647873520851135\n",
      "iteration 42685: loss: 0.216478630900383\n",
      "iteration 42686: loss: 0.21647851169109344\n",
      "iteration 42687: loss: 0.2164783924818039\n",
      "iteration 42688: loss: 0.21647831797599792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 42689: loss: 0.2164781540632248\n",
      "iteration 42690: loss: 0.2164781093597412\n",
      "iteration 42691: loss: 0.21647794544696808\n",
      "iteration 42692: loss: 0.21647782623767853\n",
      "iteration 42693: loss: 0.21647778153419495\n",
      "iteration 42694: loss: 0.216477632522583\n",
      "iteration 42695: loss: 0.21647746860980988\n",
      "iteration 42696: loss: 0.21647736430168152\n",
      "iteration 42697: loss: 0.21647727489471436\n",
      "iteration 42698: loss: 0.21647712588310242\n",
      "iteration 42699: loss: 0.21647699177265167\n",
      "iteration 42700: loss: 0.2164769470691681\n",
      "iteration 42701: loss: 0.21647687256336212\n",
      "iteration 42702: loss: 0.2164766788482666\n",
      "iteration 42703: loss: 0.21647658944129944\n",
      "iteration 42704: loss: 0.21647648513317108\n",
      "iteration 42705: loss: 0.21647639572620392\n",
      "iteration 42706: loss: 0.21647624671459198\n",
      "iteration 42707: loss: 0.21647608280181885\n",
      "iteration 42708: loss: 0.2164759337902069\n",
      "iteration 42709: loss: 0.21647587418556213\n",
      "iteration 42710: loss: 0.21647576987743378\n",
      "iteration 42711: loss: 0.21647563576698303\n",
      "iteration 42712: loss: 0.2164754867553711\n",
      "iteration 42713: loss: 0.21647539734840393\n",
      "iteration 42714: loss: 0.216475248336792\n",
      "iteration 42715: loss: 0.21647515892982483\n",
      "iteration 42716: loss: 0.21647503972053528\n",
      "iteration 42717: loss: 0.21647489070892334\n",
      "iteration 42718: loss: 0.21647484600543976\n",
      "iteration 42719: loss: 0.2164747267961502\n",
      "iteration 42720: loss: 0.2164744883775711\n",
      "iteration 42721: loss: 0.2164744883775711\n",
      "iteration 42722: loss: 0.21647436916828156\n",
      "iteration 42723: loss: 0.216474249958992\n",
      "iteration 42724: loss: 0.21647405624389648\n",
      "iteration 42725: loss: 0.21647398173809052\n",
      "iteration 42726: loss: 0.21647381782531738\n",
      "iteration 42727: loss: 0.2164737731218338\n",
      "iteration 42728: loss: 0.21647369861602783\n",
      "iteration 42729: loss: 0.21647343039512634\n",
      "iteration 42730: loss: 0.21647341549396515\n",
      "iteration 42731: loss: 0.2164732962846756\n",
      "iteration 42732: loss: 0.21647319197654724\n",
      "iteration 42733: loss: 0.21647310256958008\n",
      "iteration 42734: loss: 0.21647298336029053\n",
      "iteration 42735: loss: 0.21647286415100098\n",
      "iteration 42736: loss: 0.21647267043590546\n",
      "iteration 42737: loss: 0.2164725363254547\n",
      "iteration 42738: loss: 0.21647243201732635\n",
      "iteration 42739: loss: 0.2164723128080368\n",
      "iteration 42740: loss: 0.2164722979068756\n",
      "iteration 42741: loss: 0.2164720743894577\n",
      "iteration 42742: loss: 0.21647199988365173\n",
      "iteration 42743: loss: 0.216471865773201\n",
      "iteration 42744: loss: 0.21647176146507263\n",
      "iteration 42745: loss: 0.21647170186042786\n",
      "iteration 42746: loss: 0.21647152304649353\n",
      "iteration 42747: loss: 0.21647143363952637\n",
      "iteration 42748: loss: 0.21647126972675323\n",
      "iteration 42749: loss: 0.21647115051746368\n",
      "iteration 42750: loss: 0.21647104620933533\n",
      "iteration 42751: loss: 0.21647095680236816\n",
      "iteration 42752: loss: 0.216470867395401\n",
      "iteration 42753: loss: 0.21647071838378906\n",
      "iteration 42754: loss: 0.21647056937217712\n",
      "iteration 42755: loss: 0.21647050976753235\n",
      "iteration 42756: loss: 0.21647033095359802\n",
      "iteration 42757: loss: 0.21647019684314728\n",
      "iteration 42758: loss: 0.21647009253501892\n",
      "iteration 42759: loss: 0.21647004783153534\n",
      "iteration 42760: loss: 0.2164699286222458\n",
      "iteration 42761: loss: 0.21646979451179504\n",
      "iteration 42762: loss: 0.2164696902036667\n",
      "iteration 42763: loss: 0.21646957099437714\n",
      "iteration 42764: loss: 0.21646937727928162\n",
      "iteration 42765: loss: 0.21646931767463684\n",
      "iteration 42766: loss: 0.21646924316883087\n",
      "iteration 42767: loss: 0.21646912395954132\n",
      "iteration 42768: loss: 0.21646897494792938\n",
      "iteration 42769: loss: 0.21646885573863983\n",
      "iteration 42770: loss: 0.2164687216281891\n",
      "iteration 42771: loss: 0.2164686620235443\n",
      "iteration 42772: loss: 0.21646852791309357\n",
      "iteration 42773: loss: 0.21646836400032043\n",
      "iteration 42774: loss: 0.21646830439567566\n",
      "iteration 42775: loss: 0.2164681851863861\n",
      "iteration 42776: loss: 0.21646806597709656\n",
      "iteration 42777: loss: 0.2164679318666458\n",
      "iteration 42778: loss: 0.21646788716316223\n",
      "iteration 42779: loss: 0.2164677083492279\n",
      "iteration 42780: loss: 0.21646758913993835\n",
      "iteration 42781: loss: 0.2164674699306488\n",
      "iteration 42782: loss: 0.21646729111671448\n",
      "iteration 42783: loss: 0.2164672166109085\n",
      "iteration 42784: loss: 0.21646709740161896\n",
      "iteration 42785: loss: 0.2164669930934906\n",
      "iteration 42786: loss: 0.21646687388420105\n",
      "iteration 42787: loss: 0.2164667546749115\n",
      "iteration 42788: loss: 0.21646666526794434\n",
      "iteration 42789: loss: 0.21646654605865479\n",
      "iteration 42790: loss: 0.21646645665168762\n",
      "iteration 42791: loss: 0.2164662778377533\n",
      "iteration 42792: loss: 0.21646621823310852\n",
      "iteration 42793: loss: 0.21646606922149658\n",
      "iteration 42794: loss: 0.21646597981452942\n",
      "iteration 42795: loss: 0.21646587550640106\n",
      "iteration 42796: loss: 0.21646571159362793\n",
      "iteration 42797: loss: 0.21646563708782196\n",
      "iteration 42798: loss: 0.21646547317504883\n",
      "iteration 42799: loss: 0.21646535396575928\n",
      "iteration 42800: loss: 0.21646520495414734\n",
      "iteration 42801: loss: 0.21646514534950256\n",
      "iteration 42802: loss: 0.21646499633789062\n",
      "iteration 42803: loss: 0.21646492183208466\n",
      "iteration 42804: loss: 0.2164647877216339\n",
      "iteration 42805: loss: 0.21646463871002197\n",
      "iteration 42806: loss: 0.216464564204216\n",
      "iteration 42807: loss: 0.21646437048912048\n",
      "iteration 42808: loss: 0.2164643257856369\n",
      "iteration 42809: loss: 0.21646419167518616\n",
      "iteration 42810: loss: 0.21646399796009064\n",
      "iteration 42811: loss: 0.21646396815776825\n",
      "iteration 42812: loss: 0.2164638489484787\n",
      "iteration 42813: loss: 0.21646368503570557\n",
      "iteration 42814: loss: 0.21646364033222198\n",
      "iteration 42815: loss: 0.21646347641944885\n",
      "iteration 42816: loss: 0.21646332740783691\n",
      "iteration 42817: loss: 0.21646320819854736\n",
      "iteration 42818: loss: 0.21646316349506378\n",
      "iteration 42819: loss: 0.21646304428577423\n",
      "iteration 42820: loss: 0.2164628952741623\n",
      "iteration 42821: loss: 0.21646270155906677\n",
      "iteration 42822: loss: 0.216462641954422\n",
      "iteration 42823: loss: 0.21646249294281006\n",
      "iteration 42824: loss: 0.2164623737335205\n",
      "iteration 42825: loss: 0.21646232903003693\n",
      "iteration 42826: loss: 0.21646222472190857\n",
      "iteration 42827: loss: 0.21646209061145782\n",
      "iteration 42828: loss: 0.21646197140216827\n",
      "iteration 42829: loss: 0.21646182239055634\n",
      "iteration 42830: loss: 0.21646173298358917\n",
      "iteration 42831: loss: 0.21646161377429962\n",
      "iteration 42832: loss: 0.21646158397197723\n",
      "iteration 42833: loss: 0.21646130084991455\n",
      "iteration 42834: loss: 0.21646127104759216\n",
      "iteration 42835: loss: 0.21646113693714142\n",
      "iteration 42836: loss: 0.21646103262901306\n",
      "iteration 42837: loss: 0.21646089851856232\n",
      "iteration 42838: loss: 0.21646074950695038\n",
      "iteration 42839: loss: 0.21646061539649963\n",
      "iteration 42840: loss: 0.21646061539649963\n",
      "iteration 42841: loss: 0.2164604216814041\n",
      "iteration 42842: loss: 0.21646027266979218\n",
      "iteration 42843: loss: 0.2164601981639862\n",
      "iteration 42844: loss: 0.21646007895469666\n",
      "iteration 42845: loss: 0.2164599597454071\n",
      "iteration 42846: loss: 0.21645990014076233\n",
      "iteration 42847: loss: 0.2164597511291504\n",
      "iteration 42848: loss: 0.21645960211753845\n",
      "iteration 42849: loss: 0.2164595127105713\n",
      "iteration 42850: loss: 0.21645930409431458\n",
      "iteration 42851: loss: 0.21645930409431458\n",
      "iteration 42852: loss: 0.21645911037921906\n",
      "iteration 42853: loss: 0.21645903587341309\n",
      "iteration 42854: loss: 0.21645891666412354\n",
      "iteration 42855: loss: 0.2164587676525116\n",
      "iteration 42856: loss: 0.21645872294902802\n",
      "iteration 42857: loss: 0.21645855903625488\n",
      "iteration 42858: loss: 0.21645843982696533\n",
      "iteration 42859: loss: 0.21645835041999817\n",
      "iteration 42860: loss: 0.21645823121070862\n",
      "iteration 42861: loss: 0.21645808219909668\n",
      "iteration 42862: loss: 0.21645796298980713\n",
      "iteration 42863: loss: 0.21645788848400116\n",
      "iteration 42864: loss: 0.2164577692747116\n",
      "iteration 42865: loss: 0.21645765006542206\n",
      "iteration 42866: loss: 0.2164575308561325\n",
      "iteration 42867: loss: 0.21645736694335938\n",
      "iteration 42868: loss: 0.2164572924375534\n",
      "iteration 42869: loss: 0.21645715832710266\n",
      "iteration 42870: loss: 0.21645709872245789\n",
      "iteration 42871: loss: 0.21645693480968475\n",
      "iteration 42872: loss: 0.21645686030387878\n",
      "iteration 42873: loss: 0.21645665168762207\n",
      "iteration 42874: loss: 0.2164565622806549\n",
      "iteration 42875: loss: 0.21645644307136536\n",
      "iteration 42876: loss: 0.21645638346672058\n",
      "iteration 42877: loss: 0.21645621955394745\n",
      "iteration 42878: loss: 0.2164560854434967\n",
      "iteration 42879: loss: 0.21645596623420715\n",
      "iteration 42880: loss: 0.2164558470249176\n",
      "iteration 42881: loss: 0.21645574271678925\n",
      "iteration 42882: loss: 0.21645565330982208\n",
      "iteration 42883: loss: 0.21645553410053253\n",
      "iteration 42884: loss: 0.21645542979240417\n",
      "iteration 42885: loss: 0.21645526587963104\n",
      "iteration 42886: loss: 0.21645525097846985\n",
      "iteration 42887: loss: 0.21645507216453552\n",
      "iteration 42888: loss: 0.2164548933506012\n",
      "iteration 42889: loss: 0.21645481884479523\n",
      "iteration 42890: loss: 0.21645469963550568\n",
      "iteration 42891: loss: 0.2164546251296997\n",
      "iteration 42892: loss: 0.21645447611808777\n",
      "iteration 42893: loss: 0.21645434200763702\n",
      "iteration 42894: loss: 0.21645426750183105\n",
      "iteration 42895: loss: 0.21645411849021912\n",
      "iteration 42896: loss: 0.21645402908325195\n",
      "iteration 42897: loss: 0.2164539396762848\n",
      "iteration 42898: loss: 0.21645383536815643\n",
      "iteration 42899: loss: 0.2164536416530609\n",
      "iteration 42900: loss: 0.21645355224609375\n",
      "iteration 42901: loss: 0.2164534032344818\n",
      "iteration 42902: loss: 0.21645328402519226\n",
      "iteration 42903: loss: 0.21645322442054749\n",
      "iteration 42904: loss: 0.21645304560661316\n",
      "iteration 42905: loss: 0.216452956199646\n",
      "iteration 42906: loss: 0.21645283699035645\n",
      "iteration 42907: loss: 0.2164527177810669\n",
      "iteration 42908: loss: 0.21645256876945496\n",
      "iteration 42909: loss: 0.2164524793624878\n",
      "iteration 42910: loss: 0.21645238995552063\n",
      "iteration 42911: loss: 0.2164521962404251\n",
      "iteration 42912: loss: 0.21645207703113556\n",
      "iteration 42913: loss: 0.2164519727230072\n",
      "iteration 42914: loss: 0.21645188331604004\n",
      "iteration 42915: loss: 0.2164517343044281\n",
      "iteration 42916: loss: 0.21645164489746094\n",
      "iteration 42917: loss: 0.21645157039165497\n",
      "iteration 42918: loss: 0.21645140647888184\n",
      "iteration 42919: loss: 0.21645131707191467\n",
      "iteration 42920: loss: 0.21645121276378632\n",
      "iteration 42921: loss: 0.21645107865333557\n",
      "iteration 42922: loss: 0.21645092964172363\n",
      "iteration 42923: loss: 0.2164507806301117\n",
      "iteration 42924: loss: 0.21645066142082214\n",
      "iteration 42925: loss: 0.21645060181617737\n",
      "iteration 42926: loss: 0.21645048260688782\n",
      "iteration 42927: loss: 0.21645036339759827\n",
      "iteration 42928: loss: 0.21645018458366394\n",
      "iteration 42929: loss: 0.21645013988018036\n",
      "iteration 42930: loss: 0.2164500504732132\n",
      "iteration 42931: loss: 0.21644993126392365\n",
      "iteration 42932: loss: 0.2164497822523117\n",
      "iteration 42933: loss: 0.21644961833953857\n",
      "iteration 42934: loss: 0.216449573636055\n",
      "iteration 42935: loss: 0.21644937992095947\n",
      "iteration 42936: loss: 0.2164492905139923\n",
      "iteration 42937: loss: 0.21644917130470276\n",
      "iteration 42938: loss: 0.2164490669965744\n",
      "iteration 42939: loss: 0.21644893288612366\n",
      "iteration 42940: loss: 0.2164488136768341\n",
      "iteration 42941: loss: 0.21644873917102814\n",
      "iteration 42942: loss: 0.216448575258255\n",
      "iteration 42943: loss: 0.21644851565361023\n",
      "iteration 42944: loss: 0.21644839644432068\n",
      "iteration 42945: loss: 0.21644826233386993\n",
      "iteration 42946: loss: 0.216448113322258\n",
      "iteration 42947: loss: 0.21644797921180725\n",
      "iteration 42948: loss: 0.2164478749036789\n",
      "iteration 42949: loss: 0.21644780039787292\n",
      "iteration 42950: loss: 0.21644768118858337\n",
      "iteration 42951: loss: 0.21644756197929382\n",
      "iteration 42952: loss: 0.21644744277000427\n",
      "iteration 42953: loss: 0.21644730865955353\n",
      "iteration 42954: loss: 0.21644720435142517\n",
      "iteration 42955: loss: 0.21644704043865204\n",
      "iteration 42956: loss: 0.21644696593284607\n",
      "iteration 42957: loss: 0.2164469212293625\n",
      "iteration 42958: loss: 0.21644671261310577\n",
      "iteration 42959: loss: 0.21644660830497742\n",
      "iteration 42960: loss: 0.21644647419452667\n",
      "iteration 42961: loss: 0.21644635498523712\n",
      "iteration 42962: loss: 0.21644625067710876\n",
      "iteration 42963: loss: 0.21644611656665802\n",
      "iteration 42964: loss: 0.21644601225852966\n",
      "iteration 42965: loss: 0.21644580364227295\n",
      "iteration 42966: loss: 0.21644577383995056\n",
      "iteration 42967: loss: 0.21644572913646698\n",
      "iteration 42968: loss: 0.21644559502601624\n",
      "iteration 42969: loss: 0.2164454162120819\n",
      "iteration 42970: loss: 0.21644529700279236\n",
      "iteration 42971: loss: 0.2164452075958252\n",
      "iteration 42972: loss: 0.21644505858421326\n",
      "iteration 42973: loss: 0.2164449691772461\n",
      "iteration 42974: loss: 0.21644484996795654\n",
      "iteration 42975: loss: 0.216444730758667\n",
      "iteration 42976: loss: 0.21644464135169983\n",
      "iteration 42977: loss: 0.21644452214241028\n",
      "iteration 42978: loss: 0.21644429862499237\n",
      "iteration 42979: loss: 0.2164442539215088\n",
      "iteration 42980: loss: 0.21644410490989685\n",
      "iteration 42981: loss: 0.21644404530525208\n",
      "iteration 42982: loss: 0.21644392609596252\n",
      "iteration 42983: loss: 0.21644377708435059\n",
      "iteration 42984: loss: 0.21644365787506104\n",
      "iteration 42985: loss: 0.21644356846809387\n",
      "iteration 42986: loss: 0.21644344925880432\n",
      "iteration 42987: loss: 0.21644327044487\n",
      "iteration 42988: loss: 0.21644315123558044\n",
      "iteration 42989: loss: 0.21644309163093567\n",
      "iteration 42990: loss: 0.21644297242164612\n",
      "iteration 42991: loss: 0.21644285321235657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 42992: loss: 0.21644270420074463\n",
      "iteration 42993: loss: 0.2164425551891327\n",
      "iteration 42994: loss: 0.21644246578216553\n",
      "iteration 42995: loss: 0.21644239127635956\n",
      "iteration 42996: loss: 0.2164423018693924\n",
      "iteration 42997: loss: 0.21644210815429688\n",
      "iteration 42998: loss: 0.2164420187473297\n",
      "iteration 42999: loss: 0.21644189953804016\n",
      "iteration 43000: loss: 0.2164417803287506\n",
      "iteration 43001: loss: 0.21644163131713867\n",
      "iteration 43002: loss: 0.21644148230552673\n",
      "iteration 43003: loss: 0.21644143760204315\n",
      "iteration 43004: loss: 0.2164413183927536\n",
      "iteration 43005: loss: 0.21644118428230286\n",
      "iteration 43006: loss: 0.21644103527069092\n",
      "iteration 43007: loss: 0.21644100546836853\n",
      "iteration 43008: loss: 0.2164408415555954\n",
      "iteration 43009: loss: 0.21644075214862823\n",
      "iteration 43010: loss: 0.2164405882358551\n",
      "iteration 43011: loss: 0.21644048392772675\n",
      "iteration 43012: loss: 0.2164403647184372\n",
      "iteration 43013: loss: 0.21644023060798645\n",
      "iteration 43014: loss: 0.2164401262998581\n",
      "iteration 43015: loss: 0.21644005179405212\n",
      "iteration 43016: loss: 0.216439887881279\n",
      "iteration 43017: loss: 0.21643976867198944\n",
      "iteration 43018: loss: 0.21643969416618347\n",
      "iteration 43019: loss: 0.21643957495689392\n",
      "iteration 43020: loss: 0.2164394110441208\n",
      "iteration 43021: loss: 0.21643929183483124\n",
      "iteration 43022: loss: 0.2164391279220581\n",
      "iteration 43023: loss: 0.21643909811973572\n",
      "iteration 43024: loss: 0.21643893420696259\n",
      "iteration 43025: loss: 0.21643893420696259\n",
      "iteration 43026: loss: 0.21643874049186707\n",
      "iteration 43027: loss: 0.21643860638141632\n",
      "iteration 43028: loss: 0.21643848717212677\n",
      "iteration 43029: loss: 0.21643833816051483\n",
      "iteration 43030: loss: 0.21643829345703125\n",
      "iteration 43031: loss: 0.2164381444454193\n",
      "iteration 43032: loss: 0.21643798053264618\n",
      "iteration 43033: loss: 0.21643796563148499\n",
      "iteration 43034: loss: 0.21643777191638947\n",
      "iteration 43035: loss: 0.21643765270709991\n",
      "iteration 43036: loss: 0.21643754839897156\n",
      "iteration 43037: loss: 0.2164374589920044\n",
      "iteration 43038: loss: 0.21643733978271484\n",
      "iteration 43039: loss: 0.2164372205734253\n",
      "iteration 43040: loss: 0.21643705666065216\n",
      "iteration 43041: loss: 0.2164369523525238\n",
      "iteration 43042: loss: 0.21643686294555664\n",
      "iteration 43043: loss: 0.2164367139339447\n",
      "iteration 43044: loss: 0.21643662452697754\n",
      "iteration 43045: loss: 0.2164364755153656\n",
      "iteration 43046: loss: 0.21643635630607605\n",
      "iteration 43047: loss: 0.21643629670143127\n",
      "iteration 43048: loss: 0.21643610298633575\n",
      "iteration 43049: loss: 0.2164359986782074\n",
      "iteration 43050: loss: 0.21643586456775665\n",
      "iteration 43051: loss: 0.2164357453584671\n",
      "iteration 43052: loss: 0.21643564105033875\n",
      "iteration 43053: loss: 0.2164355218410492\n",
      "iteration 43054: loss: 0.21643546223640442\n",
      "iteration 43055: loss: 0.2164352685213089\n",
      "iteration 43056: loss: 0.21643514931201935\n",
      "iteration 43057: loss: 0.21643507480621338\n",
      "iteration 43058: loss: 0.21643495559692383\n",
      "iteration 43059: loss: 0.21643488109111786\n",
      "iteration 43060: loss: 0.21643467247486115\n",
      "iteration 43061: loss: 0.2164345681667328\n",
      "iteration 43062: loss: 0.21643447875976562\n",
      "iteration 43063: loss: 0.21643438935279846\n",
      "iteration 43064: loss: 0.21643424034118652\n",
      "iteration 43065: loss: 0.21643409132957458\n",
      "iteration 43066: loss: 0.2164340317249298\n",
      "iteration 43067: loss: 0.21643385291099548\n",
      "iteration 43068: loss: 0.21643376350402832\n",
      "iteration 43069: loss: 0.21643364429473877\n",
      "iteration 43070: loss: 0.2164335697889328\n",
      "iteration 43071: loss: 0.21643348038196564\n",
      "iteration 43072: loss: 0.21643328666687012\n",
      "iteration 43073: loss: 0.21643321216106415\n",
      "iteration 43074: loss: 0.21643312275409698\n",
      "iteration 43075: loss: 0.21643292903900146\n",
      "iteration 43076: loss: 0.2164328545331955\n",
      "iteration 43077: loss: 0.21643273532390594\n",
      "iteration 43078: loss: 0.2164325714111328\n",
      "iteration 43079: loss: 0.21643249690532684\n",
      "iteration 43080: loss: 0.21643240749835968\n",
      "iteration 43081: loss: 0.21643221378326416\n",
      "iteration 43082: loss: 0.216432124376297\n",
      "iteration 43083: loss: 0.21643200516700745\n",
      "iteration 43084: loss: 0.2164318561553955\n",
      "iteration 43085: loss: 0.21643176674842834\n",
      "iteration 43086: loss: 0.2164316475391388\n",
      "iteration 43087: loss: 0.21643154323101044\n",
      "iteration 43088: loss: 0.2164314240217209\n",
      "iteration 43089: loss: 0.21643128991127014\n",
      "iteration 43090: loss: 0.2164311408996582\n",
      "iteration 43091: loss: 0.21643109619617462\n",
      "iteration 43092: loss: 0.21643094718456268\n",
      "iteration 43093: loss: 0.2164308726787567\n",
      "iteration 43094: loss: 0.2164306938648224\n",
      "iteration 43095: loss: 0.21643057465553284\n",
      "iteration 43096: loss: 0.2164304256439209\n",
      "iteration 43097: loss: 0.21643027663230896\n",
      "iteration 43098: loss: 0.21643026173114777\n",
      "iteration 43099: loss: 0.21643014252185822\n",
      "iteration 43100: loss: 0.21642999351024628\n",
      "iteration 43101: loss: 0.21642987430095673\n",
      "iteration 43102: loss: 0.21642974019050598\n",
      "iteration 43103: loss: 0.21642959117889404\n",
      "iteration 43104: loss: 0.21642951667308807\n",
      "iteration 43105: loss: 0.2164294272661209\n",
      "iteration 43106: loss: 0.21642927825450897\n",
      "iteration 43107: loss: 0.21642911434173584\n",
      "iteration 43108: loss: 0.21642903983592987\n",
      "iteration 43109: loss: 0.2164289504289627\n",
      "iteration 43110: loss: 0.21642883121967316\n",
      "iteration 43111: loss: 0.21642863750457764\n",
      "iteration 43112: loss: 0.21642859280109406\n",
      "iteration 43113: loss: 0.21642844378948212\n",
      "iteration 43114: loss: 0.21642832458019257\n",
      "iteration 43115: loss: 0.2164282351732254\n",
      "iteration 43116: loss: 0.21642804145812988\n",
      "iteration 43117: loss: 0.21642795205116272\n",
      "iteration 43118: loss: 0.21642787754535675\n",
      "iteration 43119: loss: 0.21642771363258362\n",
      "iteration 43120: loss: 0.21642765402793884\n",
      "iteration 43121: loss: 0.2164275199174881\n",
      "iteration 43122: loss: 0.21642737090587616\n",
      "iteration 43123: loss: 0.21642720699310303\n",
      "iteration 43124: loss: 0.2164270579814911\n",
      "iteration 43125: loss: 0.2164270579814911\n",
      "iteration 43126: loss: 0.21642689406871796\n",
      "iteration 43127: loss: 0.2164268046617508\n",
      "iteration 43128: loss: 0.21642665565013885\n",
      "iteration 43129: loss: 0.2164265364408493\n",
      "iteration 43130: loss: 0.21642644703388214\n",
      "iteration 43131: loss: 0.2164262980222702\n",
      "iteration 43132: loss: 0.21642625331878662\n",
      "iteration 43133: loss: 0.2164260596036911\n",
      "iteration 43134: loss: 0.21642594039440155\n",
      "iteration 43135: loss: 0.216425821185112\n",
      "iteration 43136: loss: 0.21642574667930603\n",
      "iteration 43137: loss: 0.21642565727233887\n",
      "iteration 43138: loss: 0.21642546355724335\n",
      "iteration 43139: loss: 0.21642544865608215\n",
      "iteration 43140: loss: 0.21642521023750305\n",
      "iteration 43141: loss: 0.21642515063285828\n",
      "iteration 43142: loss: 0.21642503142356873\n",
      "iteration 43143: loss: 0.21642491221427917\n",
      "iteration 43144: loss: 0.21642474830150604\n",
      "iteration 43145: loss: 0.2164246141910553\n",
      "iteration 43146: loss: 0.21642449498176575\n",
      "iteration 43147: loss: 0.21642446517944336\n",
      "iteration 43148: loss: 0.21642427146434784\n",
      "iteration 43149: loss: 0.21642419695854187\n",
      "iteration 43150: loss: 0.2164241373538971\n",
      "iteration 43151: loss: 0.21642403304576874\n",
      "iteration 43152: loss: 0.21642382442951202\n",
      "iteration 43153: loss: 0.21642370522022247\n",
      "iteration 43154: loss: 0.21642358601093292\n",
      "iteration 43155: loss: 0.21642346680164337\n",
      "iteration 43156: loss: 0.21642336249351501\n",
      "iteration 43157: loss: 0.21642327308654785\n",
      "iteration 43158: loss: 0.21642310917377472\n",
      "iteration 43159: loss: 0.21642306447029114\n",
      "iteration 43160: loss: 0.2164228856563568\n",
      "iteration 43161: loss: 0.21642276644706726\n",
      "iteration 43162: loss: 0.2164226770401001\n",
      "iteration 43163: loss: 0.21642251312732697\n",
      "iteration 43164: loss: 0.21642246842384338\n",
      "iteration 43165: loss: 0.21642231941223145\n",
      "iteration 43166: loss: 0.2164222002029419\n",
      "iteration 43167: loss: 0.21642208099365234\n",
      "iteration 43168: loss: 0.2164219319820404\n",
      "iteration 43169: loss: 0.21642184257507324\n",
      "iteration 43170: loss: 0.21642164885997772\n",
      "iteration 43171: loss: 0.21642157435417175\n",
      "iteration 43172: loss: 0.2164214849472046\n",
      "iteration 43173: loss: 0.21642136573791504\n",
      "iteration 43174: loss: 0.2164212167263031\n",
      "iteration 43175: loss: 0.21642112731933594\n",
      "iteration 43176: loss: 0.216420978307724\n",
      "iteration 43177: loss: 0.21642088890075684\n",
      "iteration 43178: loss: 0.2164207398891449\n",
      "iteration 43179: loss: 0.21642062067985535\n",
      "iteration 43180: loss: 0.21642056107521057\n",
      "iteration 43181: loss: 0.21642038226127625\n",
      "iteration 43182: loss: 0.2164202630519867\n",
      "iteration 43183: loss: 0.21642017364501953\n",
      "iteration 43184: loss: 0.2164200097322464\n",
      "iteration 43185: loss: 0.21641996502876282\n",
      "iteration 43186: loss: 0.21641981601715088\n",
      "iteration 43187: loss: 0.21641966700553894\n",
      "iteration 43188: loss: 0.2164195328950882\n",
      "iteration 43189: loss: 0.21641938388347626\n",
      "iteration 43190: loss: 0.21641941368579865\n",
      "iteration 43191: loss: 0.21641919016838074\n",
      "iteration 43192: loss: 0.2164190709590912\n",
      "iteration 43193: loss: 0.21641893684864044\n",
      "iteration 43194: loss: 0.21641889214515686\n",
      "iteration 43195: loss: 0.21641869843006134\n",
      "iteration 43196: loss: 0.21641866862773895\n",
      "iteration 43197: loss: 0.21641850471496582\n",
      "iteration 43198: loss: 0.21641835570335388\n",
      "iteration 43199: loss: 0.21641826629638672\n",
      "iteration 43200: loss: 0.21641811728477478\n",
      "iteration 43201: loss: 0.21641802787780762\n",
      "iteration 43202: loss: 0.21641790866851807\n",
      "iteration 43203: loss: 0.21641775965690613\n",
      "iteration 43204: loss: 0.21641770005226135\n",
      "iteration 43205: loss: 0.21641755104064941\n",
      "iteration 43206: loss: 0.21641743183135986\n",
      "iteration 43207: loss: 0.2164173126220703\n",
      "iteration 43208: loss: 0.21641723811626434\n",
      "iteration 43209: loss: 0.2164170742034912\n",
      "iteration 43210: loss: 0.21641698479652405\n",
      "iteration 43211: loss: 0.2164168357849121\n",
      "iteration 43212: loss: 0.21641668677330017\n",
      "iteration 43213: loss: 0.2164166420698166\n",
      "iteration 43214: loss: 0.21641650795936584\n",
      "iteration 43215: loss: 0.2164163589477539\n",
      "iteration 43216: loss: 0.21641628444194794\n",
      "iteration 43217: loss: 0.2164161652326584\n",
      "iteration 43218: loss: 0.21641600131988525\n",
      "iteration 43219: loss: 0.2164158821105957\n",
      "iteration 43220: loss: 0.21641583740711212\n",
      "iteration 43221: loss: 0.21641559898853302\n",
      "iteration 43222: loss: 0.21641549468040466\n",
      "iteration 43223: loss: 0.2164154350757599\n",
      "iteration 43224: loss: 0.21641525626182556\n",
      "iteration 43225: loss: 0.2164151668548584\n",
      "iteration 43226: loss: 0.21641504764556885\n",
      "iteration 43227: loss: 0.21641497313976288\n",
      "iteration 43228: loss: 0.21641485393047333\n",
      "iteration 43229: loss: 0.21641473472118378\n",
      "iteration 43230: loss: 0.21641460061073303\n",
      "iteration 43231: loss: 0.21641449630260468\n",
      "iteration 43232: loss: 0.21641437709331512\n",
      "iteration 43233: loss: 0.216414213180542\n",
      "iteration 43234: loss: 0.21641406416893005\n",
      "iteration 43235: loss: 0.2164139747619629\n",
      "iteration 43236: loss: 0.21641382575035095\n",
      "iteration 43237: loss: 0.2164137065410614\n",
      "iteration 43238: loss: 0.2164136916399002\n",
      "iteration 43239: loss: 0.2164134979248047\n",
      "iteration 43240: loss: 0.21641337871551514\n",
      "iteration 43241: loss: 0.2164132297039032\n",
      "iteration 43242: loss: 0.21641311049461365\n",
      "iteration 43243: loss: 0.21641305088996887\n",
      "iteration 43244: loss: 0.21641293168067932\n",
      "iteration 43245: loss: 0.21641281247138977\n",
      "iteration 43246: loss: 0.2164127379655838\n",
      "iteration 43247: loss: 0.21641258895397186\n",
      "iteration 43248: loss: 0.21641242504119873\n",
      "iteration 43249: loss: 0.21641238033771515\n",
      "iteration 43250: loss: 0.21641214191913605\n",
      "iteration 43251: loss: 0.21641215682029724\n",
      "iteration 43252: loss: 0.21641194820404053\n",
      "iteration 43253: loss: 0.21641182899475098\n",
      "iteration 43254: loss: 0.2164117395877838\n",
      "iteration 43255: loss: 0.2164115160703659\n",
      "iteration 43256: loss: 0.21641147136688232\n",
      "iteration 43257: loss: 0.21641135215759277\n",
      "iteration 43258: loss: 0.21641118824481964\n",
      "iteration 43259: loss: 0.21641118824481964\n",
      "iteration 43260: loss: 0.2164110243320465\n",
      "iteration 43261: loss: 0.21641094982624054\n",
      "iteration 43262: loss: 0.2164108008146286\n",
      "iteration 43263: loss: 0.21641066670417786\n",
      "iteration 43264: loss: 0.2164105474948883\n",
      "iteration 43265: loss: 0.21641042828559875\n",
      "iteration 43266: loss: 0.2164103239774704\n",
      "iteration 43267: loss: 0.21641016006469727\n",
      "iteration 43268: loss: 0.2164100706577301\n",
      "iteration 43269: loss: 0.21640992164611816\n",
      "iteration 43270: loss: 0.2164098024368286\n",
      "iteration 43271: loss: 0.21640971302986145\n",
      "iteration 43272: loss: 0.21640953421592712\n",
      "iteration 43273: loss: 0.21640944480895996\n",
      "iteration 43274: loss: 0.21640929579734802\n",
      "iteration 43275: loss: 0.21640923619270325\n",
      "iteration 43276: loss: 0.21640904247760773\n",
      "iteration 43277: loss: 0.21640899777412415\n",
      "iteration 43278: loss: 0.2164088934659958\n",
      "iteration 43279: loss: 0.21640881896018982\n",
      "iteration 43280: loss: 0.21640868484973907\n",
      "iteration 43281: loss: 0.21640856564044952\n",
      "iteration 43282: loss: 0.21640846133232117\n",
      "iteration 43283: loss: 0.21640825271606445\n",
      "iteration 43284: loss: 0.21640817821025848\n",
      "iteration 43285: loss: 0.21640805900096893\n",
      "iteration 43286: loss: 0.2164078950881958\n",
      "iteration 43287: loss: 0.21640780568122864\n",
      "iteration 43288: loss: 0.2164076268672943\n",
      "iteration 43289: loss: 0.21640756726264954\n",
      "iteration 43290: loss: 0.21640744805335999\n",
      "iteration 43291: loss: 0.21640729904174805\n",
      "iteration 43292: loss: 0.2164071798324585\n",
      "iteration 43293: loss: 0.21640706062316895\n",
      "iteration 43294: loss: 0.21640698611736298\n",
      "iteration 43295: loss: 0.21640682220458984\n",
      "iteration 43296: loss: 0.21640673279762268\n",
      "iteration 43297: loss: 0.21640662848949432\n",
      "iteration 43298: loss: 0.2164064347743988\n",
      "iteration 43299: loss: 0.21640639007091522\n",
      "iteration 43300: loss: 0.21640627086162567\n",
      "iteration 43301: loss: 0.21640606224536896\n",
      "iteration 43302: loss: 0.216405987739563\n",
      "iteration 43303: loss: 0.21640591323375702\n",
      "iteration 43304: loss: 0.21640577912330627\n",
      "iteration 43305: loss: 0.2164057046175003\n",
      "iteration 43306: loss: 0.21640555560588837\n",
      "iteration 43307: loss: 0.21640542149543762\n",
      "iteration 43308: loss: 0.21640527248382568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 43309: loss: 0.21640519797801971\n",
      "iteration 43310: loss: 0.21640506386756897\n",
      "iteration 43311: loss: 0.21640494465827942\n",
      "iteration 43312: loss: 0.21640482544898987\n",
      "iteration 43313: loss: 0.21640467643737793\n",
      "iteration 43314: loss: 0.21640463173389435\n",
      "iteration 43315: loss: 0.216404527425766\n",
      "iteration 43316: loss: 0.21640434861183167\n",
      "iteration 43317: loss: 0.21640419960021973\n",
      "iteration 43318: loss: 0.2164040505886078\n",
      "iteration 43319: loss: 0.21640396118164062\n",
      "iteration 43320: loss: 0.21640391647815704\n",
      "iteration 43321: loss: 0.2164037525653839\n",
      "iteration 43322: loss: 0.21640364825725555\n",
      "iteration 43323: loss: 0.21640348434448242\n",
      "iteration 43324: loss: 0.21640336513519287\n",
      "iteration 43325: loss: 0.2164032757282257\n",
      "iteration 43326: loss: 0.21640315651893616\n",
      "iteration 43327: loss: 0.21640300750732422\n",
      "iteration 43328: loss: 0.21640291810035706\n",
      "iteration 43329: loss: 0.21640276908874512\n",
      "iteration 43330: loss: 0.21640267968177795\n",
      "iteration 43331: loss: 0.2164025604724884\n",
      "iteration 43332: loss: 0.21640248596668243\n",
      "iteration 43333: loss: 0.21640229225158691\n",
      "iteration 43334: loss: 0.21640217304229736\n",
      "iteration 43335: loss: 0.2164020538330078\n",
      "iteration 43336: loss: 0.21640193462371826\n",
      "iteration 43337: loss: 0.2164018601179123\n",
      "iteration 43338: loss: 0.21640172600746155\n",
      "iteration 43339: loss: 0.21640154719352722\n",
      "iteration 43340: loss: 0.21640148758888245\n",
      "iteration 43341: loss: 0.21640141308307648\n",
      "iteration 43342: loss: 0.21640124917030334\n",
      "iteration 43343: loss: 0.21640107035636902\n",
      "iteration 43344: loss: 0.21640105545520782\n",
      "iteration 43345: loss: 0.2164009064435959\n",
      "iteration 43346: loss: 0.21640077233314514\n",
      "iteration 43347: loss: 0.2164006233215332\n",
      "iteration 43348: loss: 0.21640054881572723\n",
      "iteration 43349: loss: 0.2164004147052765\n",
      "iteration 43350: loss: 0.21640031039714813\n",
      "iteration 43351: loss: 0.216400146484375\n",
      "iteration 43352: loss: 0.21640002727508545\n",
      "iteration 43353: loss: 0.2163998782634735\n",
      "iteration 43354: loss: 0.21639975905418396\n",
      "iteration 43355: loss: 0.21639971435070038\n",
      "iteration 43356: loss: 0.21639959514141083\n",
      "iteration 43357: loss: 0.2163994312286377\n",
      "iteration 43358: loss: 0.21639931201934814\n",
      "iteration 43359: loss: 0.21639922261238098\n",
      "iteration 43360: loss: 0.21639911830425262\n",
      "iteration 43361: loss: 0.21639898419380188\n",
      "iteration 43362: loss: 0.21639886498451233\n",
      "iteration 43363: loss: 0.21639874577522278\n",
      "iteration 43364: loss: 0.21639864146709442\n",
      "iteration 43365: loss: 0.21639850735664368\n",
      "iteration 43366: loss: 0.21639835834503174\n",
      "iteration 43367: loss: 0.2163982093334198\n",
      "iteration 43368: loss: 0.21639816462993622\n",
      "iteration 43369: loss: 0.21639803051948547\n",
      "iteration 43370: loss: 0.21639792621135712\n",
      "iteration 43371: loss: 0.21639780700206757\n",
      "iteration 43372: loss: 0.21639767289161682\n",
      "iteration 43373: loss: 0.21639752388000488\n",
      "iteration 43374: loss: 0.21639740467071533\n",
      "iteration 43375: loss: 0.2163972556591034\n",
      "iteration 43376: loss: 0.21639719605445862\n",
      "iteration 43377: loss: 0.21639713644981384\n",
      "iteration 43378: loss: 0.21639689803123474\n",
      "iteration 43379: loss: 0.21639680862426758\n",
      "iteration 43380: loss: 0.21639668941497803\n",
      "iteration 43381: loss: 0.21639660000801086\n",
      "iteration 43382: loss: 0.21639642119407654\n",
      "iteration 43383: loss: 0.21639633178710938\n",
      "iteration 43384: loss: 0.2163962423801422\n",
      "iteration 43385: loss: 0.21639613807201385\n",
      "iteration 43386: loss: 0.2163960039615631\n",
      "iteration 43387: loss: 0.21639585494995117\n",
      "iteration 43388: loss: 0.21639573574066162\n",
      "iteration 43389: loss: 0.21639566123485565\n",
      "iteration 43390: loss: 0.21639549732208252\n",
      "iteration 43391: loss: 0.21639537811279297\n",
      "iteration 43392: loss: 0.216395303606987\n",
      "iteration 43393: loss: 0.21639516949653625\n",
      "iteration 43394: loss: 0.2163950502872467\n",
      "iteration 43395: loss: 0.21639493107795715\n",
      "iteration 43396: loss: 0.2163948118686676\n",
      "iteration 43397: loss: 0.21639466285705566\n",
      "iteration 43398: loss: 0.2163945734500885\n",
      "iteration 43399: loss: 0.21639445424079895\n",
      "iteration 43400: loss: 0.2163943350315094\n",
      "iteration 43401: loss: 0.21639415621757507\n",
      "iteration 43402: loss: 0.2163940966129303\n",
      "iteration 43403: loss: 0.21639394760131836\n",
      "iteration 43404: loss: 0.2163938283920288\n",
      "iteration 43405: loss: 0.21639370918273926\n",
      "iteration 43406: loss: 0.2163936197757721\n",
      "iteration 43407: loss: 0.21639347076416016\n",
      "iteration 43408: loss: 0.216393381357193\n",
      "iteration 43409: loss: 0.21639320254325867\n",
      "iteration 43410: loss: 0.21639308333396912\n",
      "iteration 43411: loss: 0.21639302372932434\n",
      "iteration 43412: loss: 0.2163929045200348\n",
      "iteration 43413: loss: 0.21639280021190643\n",
      "iteration 43414: loss: 0.2163926064968109\n",
      "iteration 43415: loss: 0.21639251708984375\n",
      "iteration 43416: loss: 0.2163923680782318\n",
      "iteration 43417: loss: 0.21639224886894226\n",
      "iteration 43418: loss: 0.21639211475849152\n",
      "iteration 43419: loss: 0.21639207005500793\n",
      "iteration 43420: loss: 0.216391921043396\n",
      "iteration 43421: loss: 0.21639183163642883\n",
      "iteration 43422: loss: 0.2163916826248169\n",
      "iteration 43423: loss: 0.21639151871204376\n",
      "iteration 43424: loss: 0.2163913995027542\n",
      "iteration 43425: loss: 0.21639125049114227\n",
      "iteration 43426: loss: 0.2163912057876587\n",
      "iteration 43427: loss: 0.21639108657836914\n",
      "iteration 43428: loss: 0.21639089286327362\n",
      "iteration 43429: loss: 0.21639081835746765\n",
      "iteration 43430: loss: 0.2163906991481781\n",
      "iteration 43431: loss: 0.21639057993888855\n",
      "iteration 43432: loss: 0.2163904905319214\n",
      "iteration 43433: loss: 0.21639032661914825\n",
      "iteration 43434: loss: 0.21639028191566467\n",
      "iteration 43435: loss: 0.21639013290405273\n",
      "iteration 43436: loss: 0.2163899838924408\n",
      "iteration 43437: loss: 0.21638986468315125\n",
      "iteration 43438: loss: 0.21638980507850647\n",
      "iteration 43439: loss: 0.21638965606689453\n",
      "iteration 43440: loss: 0.2163895070552826\n",
      "iteration 43441: loss: 0.21638941764831543\n",
      "iteration 43442: loss: 0.21638929843902588\n",
      "iteration 43443: loss: 0.21638910472393036\n",
      "iteration 43444: loss: 0.21638910472393036\n",
      "iteration 43445: loss: 0.21638891100883484\n",
      "iteration 43446: loss: 0.2163887917995453\n",
      "iteration 43447: loss: 0.21638870239257812\n",
      "iteration 43448: loss: 0.21638858318328857\n",
      "iteration 43449: loss: 0.21638838946819305\n",
      "iteration 43450: loss: 0.21638834476470947\n",
      "iteration 43451: loss: 0.21638819575309753\n",
      "iteration 43452: loss: 0.2163880318403244\n",
      "iteration 43453: loss: 0.2163880169391632\n",
      "iteration 43454: loss: 0.21638786792755127\n",
      "iteration 43455: loss: 0.21638774871826172\n",
      "iteration 43456: loss: 0.2163875848054886\n",
      "iteration 43457: loss: 0.21638751029968262\n",
      "iteration 43458: loss: 0.21638734638690948\n",
      "iteration 43459: loss: 0.21638724207878113\n",
      "iteration 43460: loss: 0.21638712286949158\n",
      "iteration 43461: loss: 0.216387078166008\n",
      "iteration 43462: loss: 0.21638686954975128\n",
      "iteration 43463: loss: 0.2163867950439453\n",
      "iteration 43464: loss: 0.2163865864276886\n",
      "iteration 43465: loss: 0.21638652682304382\n",
      "iteration 43466: loss: 0.21638640761375427\n",
      "iteration 43467: loss: 0.2163863182067871\n",
      "iteration 43468: loss: 0.21638615429401398\n",
      "iteration 43469: loss: 0.216386079788208\n",
      "iteration 43470: loss: 0.21638593077659607\n",
      "iteration 43471: loss: 0.2163858413696289\n",
      "iteration 43472: loss: 0.21638569235801697\n",
      "iteration 43473: loss: 0.21638555824756622\n",
      "iteration 43474: loss: 0.21638545393943787\n",
      "iteration 43475: loss: 0.21638533473014832\n",
      "iteration 43476: loss: 0.21638517081737518\n",
      "iteration 43477: loss: 0.2163851261138916\n",
      "iteration 43478: loss: 0.21638497710227966\n",
      "iteration 43479: loss: 0.2163848876953125\n",
      "iteration 43480: loss: 0.21638473868370056\n",
      "iteration 43481: loss: 0.21638460457324982\n",
      "iteration 43482: loss: 0.21638448536396027\n",
      "iteration 43483: loss: 0.21638436615467072\n",
      "iteration 43484: loss: 0.21638421714305878\n",
      "iteration 43485: loss: 0.2163841277360916\n",
      "iteration 43486: loss: 0.21638402342796326\n",
      "iteration 43487: loss: 0.21638397872447968\n",
      "iteration 43488: loss: 0.21638374030590057\n",
      "iteration 43489: loss: 0.21638362109661102\n",
      "iteration 43490: loss: 0.21638350188732147\n",
      "iteration 43491: loss: 0.21638338267803192\n",
      "iteration 43492: loss: 0.21638330817222595\n",
      "iteration 43493: loss: 0.2163831740617752\n",
      "iteration 43494: loss: 0.21638301014900208\n",
      "iteration 43495: loss: 0.2163829356431961\n",
      "iteration 43496: loss: 0.21638278663158417\n",
      "iteration 43497: loss: 0.216382697224617\n",
      "iteration 43498: loss: 0.21638262271881104\n",
      "iteration 43499: loss: 0.21638250350952148\n",
      "iteration 43500: loss: 0.21638230979442596\n",
      "iteration 43501: loss: 0.2163822203874588\n",
      "iteration 43502: loss: 0.21638210117816925\n",
      "iteration 43503: loss: 0.21638190746307373\n",
      "iteration 43504: loss: 0.21638187766075134\n",
      "iteration 43505: loss: 0.21638169884681702\n",
      "iteration 43506: loss: 0.21638157963752747\n",
      "iteration 43507: loss: 0.2163814753293991\n",
      "iteration 43508: loss: 0.21638138592243195\n",
      "iteration 43509: loss: 0.2163812667131424\n",
      "iteration 43510: loss: 0.21638111770153046\n",
      "iteration 43511: loss: 0.21638095378875732\n",
      "iteration 43512: loss: 0.21638083457946777\n",
      "iteration 43513: loss: 0.2163807898759842\n",
      "iteration 43514: loss: 0.21638064086437225\n",
      "iteration 43515: loss: 0.21638056635856628\n",
      "iteration 43516: loss: 0.21638043224811554\n",
      "iteration 43517: loss: 0.2163802683353424\n",
      "iteration 43518: loss: 0.21638020873069763\n",
      "iteration 43519: loss: 0.21638014912605286\n",
      "iteration 43520: loss: 0.21637992560863495\n",
      "iteration 43521: loss: 0.2163798063993454\n",
      "iteration 43522: loss: 0.21637964248657227\n",
      "iteration 43523: loss: 0.2163795530796051\n",
      "iteration 43524: loss: 0.21637947857379913\n",
      "iteration 43525: loss: 0.2163793295621872\n",
      "iteration 43526: loss: 0.21637916564941406\n",
      "iteration 43527: loss: 0.2163790762424469\n",
      "iteration 43528: loss: 0.21637897193431854\n",
      "iteration 43529: loss: 0.2163788378238678\n",
      "iteration 43530: loss: 0.21637868881225586\n",
      "iteration 43531: loss: 0.2163785994052887\n",
      "iteration 43532: loss: 0.21637849509716034\n",
      "iteration 43533: loss: 0.2163783758878708\n",
      "iteration 43534: loss: 0.21637824177742004\n",
      "iteration 43535: loss: 0.2163781225681305\n",
      "iteration 43536: loss: 0.21637797355651855\n",
      "iteration 43537: loss: 0.21637782454490662\n",
      "iteration 43538: loss: 0.21637780964374542\n",
      "iteration 43539: loss: 0.2163776159286499\n",
      "iteration 43540: loss: 0.21637749671936035\n",
      "iteration 43541: loss: 0.2163773775100708\n",
      "iteration 43542: loss: 0.21637721359729767\n",
      "iteration 43543: loss: 0.2163771688938141\n",
      "iteration 43544: loss: 0.21637699007987976\n",
      "iteration 43545: loss: 0.21637694537639618\n",
      "iteration 43546: loss: 0.21637675166130066\n",
      "iteration 43547: loss: 0.2163766622543335\n",
      "iteration 43548: loss: 0.21637657284736633\n",
      "iteration 43549: loss: 0.21637645363807678\n",
      "iteration 43550: loss: 0.21637633442878723\n",
      "iteration 43551: loss: 0.21637623012065887\n",
      "iteration 43552: loss: 0.21637611091136932\n",
      "iteration 43553: loss: 0.21637597680091858\n",
      "iteration 43554: loss: 0.21637575328350067\n",
      "iteration 43555: loss: 0.2163756787776947\n",
      "iteration 43556: loss: 0.21637558937072754\n",
      "iteration 43557: loss: 0.21637549996376038\n",
      "iteration 43558: loss: 0.21637538075447083\n",
      "iteration 43559: loss: 0.2163752019405365\n",
      "iteration 43560: loss: 0.21637506783008575\n",
      "iteration 43561: loss: 0.21637502312660217\n",
      "iteration 43562: loss: 0.21637491881847382\n",
      "iteration 43563: loss: 0.2163747251033783\n",
      "iteration 43564: loss: 0.21637463569641113\n",
      "iteration 43565: loss: 0.21637439727783203\n",
      "iteration 43566: loss: 0.21637432277202606\n",
      "iteration 43567: loss: 0.21637427806854248\n",
      "iteration 43568: loss: 0.21637418866157532\n",
      "iteration 43569: loss: 0.216374009847641\n",
      "iteration 43570: loss: 0.21637392044067383\n",
      "iteration 43571: loss: 0.2163737565279007\n",
      "iteration 43572: loss: 0.21637365221977234\n",
      "iteration 43573: loss: 0.21637359261512756\n",
      "iteration 43574: loss: 0.2163734883069992\n",
      "iteration 43575: loss: 0.21637332439422607\n",
      "iteration 43576: loss: 0.21637317538261414\n",
      "iteration 43577: loss: 0.21637305617332458\n",
      "iteration 43578: loss: 0.21637289226055145\n",
      "iteration 43579: loss: 0.21637281775474548\n",
      "iteration 43580: loss: 0.21637268364429474\n",
      "iteration 43581: loss: 0.21637257933616638\n",
      "iteration 43582: loss: 0.2163725346326828\n",
      "iteration 43583: loss: 0.2163722962141037\n",
      "iteration 43584: loss: 0.21637222170829773\n",
      "iteration 43585: loss: 0.21637213230133057\n",
      "iteration 43586: loss: 0.21637193858623505\n",
      "iteration 43587: loss: 0.21637186408042908\n",
      "iteration 43588: loss: 0.21637177467346191\n",
      "iteration 43589: loss: 0.2163715660572052\n",
      "iteration 43590: loss: 0.2163715362548828\n",
      "iteration 43591: loss: 0.21637137234210968\n",
      "iteration 43592: loss: 0.21637126803398132\n",
      "iteration 43593: loss: 0.216371089220047\n",
      "iteration 43594: loss: 0.21637098491191864\n",
      "iteration 43595: loss: 0.21637089550495148\n",
      "iteration 43596: loss: 0.21637077629566193\n",
      "iteration 43597: loss: 0.21637070178985596\n",
      "iteration 43598: loss: 0.21637049317359924\n",
      "iteration 43599: loss: 0.2163703739643097\n",
      "iteration 43600: loss: 0.2163703441619873\n",
      "iteration 43601: loss: 0.21637018024921417\n",
      "iteration 43602: loss: 0.21637003123760223\n",
      "iteration 43603: loss: 0.21636994183063507\n",
      "iteration 43604: loss: 0.2163698375225067\n",
      "iteration 43605: loss: 0.21636967360973358\n",
      "iteration 43606: loss: 0.21636962890625\n",
      "iteration 43607: loss: 0.21636943519115448\n",
      "iteration 43608: loss: 0.2163693606853485\n",
      "iteration 43609: loss: 0.21636919677257538\n",
      "iteration 43610: loss: 0.21636900305747986\n",
      "iteration 43611: loss: 0.21636900305747986\n",
      "iteration 43612: loss: 0.2163688689470291\n",
      "iteration 43613: loss: 0.21636871993541718\n",
      "iteration 43614: loss: 0.21636858582496643\n",
      "iteration 43615: loss: 0.21636846661567688\n",
      "iteration 43616: loss: 0.2163683921098709\n",
      "iteration 43617: loss: 0.21636822819709778\n",
      "iteration 43618: loss: 0.21636810898780823\n",
      "iteration 43619: loss: 0.21636800467967987\n",
      "iteration 43620: loss: 0.2163679152727127\n",
      "iteration 43621: loss: 0.2163676917552948\n",
      "iteration 43622: loss: 0.21636757254600525\n",
      "iteration 43623: loss: 0.2163674533367157\n",
      "iteration 43624: loss: 0.21636739373207092\n",
      "iteration 43625: loss: 0.21636724472045898\n",
      "iteration 43626: loss: 0.21636717021465302\n",
      "iteration 43627: loss: 0.21636703610420227\n",
      "iteration 43628: loss: 0.21636685729026794\n",
      "iteration 43629: loss: 0.21636681258678436\n",
      "iteration 43630: loss: 0.21636664867401123\n",
      "iteration 43631: loss: 0.2163664996623993\n",
      "iteration 43632: loss: 0.21636638045310974\n",
      "iteration 43633: loss: 0.21636636555194855\n",
      "iteration 43634: loss: 0.21636620163917542\n",
      "iteration 43635: loss: 0.21636605262756348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 43636: loss: 0.21636593341827393\n",
      "iteration 43637: loss: 0.21636581420898438\n",
      "iteration 43638: loss: 0.2163657397031784\n",
      "iteration 43639: loss: 0.21636557579040527\n",
      "iteration 43640: loss: 0.21636542677879333\n",
      "iteration 43641: loss: 0.21636533737182617\n",
      "iteration 43642: loss: 0.21636521816253662\n",
      "iteration 43643: loss: 0.21636509895324707\n",
      "iteration 43644: loss: 0.2163650244474411\n",
      "iteration 43645: loss: 0.21636486053466797\n",
      "iteration 43646: loss: 0.21636474132537842\n",
      "iteration 43647: loss: 0.21636459231376648\n",
      "iteration 43648: loss: 0.21636442840099335\n",
      "iteration 43649: loss: 0.21636435389518738\n",
      "iteration 43650: loss: 0.21636423468589783\n",
      "iteration 43651: loss: 0.21636410057544708\n",
      "iteration 43652: loss: 0.21636399626731873\n",
      "iteration 43653: loss: 0.2163638174533844\n",
      "iteration 43654: loss: 0.21636371314525604\n",
      "iteration 43655: loss: 0.2163635939359665\n",
      "iteration 43656: loss: 0.2163635790348053\n",
      "iteration 43657: loss: 0.21636345982551575\n",
      "iteration 43658: loss: 0.21636326611042023\n",
      "iteration 43659: loss: 0.21636316180229187\n",
      "iteration 43660: loss: 0.21636304259300232\n",
      "iteration 43661: loss: 0.21636290848255157\n",
      "iteration 43662: loss: 0.21636275947093964\n",
      "iteration 43663: loss: 0.21636267006397247\n",
      "iteration 43664: loss: 0.21636250615119934\n",
      "iteration 43665: loss: 0.2163623869419098\n",
      "iteration 43666: loss: 0.21636228263378143\n",
      "iteration 43667: loss: 0.21636220812797546\n",
      "iteration 43668: loss: 0.21636204421520233\n",
      "iteration 43669: loss: 0.21636192500591278\n",
      "iteration 43670: loss: 0.21636180579662323\n",
      "iteration 43671: loss: 0.21636167168617249\n",
      "iteration 43672: loss: 0.21636156737804413\n",
      "iteration 43673: loss: 0.21636144816875458\n",
      "iteration 43674: loss: 0.21636128425598145\n",
      "iteration 43675: loss: 0.21636125445365906\n",
      "iteration 43676: loss: 0.21636104583740234\n",
      "iteration 43677: loss: 0.21636095643043518\n",
      "iteration 43678: loss: 0.21636083722114563\n",
      "iteration 43679: loss: 0.21636073291301727\n",
      "iteration 43680: loss: 0.21636059880256653\n",
      "iteration 43681: loss: 0.21636056900024414\n",
      "iteration 43682: loss: 0.216360405087471\n",
      "iteration 43683: loss: 0.21636024117469788\n",
      "iteration 43684: loss: 0.2163601666688919\n",
      "iteration 43685: loss: 0.21636000275611877\n",
      "iteration 43686: loss: 0.21635988354682922\n",
      "iteration 43687: loss: 0.21635973453521729\n",
      "iteration 43688: loss: 0.2163596898317337\n",
      "iteration 43689: loss: 0.21635952591896057\n",
      "iteration 43690: loss: 0.21635937690734863\n",
      "iteration 43691: loss: 0.21635934710502625\n",
      "iteration 43692: loss: 0.2163591831922531\n",
      "iteration 43693: loss: 0.21635910868644714\n",
      "iteration 43694: loss: 0.21635887026786804\n",
      "iteration 43695: loss: 0.21635878086090088\n",
      "iteration 43696: loss: 0.21635869145393372\n",
      "iteration 43697: loss: 0.21635857224464417\n",
      "iteration 43698: loss: 0.21635837852954865\n",
      "iteration 43699: loss: 0.21635830402374268\n",
      "iteration 43700: loss: 0.21635818481445312\n",
      "iteration 43701: loss: 0.21635806560516357\n",
      "iteration 43702: loss: 0.21635794639587402\n",
      "iteration 43703: loss: 0.21635785698890686\n",
      "iteration 43704: loss: 0.21635766327381134\n",
      "iteration 43705: loss: 0.21635755896568298\n",
      "iteration 43706: loss: 0.21635743975639343\n",
      "iteration 43707: loss: 0.21635732054710388\n",
      "iteration 43708: loss: 0.21635720133781433\n",
      "iteration 43709: loss: 0.21635708212852478\n",
      "iteration 43710: loss: 0.21635699272155762\n",
      "iteration 43711: loss: 0.21635684370994568\n",
      "iteration 43712: loss: 0.21635672450065613\n",
      "iteration 43713: loss: 0.21635663509368896\n",
      "iteration 43714: loss: 0.21635647118091583\n",
      "iteration 43715: loss: 0.21635636687278748\n",
      "iteration 43716: loss: 0.21635623276233673\n",
      "iteration 43717: loss: 0.21635618805885315\n",
      "iteration 43718: loss: 0.21635596454143524\n",
      "iteration 43719: loss: 0.21635587513446808\n",
      "iteration 43720: loss: 0.21635577082633972\n",
      "iteration 43721: loss: 0.21635563671588898\n",
      "iteration 43722: loss: 0.21635548770427704\n",
      "iteration 43723: loss: 0.21635541319847107\n",
      "iteration 43724: loss: 0.21635529398918152\n",
      "iteration 43725: loss: 0.21635517477989197\n",
      "iteration 43726: loss: 0.21635499596595764\n",
      "iteration 43727: loss: 0.2163548469543457\n",
      "iteration 43728: loss: 0.21635481715202332\n",
      "iteration 43729: loss: 0.21635465323925018\n",
      "iteration 43730: loss: 0.21635453402996063\n",
      "iteration 43731: loss: 0.21635445952415466\n",
      "iteration 43732: loss: 0.21635429561138153\n",
      "iteration 43733: loss: 0.21635417640209198\n",
      "iteration 43734: loss: 0.21635405719280243\n",
      "iteration 43735: loss: 0.21635396778583527\n",
      "iteration 43736: loss: 0.21635372936725616\n",
      "iteration 43737: loss: 0.21635369956493378\n",
      "iteration 43738: loss: 0.21635358035564423\n",
      "iteration 43739: loss: 0.21635346114635468\n",
      "iteration 43740: loss: 0.2163533717393875\n",
      "iteration 43741: loss: 0.216353178024292\n",
      "iteration 43742: loss: 0.21635302901268005\n",
      "iteration 43743: loss: 0.21635298430919647\n",
      "iteration 43744: loss: 0.2163528949022293\n",
      "iteration 43745: loss: 0.21635274589061737\n",
      "iteration 43746: loss: 0.21635261178016663\n",
      "iteration 43747: loss: 0.21635250747203827\n",
      "iteration 43748: loss: 0.21635234355926514\n",
      "iteration 43749: loss: 0.21635231375694275\n",
      "iteration 43750: loss: 0.21635213494300842\n",
      "iteration 43751: loss: 0.21635203063488007\n",
      "iteration 43752: loss: 0.21635183691978455\n",
      "iteration 43753: loss: 0.21635174751281738\n",
      "iteration 43754: loss: 0.21635165810585022\n",
      "iteration 43755: loss: 0.21635150909423828\n",
      "iteration 43756: loss: 0.2163514345884323\n",
      "iteration 43757: loss: 0.21635131537914276\n",
      "iteration 43758: loss: 0.21635112166404724\n",
      "iteration 43759: loss: 0.21635103225708008\n",
      "iteration 43760: loss: 0.21635091304779053\n",
      "iteration 43761: loss: 0.21635082364082336\n",
      "iteration 43762: loss: 0.21635062992572784\n",
      "iteration 43763: loss: 0.2163504809141159\n",
      "iteration 43764: loss: 0.2163504660129547\n",
      "iteration 43765: loss: 0.21635034680366516\n",
      "iteration 43766: loss: 0.21635016798973083\n",
      "iteration 43767: loss: 0.21635010838508606\n",
      "iteration 43768: loss: 0.21634991466999054\n",
      "iteration 43769: loss: 0.2163497656583786\n",
      "iteration 43770: loss: 0.21634964644908905\n",
      "iteration 43771: loss: 0.21634957194328308\n",
      "iteration 43772: loss: 0.21634943783283234\n",
      "iteration 43773: loss: 0.21634939312934875\n",
      "iteration 43774: loss: 0.21634915471076965\n",
      "iteration 43775: loss: 0.21634909510612488\n",
      "iteration 43776: loss: 0.21634900569915771\n",
      "iteration 43777: loss: 0.21634884178638458\n",
      "iteration 43778: loss: 0.21634869277477264\n",
      "iteration 43779: loss: 0.2163485586643219\n",
      "iteration 43780: loss: 0.21634849905967712\n",
      "iteration 43781: loss: 0.216348335146904\n",
      "iteration 43782: loss: 0.2163483202457428\n",
      "iteration 43783: loss: 0.21634812653064728\n",
      "iteration 43784: loss: 0.21634800732135773\n",
      "iteration 43785: loss: 0.21634790301322937\n",
      "iteration 43786: loss: 0.21634776890277863\n",
      "iteration 43787: loss: 0.21634769439697266\n",
      "iteration 43788: loss: 0.21634754538536072\n",
      "iteration 43789: loss: 0.21634741127490997\n",
      "iteration 43790: loss: 0.21634724736213684\n",
      "iteration 43791: loss: 0.2163471281528473\n",
      "iteration 43792: loss: 0.21634697914123535\n",
      "iteration 43793: loss: 0.21634694933891296\n",
      "iteration 43794: loss: 0.21634678542613983\n",
      "iteration 43795: loss: 0.2163465917110443\n",
      "iteration 43796: loss: 0.21634650230407715\n",
      "iteration 43797: loss: 0.21634641289710999\n",
      "iteration 43798: loss: 0.21634630858898163\n",
      "iteration 43799: loss: 0.21634618937969208\n",
      "iteration 43800: loss: 0.21634602546691895\n",
      "iteration 43801: loss: 0.21634602546691895\n",
      "iteration 43802: loss: 0.21634575724601746\n",
      "iteration 43803: loss: 0.21634571254253387\n",
      "iteration 43804: loss: 0.21634554862976074\n",
      "iteration 43805: loss: 0.21634545922279358\n",
      "iteration 43806: loss: 0.21634535491466522\n",
      "iteration 43807: loss: 0.2163451611995697\n",
      "iteration 43808: loss: 0.21634507179260254\n",
      "iteration 43809: loss: 0.2163449078798294\n",
      "iteration 43810: loss: 0.21634478867053986\n",
      "iteration 43811: loss: 0.21634478867053986\n",
      "iteration 43812: loss: 0.21634455025196075\n",
      "iteration 43813: loss: 0.21634450554847717\n",
      "iteration 43814: loss: 0.21634426712989807\n",
      "iteration 43815: loss: 0.21634426712989807\n",
      "iteration 43816: loss: 0.21634408831596375\n",
      "iteration 43817: loss: 0.21634404361248016\n",
      "iteration 43818: loss: 0.21634384989738464\n",
      "iteration 43819: loss: 0.2163437306880951\n",
      "iteration 43820: loss: 0.21634359657764435\n",
      "iteration 43821: loss: 0.2163434475660324\n",
      "iteration 43822: loss: 0.21634340286254883\n",
      "iteration 43823: loss: 0.2163432538509369\n",
      "iteration 43824: loss: 0.21634308993816376\n",
      "iteration 43825: loss: 0.2163429707288742\n",
      "iteration 43826: loss: 0.21634288132190704\n",
      "iteration 43827: loss: 0.21634280681610107\n",
      "iteration 43828: loss: 0.21634265780448914\n",
      "iteration 43829: loss: 0.2163425236940384\n",
      "iteration 43830: loss: 0.21634235978126526\n",
      "iteration 43831: loss: 0.2163422852754593\n",
      "iteration 43832: loss: 0.21634216606616974\n",
      "iteration 43833: loss: 0.2163420468568802\n",
      "iteration 43834: loss: 0.21634185314178467\n",
      "iteration 43835: loss: 0.21634173393249512\n",
      "iteration 43836: loss: 0.21634165942668915\n",
      "iteration 43837: loss: 0.2163415402173996\n",
      "iteration 43838: loss: 0.21634145081043243\n",
      "iteration 43839: loss: 0.21634125709533691\n",
      "iteration 43840: loss: 0.21634116768836975\n",
      "iteration 43841: loss: 0.21634110808372498\n",
      "iteration 43842: loss: 0.21634092926979065\n",
      "iteration 43843: loss: 0.2163408249616623\n",
      "iteration 43844: loss: 0.21634063124656677\n",
      "iteration 43845: loss: 0.2163405418395996\n",
      "iteration 43846: loss: 0.21634039282798767\n",
      "iteration 43847: loss: 0.21634027361869812\n",
      "iteration 43848: loss: 0.21634021401405334\n",
      "iteration 43849: loss: 0.21634003520011902\n",
      "iteration 43850: loss: 0.21633994579315186\n",
      "iteration 43851: loss: 0.2163398563861847\n",
      "iteration 43852: loss: 0.21633978188037872\n",
      "iteration 43853: loss: 0.2163395881652832\n",
      "iteration 43854: loss: 0.21633943915367126\n",
      "iteration 43855: loss: 0.2163393795490265\n",
      "iteration 43856: loss: 0.21633920073509216\n",
      "iteration 43857: loss: 0.2163390815258026\n",
      "iteration 43858: loss: 0.21633899211883545\n",
      "iteration 43859: loss: 0.2163388729095459\n",
      "iteration 43860: loss: 0.21633875370025635\n",
      "iteration 43861: loss: 0.21633855998516083\n",
      "iteration 43862: loss: 0.21633848547935486\n",
      "iteration 43863: loss: 0.2163383662700653\n",
      "iteration 43864: loss: 0.21633824706077576\n",
      "iteration 43865: loss: 0.2163381278514862\n",
      "iteration 43866: loss: 0.21633803844451904\n",
      "iteration 43867: loss: 0.2163378745317459\n",
      "iteration 43868: loss: 0.21633777022361755\n",
      "iteration 43869: loss: 0.21633760631084442\n",
      "iteration 43870: loss: 0.21633753180503845\n",
      "iteration 43871: loss: 0.2163374125957489\n",
      "iteration 43872: loss: 0.21633724868297577\n",
      "iteration 43873: loss: 0.2163371741771698\n",
      "iteration 43874: loss: 0.21633705496788025\n",
      "iteration 43875: loss: 0.21633689105510712\n",
      "iteration 43876: loss: 0.21633681654930115\n",
      "iteration 43877: loss: 0.21633663773536682\n",
      "iteration 43878: loss: 0.21633651852607727\n",
      "iteration 43879: loss: 0.2163364142179489\n",
      "iteration 43880: loss: 0.21633628010749817\n",
      "iteration 43881: loss: 0.2163361757993698\n",
      "iteration 43882: loss: 0.2163359820842743\n",
      "iteration 43883: loss: 0.21633592247962952\n",
      "iteration 43884: loss: 0.21633580327033997\n",
      "iteration 43885: loss: 0.21633568406105042\n",
      "iteration 43886: loss: 0.2163355052471161\n",
      "iteration 43887: loss: 0.2163354456424713\n",
      "iteration 43888: loss: 0.21633529663085938\n",
      "iteration 43889: loss: 0.21633517742156982\n",
      "iteration 43890: loss: 0.21633510291576385\n",
      "iteration 43891: loss: 0.21633490920066833\n",
      "iteration 43892: loss: 0.21633484959602356\n",
      "iteration 43893: loss: 0.2163347750902176\n",
      "iteration 43894: loss: 0.21633455157279968\n",
      "iteration 43895: loss: 0.21633446216583252\n",
      "iteration 43896: loss: 0.21633434295654297\n",
      "iteration 43897: loss: 0.21633422374725342\n",
      "iteration 43898: loss: 0.21633410453796387\n",
      "iteration 43899: loss: 0.21633391082286835\n",
      "iteration 43900: loss: 0.21633389592170715\n",
      "iteration 43901: loss: 0.21633371710777283\n",
      "iteration 43902: loss: 0.2163335531949997\n",
      "iteration 43903: loss: 0.21633347868919373\n",
      "iteration 43904: loss: 0.2163333147764206\n",
      "iteration 43905: loss: 0.21633324027061462\n",
      "iteration 43906: loss: 0.2163330614566803\n",
      "iteration 43907: loss: 0.21633294224739075\n",
      "iteration 43908: loss: 0.21633286774158478\n",
      "iteration 43909: loss: 0.21633274853229523\n",
      "iteration 43910: loss: 0.21633264422416687\n",
      "iteration 43911: loss: 0.21633251011371613\n",
      "iteration 43912: loss: 0.2163323611021042\n",
      "iteration 43913: loss: 0.21633224189281464\n",
      "iteration 43914: loss: 0.21633215248584747\n",
      "iteration 43915: loss: 0.21633195877075195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 43916: loss: 0.2163318693637848\n",
      "iteration 43917: loss: 0.21633172035217285\n",
      "iteration 43918: loss: 0.21633164584636688\n",
      "iteration 43919: loss: 0.21633151173591614\n",
      "iteration 43920: loss: 0.2163313925266266\n",
      "iteration 43921: loss: 0.21633128821849823\n",
      "iteration 43922: loss: 0.2163310945034027\n",
      "iteration 43923: loss: 0.21633103489875793\n",
      "iteration 43924: loss: 0.2163308560848236\n",
      "iteration 43925: loss: 0.2163308560848236\n",
      "iteration 43926: loss: 0.21633067727088928\n",
      "iteration 43927: loss: 0.21633048355579376\n",
      "iteration 43928: loss: 0.21633048355579376\n",
      "iteration 43929: loss: 0.21633026003837585\n",
      "iteration 43930: loss: 0.2163301408290863\n",
      "iteration 43931: loss: 0.21633005142211914\n",
      "iteration 43932: loss: 0.21633000671863556\n",
      "iteration 43933: loss: 0.21632976830005646\n",
      "iteration 43934: loss: 0.2163296639919281\n",
      "iteration 43935: loss: 0.21632950007915497\n",
      "iteration 43936: loss: 0.21632938086986542\n",
      "iteration 43937: loss: 0.21632936596870422\n",
      "iteration 43938: loss: 0.21632921695709229\n",
      "iteration 43939: loss: 0.21632902324199677\n",
      "iteration 43940: loss: 0.2163289338350296\n",
      "iteration 43941: loss: 0.21632881462574005\n",
      "iteration 43942: loss: 0.2163286954164505\n",
      "iteration 43943: loss: 0.21632859110832214\n",
      "iteration 43944: loss: 0.2163284569978714\n",
      "iteration 43945: loss: 0.21632829308509827\n",
      "iteration 43946: loss: 0.2163282185792923\n",
      "iteration 43947: loss: 0.21632805466651917\n",
      "iteration 43948: loss: 0.2163279503583908\n",
      "iteration 43949: loss: 0.21632786095142365\n",
      "iteration 43950: loss: 0.2163277119398117\n",
      "iteration 43951: loss: 0.21632762253284454\n",
      "iteration 43952: loss: 0.2163274586200714\n",
      "iteration 43953: loss: 0.21632738411426544\n",
      "iteration 43954: loss: 0.21632727980613708\n",
      "iteration 43955: loss: 0.21632711589336395\n",
      "iteration 43956: loss: 0.2163270264863968\n",
      "iteration 43957: loss: 0.21632683277130127\n",
      "iteration 43958: loss: 0.2163267582654953\n",
      "iteration 43959: loss: 0.21632659435272217\n",
      "iteration 43960: loss: 0.2163265496492386\n",
      "iteration 43961: loss: 0.21632632613182068\n",
      "iteration 43962: loss: 0.2163262814283371\n",
      "iteration 43963: loss: 0.21632614731788635\n",
      "iteration 43964: loss: 0.21632596850395203\n",
      "iteration 43965: loss: 0.21632587909698486\n",
      "iteration 43966: loss: 0.2163257896900177\n",
      "iteration 43967: loss: 0.21632561087608337\n",
      "iteration 43968: loss: 0.21632544696331024\n",
      "iteration 43969: loss: 0.21632537245750427\n",
      "iteration 43970: loss: 0.2163252830505371\n",
      "iteration 43971: loss: 0.21632513403892517\n",
      "iteration 43972: loss: 0.21632501482963562\n",
      "iteration 43973: loss: 0.21632489562034607\n",
      "iteration 43974: loss: 0.21632476150989532\n",
      "iteration 43975: loss: 0.2163246124982834\n",
      "iteration 43976: loss: 0.2163245677947998\n",
      "iteration 43977: loss: 0.21632440388202667\n",
      "iteration 43978: loss: 0.21632429957389832\n",
      "iteration 43979: loss: 0.21632418036460876\n",
      "iteration 43980: loss: 0.2163240909576416\n",
      "iteration 43981: loss: 0.21632392704486847\n",
      "iteration 43982: loss: 0.2163238525390625\n",
      "iteration 43983: loss: 0.21632370352745056\n",
      "iteration 43984: loss: 0.21632356941699982\n",
      "iteration 43985: loss: 0.21632346510887146\n",
      "iteration 43986: loss: 0.21632333099842072\n",
      "iteration 43987: loss: 0.2163231372833252\n",
      "iteration 43988: loss: 0.2163230925798416\n",
      "iteration 43989: loss: 0.2163228690624237\n",
      "iteration 43990: loss: 0.21632280945777893\n",
      "iteration 43991: loss: 0.21632270514965057\n",
      "iteration 43992: loss: 0.21632254123687744\n",
      "iteration 43993: loss: 0.2163224220275879\n",
      "iteration 43994: loss: 0.21632234752178192\n",
      "iteration 43995: loss: 0.21632225811481476\n",
      "iteration 43996: loss: 0.2163221538066864\n",
      "iteration 43997: loss: 0.2163219451904297\n",
      "iteration 43998: loss: 0.21632185578346252\n",
      "iteration 43999: loss: 0.21632170677185059\n",
      "iteration 44000: loss: 0.21632163226604462\n",
      "iteration 44001: loss: 0.21632146835327148\n",
      "iteration 44002: loss: 0.21632131934165955\n",
      "iteration 44003: loss: 0.21632125973701477\n",
      "iteration 44004: loss: 0.21632111072540283\n",
      "iteration 44005: loss: 0.21632102131843567\n",
      "iteration 44006: loss: 0.21632079780101776\n",
      "iteration 44007: loss: 0.21632078289985657\n",
      "iteration 44008: loss: 0.21632060408592224\n",
      "iteration 44009: loss: 0.21632051467895508\n",
      "iteration 44010: loss: 0.21632036566734314\n",
      "iteration 44011: loss: 0.21632020175457\n",
      "iteration 44012: loss: 0.21632008254528046\n",
      "iteration 44013: loss: 0.2163200080394745\n",
      "iteration 44014: loss: 0.21631987392902374\n",
      "iteration 44015: loss: 0.2163197547197342\n",
      "iteration 44016: loss: 0.21631959080696106\n",
      "iteration 44017: loss: 0.2163194864988327\n",
      "iteration 44018: loss: 0.21631939709186554\n",
      "iteration 44019: loss: 0.2163192331790924\n",
      "iteration 44020: loss: 0.21631912887096405\n",
      "iteration 44021: loss: 0.2163189947605133\n",
      "iteration 44022: loss: 0.21631887555122375\n",
      "iteration 44023: loss: 0.21631881594657898\n",
      "iteration 44024: loss: 0.21631860733032227\n",
      "iteration 44025: loss: 0.21631845831871033\n",
      "iteration 44026: loss: 0.21631833910942078\n",
      "iteration 44027: loss: 0.21631821990013123\n",
      "iteration 44028: loss: 0.21631810069084167\n",
      "iteration 44029: loss: 0.2163180112838745\n",
      "iteration 44030: loss: 0.21631786227226257\n",
      "iteration 44031: loss: 0.2163178026676178\n",
      "iteration 44032: loss: 0.21631765365600586\n",
      "iteration 44033: loss: 0.2163175642490387\n",
      "iteration 44034: loss: 0.21631738543510437\n",
      "iteration 44035: loss: 0.2163173258304596\n",
      "iteration 44036: loss: 0.21631714701652527\n",
      "iteration 44037: loss: 0.21631698310375214\n",
      "iteration 44038: loss: 0.21631689369678497\n",
      "iteration 44039: loss: 0.216316819190979\n",
      "iteration 44040: loss: 0.21631667017936707\n",
      "iteration 44041: loss: 0.21631655097007751\n",
      "iteration 44042: loss: 0.21631646156311035\n",
      "iteration 44043: loss: 0.2163163125514984\n",
      "iteration 44044: loss: 0.21631625294685364\n",
      "iteration 44045: loss: 0.2163161039352417\n",
      "iteration 44046: loss: 0.21631589531898499\n",
      "iteration 44047: loss: 0.2163158655166626\n",
      "iteration 44048: loss: 0.21631570160388947\n",
      "iteration 44049: loss: 0.21631558239459991\n",
      "iteration 44050: loss: 0.21631541848182678\n",
      "iteration 44051: loss: 0.21631531417369843\n",
      "iteration 44052: loss: 0.2163151502609253\n",
      "iteration 44053: loss: 0.21631507575511932\n",
      "iteration 44054: loss: 0.21631494164466858\n",
      "iteration 44055: loss: 0.21631482243537903\n",
      "iteration 44056: loss: 0.21631470322608948\n",
      "iteration 44057: loss: 0.21631458401679993\n",
      "iteration 44058: loss: 0.216314435005188\n",
      "iteration 44059: loss: 0.21631434559822083\n",
      "iteration 44060: loss: 0.2163141965866089\n",
      "iteration 44061: loss: 0.21631412208080292\n",
      "iteration 44062: loss: 0.2163139134645462\n",
      "iteration 44063: loss: 0.21631386876106262\n",
      "iteration 44064: loss: 0.21631371974945068\n",
      "iteration 44065: loss: 0.21631363034248352\n",
      "iteration 44066: loss: 0.2163134515285492\n",
      "iteration 44067: loss: 0.21631333231925964\n",
      "iteration 44068: loss: 0.2163132131099701\n",
      "iteration 44069: loss: 0.21631307899951935\n",
      "iteration 44070: loss: 0.21631300449371338\n",
      "iteration 44071: loss: 0.21631291508674622\n",
      "iteration 44072: loss: 0.21631276607513428\n",
      "iteration 44073: loss: 0.21631257236003876\n",
      "iteration 44074: loss: 0.216312438249588\n",
      "iteration 44075: loss: 0.21631236374378204\n",
      "iteration 44076: loss: 0.2163122594356537\n",
      "iteration 44077: loss: 0.21631214022636414\n",
      "iteration 44078: loss: 0.2163120061159134\n",
      "iteration 44079: loss: 0.21631188690662384\n",
      "iteration 44080: loss: 0.2163117378950119\n",
      "iteration 44081: loss: 0.21631164848804474\n",
      "iteration 44082: loss: 0.21631145477294922\n",
      "iteration 44083: loss: 0.21631141006946564\n",
      "iteration 44084: loss: 0.2163112610578537\n",
      "iteration 44085: loss: 0.21631109714508057\n",
      "iteration 44086: loss: 0.21631105244159698\n",
      "iteration 44087: loss: 0.21631088852882385\n",
      "iteration 44088: loss: 0.2163107693195343\n",
      "iteration 44089: loss: 0.21631062030792236\n",
      "iteration 44090: loss: 0.2163105458021164\n",
      "iteration 44091: loss: 0.21631041169166565\n",
      "iteration 44092: loss: 0.21631033718585968\n",
      "iteration 44093: loss: 0.21631017327308655\n",
      "iteration 44094: loss: 0.21630997955799103\n",
      "iteration 44095: loss: 0.21630994975566864\n",
      "iteration 44096: loss: 0.21630975604057312\n",
      "iteration 44097: loss: 0.21630962193012238\n",
      "iteration 44098: loss: 0.21630950272083282\n",
      "iteration 44099: loss: 0.21630938351154327\n",
      "iteration 44100: loss: 0.21630926430225372\n",
      "iteration 44101: loss: 0.21630915999412537\n",
      "iteration 44102: loss: 0.21630904078483582\n",
      "iteration 44103: loss: 0.2163088619709015\n",
      "iteration 44104: loss: 0.21630878746509552\n",
      "iteration 44105: loss: 0.21630866825580597\n",
      "iteration 44106: loss: 0.21630854904651642\n",
      "iteration 44107: loss: 0.21630844473838806\n",
      "iteration 44108: loss: 0.21630826592445374\n",
      "iteration 44109: loss: 0.21630823612213135\n",
      "iteration 44110: loss: 0.21630802750587463\n",
      "iteration 44111: loss: 0.21630792319774628\n",
      "iteration 44112: loss: 0.2163078784942627\n",
      "iteration 44113: loss: 0.21630766987800598\n",
      "iteration 44114: loss: 0.21630752086639404\n",
      "iteration 44115: loss: 0.21630744636058807\n",
      "iteration 44116: loss: 0.21630725264549255\n",
      "iteration 44117: loss: 0.2163071632385254\n",
      "iteration 44118: loss: 0.21630699932575226\n",
      "iteration 44119: loss: 0.21630695462226868\n",
      "iteration 44120: loss: 0.21630677580833435\n",
      "iteration 44121: loss: 0.2163066864013672\n",
      "iteration 44122: loss: 0.21630647778511047\n",
      "iteration 44123: loss: 0.21630644798278809\n",
      "iteration 44124: loss: 0.21630632877349854\n",
      "iteration 44125: loss: 0.21630609035491943\n",
      "iteration 44126: loss: 0.21630609035491943\n",
      "iteration 44127: loss: 0.21630597114562988\n",
      "iteration 44128: loss: 0.21630577743053436\n",
      "iteration 44129: loss: 0.21630573272705078\n",
      "iteration 44130: loss: 0.21630553901195526\n",
      "iteration 44131: loss: 0.21630549430847168\n",
      "iteration 44132: loss: 0.21630533039569855\n",
      "iteration 44133: loss: 0.21630516648292542\n",
      "iteration 44134: loss: 0.21630506217479706\n",
      "iteration 44135: loss: 0.21630486845970154\n",
      "iteration 44136: loss: 0.21630489826202393\n",
      "iteration 44137: loss: 0.216304749250412\n",
      "iteration 44138: loss: 0.21630461513996124\n",
      "iteration 44139: loss: 0.2163044661283493\n",
      "iteration 44140: loss: 0.21630433201789856\n",
      "iteration 44141: loss: 0.2163042277097702\n",
      "iteration 44142: loss: 0.21630409359931946\n",
      "iteration 44143: loss: 0.21630394458770752\n",
      "iteration 44144: loss: 0.21630382537841797\n",
      "iteration 44145: loss: 0.2163037359714508\n",
      "iteration 44146: loss: 0.21630358695983887\n",
      "iteration 44147: loss: 0.21630346775054932\n",
      "iteration 44148: loss: 0.21630334854125977\n",
      "iteration 44149: loss: 0.21630322933197021\n",
      "iteration 44150: loss: 0.21630311012268066\n",
      "iteration 44151: loss: 0.21630294620990753\n",
      "iteration 44152: loss: 0.21630284190177917\n",
      "iteration 44153: loss: 0.2163027822971344\n",
      "iteration 44154: loss: 0.2163025587797165\n",
      "iteration 44155: loss: 0.21630248427391052\n",
      "iteration 44156: loss: 0.21630236506462097\n",
      "iteration 44157: loss: 0.2163022756576538\n",
      "iteration 44158: loss: 0.21630211174488068\n",
      "iteration 44159: loss: 0.21630196273326874\n",
      "iteration 44160: loss: 0.21630188822746277\n",
      "iteration 44161: loss: 0.21630172431468964\n",
      "iteration 44162: loss: 0.21630163490772247\n",
      "iteration 44163: loss: 0.21630148589611053\n",
      "iteration 44164: loss: 0.2163013517856598\n",
      "iteration 44165: loss: 0.21630124747753143\n",
      "iteration 44166: loss: 0.21630115807056427\n",
      "iteration 44167: loss: 0.21630099415779114\n",
      "iteration 44168: loss: 0.2163008749485016\n",
      "iteration 44169: loss: 0.21630072593688965\n",
      "iteration 44170: loss: 0.21630065143108368\n",
      "iteration 44171: loss: 0.21630045771598816\n",
      "iteration 44172: loss: 0.21630029380321503\n",
      "iteration 44173: loss: 0.21630027890205383\n",
      "iteration 44174: loss: 0.2163001000881195\n",
      "iteration 44175: loss: 0.21629996597766876\n",
      "iteration 44176: loss: 0.2162998616695404\n",
      "iteration 44177: loss: 0.21629974246025085\n",
      "iteration 44178: loss: 0.2162996083498001\n",
      "iteration 44179: loss: 0.21629948914051056\n",
      "iteration 44180: loss: 0.2162994146347046\n",
      "iteration 44181: loss: 0.21629920601844788\n",
      "iteration 44182: loss: 0.21629910171031952\n",
      "iteration 44183: loss: 0.21629898250102997\n",
      "iteration 44184: loss: 0.21629886329174042\n",
      "iteration 44185: loss: 0.21629872918128967\n",
      "iteration 44186: loss: 0.21629862487316132\n",
      "iteration 44187: loss: 0.21629849076271057\n",
      "iteration 44188: loss: 0.21629831194877625\n",
      "iteration 44189: loss: 0.21629825234413147\n",
      "iteration 44190: loss: 0.2162981480360031\n",
      "iteration 44191: loss: 0.21629798412322998\n",
      "iteration 44192: loss: 0.21629783511161804\n",
      "iteration 44193: loss: 0.21629782021045685\n",
      "iteration 44194: loss: 0.2162976711988449\n",
      "iteration 44195: loss: 0.21629753708839417\n",
      "iteration 44196: loss: 0.21629735827445984\n",
      "iteration 44197: loss: 0.21629726886749268\n",
      "iteration 44198: loss: 0.21629714965820312\n",
      "iteration 44199: loss: 0.21629698574543\n",
      "iteration 44200: loss: 0.2162969410419464\n",
      "iteration 44201: loss: 0.21629676222801208\n",
      "iteration 44202: loss: 0.21629659831523895\n",
      "iteration 44203: loss: 0.21629652380943298\n",
      "iteration 44204: loss: 0.21629640460014343\n",
      "iteration 44205: loss: 0.21629628539085388\n",
      "iteration 44206: loss: 0.21629615128040314\n",
      "iteration 44207: loss: 0.21629607677459717\n",
      "iteration 44208: loss: 0.21629591286182404\n",
      "iteration 44209: loss: 0.21629579365253448\n",
      "iteration 44210: loss: 0.21629564464092255\n",
      "iteration 44211: loss: 0.2162955105304718\n",
      "iteration 44212: loss: 0.21629539132118225\n",
      "iteration 44213: loss: 0.21629533171653748\n",
      "iteration 44214: loss: 0.21629516780376434\n",
      "iteration 44215: loss: 0.2162950336933136\n",
      "iteration 44216: loss: 0.21629495918750763\n",
      "iteration 44217: loss: 0.2162947952747345\n",
      "iteration 44218: loss: 0.21629469096660614\n",
      "iteration 44219: loss: 0.21629449725151062\n",
      "iteration 44220: loss: 0.21629440784454346\n",
      "iteration 44221: loss: 0.2162942886352539\n",
      "iteration 44222: loss: 0.21629416942596436\n",
      "iteration 44223: loss: 0.21629400551319122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 44224: loss: 0.21629396080970764\n",
      "iteration 44225: loss: 0.21629376709461212\n",
      "iteration 44226: loss: 0.21629369258880615\n",
      "iteration 44227: loss: 0.2162935435771942\n",
      "iteration 44228: loss: 0.21629340946674347\n",
      "iteration 44229: loss: 0.2162933647632599\n",
      "iteration 44230: loss: 0.21629318594932556\n",
      "iteration 44231: loss: 0.21629305183887482\n",
      "iteration 44232: loss: 0.21629294753074646\n",
      "iteration 44233: loss: 0.21629281342029572\n",
      "iteration 44234: loss: 0.21629269421100616\n",
      "iteration 44235: loss: 0.2162925750017166\n",
      "iteration 44236: loss: 0.21629242599010468\n",
      "iteration 44237: loss: 0.21629229187965393\n",
      "iteration 44238: loss: 0.21629218757152557\n",
      "iteration 44239: loss: 0.21629205346107483\n",
      "iteration 44240: loss: 0.21629199385643005\n",
      "iteration 44241: loss: 0.21629181504249573\n",
      "iteration 44242: loss: 0.2162916660308838\n",
      "iteration 44243: loss: 0.21629157662391663\n",
      "iteration 44244: loss: 0.21629145741462708\n",
      "iteration 44245: loss: 0.21629126369953156\n",
      "iteration 44246: loss: 0.21629118919372559\n",
      "iteration 44247: loss: 0.21629102528095245\n",
      "iteration 44248: loss: 0.21629098057746887\n",
      "iteration 44249: loss: 0.21629083156585693\n",
      "iteration 44250: loss: 0.21629071235656738\n",
      "iteration 44251: loss: 0.21629054844379425\n",
      "iteration 44252: loss: 0.21629051864147186\n",
      "iteration 44253: loss: 0.21629028022289276\n",
      "iteration 44254: loss: 0.21629023551940918\n",
      "iteration 44255: loss: 0.21629008650779724\n",
      "iteration 44256: loss: 0.2162899225950241\n",
      "iteration 44257: loss: 0.21628984808921814\n",
      "iteration 44258: loss: 0.2162897288799286\n",
      "iteration 44259: loss: 0.21628959476947784\n",
      "iteration 44260: loss: 0.2162894308567047\n",
      "iteration 44261: loss: 0.21628932654857635\n",
      "iteration 44262: loss: 0.2162892371416092\n",
      "iteration 44263: loss: 0.21628904342651367\n",
      "iteration 44264: loss: 0.2162889987230301\n",
      "iteration 44265: loss: 0.21628880500793457\n",
      "iteration 44266: loss: 0.2162887156009674\n",
      "iteration 44267: loss: 0.21628861129283905\n",
      "iteration 44268: loss: 0.2162884771823883\n",
      "iteration 44269: loss: 0.21628835797309875\n",
      "iteration 44270: loss: 0.21628820896148682\n",
      "iteration 44271: loss: 0.21628804504871368\n",
      "iteration 44272: loss: 0.21628794074058533\n",
      "iteration 44273: loss: 0.21628785133361816\n",
      "iteration 44274: loss: 0.216287761926651\n",
      "iteration 44275: loss: 0.2162875384092331\n",
      "iteration 44276: loss: 0.21628746390342712\n",
      "iteration 44277: loss: 0.21628734469413757\n",
      "iteration 44278: loss: 0.21628721058368683\n",
      "iteration 44279: loss: 0.21628713607788086\n",
      "iteration 44280: loss: 0.21628697216510773\n",
      "iteration 44281: loss: 0.21628685295581818\n",
      "iteration 44282: loss: 0.21628670394420624\n",
      "iteration 44283: loss: 0.21628661453723907\n",
      "iteration 44284: loss: 0.21628646552562714\n",
      "iteration 44285: loss: 0.21628627181053162\n",
      "iteration 44286: loss: 0.21628621220588684\n",
      "iteration 44287: loss: 0.2162860631942749\n",
      "iteration 44288: loss: 0.21628594398498535\n",
      "iteration 44289: loss: 0.21628575026988983\n",
      "iteration 44290: loss: 0.21628573536872864\n",
      "iteration 44291: loss: 0.21628563106060028\n",
      "iteration 44292: loss: 0.21628546714782715\n",
      "iteration 44293: loss: 0.21628525853157043\n",
      "iteration 44294: loss: 0.21628522872924805\n",
      "iteration 44295: loss: 0.2162851095199585\n",
      "iteration 44296: loss: 0.21628502011299133\n",
      "iteration 44297: loss: 0.21628479659557343\n",
      "iteration 44298: loss: 0.21628467738628387\n",
      "iteration 44299: loss: 0.21628454327583313\n",
      "iteration 44300: loss: 0.21628446877002716\n",
      "iteration 44301: loss: 0.2162843644618988\n",
      "iteration 44302: loss: 0.21628418564796448\n",
      "iteration 44303: loss: 0.21628403663635254\n",
      "iteration 44304: loss: 0.21628394722938538\n",
      "iteration 44305: loss: 0.21628379821777344\n",
      "iteration 44306: loss: 0.21628375351428986\n",
      "iteration 44307: loss: 0.21628360450267792\n",
      "iteration 44308: loss: 0.2162834107875824\n",
      "iteration 44309: loss: 0.2162833958864212\n",
      "iteration 44310: loss: 0.21628320217132568\n",
      "iteration 44311: loss: 0.21628311276435852\n",
      "iteration 44312: loss: 0.21628299355506897\n",
      "iteration 44313: loss: 0.21628275513648987\n",
      "iteration 44314: loss: 0.2162826806306839\n",
      "iteration 44315: loss: 0.21628260612487793\n",
      "iteration 44316: loss: 0.2162824422121048\n",
      "iteration 44317: loss: 0.21628233790397644\n",
      "iteration 44318: loss: 0.2162822186946869\n",
      "iteration 44319: loss: 0.21628209948539734\n",
      "iteration 44320: loss: 0.21628203988075256\n",
      "iteration 44321: loss: 0.21628186106681824\n",
      "iteration 44322: loss: 0.2162817418575287\n",
      "iteration 44323: loss: 0.21628157794475555\n",
      "iteration 44324: loss: 0.216281458735466\n",
      "iteration 44325: loss: 0.21628136932849884\n",
      "iteration 44326: loss: 0.21628117561340332\n",
      "iteration 44327: loss: 0.21628108620643616\n",
      "iteration 44328: loss: 0.2162809818983078\n",
      "iteration 44329: loss: 0.21628089249134064\n",
      "iteration 44330: loss: 0.21628069877624512\n",
      "iteration 44331: loss: 0.21628054976463318\n",
      "iteration 44332: loss: 0.21628046035766602\n",
      "iteration 44333: loss: 0.21628041565418243\n",
      "iteration 44334: loss: 0.21628022193908691\n",
      "iteration 44335: loss: 0.21628007292747498\n",
      "iteration 44336: loss: 0.21627993881702423\n",
      "iteration 44337: loss: 0.21627989411354065\n",
      "iteration 44338: loss: 0.2162797451019287\n",
      "iteration 44339: loss: 0.21627958118915558\n",
      "iteration 44340: loss: 0.21627947688102722\n",
      "iteration 44341: loss: 0.2162793129682541\n",
      "iteration 44342: loss: 0.21627922356128693\n",
      "iteration 44343: loss: 0.21627911925315857\n",
      "iteration 44344: loss: 0.21627894043922424\n",
      "iteration 44345: loss: 0.21627891063690186\n",
      "iteration 44346: loss: 0.21627870202064514\n",
      "iteration 44347: loss: 0.2162785530090332\n",
      "iteration 44348: loss: 0.21627850830554962\n",
      "iteration 44349: loss: 0.2162783443927765\n",
      "iteration 44350: loss: 0.21627822518348694\n",
      "iteration 44351: loss: 0.216278076171875\n",
      "iteration 44352: loss: 0.21627803146839142\n",
      "iteration 44353: loss: 0.2162778079509735\n",
      "iteration 44354: loss: 0.21627774834632874\n",
      "iteration 44355: loss: 0.21627764403820038\n",
      "iteration 44356: loss: 0.21627750992774963\n",
      "iteration 44357: loss: 0.2162773609161377\n",
      "iteration 44358: loss: 0.21627719700336456\n",
      "iteration 44359: loss: 0.2162770926952362\n",
      "iteration 44360: loss: 0.21627692878246307\n",
      "iteration 44361: loss: 0.2162768542766571\n",
      "iteration 44362: loss: 0.21627673506736755\n",
      "iteration 44363: loss: 0.21627657115459442\n",
      "iteration 44364: loss: 0.21627648174762726\n",
      "iteration 44365: loss: 0.21627631783485413\n",
      "iteration 44366: loss: 0.21627625823020935\n",
      "iteration 44367: loss: 0.2162761241197586\n",
      "iteration 44368: loss: 0.21627597510814667\n",
      "iteration 44369: loss: 0.21627584099769592\n",
      "iteration 44370: loss: 0.21627576649188995\n",
      "iteration 44371: loss: 0.21627557277679443\n",
      "iteration 44372: loss: 0.21627548336982727\n",
      "iteration 44373: loss: 0.2162753790616989\n",
      "iteration 44374: loss: 0.2162751853466034\n",
      "iteration 44375: loss: 0.2162751406431198\n",
      "iteration 44376: loss: 0.21627497673034668\n",
      "iteration 44377: loss: 0.21627482771873474\n",
      "iteration 44378: loss: 0.216274693608284\n",
      "iteration 44379: loss: 0.21627461910247803\n",
      "iteration 44380: loss: 0.2162744700908661\n",
      "iteration 44381: loss: 0.21627438068389893\n",
      "iteration 44382: loss: 0.21627426147460938\n",
      "iteration 44383: loss: 0.2162741720676422\n",
      "iteration 44384: loss: 0.2162739783525467\n",
      "iteration 44385: loss: 0.2162739485502243\n",
      "iteration 44386: loss: 0.2162737101316452\n",
      "iteration 44387: loss: 0.21627362072467804\n",
      "iteration 44388: loss: 0.2162734717130661\n",
      "iteration 44389: loss: 0.21627333760261536\n",
      "iteration 44390: loss: 0.2162732183933258\n",
      "iteration 44391: loss: 0.21627311408519745\n",
      "iteration 44392: loss: 0.21627302467823029\n",
      "iteration 44393: loss: 0.21627290546894073\n",
      "iteration 44394: loss: 0.2162727415561676\n",
      "iteration 44395: loss: 0.21627259254455566\n",
      "iteration 44396: loss: 0.2162724733352661\n",
      "iteration 44397: loss: 0.21627235412597656\n",
      "iteration 44398: loss: 0.21627219021320343\n",
      "iteration 44399: loss: 0.2162720263004303\n",
      "iteration 44400: loss: 0.2162719964981079\n",
      "iteration 44401: loss: 0.21627183258533478\n",
      "iteration 44402: loss: 0.21627168357372284\n",
      "iteration 44403: loss: 0.21627160906791687\n",
      "iteration 44404: loss: 0.21627144515514374\n",
      "iteration 44405: loss: 0.21627135574817657\n",
      "iteration 44406: loss: 0.21627119183540344\n",
      "iteration 44407: loss: 0.21627108752727509\n",
      "iteration 44408: loss: 0.21627101302146912\n",
      "iteration 44409: loss: 0.2162708342075348\n",
      "iteration 44410: loss: 0.21627075970172882\n",
      "iteration 44411: loss: 0.21627064049243927\n",
      "iteration 44412: loss: 0.21627041697502136\n",
      "iteration 44413: loss: 0.21627037227153778\n",
      "iteration 44414: loss: 0.21627017855644226\n",
      "iteration 44415: loss: 0.21627011895179749\n",
      "iteration 44416: loss: 0.21627001464366913\n",
      "iteration 44417: loss: 0.21626980602741241\n",
      "iteration 44418: loss: 0.21626965701580048\n",
      "iteration 44419: loss: 0.2162696123123169\n",
      "iteration 44420: loss: 0.21626946330070496\n",
      "iteration 44421: loss: 0.2162693440914154\n",
      "iteration 44422: loss: 0.21626925468444824\n",
      "iteration 44423: loss: 0.21626906096935272\n",
      "iteration 44424: loss: 0.21626901626586914\n",
      "iteration 44425: loss: 0.216268852353096\n",
      "iteration 44426: loss: 0.21626877784729004\n",
      "iteration 44427: loss: 0.21626858413219452\n",
      "iteration 44428: loss: 0.21626849472522736\n",
      "iteration 44429: loss: 0.21626833081245422\n",
      "iteration 44430: loss: 0.21626822650432587\n",
      "iteration 44431: loss: 0.21626806259155273\n",
      "iteration 44432: loss: 0.21626798808574677\n",
      "iteration 44433: loss: 0.21626782417297363\n",
      "iteration 44434: loss: 0.21626773476600647\n",
      "iteration 44435: loss: 0.21626758575439453\n",
      "iteration 44436: loss: 0.2162674367427826\n",
      "iteration 44437: loss: 0.21626731753349304\n",
      "iteration 44438: loss: 0.2162671536207199\n",
      "iteration 44439: loss: 0.21626710891723633\n",
      "iteration 44440: loss: 0.21626701951026917\n",
      "iteration 44441: loss: 0.21626687049865723\n",
      "iteration 44442: loss: 0.2162667065858841\n",
      "iteration 44443: loss: 0.21626660227775574\n",
      "iteration 44444: loss: 0.2162664383649826\n",
      "iteration 44445: loss: 0.21626631915569305\n",
      "iteration 44446: loss: 0.2162662297487259\n",
      "iteration 44447: loss: 0.21626608073711395\n",
      "iteration 44448: loss: 0.2162659615278244\n",
      "iteration 44449: loss: 0.21626579761505127\n",
      "iteration 44450: loss: 0.21626567840576172\n",
      "iteration 44451: loss: 0.21626555919647217\n",
      "iteration 44452: loss: 0.21626541018486023\n",
      "iteration 44453: loss: 0.21626539528369904\n",
      "iteration 44454: loss: 0.2162652313709259\n",
      "iteration 44455: loss: 0.21626512706279755\n",
      "iteration 44456: loss: 0.21626503765583038\n",
      "iteration 44457: loss: 0.21626481413841248\n",
      "iteration 44458: loss: 0.21626469492912292\n",
      "iteration 44459: loss: 0.21626456081867218\n",
      "iteration 44460: loss: 0.21626445651054382\n",
      "iteration 44461: loss: 0.21626432240009308\n",
      "iteration 44462: loss: 0.21626420319080353\n",
      "iteration 44463: loss: 0.21626409888267517\n",
      "iteration 44464: loss: 0.21626396477222443\n",
      "iteration 44465: loss: 0.2162638008594513\n",
      "iteration 44466: loss: 0.21626369655132294\n",
      "iteration 44467: loss: 0.21626360714435577\n",
      "iteration 44468: loss: 0.21626338362693787\n",
      "iteration 44469: loss: 0.21626326441764832\n",
      "iteration 44470: loss: 0.21626313030719757\n",
      "iteration 44471: loss: 0.21626310050487518\n",
      "iteration 44472: loss: 0.21626293659210205\n",
      "iteration 44473: loss: 0.2162628173828125\n",
      "iteration 44474: loss: 0.21626272797584534\n",
      "iteration 44475: loss: 0.2162625789642334\n",
      "iteration 44476: loss: 0.21626242995262146\n",
      "iteration 44477: loss: 0.2162623405456543\n",
      "iteration 44478: loss: 0.21626222133636475\n",
      "iteration 44479: loss: 0.2162620574235916\n",
      "iteration 44480: loss: 0.21626190841197968\n",
      "iteration 44481: loss: 0.2162618637084961\n",
      "iteration 44482: loss: 0.21626169979572296\n",
      "iteration 44483: loss: 0.21626153588294983\n",
      "iteration 44484: loss: 0.21626143157482147\n",
      "iteration 44485: loss: 0.21626129746437073\n",
      "iteration 44486: loss: 0.21626119315624237\n",
      "iteration 44487: loss: 0.2162611037492752\n",
      "iteration 44488: loss: 0.21626095473766327\n",
      "iteration 44489: loss: 0.21626082062721252\n",
      "iteration 44490: loss: 0.2162606418132782\n",
      "iteration 44491: loss: 0.21626058220863342\n",
      "iteration 44492: loss: 0.2162603884935379\n",
      "iteration 44493: loss: 0.21626034379005432\n",
      "iteration 44494: loss: 0.21626016497612\n",
      "iteration 44495: loss: 0.21626010537147522\n",
      "iteration 44496: loss: 0.21625995635986328\n",
      "iteration 44497: loss: 0.21625980734825134\n",
      "iteration 44498: loss: 0.2162596881389618\n",
      "iteration 44499: loss: 0.21625952422618866\n",
      "iteration 44500: loss: 0.2162594050168991\n",
      "iteration 44501: loss: 0.21625933051109314\n",
      "iteration 44502: loss: 0.2162591516971588\n",
      "iteration 44503: loss: 0.21625904738903046\n",
      "iteration 44504: loss: 0.21625888347625732\n",
      "iteration 44505: loss: 0.21625876426696777\n",
      "iteration 44506: loss: 0.2162586748600006\n",
      "iteration 44507: loss: 0.21625855565071106\n",
      "iteration 44508: loss: 0.2162584364414215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 44509: loss: 0.21625831723213196\n",
      "iteration 44510: loss: 0.21625816822052002\n",
      "iteration 44511: loss: 0.21625807881355286\n",
      "iteration 44512: loss: 0.21625789999961853\n",
      "iteration 44513: loss: 0.21625776588916779\n",
      "iteration 44514: loss: 0.21625769138336182\n",
      "iteration 44515: loss: 0.2162574976682663\n",
      "iteration 44516: loss: 0.21625740826129913\n",
      "iteration 44517: loss: 0.21625728905200958\n",
      "iteration 44518: loss: 0.21625712513923645\n",
      "iteration 44519: loss: 0.2162570059299469\n",
      "iteration 44520: loss: 0.21625694632530212\n",
      "iteration 44521: loss: 0.216256782412529\n",
      "iteration 44522: loss: 0.21625666320323944\n",
      "iteration 44523: loss: 0.2162565290927887\n",
      "iteration 44524: loss: 0.21625642478466034\n",
      "iteration 44525: loss: 0.2162563055753708\n",
      "iteration 44526: loss: 0.21625618636608124\n",
      "iteration 44527: loss: 0.21625594794750214\n",
      "iteration 44528: loss: 0.21625590324401855\n",
      "iteration 44529: loss: 0.216255784034729\n",
      "iteration 44530: loss: 0.21625562012195587\n",
      "iteration 44531: loss: 0.21625551581382751\n",
      "iteration 44532: loss: 0.21625539660453796\n",
      "iteration 44533: loss: 0.2162552773952484\n",
      "iteration 44534: loss: 0.21625511348247528\n",
      "iteration 44535: loss: 0.21625497937202454\n",
      "iteration 44536: loss: 0.21625491976737976\n",
      "iteration 44537: loss: 0.21625474095344543\n",
      "iteration 44538: loss: 0.21625471115112305\n",
      "iteration 44539: loss: 0.21625447273254395\n",
      "iteration 44540: loss: 0.21625438332557678\n",
      "iteration 44541: loss: 0.21625426411628723\n",
      "iteration 44542: loss: 0.2162541151046753\n",
      "iteration 44543: loss: 0.21625404059886932\n",
      "iteration 44544: loss: 0.2162538766860962\n",
      "iteration 44545: loss: 0.21625371277332306\n",
      "iteration 44546: loss: 0.2162536084651947\n",
      "iteration 44547: loss: 0.21625347435474396\n",
      "iteration 44548: loss: 0.21625342965126038\n",
      "iteration 44549: loss: 0.21625328063964844\n",
      "iteration 44550: loss: 0.21625320613384247\n",
      "iteration 44551: loss: 0.21625296771526337\n",
      "iteration 44552: loss: 0.21625284850597382\n",
      "iteration 44553: loss: 0.21625280380249023\n",
      "iteration 44554: loss: 0.21625261008739471\n",
      "iteration 44555: loss: 0.21625252068042755\n",
      "iteration 44556: loss: 0.2162523716688156\n",
      "iteration 44557: loss: 0.21625228226184845\n",
      "iteration 44558: loss: 0.2162521332502365\n",
      "iteration 44559: loss: 0.216251939535141\n",
      "iteration 44560: loss: 0.21625187993049622\n",
      "iteration 44561: loss: 0.21625176072120667\n",
      "iteration 44562: loss: 0.21625161170959473\n",
      "iteration 44563: loss: 0.2162514477968216\n",
      "iteration 44564: loss: 0.2162514477968216\n",
      "iteration 44565: loss: 0.2162512093782425\n",
      "iteration 44566: loss: 0.21625106036663055\n",
      "iteration 44567: loss: 0.216250941157341\n",
      "iteration 44568: loss: 0.21625086665153503\n",
      "iteration 44569: loss: 0.2162507027387619\n",
      "iteration 44570: loss: 0.21625061333179474\n",
      "iteration 44571: loss: 0.2162504941225052\n",
      "iteration 44572: loss: 0.21625034511089325\n",
      "iteration 44573: loss: 0.2162502110004425\n",
      "iteration 44574: loss: 0.21625009179115295\n",
      "iteration 44575: loss: 0.21625001728534698\n",
      "iteration 44576: loss: 0.21624982357025146\n",
      "iteration 44577: loss: 0.21624965965747833\n",
      "iteration 44578: loss: 0.21624961495399475\n",
      "iteration 44579: loss: 0.2162494659423828\n",
      "iteration 44580: loss: 0.21624934673309326\n",
      "iteration 44581: loss: 0.21624919772148132\n",
      "iteration 44582: loss: 0.2162490338087082\n",
      "iteration 44583: loss: 0.21624895930290222\n",
      "iteration 44584: loss: 0.21624882519245148\n",
      "iteration 44585: loss: 0.21624870598316193\n",
      "iteration 44586: loss: 0.21624858677387238\n",
      "iteration 44587: loss: 0.21624843776226044\n",
      "iteration 44588: loss: 0.2162483185529709\n",
      "iteration 44589: loss: 0.21624819934368134\n",
      "iteration 44590: loss: 0.21624810993671417\n",
      "iteration 44591: loss: 0.21624796092510223\n",
      "iteration 44592: loss: 0.2162477970123291\n",
      "iteration 44593: loss: 0.21624775230884552\n",
      "iteration 44594: loss: 0.2162475883960724\n",
      "iteration 44595: loss: 0.21624751389026642\n",
      "iteration 44596: loss: 0.2162472903728485\n",
      "iteration 44597: loss: 0.21624720096588135\n",
      "iteration 44598: loss: 0.21624703705310822\n",
      "iteration 44599: loss: 0.21624693274497986\n",
      "iteration 44600: loss: 0.21624675393104553\n",
      "iteration 44601: loss: 0.21624675393104553\n",
      "iteration 44602: loss: 0.21624651551246643\n",
      "iteration 44603: loss: 0.21624644100666046\n",
      "iteration 44604: loss: 0.21624627709388733\n",
      "iteration 44605: loss: 0.21624617278575897\n",
      "iteration 44606: loss: 0.21624603867530823\n",
      "iteration 44607: loss: 0.21624593436717987\n",
      "iteration 44608: loss: 0.21624580025672913\n",
      "iteration 44609: loss: 0.2162456214427948\n",
      "iteration 44610: loss: 0.21624556183815002\n",
      "iteration 44611: loss: 0.21624544262886047\n",
      "iteration 44612: loss: 0.21624524891376495\n",
      "iteration 44613: loss: 0.21624520421028137\n",
      "iteration 44614: loss: 0.21624502539634705\n",
      "iteration 44615: loss: 0.21624496579170227\n",
      "iteration 44616: loss: 0.21624481678009033\n",
      "iteration 44617: loss: 0.2162446230649948\n",
      "iteration 44618: loss: 0.21624454855918884\n",
      "iteration 44619: loss: 0.2162444144487381\n",
      "iteration 44620: loss: 0.21624425053596497\n",
      "iteration 44621: loss: 0.216244176030159\n",
      "iteration 44622: loss: 0.21624402701854706\n",
      "iteration 44623: loss: 0.2162439078092575\n",
      "iteration 44624: loss: 0.21624378859996796\n",
      "iteration 44625: loss: 0.2162436693906784\n",
      "iteration 44626: loss: 0.21624350547790527\n",
      "iteration 44627: loss: 0.2162434160709381\n",
      "iteration 44628: loss: 0.21624331176280975\n",
      "iteration 44629: loss: 0.21624311804771423\n",
      "iteration 44630: loss: 0.21624307334423065\n",
      "iteration 44631: loss: 0.21624287962913513\n",
      "iteration 44632: loss: 0.2162427008152008\n",
      "iteration 44633: loss: 0.21624264121055603\n",
      "iteration 44634: loss: 0.21624250710010529\n",
      "iteration 44635: loss: 0.21624234318733215\n",
      "iteration 44636: loss: 0.2162422239780426\n",
      "iteration 44637: loss: 0.21624210476875305\n",
      "iteration 44638: loss: 0.2162419855594635\n",
      "iteration 44639: loss: 0.21624186635017395\n",
      "iteration 44640: loss: 0.2162417620420456\n",
      "iteration 44641: loss: 0.21624162793159485\n",
      "iteration 44642: loss: 0.21624144911766052\n",
      "iteration 44643: loss: 0.21624132990837097\n",
      "iteration 44644: loss: 0.2162412703037262\n",
      "iteration 44645: loss: 0.21624112129211426\n",
      "iteration 44646: loss: 0.2162410020828247\n",
      "iteration 44647: loss: 0.21624088287353516\n",
      "iteration 44648: loss: 0.216240793466568\n",
      "iteration 44649: loss: 0.21624064445495605\n",
      "iteration 44650: loss: 0.21624043583869934\n",
      "iteration 44651: loss: 0.21624036133289337\n",
      "iteration 44652: loss: 0.21624019742012024\n",
      "iteration 44653: loss: 0.21624012291431427\n",
      "iteration 44654: loss: 0.21623997390270233\n",
      "iteration 44655: loss: 0.2162397801876068\n",
      "iteration 44656: loss: 0.21623969078063965\n",
      "iteration 44657: loss: 0.2162395417690277\n",
      "iteration 44658: loss: 0.21623942255973816\n",
      "iteration 44659: loss: 0.21623936295509338\n",
      "iteration 44660: loss: 0.21623918414115906\n",
      "iteration 44661: loss: 0.2162390947341919\n",
      "iteration 44662: loss: 0.21623894572257996\n",
      "iteration 44663: loss: 0.2162388563156128\n",
      "iteration 44664: loss: 0.21623873710632324\n",
      "iteration 44665: loss: 0.21623849868774414\n",
      "iteration 44666: loss: 0.21623840928077698\n",
      "iteration 44667: loss: 0.2162383496761322\n",
      "iteration 44668: loss: 0.21623818576335907\n",
      "iteration 44669: loss: 0.2162380963563919\n",
      "iteration 44670: loss: 0.21623793244361877\n",
      "iteration 44671: loss: 0.21623782813549042\n",
      "iteration 44672: loss: 0.2162376344203949\n",
      "iteration 44673: loss: 0.21623757481575012\n",
      "iteration 44674: loss: 0.21623750030994415\n",
      "iteration 44675: loss: 0.21623730659484863\n",
      "iteration 44676: loss: 0.2162371575832367\n",
      "iteration 44677: loss: 0.21623703837394714\n",
      "iteration 44678: loss: 0.2162369191646576\n",
      "iteration 44679: loss: 0.21623678505420685\n",
      "iteration 44680: loss: 0.21623662114143372\n",
      "iteration 44681: loss: 0.21623654663562775\n",
      "iteration 44682: loss: 0.2162364423274994\n",
      "iteration 44683: loss: 0.21623632311820984\n",
      "iteration 44684: loss: 0.2162361890077591\n",
      "iteration 44685: loss: 0.21623602509498596\n",
      "iteration 44686: loss: 0.2162359058856964\n",
      "iteration 44687: loss: 0.21623575687408447\n",
      "iteration 44688: loss: 0.21623563766479492\n",
      "iteration 44689: loss: 0.21623551845550537\n",
      "iteration 44690: loss: 0.2162354439496994\n",
      "iteration 44691: loss: 0.21623530983924866\n",
      "iteration 44692: loss: 0.21623513102531433\n",
      "iteration 44693: loss: 0.21623501181602478\n",
      "iteration 44694: loss: 0.2162349671125412\n",
      "iteration 44695: loss: 0.21623480319976807\n",
      "iteration 44696: loss: 0.21623460948467255\n",
      "iteration 44697: loss: 0.21623456478118896\n",
      "iteration 44698: loss: 0.21623435616493225\n",
      "iteration 44699: loss: 0.2162342518568039\n",
      "iteration 44700: loss: 0.21623408794403076\n",
      "iteration 44701: loss: 0.2162339985370636\n",
      "iteration 44702: loss: 0.21623387932777405\n",
      "iteration 44703: loss: 0.21623380482196808\n",
      "iteration 44704: loss: 0.21623365581035614\n",
      "iteration 44705: loss: 0.2162335216999054\n",
      "iteration 44706: loss: 0.21623340249061584\n",
      "iteration 44707: loss: 0.2162332534790039\n",
      "iteration 44708: loss: 0.21623316407203674\n",
      "iteration 44709: loss: 0.2162330448627472\n",
      "iteration 44710: loss: 0.21623282134532928\n",
      "iteration 44711: loss: 0.21623273193836212\n",
      "iteration 44712: loss: 0.21623262763023376\n",
      "iteration 44713: loss: 0.21623246371746063\n",
      "iteration 44714: loss: 0.21623237431049347\n",
      "iteration 44715: loss: 0.2162322700023651\n",
      "iteration 44716: loss: 0.21623215079307556\n",
      "iteration 44717: loss: 0.21623201668262482\n",
      "iteration 44718: loss: 0.2162318229675293\n",
      "iteration 44719: loss: 0.21623165905475616\n",
      "iteration 44720: loss: 0.21623162925243378\n",
      "iteration 44721: loss: 0.21623143553733826\n",
      "iteration 44722: loss: 0.2162313163280487\n",
      "iteration 44723: loss: 0.21623115241527557\n",
      "iteration 44724: loss: 0.2162310779094696\n",
      "iteration 44725: loss: 0.21623098850250244\n",
      "iteration 44726: loss: 0.2162308245897293\n",
      "iteration 44727: loss: 0.21623070538043976\n",
      "iteration 44728: loss: 0.21623055636882782\n",
      "iteration 44729: loss: 0.21623042225837708\n",
      "iteration 44730: loss: 0.2162303477525711\n",
      "iteration 44731: loss: 0.21623018383979797\n",
      "iteration 44732: loss: 0.21623006463050842\n",
      "iteration 44733: loss: 0.21622994542121887\n",
      "iteration 44734: loss: 0.21622984111309052\n",
      "iteration 44735: loss: 0.21622970700263977\n",
      "iteration 44736: loss: 0.21622952818870544\n",
      "iteration 44737: loss: 0.21622943878173828\n",
      "iteration 44738: loss: 0.21622931957244873\n",
      "iteration 44739: loss: 0.2162291258573532\n",
      "iteration 44740: loss: 0.21622905135154724\n",
      "iteration 44741: loss: 0.2162289321422577\n",
      "iteration 44742: loss: 0.21622881293296814\n",
      "iteration 44743: loss: 0.2162286788225174\n",
      "iteration 44744: loss: 0.21622852981090546\n",
      "iteration 44745: loss: 0.2162284106016159\n",
      "iteration 44746: loss: 0.21622832119464874\n",
      "iteration 44747: loss: 0.2162281572818756\n",
      "iteration 44748: loss: 0.21622800827026367\n",
      "iteration 44749: loss: 0.2162279188632965\n",
      "iteration 44750: loss: 0.21622776985168457\n",
      "iteration 44751: loss: 0.21622760593891144\n",
      "iteration 44752: loss: 0.2162274420261383\n",
      "iteration 44753: loss: 0.21622738242149353\n",
      "iteration 44754: loss: 0.2162272185087204\n",
      "iteration 44755: loss: 0.21622714400291443\n",
      "iteration 44756: loss: 0.2162269651889801\n",
      "iteration 44757: loss: 0.21622684597969055\n",
      "iteration 44758: loss: 0.21622677147388458\n",
      "iteration 44759: loss: 0.21622666716575623\n",
      "iteration 44760: loss: 0.21622653305530548\n",
      "iteration 44761: loss: 0.21622638404369354\n",
      "iteration 44762: loss: 0.2162262201309204\n",
      "iteration 44763: loss: 0.21622613072395325\n",
      "iteration 44764: loss: 0.21622595191001892\n",
      "iteration 44765: loss: 0.21622583270072937\n",
      "iteration 44766: loss: 0.21622571349143982\n",
      "iteration 44767: loss: 0.21622557938098907\n",
      "iteration 44768: loss: 0.21622546017169952\n",
      "iteration 44769: loss: 0.21622535586357117\n",
      "iteration 44770: loss: 0.21622523665428162\n",
      "iteration 44771: loss: 0.21622511744499207\n",
      "iteration 44772: loss: 0.21622495353221893\n",
      "iteration 44773: loss: 0.21622486412525177\n",
      "iteration 44774: loss: 0.21622474491596222\n",
      "iteration 44775: loss: 0.21622462570667267\n",
      "iteration 44776: loss: 0.21622450649738312\n",
      "iteration 44777: loss: 0.21622435748577118\n",
      "iteration 44778: loss: 0.21622423827648163\n",
      "iteration 44779: loss: 0.21622402966022491\n",
      "iteration 44780: loss: 0.21622392535209656\n",
      "iteration 44781: loss: 0.216223806142807\n",
      "iteration 44782: loss: 0.21622368693351746\n",
      "iteration 44783: loss: 0.2162235975265503\n",
      "iteration 44784: loss: 0.21622344851493835\n",
      "iteration 44785: loss: 0.2162233144044876\n",
      "iteration 44786: loss: 0.21622316539287567\n",
      "iteration 44787: loss: 0.21622300148010254\n",
      "iteration 44788: loss: 0.21622292697429657\n",
      "iteration 44789: loss: 0.21622276306152344\n",
      "iteration 44790: loss: 0.21622267365455627\n",
      "iteration 44791: loss: 0.21622255444526672\n",
      "iteration 44792: loss: 0.21622243523597717\n",
      "iteration 44793: loss: 0.21622225642204285\n",
      "iteration 44794: loss: 0.21622209250926971\n",
      "iteration 44795: loss: 0.21622201800346375\n",
      "iteration 44796: loss: 0.21622195839881897\n",
      "iteration 44797: loss: 0.21622176468372345\n",
      "iteration 44798: loss: 0.2162216156721115\n",
      "iteration 44799: loss: 0.21622148156166077\n",
      "iteration 44800: loss: 0.2162214070558548\n",
      "iteration 44801: loss: 0.21622128784656525\n",
      "iteration 44802: loss: 0.21622109413146973\n",
      "iteration 44803: loss: 0.21622100472450256\n",
      "iteration 44804: loss: 0.2162209004163742\n",
      "iteration 44805: loss: 0.21622076630592346\n",
      "iteration 44806: loss: 0.2162206918001175\n",
      "iteration 44807: loss: 0.21622052788734436\n",
      "iteration 44808: loss: 0.21622034907341003\n",
      "iteration 44809: loss: 0.21622028946876526\n",
      "iteration 44810: loss: 0.21622011065483093\n",
      "iteration 44811: loss: 0.21621999144554138\n",
      "iteration 44812: loss: 0.21621990203857422\n",
      "iteration 44813: loss: 0.2162197083234787\n",
      "iteration 44814: loss: 0.21621958911418915\n",
      "iteration 44815: loss: 0.2162194699048996\n",
      "iteration 44816: loss: 0.21621933579444885\n",
      "iteration 44817: loss: 0.2162192165851593\n",
      "iteration 44818: loss: 0.21621909737586975\n",
      "iteration 44819: loss: 0.21621890366077423\n",
      "iteration 44820: loss: 0.21621878445148468\n",
      "iteration 44821: loss: 0.21621866524219513\n",
      "iteration 44822: loss: 0.21621859073638916\n",
      "iteration 44823: loss: 0.216218501329422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 44824: loss: 0.21621832251548767\n",
      "iteration 44825: loss: 0.21621815860271454\n",
      "iteration 44826: loss: 0.21621808409690857\n",
      "iteration 44827: loss: 0.21621794998645782\n",
      "iteration 44828: loss: 0.21621784567832947\n",
      "iteration 44829: loss: 0.21621768176555634\n",
      "iteration 44830: loss: 0.2162175178527832\n",
      "iteration 44831: loss: 0.2162175178527832\n",
      "iteration 44832: loss: 0.21621732413768768\n",
      "iteration 44833: loss: 0.21621716022491455\n",
      "iteration 44834: loss: 0.2162170708179474\n",
      "iteration 44835: loss: 0.21621695160865784\n",
      "iteration 44836: loss: 0.2162167727947235\n",
      "iteration 44837: loss: 0.21621668338775635\n",
      "iteration 44838: loss: 0.2162165343761444\n",
      "iteration 44839: loss: 0.21621641516685486\n",
      "iteration 44840: loss: 0.2162162810564041\n",
      "iteration 44841: loss: 0.21621613204479218\n",
      "iteration 44842: loss: 0.21621596813201904\n",
      "iteration 44843: loss: 0.2162158489227295\n",
      "iteration 44844: loss: 0.21621575951576233\n",
      "iteration 44845: loss: 0.21621565520763397\n",
      "iteration 44846: loss: 0.21621552109718323\n",
      "iteration 44847: loss: 0.21621541678905487\n",
      "iteration 44848: loss: 0.21621520817279816\n",
      "iteration 44849: loss: 0.21621517837047577\n",
      "iteration 44850: loss: 0.21621501445770264\n",
      "iteration 44851: loss: 0.21621493995189667\n",
      "iteration 44852: loss: 0.21621474623680115\n",
      "iteration 44853: loss: 0.21621465682983398\n",
      "iteration 44854: loss: 0.21621453762054443\n",
      "iteration 44855: loss: 0.2162143886089325\n",
      "iteration 44856: loss: 0.21621426939964294\n",
      "iteration 44857: loss: 0.2162141352891922\n",
      "iteration 44858: loss: 0.21621401607990265\n",
      "iteration 44859: loss: 0.21621385216712952\n",
      "iteration 44860: loss: 0.21621373295783997\n",
      "iteration 44861: loss: 0.21621361374855042\n",
      "iteration 44862: loss: 0.21621346473693848\n",
      "iteration 44863: loss: 0.21621334552764893\n",
      "iteration 44864: loss: 0.21621325612068176\n",
      "iteration 44865: loss: 0.21621307730674744\n",
      "iteration 44866: loss: 0.2162129133939743\n",
      "iteration 44867: loss: 0.2162129133939743\n",
      "iteration 44868: loss: 0.2162126749753952\n",
      "iteration 44869: loss: 0.21621260046958923\n",
      "iteration 44870: loss: 0.2162124216556549\n",
      "iteration 44871: loss: 0.21621231734752655\n",
      "iteration 44872: loss: 0.21621212363243103\n",
      "iteration 44873: loss: 0.21621207892894745\n",
      "iteration 44874: loss: 0.21621191501617432\n",
      "iteration 44875: loss: 0.21621175110340118\n",
      "iteration 44876: loss: 0.21621167659759521\n",
      "iteration 44877: loss: 0.21621155738830566\n",
      "iteration 44878: loss: 0.21621151268482208\n",
      "iteration 44879: loss: 0.21621131896972656\n",
      "iteration 44880: loss: 0.21621112525463104\n",
      "iteration 44881: loss: 0.21621108055114746\n",
      "iteration 44882: loss: 0.21621091663837433\n",
      "iteration 44883: loss: 0.2162107676267624\n",
      "iteration 44884: loss: 0.21621067821979523\n",
      "iteration 44885: loss: 0.2162105292081833\n",
      "iteration 44886: loss: 0.21621043980121613\n",
      "iteration 44887: loss: 0.21621021628379822\n",
      "iteration 44888: loss: 0.21621012687683105\n",
      "iteration 44889: loss: 0.21620997786521912\n",
      "iteration 44890: loss: 0.21620981395244598\n",
      "iteration 44891: loss: 0.21620972454547882\n",
      "iteration 44892: loss: 0.21620969474315643\n",
      "iteration 44893: loss: 0.21620957553386688\n",
      "iteration 44894: loss: 0.21620936691761017\n",
      "iteration 44895: loss: 0.21620924770832062\n",
      "iteration 44896: loss: 0.21620912849903107\n",
      "iteration 44897: loss: 0.21620900928974152\n",
      "iteration 44898: loss: 0.21620889008045197\n",
      "iteration 44899: loss: 0.21620869636535645\n",
      "iteration 44900: loss: 0.2162085324525833\n",
      "iteration 44901: loss: 0.21620845794677734\n",
      "iteration 44902: loss: 0.2162083089351654\n",
      "iteration 44903: loss: 0.21620826423168182\n",
      "iteration 44904: loss: 0.2162080705165863\n",
      "iteration 44905: loss: 0.21620798110961914\n",
      "iteration 44906: loss: 0.21620774269104004\n",
      "iteration 44907: loss: 0.21620769798755646\n",
      "iteration 44908: loss: 0.21620754897594452\n",
      "iteration 44909: loss: 0.21620742976665497\n",
      "iteration 44910: loss: 0.21620731055736542\n",
      "iteration 44911: loss: 0.21620717644691467\n",
      "iteration 44912: loss: 0.21620699763298035\n",
      "iteration 44913: loss: 0.21620693802833557\n",
      "iteration 44914: loss: 0.21620678901672363\n",
      "iteration 44915: loss: 0.2162066400051117\n",
      "iteration 44916: loss: 0.21620655059814453\n",
      "iteration 44917: loss: 0.2162064015865326\n",
      "iteration 44918: loss: 0.21620628237724304\n",
      "iteration 44919: loss: 0.2162061631679535\n",
      "iteration 44920: loss: 0.21620607376098633\n",
      "iteration 44921: loss: 0.2162058800458908\n",
      "iteration 44922: loss: 0.21620576083660126\n",
      "iteration 44923: loss: 0.2162056416273117\n",
      "iteration 44924: loss: 0.21620552241802216\n",
      "iteration 44925: loss: 0.2162054479122162\n",
      "iteration 44926: loss: 0.21620523929595947\n",
      "iteration 44927: loss: 0.21620512008666992\n",
      "iteration 44928: loss: 0.21620504558086395\n",
      "iteration 44929: loss: 0.2162049263715744\n",
      "iteration 44930: loss: 0.21620473265647888\n",
      "iteration 44931: loss: 0.21620464324951172\n",
      "iteration 44932: loss: 0.2162044793367386\n",
      "iteration 44933: loss: 0.21620433032512665\n",
      "iteration 44934: loss: 0.21620425581932068\n",
      "iteration 44935: loss: 0.21620401740074158\n",
      "iteration 44936: loss: 0.2162039577960968\n",
      "iteration 44937: loss: 0.21620380878448486\n",
      "iteration 44938: loss: 0.21620365977287292\n",
      "iteration 44939: loss: 0.21620360016822815\n",
      "iteration 44940: loss: 0.21620342135429382\n",
      "iteration 44941: loss: 0.21620337665081024\n",
      "iteration 44942: loss: 0.21620313823223114\n",
      "iteration 44943: loss: 0.21620306372642517\n",
      "iteration 44944: loss: 0.21620294451713562\n",
      "iteration 44945: loss: 0.21620281040668488\n",
      "iteration 44946: loss: 0.21620269119739532\n",
      "iteration 44947: loss: 0.2162025421857834\n",
      "iteration 44948: loss: 0.21620242297649384\n",
      "iteration 44949: loss: 0.2162022590637207\n",
      "iteration 44950: loss: 0.21620216965675354\n",
      "iteration 44951: loss: 0.2162020206451416\n",
      "iteration 44952: loss: 0.21620190143585205\n",
      "iteration 44953: loss: 0.2162017822265625\n",
      "iteration 44954: loss: 0.21620166301727295\n",
      "iteration 44955: loss: 0.2162015438079834\n",
      "iteration 44956: loss: 0.21620137989521027\n",
      "iteration 44957: loss: 0.2162013053894043\n",
      "iteration 44958: loss: 0.21620111167430878\n",
      "iteration 44959: loss: 0.21620099246501923\n",
      "iteration 44960: loss: 0.21620087325572968\n",
      "iteration 44961: loss: 0.21620075404644012\n",
      "iteration 44962: loss: 0.216200590133667\n",
      "iteration 44963: loss: 0.21620050072669983\n",
      "iteration 44964: loss: 0.2162003070116043\n",
      "iteration 44965: loss: 0.21620023250579834\n",
      "iteration 44966: loss: 0.21620015799999237\n",
      "iteration 44967: loss: 0.21619996428489685\n",
      "iteration 44968: loss: 0.21619991958141327\n",
      "iteration 44969: loss: 0.21619971096515656\n",
      "iteration 44970: loss: 0.21619963645935059\n",
      "iteration 44971: loss: 0.21619948744773865\n",
      "iteration 44972: loss: 0.2161993682384491\n",
      "iteration 44973: loss: 0.21619924902915955\n",
      "iteration 44974: loss: 0.21619915962219238\n",
      "iteration 44975: loss: 0.21619899570941925\n",
      "iteration 44976: loss: 0.2161988466978073\n",
      "iteration 44977: loss: 0.21619871258735657\n",
      "iteration 44978: loss: 0.21619859337806702\n",
      "iteration 44979: loss: 0.2161983996629715\n",
      "iteration 44980: loss: 0.21619832515716553\n",
      "iteration 44981: loss: 0.2161981761455536\n",
      "iteration 44982: loss: 0.21619801223278046\n",
      "iteration 44983: loss: 0.2161979228258133\n",
      "iteration 44984: loss: 0.21619777381420135\n",
      "iteration 44985: loss: 0.21619769930839539\n",
      "iteration 44986: loss: 0.21619752049446106\n",
      "iteration 44987: loss: 0.2161974012851715\n",
      "iteration 44988: loss: 0.21619728207588196\n",
      "iteration 44989: loss: 0.21619713306427002\n",
      "iteration 44990: loss: 0.21619708836078644\n",
      "iteration 44991: loss: 0.21619689464569092\n",
      "iteration 44992: loss: 0.21619677543640137\n",
      "iteration 44993: loss: 0.21619662642478943\n",
      "iteration 44994: loss: 0.21619653701782227\n",
      "iteration 44995: loss: 0.21619637310504913\n",
      "iteration 44996: loss: 0.216196209192276\n",
      "iteration 44997: loss: 0.21619613468647003\n",
      "iteration 44998: loss: 0.2161959856748581\n",
      "iteration 44999: loss: 0.21619582176208496\n",
      "iteration 45000: loss: 0.2161957323551178\n",
      "iteration 45001: loss: 0.21619558334350586\n",
      "iteration 45002: loss: 0.21619538962841034\n",
      "iteration 45003: loss: 0.21619534492492676\n",
      "iteration 45004: loss: 0.2161952555179596\n",
      "iteration 45005: loss: 0.21619510650634766\n",
      "iteration 45006: loss: 0.2161949872970581\n",
      "iteration 45007: loss: 0.21619483828544617\n",
      "iteration 45008: loss: 0.21619471907615662\n",
      "iteration 45009: loss: 0.21619459986686707\n",
      "iteration 45010: loss: 0.21619446575641632\n",
      "iteration 45011: loss: 0.21619434654712677\n",
      "iteration 45012: loss: 0.21619418263435364\n",
      "iteration 45013: loss: 0.2161940336227417\n",
      "iteration 45014: loss: 0.21619386970996857\n",
      "iteration 45015: loss: 0.21619375050067902\n",
      "iteration 45016: loss: 0.21619370579719543\n",
      "iteration 45017: loss: 0.21619351208209991\n",
      "iteration 45018: loss: 0.21619340777397156\n",
      "iteration 45019: loss: 0.2161932736635208\n",
      "iteration 45020: loss: 0.21619315445423126\n",
      "iteration 45021: loss: 0.2161930352449417\n",
      "iteration 45022: loss: 0.21619293093681335\n",
      "iteration 45023: loss: 0.2161927968263626\n",
      "iteration 45024: loss: 0.21619267761707306\n",
      "iteration 45025: loss: 0.21619248390197754\n",
      "iteration 45026: loss: 0.216192364692688\n",
      "iteration 45027: loss: 0.21619221568107605\n",
      "iteration 45028: loss: 0.2161921262741089\n",
      "iteration 45029: loss: 0.21619203686714172\n",
      "iteration 45030: loss: 0.21619188785552979\n",
      "iteration 45031: loss: 0.21619172394275665\n",
      "iteration 45032: loss: 0.21619167923927307\n",
      "iteration 45033: loss: 0.21619145572185516\n",
      "iteration 45034: loss: 0.21619132161140442\n",
      "iteration 45035: loss: 0.21619120240211487\n",
      "iteration 45036: loss: 0.21619108319282532\n",
      "iteration 45037: loss: 0.21619100868701935\n",
      "iteration 45038: loss: 0.21619084477424622\n",
      "iteration 45039: loss: 0.21619072556495667\n",
      "iteration 45040: loss: 0.21619060635566711\n",
      "iteration 45041: loss: 0.2161904275417328\n",
      "iteration 45042: loss: 0.216190367937088\n",
      "iteration 45043: loss: 0.21619009971618652\n",
      "iteration 45044: loss: 0.21619006991386414\n",
      "iteration 45045: loss: 0.216189906001091\n",
      "iteration 45046: loss: 0.21618981659412384\n",
      "iteration 45047: loss: 0.21618971228599548\n",
      "iteration 45048: loss: 0.21618962287902832\n",
      "iteration 45049: loss: 0.21618938446044922\n",
      "iteration 45050: loss: 0.21618929505348206\n",
      "iteration 45051: loss: 0.2161891907453537\n",
      "iteration 45052: loss: 0.21618899703025818\n",
      "iteration 45053: loss: 0.21618886291980743\n",
      "iteration 45054: loss: 0.21618874371051788\n",
      "iteration 45055: loss: 0.21618863940238953\n",
      "iteration 45056: loss: 0.21618850529193878\n",
      "iteration 45057: loss: 0.21618834137916565\n",
      "iteration 45058: loss: 0.21618826687335968\n",
      "iteration 45059: loss: 0.21618811786174774\n",
      "iteration 45060: loss: 0.21618792414665222\n",
      "iteration 45061: loss: 0.21618783473968506\n",
      "iteration 45062: loss: 0.21618767082691193\n",
      "iteration 45063: loss: 0.21618755161762238\n",
      "iteration 45064: loss: 0.2161875069141388\n",
      "iteration 45065: loss: 0.21618735790252686\n",
      "iteration 45066: loss: 0.21618719398975372\n",
      "iteration 45067: loss: 0.21618707478046417\n",
      "iteration 45068: loss: 0.21618697047233582\n",
      "iteration 45069: loss: 0.21618688106536865\n",
      "iteration 45070: loss: 0.21618668735027313\n",
      "iteration 45071: loss: 0.2161865234375\n",
      "iteration 45072: loss: 0.21618643403053284\n",
      "iteration 45073: loss: 0.21618624031543732\n",
      "iteration 45074: loss: 0.21618613600730896\n",
      "iteration 45075: loss: 0.2161860167980194\n",
      "iteration 45076: loss: 0.21618595719337463\n",
      "iteration 45077: loss: 0.21618585288524628\n",
      "iteration 45078: loss: 0.21618559956550598\n",
      "iteration 45079: loss: 0.2161855697631836\n",
      "iteration 45080: loss: 0.21618537604808807\n",
      "iteration 45081: loss: 0.21618518233299255\n",
      "iteration 45082: loss: 0.21618513762950897\n",
      "iteration 45083: loss: 0.21618500351905823\n",
      "iteration 45084: loss: 0.21618489921092987\n",
      "iteration 45085: loss: 0.21618476510047913\n",
      "iteration 45086: loss: 0.2161845862865448\n",
      "iteration 45087: loss: 0.21618452668190002\n",
      "iteration 45088: loss: 0.21618440747261047\n",
      "iteration 45089: loss: 0.21618422865867615\n",
      "iteration 45090: loss: 0.2161840945482254\n",
      "iteration 45091: loss: 0.21618394553661346\n",
      "iteration 45092: loss: 0.21618381142616272\n",
      "iteration 45093: loss: 0.21618369221687317\n",
      "iteration 45094: loss: 0.21618351340293884\n",
      "iteration 45095: loss: 0.21618345379829407\n",
      "iteration 45096: loss: 0.2161833494901657\n",
      "iteration 45097: loss: 0.21618323028087616\n",
      "iteration 45098: loss: 0.21618309617042542\n",
      "iteration 45099: loss: 0.2161829024553299\n",
      "iteration 45100: loss: 0.21618279814720154\n",
      "iteration 45101: loss: 0.216182678937912\n",
      "iteration 45102: loss: 0.21618251502513885\n",
      "iteration 45103: loss: 0.2161823809146881\n",
      "iteration 45104: loss: 0.21618226170539856\n",
      "iteration 45105: loss: 0.216182142496109\n",
      "iteration 45106: loss: 0.21618203818798065\n",
      "iteration 45107: loss: 0.21618184447288513\n",
      "iteration 45108: loss: 0.21618182957172394\n",
      "iteration 45109: loss: 0.2161816656589508\n",
      "iteration 45110: loss: 0.21618151664733887\n",
      "iteration 45111: loss: 0.21618130803108215\n",
      "iteration 45112: loss: 0.21618123352527618\n",
      "iteration 45113: loss: 0.21618108451366425\n",
      "iteration 45114: loss: 0.21618103981018066\n",
      "iteration 45115: loss: 0.21618084609508514\n",
      "iteration 45116: loss: 0.21618075668811798\n",
      "iteration 45117: loss: 0.21618060767650604\n",
      "iteration 45118: loss: 0.2161804735660553\n",
      "iteration 45119: loss: 0.21618039906024933\n",
      "iteration 45120: loss: 0.21618017554283142\n",
      "iteration 45121: loss: 0.21618011593818665\n",
      "iteration 45122: loss: 0.2161799669265747\n",
      "iteration 45123: loss: 0.21617984771728516\n",
      "iteration 45124: loss: 0.21617969870567322\n",
      "iteration 45125: loss: 0.2161795198917389\n",
      "iteration 45126: loss: 0.21617941558361053\n",
      "iteration 45127: loss: 0.2161792814731598\n",
      "iteration 45128: loss: 0.21617917716503143\n",
      "iteration 45129: loss: 0.21617913246154785\n",
      "iteration 45130: loss: 0.21617889404296875\n",
      "iteration 45131: loss: 0.2161788046360016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 45132: loss: 0.21617861092090607\n",
      "iteration 45133: loss: 0.21617849171161652\n",
      "iteration 45134: loss: 0.21617837250232697\n",
      "iteration 45135: loss: 0.21617825329303741\n",
      "iteration 45136: loss: 0.21617813408374786\n",
      "iteration 45137: loss: 0.21617797017097473\n",
      "iteration 45138: loss: 0.21617789566516876\n",
      "iteration 45139: loss: 0.21617774665355682\n",
      "iteration 45140: loss: 0.21617762744426727\n",
      "iteration 45141: loss: 0.21617749333381653\n",
      "iteration 45142: loss: 0.2161773443222046\n",
      "iteration 45143: loss: 0.21617725491523743\n",
      "iteration 45144: loss: 0.2161771059036255\n",
      "iteration 45145: loss: 0.21617695689201355\n",
      "iteration 45146: loss: 0.216176837682724\n",
      "iteration 45147: loss: 0.21617662906646729\n",
      "iteration 45148: loss: 0.21617653965950012\n",
      "iteration 45149: loss: 0.21617642045021057\n",
      "iteration 45150: loss: 0.21617627143859863\n",
      "iteration 45151: loss: 0.2161761224269867\n",
      "iteration 45152: loss: 0.21617603302001953\n",
      "iteration 45153: loss: 0.21617598831653595\n",
      "iteration 45154: loss: 0.21617576479911804\n",
      "iteration 45155: loss: 0.2161756306886673\n",
      "iteration 45156: loss: 0.21617552638053894\n",
      "iteration 45157: loss: 0.2161753922700882\n",
      "iteration 45158: loss: 0.21617527306079865\n",
      "iteration 45159: loss: 0.2161751240491867\n",
      "iteration 45160: loss: 0.21617503464221954\n",
      "iteration 45161: loss: 0.2161749303340912\n",
      "iteration 45162: loss: 0.21617472171783447\n",
      "iteration 45163: loss: 0.21617457270622253\n",
      "iteration 45164: loss: 0.21617457270622253\n",
      "iteration 45165: loss: 0.21617436408996582\n",
      "iteration 45166: loss: 0.21617428958415985\n",
      "iteration 45167: loss: 0.21617409586906433\n",
      "iteration 45168: loss: 0.2161739319562912\n",
      "iteration 45169: loss: 0.21617388725280762\n",
      "iteration 45170: loss: 0.21617376804351807\n",
      "iteration 45171: loss: 0.21617355942726135\n",
      "iteration 45172: loss: 0.21617349982261658\n",
      "iteration 45173: loss: 0.21617336571216583\n",
      "iteration 45174: loss: 0.2161732167005539\n",
      "iteration 45175: loss: 0.21617305278778076\n",
      "iteration 45176: loss: 0.2161729633808136\n",
      "iteration 45177: loss: 0.21617278456687927\n",
      "iteration 45178: loss: 0.21617265045642853\n",
      "iteration 45179: loss: 0.21617253124713898\n",
      "iteration 45180: loss: 0.21617238223552704\n",
      "iteration 45181: loss: 0.2161722630262375\n",
      "iteration 45182: loss: 0.21617212891578674\n",
      "iteration 45183: loss: 0.21617206931114197\n",
      "iteration 45184: loss: 0.21617189049720764\n",
      "iteration 45185: loss: 0.21617171168327332\n",
      "iteration 45186: loss: 0.21617162227630615\n",
      "iteration 45187: loss: 0.2161715030670166\n",
      "iteration 45188: loss: 0.21617138385772705\n",
      "iteration 45189: loss: 0.2161712348461151\n",
      "iteration 45190: loss: 0.21617107093334198\n",
      "iteration 45191: loss: 0.21617093682289124\n",
      "iteration 45192: loss: 0.21617081761360168\n",
      "iteration 45193: loss: 0.21617071330547333\n",
      "iteration 45194: loss: 0.21617059409618378\n",
      "iteration 45195: loss: 0.21617043018341064\n",
      "iteration 45196: loss: 0.21617023646831512\n",
      "iteration 45197: loss: 0.21617016196250916\n",
      "iteration 45198: loss: 0.216170072555542\n",
      "iteration 45199: loss: 0.21616992354393005\n",
      "iteration 45200: loss: 0.2161697894334793\n",
      "iteration 45201: loss: 0.21616967022418976\n",
      "iteration 45202: loss: 0.21616962552070618\n",
      "iteration 45203: loss: 0.21616943180561066\n",
      "iteration 45204: loss: 0.21616926789283752\n",
      "iteration 45205: loss: 0.2161690890789032\n",
      "iteration 45206: loss: 0.2161690890789032\n",
      "iteration 45207: loss: 0.21616888046264648\n",
      "iteration 45208: loss: 0.21616876125335693\n",
      "iteration 45209: loss: 0.21616864204406738\n",
      "iteration 45210: loss: 0.21616847813129425\n",
      "iteration 45211: loss: 0.2161683589220047\n",
      "iteration 45212: loss: 0.21616825461387634\n",
      "iteration 45213: loss: 0.2161681205034256\n",
      "iteration 45214: loss: 0.21616795659065247\n",
      "iteration 45215: loss: 0.2161678820848465\n",
      "iteration 45216: loss: 0.21616773307323456\n",
      "iteration 45217: loss: 0.2161675989627838\n",
      "iteration 45218: loss: 0.21616744995117188\n",
      "iteration 45219: loss: 0.2161673605442047\n",
      "iteration 45220: loss: 0.21616721153259277\n",
      "iteration 45221: loss: 0.21616706252098083\n",
      "iteration 45222: loss: 0.2161669284105301\n",
      "iteration 45223: loss: 0.21616682410240173\n",
      "iteration 45224: loss: 0.21616670489311218\n",
      "iteration 45225: loss: 0.21616654098033905\n",
      "iteration 45226: loss: 0.2161664515733719\n",
      "iteration 45227: loss: 0.21616630256175995\n",
      "iteration 45228: loss: 0.21616613864898682\n",
      "iteration 45229: loss: 0.21616606414318085\n",
      "iteration 45230: loss: 0.2161659300327301\n",
      "iteration 45231: loss: 0.21616575121879578\n",
      "iteration 45232: loss: 0.21616563200950623\n",
      "iteration 45233: loss: 0.21616554260253906\n",
      "iteration 45234: loss: 0.21616539359092712\n",
      "iteration 45235: loss: 0.21616525948047638\n",
      "iteration 45236: loss: 0.21616511046886444\n",
      "iteration 45237: loss: 0.2161649912595749\n",
      "iteration 45238: loss: 0.21616487205028534\n",
      "iteration 45239: loss: 0.2161647528409958\n",
      "iteration 45240: loss: 0.21616466343402863\n",
      "iteration 45241: loss: 0.21616443991661072\n",
      "iteration 45242: loss: 0.21616435050964355\n",
      "iteration 45243: loss: 0.21616418659687042\n",
      "iteration 45244: loss: 0.2161640226840973\n",
      "iteration 45245: loss: 0.21616394817829132\n",
      "iteration 45246: loss: 0.21616384387016296\n",
      "iteration 45247: loss: 0.21616370975971222\n",
      "iteration 45248: loss: 0.2161634862422943\n",
      "iteration 45249: loss: 0.21616336703300476\n",
      "iteration 45250: loss: 0.2161632478237152\n",
      "iteration 45251: loss: 0.21616312861442566\n",
      "iteration 45252: loss: 0.2161630392074585\n",
      "iteration 45253: loss: 0.21616287529468536\n",
      "iteration 45254: loss: 0.2161628007888794\n",
      "iteration 45255: loss: 0.21616263687610626\n",
      "iteration 45256: loss: 0.21616248786449432\n",
      "iteration 45257: loss: 0.21616236865520477\n",
      "iteration 45258: loss: 0.21616224944591522\n",
      "iteration 45259: loss: 0.21616211533546448\n",
      "iteration 45260: loss: 0.21616196632385254\n",
      "iteration 45261: loss: 0.21616189181804657\n",
      "iteration 45262: loss: 0.21616172790527344\n",
      "iteration 45263: loss: 0.21616163849830627\n",
      "iteration 45264: loss: 0.21616148948669434\n",
      "iteration 45265: loss: 0.2161613255739212\n",
      "iteration 45266: loss: 0.21616120636463165\n",
      "iteration 45267: loss: 0.21616105735301971\n",
      "iteration 45268: loss: 0.21616092324256897\n",
      "iteration 45269: loss: 0.21616074442863464\n",
      "iteration 45270: loss: 0.21616068482398987\n",
      "iteration 45271: loss: 0.21616056561470032\n",
      "iteration 45272: loss: 0.2161603718996048\n",
      "iteration 45273: loss: 0.21616026759147644\n",
      "iteration 45274: loss: 0.2161601334810257\n",
      "iteration 45275: loss: 0.21616001427173615\n",
      "iteration 45276: loss: 0.216159850358963\n",
      "iteration 45277: loss: 0.21615977585315704\n",
      "iteration 45278: loss: 0.2161596566438675\n",
      "iteration 45279: loss: 0.21615946292877197\n",
      "iteration 45280: loss: 0.2161593735218048\n",
      "iteration 45281: loss: 0.21615919470787048\n",
      "iteration 45282: loss: 0.21615906059741974\n",
      "iteration 45283: loss: 0.21615901589393616\n",
      "iteration 45284: loss: 0.21615886688232422\n",
      "iteration 45285: loss: 0.21615871787071228\n",
      "iteration 45286: loss: 0.21615859866142273\n",
      "iteration 45287: loss: 0.21615839004516602\n",
      "iteration 45288: loss: 0.21615831553936005\n",
      "iteration 45289: loss: 0.2161581814289093\n",
      "iteration 45290: loss: 0.21615803241729736\n",
      "iteration 45291: loss: 0.2161579579114914\n",
      "iteration 45292: loss: 0.21615779399871826\n",
      "iteration 45293: loss: 0.21615763008594513\n",
      "iteration 45294: loss: 0.21615752577781677\n",
      "iteration 45295: loss: 0.21615740656852722\n",
      "iteration 45296: loss: 0.21615734696388245\n",
      "iteration 45297: loss: 0.21615715324878693\n",
      "iteration 45298: loss: 0.21615703403949738\n",
      "iteration 45299: loss: 0.21615687012672424\n",
      "iteration 45300: loss: 0.21615679562091827\n",
      "iteration 45301: loss: 0.21615667641162872\n",
      "iteration 45302: loss: 0.2161565124988556\n",
      "iteration 45303: loss: 0.21615639328956604\n",
      "iteration 45304: loss: 0.2161562144756317\n",
      "iteration 45305: loss: 0.21615609526634216\n",
      "iteration 45306: loss: 0.2161559760570526\n",
      "iteration 45307: loss: 0.21615584194660187\n",
      "iteration 45308: loss: 0.21615567803382874\n",
      "iteration 45309: loss: 0.21615557372570038\n",
      "iteration 45310: loss: 0.21615543961524963\n",
      "iteration 45311: loss: 0.2161552906036377\n",
      "iteration 45312: loss: 0.21615509688854218\n",
      "iteration 45313: loss: 0.21615508198738098\n",
      "iteration 45314: loss: 0.21615484356880188\n",
      "iteration 45315: loss: 0.2161548137664795\n",
      "iteration 45316: loss: 0.21615466475486755\n",
      "iteration 45317: loss: 0.216154545545578\n",
      "iteration 45318: loss: 0.2161543369293213\n",
      "iteration 45319: loss: 0.21615426242351532\n",
      "iteration 45320: loss: 0.21615414321422577\n",
      "iteration 45321: loss: 0.21615397930145264\n",
      "iteration 45322: loss: 0.21615388989448547\n",
      "iteration 45323: loss: 0.21615377068519592\n",
      "iteration 45324: loss: 0.2161535769701004\n",
      "iteration 45325: loss: 0.21615347266197205\n",
      "iteration 45326: loss: 0.21615329384803772\n",
      "iteration 45327: loss: 0.21615321934223175\n",
      "iteration 45328: loss: 0.2161531001329422\n",
      "iteration 45329: loss: 0.21615293622016907\n",
      "iteration 45330: loss: 0.21615278720855713\n",
      "iteration 45331: loss: 0.21615271270275116\n",
      "iteration 45332: loss: 0.21615251898765564\n",
      "iteration 45333: loss: 0.21615245938301086\n",
      "iteration 45334: loss: 0.21615226566791534\n",
      "iteration 45335: loss: 0.216152161359787\n",
      "iteration 45336: loss: 0.21615202724933624\n",
      "iteration 45337: loss: 0.2161518782377243\n",
      "iteration 45338: loss: 0.21615175902843475\n",
      "iteration 45339: loss: 0.216151624917984\n",
      "iteration 45340: loss: 0.21615156531333923\n",
      "iteration 45341: loss: 0.2161514312028885\n",
      "iteration 45342: loss: 0.21615126729011536\n",
      "iteration 45343: loss: 0.21615108847618103\n",
      "iteration 45344: loss: 0.21615099906921387\n",
      "iteration 45345: loss: 0.21615079045295715\n",
      "iteration 45346: loss: 0.21615073084831238\n",
      "iteration 45347: loss: 0.21615056693553925\n",
      "iteration 45348: loss: 0.2161504328250885\n",
      "iteration 45349: loss: 0.21615032851696014\n",
      "iteration 45350: loss: 0.21615013480186462\n",
      "iteration 45351: loss: 0.21615004539489746\n",
      "iteration 45352: loss: 0.2161499261856079\n",
      "iteration 45353: loss: 0.21614983677864075\n",
      "iteration 45354: loss: 0.21614965796470642\n",
      "iteration 45355: loss: 0.21614952385425568\n",
      "iteration 45356: loss: 0.21614937484264374\n",
      "iteration 45357: loss: 0.21614930033683777\n",
      "iteration 45358: loss: 0.21614918112754822\n",
      "iteration 45359: loss: 0.2161490023136139\n",
      "iteration 45360: loss: 0.21614885330200195\n",
      "iteration 45361: loss: 0.2161487638950348\n",
      "iteration 45362: loss: 0.21614861488342285\n",
      "iteration 45363: loss: 0.21614845097064972\n",
      "iteration 45364: loss: 0.21614837646484375\n",
      "iteration 45365: loss: 0.21614818274974823\n",
      "iteration 45366: loss: 0.21614810824394226\n",
      "iteration 45367: loss: 0.21614789962768555\n",
      "iteration 45368: loss: 0.21614781022071838\n",
      "iteration 45369: loss: 0.2161477506160736\n",
      "iteration 45370: loss: 0.2161475121974945\n",
      "iteration 45371: loss: 0.21614739298820496\n",
      "iteration 45372: loss: 0.21614737808704376\n",
      "iteration 45373: loss: 0.21614718437194824\n",
      "iteration 45374: loss: 0.2161470204591751\n",
      "iteration 45375: loss: 0.21614694595336914\n",
      "iteration 45376: loss: 0.2161467969417572\n",
      "iteration 45377: loss: 0.21614661812782288\n",
      "iteration 45378: loss: 0.21614649891853333\n",
      "iteration 45379: loss: 0.2161463499069214\n",
      "iteration 45380: loss: 0.21614627540111542\n",
      "iteration 45381: loss: 0.2161460816860199\n",
      "iteration 45382: loss: 0.21614603698253632\n",
      "iteration 45383: loss: 0.2161458283662796\n",
      "iteration 45384: loss: 0.21614570915699005\n",
      "iteration 45385: loss: 0.21614563465118408\n",
      "iteration 45386: loss: 0.21614542603492737\n",
      "iteration 45387: loss: 0.216145321726799\n",
      "iteration 45388: loss: 0.21614518761634827\n",
      "iteration 45389: loss: 0.21614503860473633\n",
      "iteration 45390: loss: 0.21614491939544678\n",
      "iteration 45391: loss: 0.21614480018615723\n",
      "iteration 45392: loss: 0.21614468097686768\n",
      "iteration 45393: loss: 0.21614453196525574\n",
      "iteration 45394: loss: 0.216144397854805\n",
      "iteration 45395: loss: 0.21614429354667664\n",
      "iteration 45396: loss: 0.2161441296339035\n",
      "iteration 45397: loss: 0.21614401042461395\n",
      "iteration 45398: loss: 0.2161439210176468\n",
      "iteration 45399: loss: 0.21614375710487366\n",
      "iteration 45400: loss: 0.21614360809326172\n",
      "iteration 45401: loss: 0.21614348888397217\n",
      "iteration 45402: loss: 0.21614333987236023\n",
      "iteration 45403: loss: 0.21614325046539307\n",
      "iteration 45404: loss: 0.21614308655261993\n",
      "iteration 45405: loss: 0.21614298224449158\n",
      "iteration 45406: loss: 0.21614277362823486\n",
      "iteration 45407: loss: 0.21614274382591248\n",
      "iteration 45408: loss: 0.21614256501197815\n",
      "iteration 45409: loss: 0.2161424458026886\n",
      "iteration 45410: loss: 0.21614232659339905\n",
      "iteration 45411: loss: 0.2161421775817871\n",
      "iteration 45412: loss: 0.21614202857017517\n",
      "iteration 45413: loss: 0.21614190936088562\n",
      "iteration 45414: loss: 0.21614181995391846\n",
      "iteration 45415: loss: 0.21614162623882294\n",
      "iteration 45416: loss: 0.2161414921283722\n",
      "iteration 45417: loss: 0.21614141762256622\n",
      "iteration 45418: loss: 0.21614126861095428\n",
      "iteration 45419: loss: 0.21614105999469757\n",
      "iteration 45420: loss: 0.2161409854888916\n",
      "iteration 45421: loss: 0.21614091098308563\n",
      "iteration 45422: loss: 0.2161407768726349\n",
      "iteration 45423: loss: 0.21614065766334534\n",
      "iteration 45424: loss: 0.21614046394824982\n",
      "iteration 45425: loss: 0.21614035964012146\n",
      "iteration 45426: loss: 0.21614015102386475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 45427: loss: 0.2161400318145752\n",
      "iteration 45428: loss: 0.21613991260528564\n",
      "iteration 45429: loss: 0.2161397635936737\n",
      "iteration 45430: loss: 0.21613967418670654\n",
      "iteration 45431: loss: 0.216139554977417\n",
      "iteration 45432: loss: 0.21613940596580505\n",
      "iteration 45433: loss: 0.2161393165588379\n",
      "iteration 45434: loss: 0.21613916754722595\n",
      "iteration 45435: loss: 0.21613900363445282\n",
      "iteration 45436: loss: 0.21613888442516327\n",
      "iteration 45437: loss: 0.21613875031471252\n",
      "iteration 45438: loss: 0.21613860130310059\n",
      "iteration 45439: loss: 0.21613843739032745\n",
      "iteration 45440: loss: 0.2161383330821991\n",
      "iteration 45441: loss: 0.21613824367523193\n",
      "iteration 45442: loss: 0.2161380499601364\n",
      "iteration 45443: loss: 0.21613797545433044\n",
      "iteration 45444: loss: 0.2161378413438797\n",
      "iteration 45445: loss: 0.21613764762878418\n",
      "iteration 45446: loss: 0.21613767743110657\n",
      "iteration 45447: loss: 0.21613743901252747\n",
      "iteration 45448: loss: 0.21613724529743195\n",
      "iteration 45449: loss: 0.2161371409893036\n",
      "iteration 45450: loss: 0.2161370813846588\n",
      "iteration 45451: loss: 0.2161368429660797\n",
      "iteration 45452: loss: 0.21613678336143494\n",
      "iteration 45453: loss: 0.21613666415214539\n",
      "iteration 45454: loss: 0.21613650023937225\n",
      "iteration 45455: loss: 0.2161364108324051\n",
      "iteration 45456: loss: 0.21613626182079315\n",
      "iteration 45457: loss: 0.21613609790802002\n",
      "iteration 45458: loss: 0.21613594889640808\n",
      "iteration 45459: loss: 0.21613585948944092\n",
      "iteration 45460: loss: 0.2161356657743454\n",
      "iteration 45461: loss: 0.21613557636737823\n",
      "iteration 45462: loss: 0.21613547205924988\n",
      "iteration 45463: loss: 0.21613533794879913\n",
      "iteration 45464: loss: 0.21613521873950958\n",
      "iteration 45465: loss: 0.21613499522209167\n",
      "iteration 45466: loss: 0.2161349356174469\n",
      "iteration 45467: loss: 0.21613478660583496\n",
      "iteration 45468: loss: 0.2161346971988678\n",
      "iteration 45469: loss: 0.21613457798957825\n",
      "iteration 45470: loss: 0.21613439917564392\n",
      "iteration 45471: loss: 0.21613426506519318\n",
      "iteration 45472: loss: 0.21613411605358124\n",
      "iteration 45473: loss: 0.2161339521408081\n",
      "iteration 45474: loss: 0.21613390743732452\n",
      "iteration 45475: loss: 0.216133713722229\n",
      "iteration 45476: loss: 0.21613356471061707\n",
      "iteration 45477: loss: 0.2161335051059723\n",
      "iteration 45478: loss: 0.21613335609436035\n",
      "iteration 45479: loss: 0.2161332070827484\n",
      "iteration 45480: loss: 0.21613311767578125\n",
      "iteration 45481: loss: 0.2161329984664917\n",
      "iteration 45482: loss: 0.21613278985023499\n",
      "iteration 45483: loss: 0.21613268554210663\n",
      "iteration 45484: loss: 0.21613259613513947\n",
      "iteration 45485: loss: 0.21613243222236633\n",
      "iteration 45486: loss: 0.216132253408432\n",
      "iteration 45487: loss: 0.21613211929798126\n",
      "iteration 45488: loss: 0.21613197028636932\n",
      "iteration 45489: loss: 0.21613188087940216\n",
      "iteration 45490: loss: 0.2161317765712738\n",
      "iteration 45491: loss: 0.21613164246082306\n",
      "iteration 45492: loss: 0.21613149344921112\n",
      "iteration 45493: loss: 0.21613135933876038\n",
      "iteration 45494: loss: 0.21613121032714844\n",
      "iteration 45495: loss: 0.21613112092018127\n",
      "iteration 45496: loss: 0.21613100171089172\n",
      "iteration 45497: loss: 0.21613088250160217\n",
      "iteration 45498: loss: 0.21613073348999023\n",
      "iteration 45499: loss: 0.21613053977489471\n",
      "iteration 45500: loss: 0.21613046526908875\n",
      "iteration 45501: loss: 0.2161303460597992\n",
      "iteration 45502: loss: 0.2161301076412201\n",
      "iteration 45503: loss: 0.21613001823425293\n",
      "iteration 45504: loss: 0.21612994372844696\n",
      "iteration 45505: loss: 0.21612977981567383\n",
      "iteration 45506: loss: 0.2161296308040619\n",
      "iteration 45507: loss: 0.21612946689128876\n",
      "iteration 45508: loss: 0.2161293476819992\n",
      "iteration 45509: loss: 0.21612925827503204\n",
      "iteration 45510: loss: 0.2161291092634201\n",
      "iteration 45511: loss: 0.21612899005413055\n",
      "iteration 45512: loss: 0.216128870844841\n",
      "iteration 45513: loss: 0.21612875163555145\n",
      "iteration 45514: loss: 0.2161286175251007\n",
      "iteration 45515: loss: 0.21612851321697235\n",
      "iteration 45516: loss: 0.21612834930419922\n",
      "iteration 45517: loss: 0.21612811088562012\n",
      "iteration 45518: loss: 0.21612806618213654\n",
      "iteration 45519: loss: 0.21612796187400818\n",
      "iteration 45520: loss: 0.21612779796123505\n",
      "iteration 45521: loss: 0.21612770855426788\n",
      "iteration 45522: loss: 0.21612754464149475\n",
      "iteration 45523: loss: 0.2161273956298828\n",
      "iteration 45524: loss: 0.21612723171710968\n",
      "iteration 45525: loss: 0.2161271870136261\n",
      "iteration 45526: loss: 0.21612703800201416\n",
      "iteration 45527: loss: 0.21612684428691864\n",
      "iteration 45528: loss: 0.21612675487995148\n",
      "iteration 45529: loss: 0.21612663567066193\n",
      "iteration 45530: loss: 0.2161264419555664\n",
      "iteration 45531: loss: 0.21612635254859924\n",
      "iteration 45532: loss: 0.2161262035369873\n",
      "iteration 45533: loss: 0.21612600982189178\n",
      "iteration 45534: loss: 0.21612592041492462\n",
      "iteration 45535: loss: 0.21612577140331268\n",
      "iteration 45536: loss: 0.2161257266998291\n",
      "iteration 45537: loss: 0.21612548828125\n",
      "iteration 45538: loss: 0.21612541377544403\n",
      "iteration 45539: loss: 0.2161252498626709\n",
      "iteration 45540: loss: 0.21612516045570374\n",
      "iteration 45541: loss: 0.21612504124641418\n",
      "iteration 45542: loss: 0.21612489223480225\n",
      "iteration 45543: loss: 0.21612481772899628\n",
      "iteration 45544: loss: 0.21612465381622314\n",
      "iteration 45545: loss: 0.21612441539764404\n",
      "iteration 45546: loss: 0.21612434089183807\n",
      "iteration 45547: loss: 0.21612422168254852\n",
      "iteration 45548: loss: 0.2161240577697754\n",
      "iteration 45549: loss: 0.21612396836280823\n",
      "iteration 45550: loss: 0.2161238193511963\n",
      "iteration 45551: loss: 0.21612370014190674\n",
      "iteration 45552: loss: 0.21612350642681122\n",
      "iteration 45553: loss: 0.21612343192100525\n",
      "iteration 45554: loss: 0.21612326800823212\n",
      "iteration 45555: loss: 0.21612314879894257\n",
      "iteration 45556: loss: 0.2161230593919754\n",
      "iteration 45557: loss: 0.21612286567687988\n",
      "iteration 45558: loss: 0.21612277626991272\n",
      "iteration 45559: loss: 0.21612262725830078\n",
      "iteration 45560: loss: 0.21612250804901123\n",
      "iteration 45561: loss: 0.2161223590373993\n",
      "iteration 45562: loss: 0.2161223143339157\n",
      "iteration 45563: loss: 0.2161220759153366\n",
      "iteration 45564: loss: 0.21612198650836945\n",
      "iteration 45565: loss: 0.2161218225955963\n",
      "iteration 45566: loss: 0.2161216288805008\n",
      "iteration 45567: loss: 0.21612155437469482\n",
      "iteration 45568: loss: 0.21612146496772766\n",
      "iteration 45569: loss: 0.21612131595611572\n",
      "iteration 45570: loss: 0.21612119674682617\n",
      "iteration 45571: loss: 0.216121107339859\n",
      "iteration 45572: loss: 0.21612095832824707\n",
      "iteration 45573: loss: 0.21612071990966797\n",
      "iteration 45574: loss: 0.2161206305027008\n",
      "iteration 45575: loss: 0.21612045168876648\n",
      "iteration 45576: loss: 0.21612033247947693\n",
      "iteration 45577: loss: 0.21612027287483215\n",
      "iteration 45578: loss: 0.21612012386322021\n",
      "iteration 45579: loss: 0.21611997485160828\n",
      "iteration 45580: loss: 0.21611984074115753\n",
      "iteration 45581: loss: 0.2161196768283844\n",
      "iteration 45582: loss: 0.21611955761909485\n",
      "iteration 45583: loss: 0.2161194533109665\n",
      "iteration 45584: loss: 0.21611925959587097\n",
      "iteration 45585: loss: 0.2161191701889038\n",
      "iteration 45586: loss: 0.21611905097961426\n",
      "iteration 45587: loss: 0.21611890196800232\n",
      "iteration 45588: loss: 0.21611876785755157\n",
      "iteration 45589: loss: 0.21611861884593964\n",
      "iteration 45590: loss: 0.21611849963665009\n",
      "iteration 45591: loss: 0.21611838042736053\n",
      "iteration 45592: loss: 0.21611818671226501\n",
      "iteration 45593: loss: 0.21611812710762024\n",
      "iteration 45594: loss: 0.2161179482936859\n",
      "iteration 45595: loss: 0.21611790359020233\n",
      "iteration 45596: loss: 0.2161177098751068\n",
      "iteration 45597: loss: 0.21611757576465607\n",
      "iteration 45598: loss: 0.2161174714565277\n",
      "iteration 45599: loss: 0.21611730754375458\n",
      "iteration 45600: loss: 0.21611717343330383\n",
      "iteration 45601: loss: 0.21611706912517548\n",
      "iteration 45602: loss: 0.21611694991588593\n",
      "iteration 45603: loss: 0.2161167412996292\n",
      "iteration 45604: loss: 0.21611657738685608\n",
      "iteration 45605: loss: 0.21611647307872772\n",
      "iteration 45606: loss: 0.21611639857292175\n",
      "iteration 45607: loss: 0.21611618995666504\n",
      "iteration 45608: loss: 0.21611616015434265\n",
      "iteration 45609: loss: 0.2161160260438919\n",
      "iteration 45610: loss: 0.21611586213111877\n",
      "iteration 45611: loss: 0.21611575782299042\n",
      "iteration 45612: loss: 0.21611562371253967\n",
      "iteration 45613: loss: 0.21611544489860535\n",
      "iteration 45614: loss: 0.21611526608467102\n",
      "iteration 45615: loss: 0.21611519157886505\n",
      "iteration 45616: loss: 0.2161150723695755\n",
      "iteration 45617: loss: 0.21611492335796356\n",
      "iteration 45618: loss: 0.21611475944519043\n",
      "iteration 45619: loss: 0.21611464023590088\n",
      "iteration 45620: loss: 0.21611452102661133\n",
      "iteration 45621: loss: 0.2161143273115158\n",
      "iteration 45622: loss: 0.21611423790454865\n",
      "iteration 45623: loss: 0.2161141186952591\n",
      "iteration 45624: loss: 0.21611396968364716\n",
      "iteration 45625: loss: 0.2161138504743576\n",
      "iteration 45626: loss: 0.21611376106739044\n",
      "iteration 45627: loss: 0.21611356735229492\n",
      "iteration 45628: loss: 0.21611349284648895\n",
      "iteration 45629: loss: 0.21611332893371582\n",
      "iteration 45630: loss: 0.21611320972442627\n",
      "iteration 45631: loss: 0.21611304581165314\n",
      "iteration 45632: loss: 0.21611294150352478\n",
      "iteration 45633: loss: 0.21611282229423523\n",
      "iteration 45634: loss: 0.2161126583814621\n",
      "iteration 45635: loss: 0.21611253917217255\n",
      "iteration 45636: loss: 0.2161124050617218\n",
      "iteration 45637: loss: 0.21611228585243225\n",
      "iteration 45638: loss: 0.21611210703849792\n",
      "iteration 45639: loss: 0.21611204743385315\n",
      "iteration 45640: loss: 0.21611180901527405\n",
      "iteration 45641: loss: 0.2161117047071457\n",
      "iteration 45642: loss: 0.21611157059669495\n",
      "iteration 45643: loss: 0.21611149609088898\n",
      "iteration 45644: loss: 0.21611127257347107\n",
      "iteration 45645: loss: 0.2161112129688263\n",
      "iteration 45646: loss: 0.21611103415489197\n",
      "iteration 45647: loss: 0.21611090004444122\n",
      "iteration 45648: loss: 0.21611082553863525\n",
      "iteration 45649: loss: 0.21611063182353973\n",
      "iteration 45650: loss: 0.21611054241657257\n",
      "iteration 45651: loss: 0.21611037850379944\n",
      "iteration 45652: loss: 0.2161102592945099\n",
      "iteration 45653: loss: 0.21611014008522034\n",
      "iteration 45654: loss: 0.2161099910736084\n",
      "iteration 45655: loss: 0.21610979735851288\n",
      "iteration 45656: loss: 0.21610966324806213\n",
      "iteration 45657: loss: 0.21610955893993378\n",
      "iteration 45658: loss: 0.21610943973064423\n",
      "iteration 45659: loss: 0.21610936522483826\n",
      "iteration 45660: loss: 0.21610918641090393\n",
      "iteration 45661: loss: 0.216109037399292\n",
      "iteration 45662: loss: 0.21610891819000244\n",
      "iteration 45663: loss: 0.2161087989807129\n",
      "iteration 45664: loss: 0.21610867977142334\n",
      "iteration 45665: loss: 0.2161085158586502\n",
      "iteration 45666: loss: 0.21610847115516663\n",
      "iteration 45667: loss: 0.2161082774400711\n",
      "iteration 45668: loss: 0.21610815823078156\n",
      "iteration 45669: loss: 0.21610796451568604\n",
      "iteration 45670: loss: 0.21610787510871887\n",
      "iteration 45671: loss: 0.21610775589942932\n",
      "iteration 45672: loss: 0.2161075621843338\n",
      "iteration 45673: loss: 0.21610744297504425\n",
      "iteration 45674: loss: 0.21610736846923828\n",
      "iteration 45675: loss: 0.21610721945762634\n",
      "iteration 45676: loss: 0.2161070555448532\n",
      "iteration 45677: loss: 0.21610693633556366\n",
      "iteration 45678: loss: 0.21610677242279053\n",
      "iteration 45679: loss: 0.21610668301582336\n",
      "iteration 45680: loss: 0.216106578707695\n",
      "iteration 45681: loss: 0.2161063849925995\n",
      "iteration 45682: loss: 0.21610626578330994\n",
      "iteration 45683: loss: 0.2161061316728592\n",
      "iteration 45684: loss: 0.21610598266124725\n",
      "iteration 45685: loss: 0.21610581874847412\n",
      "iteration 45686: loss: 0.21610577404499054\n",
      "iteration 45687: loss: 0.2161056250333786\n",
      "iteration 45688: loss: 0.21610546112060547\n",
      "iteration 45689: loss: 0.2161053717136383\n",
      "iteration 45690: loss: 0.21610517799854279\n",
      "iteration 45691: loss: 0.21610502898693085\n",
      "iteration 45692: loss: 0.21610493957996368\n",
      "iteration 45693: loss: 0.21610477566719055\n",
      "iteration 45694: loss: 0.216104656457901\n",
      "iteration 45695: loss: 0.21610450744628906\n",
      "iteration 45696: loss: 0.2161044329404831\n",
      "iteration 45697: loss: 0.21610426902770996\n",
      "iteration 45698: loss: 0.21610407531261444\n",
      "iteration 45699: loss: 0.2161039561033249\n",
      "iteration 45700: loss: 0.21610388159751892\n",
      "iteration 45701: loss: 0.21610364317893982\n",
      "iteration 45702: loss: 0.21610359847545624\n",
      "iteration 45703: loss: 0.2161034643650055\n",
      "iteration 45704: loss: 0.21610336005687714\n",
      "iteration 45705: loss: 0.21610316634178162\n",
      "iteration 45706: loss: 0.21610307693481445\n",
      "iteration 45707: loss: 0.21610291302204132\n",
      "iteration 45708: loss: 0.21610280871391296\n",
      "iteration 45709: loss: 0.21610267460346222\n",
      "iteration 45710: loss: 0.21610252559185028\n",
      "iteration 45711: loss: 0.21610233187675476\n",
      "iteration 45712: loss: 0.2161022126674652\n",
      "iteration 45713: loss: 0.21610212326049805\n",
      "iteration 45714: loss: 0.2161019742488861\n",
      "iteration 45715: loss: 0.21610184013843536\n",
      "iteration 45716: loss: 0.21610167622566223\n",
      "iteration 45717: loss: 0.21610160171985626\n",
      "iteration 45718: loss: 0.2161014825105667\n",
      "iteration 45719: loss: 0.21610131859779358\n",
      "iteration 45720: loss: 0.21610119938850403\n",
      "iteration 45721: loss: 0.2161010503768921\n",
      "iteration 45722: loss: 0.21610088646411896\n",
      "iteration 45723: loss: 0.2161007821559906\n",
      "iteration 45724: loss: 0.21610061824321747\n",
      "iteration 45725: loss: 0.21610048413276672\n",
      "iteration 45726: loss: 0.21610037982463837\n",
      "iteration 45727: loss: 0.21610024571418762\n",
      "iteration 45728: loss: 0.21610002219676971\n",
      "iteration 45729: loss: 0.2161000519990921\n",
      "iteration 45730: loss: 0.2160998284816742\n",
      "iteration 45731: loss: 0.21609970927238464\n",
      "iteration 45732: loss: 0.2160995900630951\n",
      "iteration 45733: loss: 0.21609942615032196\n",
      "iteration 45734: loss: 0.21609926223754883\n",
      "iteration 45735: loss: 0.21609918773174286\n",
      "iteration 45736: loss: 0.2160990685224533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 45737: loss: 0.21609890460968018\n",
      "iteration 45738: loss: 0.21609875559806824\n",
      "iteration 45739: loss: 0.2160985916852951\n",
      "iteration 45740: loss: 0.21609851717948914\n",
      "iteration 45741: loss: 0.216098353266716\n",
      "iteration 45742: loss: 0.21609826385974884\n",
      "iteration 45743: loss: 0.21609807014465332\n",
      "iteration 45744: loss: 0.21609792113304138\n",
      "iteration 45745: loss: 0.21609783172607422\n",
      "iteration 45746: loss: 0.21609771251678467\n",
      "iteration 45747: loss: 0.21609759330749512\n",
      "iteration 45748: loss: 0.21609742939472198\n",
      "iteration 45749: loss: 0.21609726548194885\n",
      "iteration 45750: loss: 0.21609720587730408\n",
      "iteration 45751: loss: 0.21609707176685333\n",
      "iteration 45752: loss: 0.2160969078540802\n",
      "iteration 45753: loss: 0.21609678864479065\n",
      "iteration 45754: loss: 0.2160966694355011\n",
      "iteration 45755: loss: 0.21609649062156677\n",
      "iteration 45756: loss: 0.21609635651111603\n",
      "iteration 45757: loss: 0.2160961925983429\n",
      "iteration 45758: loss: 0.21609607338905334\n",
      "iteration 45759: loss: 0.21609599888324738\n",
      "iteration 45760: loss: 0.21609576046466827\n",
      "iteration 45761: loss: 0.2160957157611847\n",
      "iteration 45762: loss: 0.21609549224376678\n",
      "iteration 45763: loss: 0.2160954773426056\n",
      "iteration 45764: loss: 0.21609528362751007\n",
      "iteration 45765: loss: 0.21609513461589813\n",
      "iteration 45766: loss: 0.21609506011009216\n",
      "iteration 45767: loss: 0.21609492599964142\n",
      "iteration 45768: loss: 0.21609477698802948\n",
      "iteration 45769: loss: 0.21609461307525635\n",
      "iteration 45770: loss: 0.21609452366828918\n",
      "iteration 45771: loss: 0.21609432995319366\n",
      "iteration 45772: loss: 0.2160942107439041\n",
      "iteration 45773: loss: 0.21609409153461456\n",
      "iteration 45774: loss: 0.21609386801719666\n",
      "iteration 45775: loss: 0.21609386801719666\n",
      "iteration 45776: loss: 0.21609370410442352\n",
      "iteration 45777: loss: 0.21609356999397278\n",
      "iteration 45778: loss: 0.21609345078468323\n",
      "iteration 45779: loss: 0.21609322726726532\n",
      "iteration 45780: loss: 0.21609313786029816\n",
      "iteration 45781: loss: 0.2160930186510086\n",
      "iteration 45782: loss: 0.21609285473823547\n",
      "iteration 45783: loss: 0.2160927802324295\n",
      "iteration 45784: loss: 0.21609266102313995\n",
      "iteration 45785: loss: 0.21609243750572205\n",
      "iteration 45786: loss: 0.21609237790107727\n",
      "iteration 45787: loss: 0.21609210968017578\n",
      "iteration 45788: loss: 0.2160920798778534\n",
      "iteration 45789: loss: 0.21609191596508026\n",
      "iteration 45790: loss: 0.21609178185462952\n",
      "iteration 45791: loss: 0.21609167754650116\n",
      "iteration 45792: loss: 0.216091588139534\n",
      "iteration 45793: loss: 0.21609142422676086\n",
      "iteration 45794: loss: 0.21609127521514893\n",
      "iteration 45795: loss: 0.216091126203537\n",
      "iteration 45796: loss: 0.21609100699424744\n",
      "iteration 45797: loss: 0.21609088778495789\n",
      "iteration 45798: loss: 0.21609070897102356\n",
      "iteration 45799: loss: 0.2160906344652176\n",
      "iteration 45800: loss: 0.21609051525592804\n",
      "iteration 45801: loss: 0.2160903960466385\n",
      "iteration 45802: loss: 0.21609023213386536\n",
      "iteration 45803: loss: 0.21609005331993103\n",
      "iteration 45804: loss: 0.2160898894071579\n",
      "iteration 45805: loss: 0.21608972549438477\n",
      "iteration 45806: loss: 0.21608969569206238\n",
      "iteration 45807: loss: 0.21608957648277283\n",
      "iteration 45808: loss: 0.21608944237232208\n",
      "iteration 45809: loss: 0.2160891741514206\n",
      "iteration 45810: loss: 0.2160891592502594\n",
      "iteration 45811: loss: 0.21608898043632507\n",
      "iteration 45812: loss: 0.21608884632587433\n",
      "iteration 45813: loss: 0.21608874201774597\n",
      "iteration 45814: loss: 0.21608860790729523\n",
      "iteration 45815: loss: 0.2160884588956833\n",
      "iteration 45816: loss: 0.21608832478523254\n",
      "iteration 45817: loss: 0.216088205575943\n",
      "iteration 45818: loss: 0.21608801186084747\n",
      "iteration 45819: loss: 0.21608789265155792\n",
      "iteration 45820: loss: 0.21608777344226837\n",
      "iteration 45821: loss: 0.21608762443065643\n",
      "iteration 45822: loss: 0.21608753502368927\n",
      "iteration 45823: loss: 0.21608734130859375\n",
      "iteration 45824: loss: 0.2160872519016266\n",
      "iteration 45825: loss: 0.21608714759349823\n",
      "iteration 45826: loss: 0.21608701348304749\n",
      "iteration 45827: loss: 0.21608686447143555\n",
      "iteration 45828: loss: 0.21608667075634003\n",
      "iteration 45829: loss: 0.21608655154705048\n",
      "iteration 45830: loss: 0.2160864621400833\n",
      "iteration 45831: loss: 0.21608629822731018\n",
      "iteration 45832: loss: 0.21608614921569824\n",
      "iteration 45833: loss: 0.2160860300064087\n",
      "iteration 45834: loss: 0.21608586609363556\n",
      "iteration 45835: loss: 0.2160857617855072\n",
      "iteration 45836: loss: 0.21608567237854004\n",
      "iteration 45837: loss: 0.21608558297157288\n",
      "iteration 45838: loss: 0.21608534455299377\n",
      "iteration 45839: loss: 0.21608516573905945\n",
      "iteration 45840: loss: 0.2160850465297699\n",
      "iteration 45841: loss: 0.21608488261699677\n",
      "iteration 45842: loss: 0.2160847932100296\n",
      "iteration 45843: loss: 0.21608467400074005\n",
      "iteration 45844: loss: 0.2160845696926117\n",
      "iteration 45845: loss: 0.21608443558216095\n",
      "iteration 45846: loss: 0.21608424186706543\n",
      "iteration 45847: loss: 0.21608415246009827\n",
      "iteration 45848: loss: 0.21608397364616394\n",
      "iteration 45849: loss: 0.2160838544368744\n",
      "iteration 45850: loss: 0.21608369052410126\n",
      "iteration 45851: loss: 0.2160836011171341\n",
      "iteration 45852: loss: 0.21608343720436096\n",
      "iteration 45853: loss: 0.216083362698555\n",
      "iteration 45854: loss: 0.21608316898345947\n",
      "iteration 45855: loss: 0.2160830944776535\n",
      "iteration 45856: loss: 0.21608296036720276\n",
      "iteration 45857: loss: 0.21608281135559082\n",
      "iteration 45858: loss: 0.2160826176404953\n",
      "iteration 45859: loss: 0.21608252823352814\n",
      "iteration 45860: loss: 0.2160823792219162\n",
      "iteration 45861: loss: 0.21608226001262665\n",
      "iteration 45862: loss: 0.2160821259021759\n",
      "iteration 45863: loss: 0.21608194708824158\n",
      "iteration 45864: loss: 0.21608185768127441\n",
      "iteration 45865: loss: 0.21608173847198486\n",
      "iteration 45866: loss: 0.2160816490650177\n",
      "iteration 45867: loss: 0.2160814255475998\n",
      "iteration 45868: loss: 0.21608133614063263\n",
      "iteration 45869: loss: 0.2160811722278595\n",
      "iteration 45870: loss: 0.21608097851276398\n",
      "iteration 45871: loss: 0.21608087420463562\n",
      "iteration 45872: loss: 0.21608078479766846\n",
      "iteration 45873: loss: 0.2160807102918625\n",
      "iteration 45874: loss: 0.2160804718732834\n",
      "iteration 45875: loss: 0.21608038246631622\n",
      "iteration 45876: loss: 0.21608027815818787\n",
      "iteration 45877: loss: 0.21608009934425354\n",
      "iteration 45878: loss: 0.21607999503612518\n",
      "iteration 45879: loss: 0.21607983112335205\n",
      "iteration 45880: loss: 0.21607966721057892\n",
      "iteration 45881: loss: 0.21607962250709534\n",
      "iteration 45882: loss: 0.21607938408851624\n",
      "iteration 45883: loss: 0.21607927978038788\n",
      "iteration 45884: loss: 0.21607914566993713\n",
      "iteration 45885: loss: 0.21607902646064758\n",
      "iteration 45886: loss: 0.21607887744903564\n",
      "iteration 45887: loss: 0.2160787582397461\n",
      "iteration 45888: loss: 0.21607863903045654\n",
      "iteration 45889: loss: 0.21607844531536102\n",
      "iteration 45890: loss: 0.21607835590839386\n",
      "iteration 45891: loss: 0.21607819199562073\n",
      "iteration 45892: loss: 0.21607808768749237\n",
      "iteration 45893: loss: 0.21607796847820282\n",
      "iteration 45894: loss: 0.21607784926891327\n",
      "iteration 45895: loss: 0.21607773005962372\n",
      "iteration 45896: loss: 0.2160775363445282\n",
      "iteration 45897: loss: 0.21607741713523865\n",
      "iteration 45898: loss: 0.21607735753059387\n",
      "iteration 45899: loss: 0.21607711911201477\n",
      "iteration 45900: loss: 0.21607697010040283\n",
      "iteration 45901: loss: 0.21607685089111328\n",
      "iteration 45902: loss: 0.21607676148414612\n",
      "iteration 45903: loss: 0.2160765826702118\n",
      "iteration 45904: loss: 0.21607644855976105\n",
      "iteration 45905: loss: 0.2160762995481491\n",
      "iteration 45906: loss: 0.21607618033885956\n",
      "iteration 45907: loss: 0.2160761058330536\n",
      "iteration 45908: loss: 0.21607597172260284\n",
      "iteration 45909: loss: 0.21607573330402374\n",
      "iteration 45910: loss: 0.2160756140947342\n",
      "iteration 45911: loss: 0.21607550978660583\n",
      "iteration 45912: loss: 0.21607539057731628\n",
      "iteration 45913: loss: 0.21607527136802673\n",
      "iteration 45914: loss: 0.2160750925540924\n",
      "iteration 45915: loss: 0.21607497334480286\n",
      "iteration 45916: loss: 0.21607479453086853\n",
      "iteration 45917: loss: 0.21607470512390137\n",
      "iteration 45918: loss: 0.21607455611228943\n",
      "iteration 45919: loss: 0.21607443690299988\n",
      "iteration 45920: loss: 0.21607431769371033\n",
      "iteration 45921: loss: 0.216074138879776\n",
      "iteration 45922: loss: 0.21607406437397003\n",
      "iteration 45923: loss: 0.21607384085655212\n",
      "iteration 45924: loss: 0.21607375144958496\n",
      "iteration 45925: loss: 0.2160736620426178\n",
      "iteration 45926: loss: 0.2160734385251999\n",
      "iteration 45927: loss: 0.21607336401939392\n",
      "iteration 45928: loss: 0.21607322990894318\n",
      "iteration 45929: loss: 0.21607306599617004\n",
      "iteration 45930: loss: 0.2160729616880417\n",
      "iteration 45931: loss: 0.21607275307178497\n",
      "iteration 45932: loss: 0.216072678565979\n",
      "iteration 45933: loss: 0.21607252955436707\n",
      "iteration 45934: loss: 0.21607236564159393\n",
      "iteration 45935: loss: 0.21607232093811035\n",
      "iteration 45936: loss: 0.21607215702533722\n",
      "iteration 45937: loss: 0.21607200801372528\n",
      "iteration 45938: loss: 0.21607181429862976\n",
      "iteration 45939: loss: 0.2160717248916626\n",
      "iteration 45940: loss: 0.21607165038585663\n",
      "iteration 45941: loss: 0.21607141196727753\n",
      "iteration 45942: loss: 0.21607127785682678\n",
      "iteration 45943: loss: 0.21607117354869843\n",
      "iteration 45944: loss: 0.21607103943824768\n",
      "iteration 45945: loss: 0.21607093513011932\n",
      "iteration 45946: loss: 0.21607080101966858\n",
      "iteration 45947: loss: 0.21607057750225067\n",
      "iteration 45948: loss: 0.2160705327987671\n",
      "iteration 45949: loss: 0.21607036888599396\n",
      "iteration 45950: loss: 0.2160702496767044\n",
      "iteration 45951: loss: 0.21607013046741486\n",
      "iteration 45952: loss: 0.21606990694999695\n",
      "iteration 45953: loss: 0.2160697877407074\n",
      "iteration 45954: loss: 0.21606972813606262\n",
      "iteration 45955: loss: 0.21606957912445068\n",
      "iteration 45956: loss: 0.21606943011283875\n",
      "iteration 45957: loss: 0.216069296002388\n",
      "iteration 45958: loss: 0.21606913208961487\n",
      "iteration 45959: loss: 0.21606901288032532\n",
      "iteration 45960: loss: 0.21606893837451935\n",
      "iteration 45961: loss: 0.21606874465942383\n",
      "iteration 45962: loss: 0.2160685807466507\n",
      "iteration 45963: loss: 0.21606847643852234\n",
      "iteration 45964: loss: 0.21606841683387756\n",
      "iteration 45965: loss: 0.21606822311878204\n",
      "iteration 45966: loss: 0.2160681039094925\n",
      "iteration 45967: loss: 0.21606793999671936\n",
      "iteration 45968: loss: 0.21606776118278503\n",
      "iteration 45969: loss: 0.21606764197349548\n",
      "iteration 45970: loss: 0.2160675972700119\n",
      "iteration 45971: loss: 0.2160673886537552\n",
      "iteration 45972: loss: 0.21606722474098206\n",
      "iteration 45973: loss: 0.21606704592704773\n",
      "iteration 45974: loss: 0.21606703102588654\n",
      "iteration 45975: loss: 0.2160668820142746\n",
      "iteration 45976: loss: 0.21606668829917908\n",
      "iteration 45977: loss: 0.21606659889221191\n",
      "iteration 45978: loss: 0.21606643497943878\n",
      "iteration 45979: loss: 0.21606631577014923\n",
      "iteration 45980: loss: 0.2160661518573761\n",
      "iteration 45981: loss: 0.21606600284576416\n",
      "iteration 45982: loss: 0.21606583893299103\n",
      "iteration 45983: loss: 0.21606571972370148\n",
      "iteration 45984: loss: 0.2160656452178955\n",
      "iteration 45985: loss: 0.21606552600860596\n",
      "iteration 45986: loss: 0.21606533229351044\n",
      "iteration 45987: loss: 0.21606525778770447\n",
      "iteration 45988: loss: 0.21606513857841492\n",
      "iteration 45989: loss: 0.21606497466564178\n",
      "iteration 45990: loss: 0.21606478095054626\n",
      "iteration 45991: loss: 0.2160646617412567\n",
      "iteration 45992: loss: 0.21606460213661194\n",
      "iteration 45993: loss: 0.21606437861919403\n",
      "iteration 45994: loss: 0.21606430411338806\n",
      "iteration 45995: loss: 0.2160641849040985\n",
      "iteration 45996: loss: 0.21606402099132538\n",
      "iteration 45997: loss: 0.21606393158435822\n",
      "iteration 45998: loss: 0.2160636931657791\n",
      "iteration 45999: loss: 0.21606358885765076\n",
      "iteration 46000: loss: 0.2160634547472\n",
      "iteration 46001: loss: 0.21606338024139404\n",
      "iteration 46002: loss: 0.2160632312297821\n",
      "iteration 46003: loss: 0.21606306731700897\n",
      "iteration 46004: loss: 0.21606293320655823\n",
      "iteration 46005: loss: 0.2160627841949463\n",
      "iteration 46006: loss: 0.21606259047985077\n",
      "iteration 46007: loss: 0.21606245636940002\n",
      "iteration 46008: loss: 0.21606239676475525\n",
      "iteration 46009: loss: 0.21606218814849854\n",
      "iteration 46010: loss: 0.21606211364269257\n",
      "iteration 46011: loss: 0.21606197953224182\n",
      "iteration 46012: loss: 0.2160617858171463\n",
      "iteration 46013: loss: 0.21606166660785675\n",
      "iteration 46014: loss: 0.21606159210205078\n",
      "iteration 46015: loss: 0.21606142818927765\n",
      "iteration 46016: loss: 0.21606126427650452\n",
      "iteration 46017: loss: 0.21606115996837616\n",
      "iteration 46018: loss: 0.216061070561409\n",
      "iteration 46019: loss: 0.21606090664863586\n",
      "iteration 46020: loss: 0.2160607874393463\n",
      "iteration 46021: loss: 0.21606066823005676\n",
      "iteration 46022: loss: 0.21606048941612244\n",
      "iteration 46023: loss: 0.2160603553056717\n",
      "iteration 46024: loss: 0.21606020629405975\n",
      "iteration 46025: loss: 0.2160600870847702\n",
      "iteration 46026: loss: 0.21605992317199707\n",
      "iteration 46027: loss: 0.2160598337650299\n",
      "iteration 46028: loss: 0.21605968475341797\n",
      "iteration 46029: loss: 0.21605953574180603\n",
      "iteration 46030: loss: 0.2160593569278717\n",
      "iteration 46031: loss: 0.21605929732322693\n",
      "iteration 46032: loss: 0.2160591185092926\n",
      "iteration 46033: loss: 0.21605893969535828\n",
      "iteration 46034: loss: 0.21605882048606873\n",
      "iteration 46035: loss: 0.21605868637561798\n",
      "iteration 46036: loss: 0.21605856716632843\n",
      "iteration 46037: loss: 0.21605844795703888\n",
      "iteration 46038: loss: 0.21605832874774933\n",
      "iteration 46039: loss: 0.2160581648349762\n",
      "iteration 46040: loss: 0.21605804562568665\n",
      "iteration 46041: loss: 0.21605786681175232\n",
      "iteration 46042: loss: 0.21605777740478516\n",
      "iteration 46043: loss: 0.21605762839317322\n",
      "iteration 46044: loss: 0.21605750918388367\n",
      "iteration 46045: loss: 0.21605734527111053\n",
      "iteration 46046: loss: 0.2160572111606598\n",
      "iteration 46047: loss: 0.21605710685253143\n",
      "iteration 46048: loss: 0.2160569429397583\n",
      "iteration 46049: loss: 0.21605682373046875\n",
      "iteration 46050: loss: 0.21605665981769562\n",
      "iteration 46051: loss: 0.2160564661026001\n",
      "iteration 46052: loss: 0.21605639159679413\n",
      "iteration 46053: loss: 0.21605625748634338\n",
      "iteration 46054: loss: 0.21605606377124786\n",
      "iteration 46055: loss: 0.21605603396892548\n",
      "iteration 46056: loss: 0.21605582535266876\n",
      "iteration 46057: loss: 0.2160557508468628\n",
      "iteration 46058: loss: 0.21605555713176727\n",
      "iteration 46059: loss: 0.2160554826259613\n",
      "iteration 46060: loss: 0.21605539321899414\n",
      "iteration 46061: loss: 0.21605515480041504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 46062: loss: 0.2160549908876419\n",
      "iteration 46063: loss: 0.21605488657951355\n",
      "iteration 46064: loss: 0.21605487167835236\n",
      "iteration 46065: loss: 0.21605464816093445\n",
      "iteration 46066: loss: 0.21605448424816132\n",
      "iteration 46067: loss: 0.21605439484119415\n",
      "iteration 46068: loss: 0.21605423092842102\n",
      "iteration 46069: loss: 0.21605408191680908\n",
      "iteration 46070: loss: 0.21605388820171356\n",
      "iteration 46071: loss: 0.2160538136959076\n",
      "iteration 46072: loss: 0.21605375409126282\n",
      "iteration 46073: loss: 0.2160535305738449\n",
      "iteration 46074: loss: 0.21605341136455536\n",
      "iteration 46075: loss: 0.21605327725410461\n",
      "iteration 46076: loss: 0.2160530984401703\n",
      "iteration 46077: loss: 0.21605297923088074\n",
      "iteration 46078: loss: 0.21605284512043\n",
      "iteration 46079: loss: 0.21605268120765686\n",
      "iteration 46080: loss: 0.2160525619983673\n",
      "iteration 46081: loss: 0.21605248749256134\n",
      "iteration 46082: loss: 0.21605229377746582\n",
      "iteration 46083: loss: 0.21605220437049866\n",
      "iteration 46084: loss: 0.21605201065540314\n",
      "iteration 46085: loss: 0.21605198085308075\n",
      "iteration 46086: loss: 0.21605181694030762\n",
      "iteration 46087: loss: 0.21605157852172852\n",
      "iteration 46088: loss: 0.21605153381824493\n",
      "iteration 46089: loss: 0.21605134010314941\n",
      "iteration 46090: loss: 0.21605126559734344\n",
      "iteration 46091: loss: 0.2160511314868927\n",
      "iteration 46092: loss: 0.21605098247528076\n",
      "iteration 46093: loss: 0.21605077385902405\n",
      "iteration 46094: loss: 0.2160506546497345\n",
      "iteration 46095: loss: 0.21605055034160614\n",
      "iteration 46096: loss: 0.21605035662651062\n",
      "iteration 46097: loss: 0.21605031192302704\n",
      "iteration 46098: loss: 0.21605011820793152\n",
      "iteration 46099: loss: 0.21604998409748077\n",
      "iteration 46100: loss: 0.21604986488819122\n",
      "iteration 46101: loss: 0.2160496711730957\n",
      "iteration 46102: loss: 0.21604958176612854\n",
      "iteration 46103: loss: 0.216049462556839\n",
      "iteration 46104: loss: 0.21604934334754944\n",
      "iteration 46105: loss: 0.2160491943359375\n",
      "iteration 46106: loss: 0.21604900062084198\n",
      "iteration 46107: loss: 0.216048926115036\n",
      "iteration 46108: loss: 0.21604876220226288\n",
      "iteration 46109: loss: 0.21604862809181213\n",
      "iteration 46110: loss: 0.2160484790802002\n",
      "iteration 46111: loss: 0.21604833006858826\n",
      "iteration 46112: loss: 0.21604831516742706\n",
      "iteration 46113: loss: 0.21604807674884796\n",
      "iteration 46114: loss: 0.2160479724407196\n",
      "iteration 46115: loss: 0.21604783833026886\n",
      "iteration 46116: loss: 0.2160477191209793\n",
      "iteration 46117: loss: 0.2160475254058838\n",
      "iteration 46118: loss: 0.21604736149311066\n",
      "iteration 46119: loss: 0.2160472422838211\n",
      "iteration 46120: loss: 0.21604707837104797\n",
      "iteration 46121: loss: 0.21604697406291962\n",
      "iteration 46122: loss: 0.21604685485363007\n",
      "iteration 46123: loss: 0.2160467654466629\n",
      "iteration 46124: loss: 0.21604657173156738\n",
      "iteration 46125: loss: 0.21604648232460022\n",
      "iteration 46126: loss: 0.2160463035106659\n",
      "iteration 46127: loss: 0.21604613959789276\n",
      "iteration 46128: loss: 0.2160460650920868\n",
      "iteration 46129: loss: 0.21604585647583008\n",
      "iteration 46130: loss: 0.2160458117723465\n",
      "iteration 46131: loss: 0.21604569256305695\n",
      "iteration 46132: loss: 0.2160455286502838\n",
      "iteration 46133: loss: 0.2160453349351883\n",
      "iteration 46134: loss: 0.21604521572589874\n",
      "iteration 46135: loss: 0.2160450667142868\n",
      "iteration 46136: loss: 0.21604497730731964\n",
      "iteration 46137: loss: 0.2160448282957077\n",
      "iteration 46138: loss: 0.21604470908641815\n",
      "iteration 46139: loss: 0.21604454517364502\n",
      "iteration 46140: loss: 0.2160443812608719\n",
      "iteration 46141: loss: 0.21604427695274353\n",
      "iteration 46142: loss: 0.21604406833648682\n",
      "iteration 46143: loss: 0.21604394912719727\n",
      "iteration 46144: loss: 0.2160438746213913\n",
      "iteration 46145: loss: 0.21604374051094055\n",
      "iteration 46146: loss: 0.216043621301651\n",
      "iteration 46147: loss: 0.21604347229003906\n",
      "iteration 46148: loss: 0.21604332327842712\n",
      "iteration 46149: loss: 0.2160431444644928\n",
      "iteration 46150: loss: 0.21604302525520325\n",
      "iteration 46151: loss: 0.21604284644126892\n",
      "iteration 46152: loss: 0.21604280173778534\n",
      "iteration 46153: loss: 0.21604256331920624\n",
      "iteration 46154: loss: 0.21604256331920624\n",
      "iteration 46155: loss: 0.21604230999946594\n",
      "iteration 46156: loss: 0.21604223549365997\n",
      "iteration 46157: loss: 0.21604207158088684\n",
      "iteration 46158: loss: 0.21604196727275848\n",
      "iteration 46159: loss: 0.21604172885417938\n",
      "iteration 46160: loss: 0.21604175865650177\n",
      "iteration 46161: loss: 0.21604153513908386\n",
      "iteration 46162: loss: 0.21604135632514954\n",
      "iteration 46163: loss: 0.21604129672050476\n",
      "iteration 46164: loss: 0.21604113280773163\n",
      "iteration 46165: loss: 0.2160409688949585\n",
      "iteration 46166: loss: 0.21604084968566895\n",
      "iteration 46167: loss: 0.216040700674057\n",
      "iteration 46168: loss: 0.21604064106941223\n",
      "iteration 46169: loss: 0.21604052186012268\n",
      "iteration 46170: loss: 0.21604032814502716\n",
      "iteration 46171: loss: 0.2160402238368988\n",
      "iteration 46172: loss: 0.2160399854183197\n",
      "iteration 46173: loss: 0.21603985130786896\n",
      "iteration 46174: loss: 0.2160397469997406\n",
      "iteration 46175: loss: 0.21603961288928986\n",
      "iteration 46176: loss: 0.2160395085811615\n",
      "iteration 46177: loss: 0.21603937447071075\n",
      "iteration 46178: loss: 0.21603922545909882\n",
      "iteration 46179: loss: 0.21603909134864807\n",
      "iteration 46180: loss: 0.2160390168428421\n",
      "iteration 46181: loss: 0.21603885293006897\n",
      "iteration 46182: loss: 0.21603867411613464\n",
      "iteration 46183: loss: 0.21603849530220032\n",
      "iteration 46184: loss: 0.21603842079639435\n",
      "iteration 46185: loss: 0.21603822708129883\n",
      "iteration 46186: loss: 0.21603813767433167\n",
      "iteration 46187: loss: 0.21603798866271973\n",
      "iteration 46188: loss: 0.21603786945343018\n",
      "iteration 46189: loss: 0.216037780046463\n",
      "iteration 46190: loss: 0.2160375565290451\n",
      "iteration 46191: loss: 0.21603742241859436\n",
      "iteration 46192: loss: 0.2160373032093048\n",
      "iteration 46193: loss: 0.21603719890117645\n",
      "iteration 46194: loss: 0.21603703498840332\n",
      "iteration 46195: loss: 0.21603688597679138\n",
      "iteration 46196: loss: 0.21603675186634064\n",
      "iteration 46197: loss: 0.2160366028547287\n",
      "iteration 46198: loss: 0.21603648364543915\n",
      "iteration 46199: loss: 0.21603640913963318\n",
      "iteration 46200: loss: 0.21603620052337646\n",
      "iteration 46201: loss: 0.2160361111164093\n",
      "iteration 46202: loss: 0.21603591740131378\n",
      "iteration 46203: loss: 0.21603575348854065\n",
      "iteration 46204: loss: 0.2160356491804123\n",
      "iteration 46205: loss: 0.21603551506996155\n",
      "iteration 46206: loss: 0.2160353660583496\n",
      "iteration 46207: loss: 0.21603524684906006\n",
      "iteration 46208: loss: 0.2160351276397705\n",
      "iteration 46209: loss: 0.2160348892211914\n",
      "iteration 46210: loss: 0.21603481471538544\n",
      "iteration 46211: loss: 0.21603474020957947\n",
      "iteration 46212: loss: 0.21603456139564514\n",
      "iteration 46213: loss: 0.2160344123840332\n",
      "iteration 46214: loss: 0.21603424847126007\n",
      "iteration 46215: loss: 0.21603412926197052\n",
      "iteration 46216: loss: 0.21603401005268097\n",
      "iteration 46217: loss: 0.21603389084339142\n",
      "iteration 46218: loss: 0.21603374183177948\n",
      "iteration 46219: loss: 0.21603357791900635\n",
      "iteration 46220: loss: 0.21603338420391083\n",
      "iteration 46221: loss: 0.21603336930274963\n",
      "iteration 46222: loss: 0.2160331755876541\n",
      "iteration 46223: loss: 0.21603302657604218\n",
      "iteration 46224: loss: 0.21603283286094666\n",
      "iteration 46225: loss: 0.21603277325630188\n",
      "iteration 46226: loss: 0.21603265404701233\n",
      "iteration 46227: loss: 0.2160324603319168\n",
      "iteration 46228: loss: 0.21603231132030487\n",
      "iteration 46229: loss: 0.2160322368144989\n",
      "iteration 46230: loss: 0.21603210270404816\n",
      "iteration 46231: loss: 0.21603187918663025\n",
      "iteration 46232: loss: 0.21603178977966309\n",
      "iteration 46233: loss: 0.21603164076805115\n",
      "iteration 46234: loss: 0.2160315215587616\n",
      "iteration 46235: loss: 0.21603140234947205\n",
      "iteration 46236: loss: 0.21603122353553772\n",
      "iteration 46237: loss: 0.21603110432624817\n",
      "iteration 46238: loss: 0.21603098511695862\n",
      "iteration 46239: loss: 0.21603083610534668\n",
      "iteration 46240: loss: 0.21603074669837952\n",
      "iteration 46241: loss: 0.216030552983284\n",
      "iteration 46242: loss: 0.21603040397167206\n",
      "iteration 46243: loss: 0.21603035926818848\n",
      "iteration 46244: loss: 0.21603019535541534\n",
      "iteration 46245: loss: 0.2160300761461258\n",
      "iteration 46246: loss: 0.21602991223335266\n",
      "iteration 46247: loss: 0.2160298377275467\n",
      "iteration 46248: loss: 0.2160295993089676\n",
      "iteration 46249: loss: 0.21602948009967804\n",
      "iteration 46250: loss: 0.2160293608903885\n",
      "iteration 46251: loss: 0.21602919697761536\n",
      "iteration 46252: loss: 0.21602900326251984\n",
      "iteration 46253: loss: 0.21602897346019745\n",
      "iteration 46254: loss: 0.21602880954742432\n",
      "iteration 46255: loss: 0.21602864563465118\n",
      "iteration 46256: loss: 0.21602857112884521\n",
      "iteration 46257: loss: 0.21602840721607208\n",
      "iteration 46258: loss: 0.21602818369865417\n",
      "iteration 46259: loss: 0.21602806448936462\n",
      "iteration 46260: loss: 0.2160278856754303\n",
      "iteration 46261: loss: 0.21602781116962433\n",
      "iteration 46262: loss: 0.2160276621580124\n",
      "iteration 46263: loss: 0.21602752804756165\n",
      "iteration 46264: loss: 0.2160274237394333\n",
      "iteration 46265: loss: 0.21602725982666016\n",
      "iteration 46266: loss: 0.21602709591388702\n",
      "iteration 46267: loss: 0.21602705121040344\n",
      "iteration 46268: loss: 0.21602682769298553\n",
      "iteration 46269: loss: 0.21602673828601837\n",
      "iteration 46270: loss: 0.21602658927440643\n",
      "iteration 46271: loss: 0.2160264551639557\n",
      "iteration 46272: loss: 0.21602633595466614\n",
      "iteration 46273: loss: 0.21602614223957062\n",
      "iteration 46274: loss: 0.21602599322795868\n",
      "iteration 46275: loss: 0.2160259485244751\n",
      "iteration 46276: loss: 0.21602579951286316\n",
      "iteration 46277: loss: 0.21602562069892883\n",
      "iteration 46278: loss: 0.21602556109428406\n",
      "iteration 46279: loss: 0.21602538228034973\n",
      "iteration 46280: loss: 0.21602515876293182\n",
      "iteration 46281: loss: 0.21602511405944824\n",
      "iteration 46282: loss: 0.2160249650478363\n",
      "iteration 46283: loss: 0.21602483093738556\n",
      "iteration 46284: loss: 0.21602466702461243\n",
      "iteration 46285: loss: 0.21602456271648407\n",
      "iteration 46286: loss: 0.21602439880371094\n",
      "iteration 46287: loss: 0.2160242795944214\n",
      "iteration 46288: loss: 0.21602411568164825\n",
      "iteration 46289: loss: 0.2160239964723587\n",
      "iteration 46290: loss: 0.21602384746074677\n",
      "iteration 46291: loss: 0.21602372825145721\n",
      "iteration 46292: loss: 0.21602356433868408\n",
      "iteration 46293: loss: 0.21602340042591095\n",
      "iteration 46294: loss: 0.21602332592010498\n",
      "iteration 46295: loss: 0.21602320671081543\n",
      "iteration 46296: loss: 0.2160230427980423\n",
      "iteration 46297: loss: 0.21602284908294678\n",
      "iteration 46298: loss: 0.21602272987365723\n",
      "iteration 46299: loss: 0.21602264046669006\n",
      "iteration 46300: loss: 0.21602249145507812\n",
      "iteration 46301: loss: 0.2160222977399826\n",
      "iteration 46302: loss: 0.21602217853069305\n",
      "iteration 46303: loss: 0.2160220593214035\n",
      "iteration 46304: loss: 0.21602192521095276\n",
      "iteration 46305: loss: 0.2160218060016632\n",
      "iteration 46306: loss: 0.21602168679237366\n",
      "iteration 46307: loss: 0.21602149307727814\n",
      "iteration 46308: loss: 0.2160213440656662\n",
      "iteration 46309: loss: 0.21602120995521545\n",
      "iteration 46310: loss: 0.21602106094360352\n",
      "iteration 46311: loss: 0.21602097153663635\n",
      "iteration 46312: loss: 0.21602079272270203\n",
      "iteration 46313: loss: 0.21602073311805725\n",
      "iteration 46314: loss: 0.2160205841064453\n",
      "iteration 46315: loss: 0.21602042019367218\n",
      "iteration 46316: loss: 0.21602025628089905\n",
      "iteration 46317: loss: 0.2160201072692871\n",
      "iteration 46318: loss: 0.21601994335651398\n",
      "iteration 46319: loss: 0.216019868850708\n",
      "iteration 46320: loss: 0.21601974964141846\n",
      "iteration 46321: loss: 0.2160196304321289\n",
      "iteration 46322: loss: 0.2160193920135498\n",
      "iteration 46323: loss: 0.21601924300193787\n",
      "iteration 46324: loss: 0.21601912379264832\n",
      "iteration 46325: loss: 0.21601900458335876\n",
      "iteration 46326: loss: 0.21601884067058563\n",
      "iteration 46327: loss: 0.21601872146129608\n",
      "iteration 46328: loss: 0.21601863205432892\n",
      "iteration 46329: loss: 0.2160184383392334\n",
      "iteration 46330: loss: 0.21601834893226624\n",
      "iteration 46331: loss: 0.21601827442646027\n",
      "iteration 46332: loss: 0.21601803600788116\n",
      "iteration 46333: loss: 0.21601787209510803\n",
      "iteration 46334: loss: 0.21601779758930206\n",
      "iteration 46335: loss: 0.21601764857769012\n",
      "iteration 46336: loss: 0.216017484664917\n",
      "iteration 46337: loss: 0.21601739525794983\n",
      "iteration 46338: loss: 0.2160172015428543\n",
      "iteration 46339: loss: 0.21601705253124237\n",
      "iteration 46340: loss: 0.21601693332195282\n",
      "iteration 46341: loss: 0.21601679921150208\n",
      "iteration 46342: loss: 0.2160167247056961\n",
      "iteration 46343: loss: 0.216016486287117\n",
      "iteration 46344: loss: 0.21601644158363342\n",
      "iteration 46345: loss: 0.21601621806621552\n",
      "iteration 46346: loss: 0.21601614356040955\n",
      "iteration 46347: loss: 0.21601596474647522\n",
      "iteration 46348: loss: 0.21601586043834686\n",
      "iteration 46349: loss: 0.21601566672325134\n",
      "iteration 46350: loss: 0.2160155326128006\n",
      "iteration 46351: loss: 0.21601541340351105\n",
      "iteration 46352: loss: 0.2160152941942215\n",
      "iteration 46353: loss: 0.21601514518260956\n",
      "iteration 46354: loss: 0.21601495146751404\n",
      "iteration 46355: loss: 0.21601489186286926\n",
      "iteration 46356: loss: 0.2160147875547409\n",
      "iteration 46357: loss: 0.21601459383964539\n",
      "iteration 46358: loss: 0.21601445972919464\n",
      "iteration 46359: loss: 0.2160143405199051\n",
      "iteration 46360: loss: 0.21601414680480957\n",
      "iteration 46361: loss: 0.21601399779319763\n",
      "iteration 46362: loss: 0.21601390838623047\n",
      "iteration 46363: loss: 0.21601369976997375\n",
      "iteration 46364: loss: 0.2160135954618454\n",
      "iteration 46365: loss: 0.21601350605487823\n",
      "iteration 46366: loss: 0.2160133421421051\n",
      "iteration 46367: loss: 0.21601323783397675\n",
      "iteration 46368: loss: 0.216013103723526\n",
      "iteration 46369: loss: 0.21601298451423645\n",
      "iteration 46370: loss: 0.21601276099681854\n",
      "iteration 46371: loss: 0.21601267158985138\n",
      "iteration 46372: loss: 0.21601255238056183\n",
      "iteration 46373: loss: 0.2160123586654663\n",
      "iteration 46374: loss: 0.21601223945617676\n",
      "iteration 46375: loss: 0.2160121500492096\n",
      "iteration 46376: loss: 0.21601195633411407\n",
      "iteration 46377: loss: 0.21601183712482452\n",
      "iteration 46378: loss: 0.21601168811321259\n",
      "iteration 46379: loss: 0.21601156890392303\n",
      "iteration 46380: loss: 0.2160114049911499\n",
      "iteration 46381: loss: 0.21601128578186035\n",
      "iteration 46382: loss: 0.2160111367702484\n",
      "iteration 46383: loss: 0.21601100265979767\n",
      "iteration 46384: loss: 0.21601085364818573\n",
      "iteration 46385: loss: 0.21601076424121857\n",
      "iteration 46386: loss: 0.21601054072380066\n",
      "iteration 46387: loss: 0.2160104215145111\n",
      "iteration 46388: loss: 0.21601030230522156\n",
      "iteration 46389: loss: 0.21601012349128723\n",
      "iteration 46390: loss: 0.21601000428199768\n",
      "iteration 46391: loss: 0.21600982546806335\n",
      "iteration 46392: loss: 0.2160097360610962\n",
      "iteration 46393: loss: 0.21600961685180664\n",
      "iteration 46394: loss: 0.21600952744483948\n",
      "iteration 46395: loss: 0.21600933372974396\n",
      "iteration 46396: loss: 0.2160092145204544\n",
      "iteration 46397: loss: 0.21600905060768127\n",
      "iteration 46398: loss: 0.21600894629955292\n",
      "iteration 46399: loss: 0.21600878238677979\n",
      "iteration 46400: loss: 0.21600863337516785\n",
      "iteration 46401: loss: 0.21600846946239471\n",
      "iteration 46402: loss: 0.21600838005542755\n",
      "iteration 46403: loss: 0.2160082757472992\n",
      "iteration 46404: loss: 0.21600814163684845\n",
      "iteration 46405: loss: 0.21600797772407532\n",
      "iteration 46406: loss: 0.216007798910141\n",
      "iteration 46407: loss: 0.21600763499736786\n",
      "iteration 46408: loss: 0.21600759029388428\n",
      "iteration 46409: loss: 0.21600739657878876\n",
      "iteration 46410: loss: 0.2160072773694992\n",
      "iteration 46411: loss: 0.2160070836544037\n",
      "iteration 46412: loss: 0.21600696444511414\n",
      "iteration 46413: loss: 0.21600691974163055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 46414: loss: 0.21600666642189026\n",
      "iteration 46415: loss: 0.21600660681724548\n",
      "iteration 46416: loss: 0.21600642800331116\n",
      "iteration 46417: loss: 0.21600636839866638\n",
      "iteration 46418: loss: 0.21600615978240967\n",
      "iteration 46419: loss: 0.21600604057312012\n",
      "iteration 46420: loss: 0.21600589156150818\n",
      "iteration 46421: loss: 0.21600572764873505\n",
      "iteration 46422: loss: 0.2160056084394455\n",
      "iteration 46423: loss: 0.21600544452667236\n",
      "iteration 46424: loss: 0.2160053551197052\n",
      "iteration 46425: loss: 0.21600517630577087\n",
      "iteration 46426: loss: 0.21600505709648132\n",
      "iteration 46427: loss: 0.216004878282547\n",
      "iteration 46428: loss: 0.21600475907325745\n",
      "iteration 46429: loss: 0.2160046398639679\n",
      "iteration 46430: loss: 0.21600446105003357\n",
      "iteration 46431: loss: 0.21600432693958282\n",
      "iteration 46432: loss: 0.2160041779279709\n",
      "iteration 46433: loss: 0.21600408852100372\n",
      "iteration 46434: loss: 0.21600398421287537\n",
      "iteration 46435: loss: 0.21600377559661865\n",
      "iteration 46436: loss: 0.2160036861896515\n",
      "iteration 46437: loss: 0.21600349247455597\n",
      "iteration 46438: loss: 0.21600337326526642\n",
      "iteration 46439: loss: 0.21600322425365448\n",
      "iteration 46440: loss: 0.2160031497478485\n",
      "iteration 46441: loss: 0.2160029411315918\n",
      "iteration 46442: loss: 0.21600285172462463\n",
      "iteration 46443: loss: 0.2160027027130127\n",
      "iteration 46444: loss: 0.21600258350372314\n",
      "iteration 46445: loss: 0.21600241959095\n",
      "iteration 46446: loss: 0.2160022258758545\n",
      "iteration 46447: loss: 0.21600210666656494\n",
      "iteration 46448: loss: 0.21600201725959778\n",
      "iteration 46449: loss: 0.21600183844566345\n",
      "iteration 46450: loss: 0.2160017192363739\n",
      "iteration 46451: loss: 0.21600158512592316\n",
      "iteration 46452: loss: 0.21600139141082764\n",
      "iteration 46453: loss: 0.21600130200386047\n",
      "iteration 46454: loss: 0.21600112318992615\n",
      "iteration 46455: loss: 0.21600106358528137\n",
      "iteration 46456: loss: 0.21600088477134705\n",
      "iteration 46457: loss: 0.2160007208585739\n",
      "iteration 46458: loss: 0.21600060164928436\n",
      "iteration 46459: loss: 0.21600055694580078\n",
      "iteration 46460: loss: 0.21600034832954407\n",
      "iteration 46461: loss: 0.21600016951560974\n",
      "iteration 46462: loss: 0.216000035405159\n",
      "iteration 46463: loss: 0.21599996089935303\n",
      "iteration 46464: loss: 0.2159997522830963\n",
      "iteration 46465: loss: 0.21599963307380676\n",
      "iteration 46466: loss: 0.2159995585680008\n",
      "iteration 46467: loss: 0.21599933505058289\n",
      "iteration 46468: loss: 0.21599920094013214\n",
      "iteration 46469: loss: 0.21599900722503662\n",
      "iteration 46470: loss: 0.21599893271923065\n",
      "iteration 46471: loss: 0.2159988433122635\n",
      "iteration 46472: loss: 0.21599869430065155\n",
      "iteration 46473: loss: 0.2159985601902008\n",
      "iteration 46474: loss: 0.21599836647510529\n",
      "iteration 46475: loss: 0.21599821746349335\n",
      "iteration 46476: loss: 0.2159980833530426\n",
      "iteration 46477: loss: 0.21599796414375305\n",
      "iteration 46478: loss: 0.2159978151321411\n",
      "iteration 46479: loss: 0.21599769592285156\n",
      "iteration 46480: loss: 0.2159976065158844\n",
      "iteration 46481: loss: 0.2159973680973053\n",
      "iteration 46482: loss: 0.2159973382949829\n",
      "iteration 46483: loss: 0.2159971445798874\n",
      "iteration 46484: loss: 0.21599698066711426\n",
      "iteration 46485: loss: 0.21599683165550232\n",
      "iteration 46486: loss: 0.21599669754505157\n",
      "iteration 46487: loss: 0.21599657833576202\n",
      "iteration 46488: loss: 0.21599642932415009\n",
      "iteration 46489: loss: 0.21599629521369934\n",
      "iteration 46490: loss: 0.2159961760044098\n",
      "iteration 46491: loss: 0.21599605679512024\n",
      "iteration 46492: loss: 0.2159958779811859\n",
      "iteration 46493: loss: 0.21599571406841278\n",
      "iteration 46494: loss: 0.2159956395626068\n",
      "iteration 46495: loss: 0.2159954309463501\n",
      "iteration 46496: loss: 0.21599534153938293\n",
      "iteration 46497: loss: 0.2159951627254486\n",
      "iteration 46498: loss: 0.21599511802196503\n",
      "iteration 46499: loss: 0.2159949094057083\n",
      "iteration 46500: loss: 0.21599479019641876\n",
      "iteration 46501: loss: 0.21599462628364563\n",
      "iteration 46502: loss: 0.2159944474697113\n",
      "iteration 46503: loss: 0.21599435806274414\n",
      "iteration 46504: loss: 0.2159942090511322\n",
      "iteration 46505: loss: 0.21599411964416504\n",
      "iteration 46506: loss: 0.2159939706325531\n",
      "iteration 46507: loss: 0.21599380671977997\n",
      "iteration 46508: loss: 0.21599364280700684\n",
      "iteration 46509: loss: 0.21599352359771729\n",
      "iteration 46510: loss: 0.21599340438842773\n",
      "iteration 46511: loss: 0.21599321067333221\n",
      "iteration 46512: loss: 0.21599309146404266\n",
      "iteration 46513: loss: 0.2159929722547531\n",
      "iteration 46514: loss: 0.21599283814430237\n",
      "iteration 46515: loss: 0.21599265933036804\n",
      "iteration 46516: loss: 0.2159925252199173\n",
      "iteration 46517: loss: 0.21599237620830536\n",
      "iteration 46518: loss: 0.2159922868013382\n",
      "iteration 46519: loss: 0.21599209308624268\n",
      "iteration 46520: loss: 0.2159920185804367\n",
      "iteration 46521: loss: 0.2159918248653412\n",
      "iteration 46522: loss: 0.2159917652606964\n",
      "iteration 46523: loss: 0.21599158644676208\n",
      "iteration 46524: loss: 0.21599142253398895\n",
      "iteration 46525: loss: 0.2159912884235382\n",
      "iteration 46526: loss: 0.21599110960960388\n",
      "iteration 46527: loss: 0.2159910500049591\n",
      "iteration 46528: loss: 0.21599087119102478\n",
      "iteration 46529: loss: 0.21599078178405762\n",
      "iteration 46530: loss: 0.21599061787128448\n",
      "iteration 46531: loss: 0.21599046885967255\n",
      "iteration 46532: loss: 0.21599026024341583\n",
      "iteration 46533: loss: 0.21599015593528748\n",
      "iteration 46534: loss: 0.2159900665283203\n",
      "iteration 46535: loss: 0.21598979830741882\n",
      "iteration 46536: loss: 0.21598978340625763\n",
      "iteration 46537: loss: 0.2159896194934845\n",
      "iteration 46538: loss: 0.21598950028419495\n",
      "iteration 46539: loss: 0.2159893810749054\n",
      "iteration 46540: loss: 0.21598923206329346\n",
      "iteration 46541: loss: 0.21598903834819794\n",
      "iteration 46542: loss: 0.2159888744354248\n",
      "iteration 46543: loss: 0.21598878502845764\n",
      "iteration 46544: loss: 0.2159886658191681\n",
      "iteration 46545: loss: 0.21598848700523376\n",
      "iteration 46546: loss: 0.21598835289478302\n",
      "iteration 46547: loss: 0.21598827838897705\n",
      "iteration 46548: loss: 0.21598811447620392\n",
      "iteration 46549: loss: 0.215987890958786\n",
      "iteration 46550: loss: 0.21598784625530243\n",
      "iteration 46551: loss: 0.2159876525402069\n",
      "iteration 46552: loss: 0.21598748862743378\n",
      "iteration 46553: loss: 0.21598736941814423\n",
      "iteration 46554: loss: 0.21598729491233826\n",
      "iteration 46555: loss: 0.21598711609840393\n",
      "iteration 46556: loss: 0.2159869223833084\n",
      "iteration 46557: loss: 0.21598681807518005\n",
      "iteration 46558: loss: 0.2159866988658905\n",
      "iteration 46559: loss: 0.21598656475543976\n",
      "iteration 46560: loss: 0.2159864455461502\n",
      "iteration 46561: loss: 0.21598637104034424\n",
      "iteration 46562: loss: 0.21598610281944275\n",
      "iteration 46563: loss: 0.2159859836101532\n",
      "iteration 46564: loss: 0.21598584949970245\n",
      "iteration 46565: loss: 0.21598570048809052\n",
      "iteration 46566: loss: 0.21598556637763977\n",
      "iteration 46567: loss: 0.21598537266254425\n",
      "iteration 46568: loss: 0.2159852683544159\n",
      "iteration 46569: loss: 0.21598513424396515\n",
      "iteration 46570: loss: 0.21598497033119202\n",
      "iteration 46571: loss: 0.21598486602306366\n",
      "iteration 46572: loss: 0.21598473191261292\n",
      "iteration 46573: loss: 0.2159845530986786\n",
      "iteration 46574: loss: 0.21598443388938904\n",
      "iteration 46575: loss: 0.2159842997789383\n",
      "iteration 46576: loss: 0.21598415076732635\n",
      "iteration 46577: loss: 0.2159840613603592\n",
      "iteration 46578: loss: 0.21598389744758606\n",
      "iteration 46579: loss: 0.21598371863365173\n",
      "iteration 46580: loss: 0.21598359942436218\n",
      "iteration 46581: loss: 0.21598351001739502\n",
      "iteration 46582: loss: 0.2159833014011383\n",
      "iteration 46583: loss: 0.21598319709300995\n",
      "iteration 46584: loss: 0.2159830778837204\n",
      "iteration 46585: loss: 0.21598288416862488\n",
      "iteration 46586: loss: 0.21598279476165771\n",
      "iteration 46587: loss: 0.21598267555236816\n",
      "iteration 46588: loss: 0.21598248183727264\n",
      "iteration 46589: loss: 0.2159823626279831\n",
      "iteration 46590: loss: 0.21598219871520996\n",
      "iteration 46591: loss: 0.21598204970359802\n",
      "iteration 46592: loss: 0.21598191559314728\n",
      "iteration 46593: loss: 0.21598176658153534\n",
      "iteration 46594: loss: 0.2159816324710846\n",
      "iteration 46595: loss: 0.21598148345947266\n",
      "iteration 46596: loss: 0.2159814089536667\n",
      "iteration 46597: loss: 0.21598120033740997\n",
      "iteration 46598: loss: 0.21598109602928162\n",
      "iteration 46599: loss: 0.21598100662231445\n",
      "iteration 46600: loss: 0.21598081290721893\n",
      "iteration 46601: loss: 0.21598076820373535\n",
      "iteration 46602: loss: 0.21598060429096222\n",
      "iteration 46603: loss: 0.2159804105758667\n",
      "iteration 46604: loss: 0.21598024666309357\n",
      "iteration 46605: loss: 0.21598008275032043\n",
      "iteration 46606: loss: 0.21598005294799805\n",
      "iteration 46607: loss: 0.21597981452941895\n",
      "iteration 46608: loss: 0.215979665517807\n",
      "iteration 46609: loss: 0.21597954630851746\n",
      "iteration 46610: loss: 0.2159794270992279\n",
      "iteration 46611: loss: 0.21597929298877716\n",
      "iteration 46612: loss: 0.21597912907600403\n",
      "iteration 46613: loss: 0.2159789800643921\n",
      "iteration 46614: loss: 0.21597886085510254\n",
      "iteration 46615: loss: 0.2159787118434906\n",
      "iteration 46616: loss: 0.21597857773303986\n",
      "iteration 46617: loss: 0.21597842872142792\n",
      "iteration 46618: loss: 0.21597833931446075\n",
      "iteration 46619: loss: 0.21597807109355927\n",
      "iteration 46620: loss: 0.21597807109355927\n",
      "iteration 46621: loss: 0.21597781777381897\n",
      "iteration 46622: loss: 0.2159777134656906\n",
      "iteration 46623: loss: 0.21597757935523987\n",
      "iteration 46624: loss: 0.21597743034362793\n",
      "iteration 46625: loss: 0.21597731113433838\n",
      "iteration 46626: loss: 0.21597722172737122\n",
      "iteration 46627: loss: 0.21597707271575928\n",
      "iteration 46628: loss: 0.21597690880298615\n",
      "iteration 46629: loss: 0.2159767597913742\n",
      "iteration 46630: loss: 0.21597662568092346\n",
      "iteration 46631: loss: 0.2159765064716339\n",
      "iteration 46632: loss: 0.2159762680530548\n",
      "iteration 46633: loss: 0.21597619354724884\n",
      "iteration 46634: loss: 0.2159760296344757\n",
      "iteration 46635: loss: 0.21597597002983093\n",
      "iteration 46636: loss: 0.2159758061170578\n",
      "iteration 46637: loss: 0.21597561240196228\n",
      "iteration 46638: loss: 0.21597552299499512\n",
      "iteration 46639: loss: 0.21597540378570557\n",
      "iteration 46640: loss: 0.21597519516944885\n",
      "iteration 46641: loss: 0.21597512066364288\n",
      "iteration 46642: loss: 0.21597492694854736\n",
      "iteration 46643: loss: 0.21597477793693542\n",
      "iteration 46644: loss: 0.21597464382648468\n",
      "iteration 46645: loss: 0.2159745693206787\n",
      "iteration 46646: loss: 0.2159743756055832\n",
      "iteration 46647: loss: 0.21597424149513245\n",
      "iteration 46648: loss: 0.2159741222858429\n",
      "iteration 46649: loss: 0.21597394347190857\n",
      "iteration 46650: loss: 0.2159738838672638\n",
      "iteration 46651: loss: 0.2159736603498459\n",
      "iteration 46652: loss: 0.21597354114055634\n",
      "iteration 46653: loss: 0.2159733772277832\n",
      "iteration 46654: loss: 0.21597325801849365\n",
      "iteration 46655: loss: 0.21597309410572052\n",
      "iteration 46656: loss: 0.21597294509410858\n",
      "iteration 46657: loss: 0.2159728705883026\n",
      "iteration 46658: loss: 0.21597270667552948\n",
      "iteration 46659: loss: 0.21597258746623993\n",
      "iteration 46660: loss: 0.21597237884998322\n",
      "iteration 46661: loss: 0.21597230434417725\n",
      "iteration 46662: loss: 0.2159721553325653\n",
      "iteration 46663: loss: 0.21597197651863098\n",
      "iteration 46664: loss: 0.21597185730934143\n",
      "iteration 46665: loss: 0.21597173810005188\n",
      "iteration 46666: loss: 0.21597155928611755\n",
      "iteration 46667: loss: 0.21597149968147278\n",
      "iteration 46668: loss: 0.21597127616405487\n",
      "iteration 46669: loss: 0.21597114205360413\n",
      "iteration 46670: loss: 0.21597103774547577\n",
      "iteration 46671: loss: 0.21597091853618622\n",
      "iteration 46672: loss: 0.21597075462341309\n",
      "iteration 46673: loss: 0.21597068011760712\n",
      "iteration 46674: loss: 0.2159704715013504\n",
      "iteration 46675: loss: 0.21597035229206085\n",
      "iteration 46676: loss: 0.21597015857696533\n",
      "iteration 46677: loss: 0.21597003936767578\n",
      "iteration 46678: loss: 0.21596992015838623\n",
      "iteration 46679: loss: 0.2159697711467743\n",
      "iteration 46680: loss: 0.21596963703632355\n",
      "iteration 46681: loss: 0.2159694880247116\n",
      "iteration 46682: loss: 0.21596932411193848\n",
      "iteration 46683: loss: 0.2159692347049713\n",
      "iteration 46684: loss: 0.2159690409898758\n",
      "iteration 46685: loss: 0.21596892178058624\n",
      "iteration 46686: loss: 0.2159688025712967\n",
      "iteration 46687: loss: 0.21596863865852356\n",
      "iteration 46688: loss: 0.2159685343503952\n",
      "iteration 46689: loss: 0.21596837043762207\n",
      "iteration 46690: loss: 0.21596820652484894\n",
      "iteration 46691: loss: 0.2159680873155594\n",
      "iteration 46692: loss: 0.21596793830394745\n",
      "iteration 46693: loss: 0.2159678190946579\n",
      "iteration 46694: loss: 0.21596765518188477\n",
      "iteration 46695: loss: 0.21596750617027283\n",
      "iteration 46696: loss: 0.21596737205982208\n",
      "iteration 46697: loss: 0.21596720814704895\n",
      "iteration 46698: loss: 0.215967059135437\n",
      "iteration 46699: loss: 0.21596701443195343\n",
      "iteration 46700: loss: 0.2159668505191803\n",
      "iteration 46701: loss: 0.21596670150756836\n",
      "iteration 46702: loss: 0.21596650779247284\n",
      "iteration 46703: loss: 0.2159663736820221\n",
      "iteration 46704: loss: 0.21596629917621613\n",
      "iteration 46705: loss: 0.21596617996692657\n",
      "iteration 46706: loss: 0.21596598625183105\n",
      "iteration 46707: loss: 0.21596582233905792\n",
      "iteration 46708: loss: 0.21596570312976837\n",
      "iteration 46709: loss: 0.21596553921699524\n",
      "iteration 46710: loss: 0.2159653604030609\n",
      "iteration 46711: loss: 0.21596527099609375\n",
      "iteration 46712: loss: 0.2159651219844818\n",
      "iteration 46713: loss: 0.21596498787403107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 46714: loss: 0.21596483886241913\n",
      "iteration 46715: loss: 0.21596470475196838\n",
      "iteration 46716: loss: 0.21596452593803406\n",
      "iteration 46717: loss: 0.2159644067287445\n",
      "iteration 46718: loss: 0.21596427261829376\n",
      "iteration 46719: loss: 0.21596412360668182\n",
      "iteration 46720: loss: 0.2159639298915863\n",
      "iteration 46721: loss: 0.21596384048461914\n",
      "iteration 46722: loss: 0.2159637212753296\n",
      "iteration 46723: loss: 0.21596357226371765\n",
      "iteration 46724: loss: 0.2159634381532669\n",
      "iteration 46725: loss: 0.21596331894397736\n",
      "iteration 46726: loss: 0.21596312522888184\n",
      "iteration 46727: loss: 0.21596300601959229\n",
      "iteration 46728: loss: 0.21596284210681915\n",
      "iteration 46729: loss: 0.2159627377986908\n",
      "iteration 46730: loss: 0.21596261858940125\n",
      "iteration 46731: loss: 0.2159624546766281\n",
      "iteration 46732: loss: 0.2159622460603714\n",
      "iteration 46733: loss: 0.21596214175224304\n",
      "iteration 46734: loss: 0.2159620076417923\n",
      "iteration 46735: loss: 0.21596185863018036\n",
      "iteration 46736: loss: 0.2159617394208908\n",
      "iteration 46737: loss: 0.21596160531044006\n",
      "iteration 46738: loss: 0.21596145629882812\n",
      "iteration 46739: loss: 0.2159613072872162\n",
      "iteration 46740: loss: 0.21596118807792664\n",
      "iteration 46741: loss: 0.21596106886863708\n",
      "iteration 46742: loss: 0.21596093475818634\n",
      "iteration 46743: loss: 0.2159607708454132\n",
      "iteration 46744: loss: 0.21596065163612366\n",
      "iteration 46745: loss: 0.21596045792102814\n",
      "iteration 46746: loss: 0.21596035361289978\n",
      "iteration 46747: loss: 0.21596011519432068\n",
      "iteration 46748: loss: 0.21595999598503113\n",
      "iteration 46749: loss: 0.21595987677574158\n",
      "iteration 46750: loss: 0.21595978736877441\n",
      "iteration 46751: loss: 0.21595963835716248\n",
      "iteration 46752: loss: 0.21595947444438934\n",
      "iteration 46753: loss: 0.21595928072929382\n",
      "iteration 46754: loss: 0.21595922112464905\n",
      "iteration 46755: loss: 0.21595904231071472\n",
      "iteration 46756: loss: 0.21595898270606995\n",
      "iteration 46757: loss: 0.21595875918865204\n",
      "iteration 46758: loss: 0.2159586250782013\n",
      "iteration 46759: loss: 0.21595855057239532\n",
      "iteration 46760: loss: 0.2159583866596222\n",
      "iteration 46761: loss: 0.21595819294452667\n",
      "iteration 46762: loss: 0.21595807373523712\n",
      "iteration 46763: loss: 0.215957909822464\n",
      "iteration 46764: loss: 0.21595783531665802\n",
      "iteration 46765: loss: 0.21595768630504608\n",
      "iteration 46766: loss: 0.21595755219459534\n",
      "iteration 46767: loss: 0.2159574031829834\n",
      "iteration 46768: loss: 0.21595725417137146\n",
      "iteration 46769: loss: 0.21595701575279236\n",
      "iteration 46770: loss: 0.21595697104930878\n",
      "iteration 46771: loss: 0.21595677733421326\n",
      "iteration 46772: loss: 0.2159566879272461\n",
      "iteration 46773: loss: 0.21595649421215057\n",
      "iteration 46774: loss: 0.21595637500286102\n",
      "iteration 46775: loss: 0.2159562110900879\n",
      "iteration 46776: loss: 0.21595613658428192\n",
      "iteration 46777: loss: 0.2159559279680252\n",
      "iteration 46778: loss: 0.21595580875873566\n",
      "iteration 46779: loss: 0.21595564484596252\n",
      "iteration 46780: loss: 0.21595554053783417\n",
      "iteration 46781: loss: 0.215955451130867\n",
      "iteration 46782: loss: 0.2159552276134491\n",
      "iteration 46783: loss: 0.21595516800880432\n",
      "iteration 46784: loss: 0.21595492959022522\n",
      "iteration 46785: loss: 0.21595482528209686\n",
      "iteration 46786: loss: 0.21595466136932373\n",
      "iteration 46787: loss: 0.21595454216003418\n",
      "iteration 46788: loss: 0.21595437824726105\n",
      "iteration 46789: loss: 0.2159542590379715\n",
      "iteration 46790: loss: 0.21595411002635956\n",
      "iteration 46791: loss: 0.2159539759159088\n",
      "iteration 46792: loss: 0.21595385670661926\n",
      "iteration 46793: loss: 0.21595370769500732\n",
      "iteration 46794: loss: 0.2159535139799118\n",
      "iteration 46795: loss: 0.21595337986946106\n",
      "iteration 46796: loss: 0.2159532755613327\n",
      "iteration 46797: loss: 0.21595314145088196\n",
      "iteration 46798: loss: 0.21595294773578644\n",
      "iteration 46799: loss: 0.2159527987241745\n",
      "iteration 46800: loss: 0.21595267951488495\n",
      "iteration 46801: loss: 0.2159525603055954\n",
      "iteration 46802: loss: 0.21595242619514465\n",
      "iteration 46803: loss: 0.21595224738121033\n",
      "iteration 46804: loss: 0.21595215797424316\n",
      "iteration 46805: loss: 0.21595200896263123\n",
      "iteration 46806: loss: 0.2159518450498581\n",
      "iteration 46807: loss: 0.21595175564289093\n",
      "iteration 46808: loss: 0.2159515619277954\n",
      "iteration 46809: loss: 0.21595144271850586\n",
      "iteration 46810: loss: 0.2159513533115387\n",
      "iteration 46811: loss: 0.21595115959644318\n",
      "iteration 46812: loss: 0.21595096588134766\n",
      "iteration 46813: loss: 0.2159508913755417\n",
      "iteration 46814: loss: 0.21595075726509094\n",
      "iteration 46815: loss: 0.21595056354999542\n",
      "iteration 46816: loss: 0.21595045924186707\n",
      "iteration 46817: loss: 0.21595029532909393\n",
      "iteration 46818: loss: 0.2159501612186432\n",
      "iteration 46819: loss: 0.21594996750354767\n",
      "iteration 46820: loss: 0.2159498631954193\n",
      "iteration 46821: loss: 0.21594974398612976\n",
      "iteration 46822: loss: 0.2159496247768402\n",
      "iteration 46823: loss: 0.21594950556755066\n",
      "iteration 46824: loss: 0.21594926714897156\n",
      "iteration 46825: loss: 0.21594922244548798\n",
      "iteration 46826: loss: 0.21594902873039246\n",
      "iteration 46827: loss: 0.21594884991645813\n",
      "iteration 46828: loss: 0.21594873070716858\n",
      "iteration 46829: loss: 0.21594861149787903\n",
      "iteration 46830: loss: 0.2159484326839447\n",
      "iteration 46831: loss: 0.21594834327697754\n",
      "iteration 46832: loss: 0.2159481793642044\n",
      "iteration 46833: loss: 0.21594803035259247\n",
      "iteration 46834: loss: 0.21594791114330292\n",
      "iteration 46835: loss: 0.2159477174282074\n",
      "iteration 46836: loss: 0.21594758331775665\n",
      "iteration 46837: loss: 0.2159474790096283\n",
      "iteration 46838: loss: 0.21594730019569397\n",
      "iteration 46839: loss: 0.2159471958875656\n",
      "iteration 46840: loss: 0.21594707667827606\n",
      "iteration 46841: loss: 0.21594691276550293\n",
      "iteration 46842: loss: 0.2159467488527298\n",
      "iteration 46843: loss: 0.21594659984111786\n",
      "iteration 46844: loss: 0.21594646573066711\n",
      "iteration 46845: loss: 0.21594636142253876\n",
      "iteration 46846: loss: 0.21594610810279846\n",
      "iteration 46847: loss: 0.21594607830047607\n",
      "iteration 46848: loss: 0.21594591438770294\n",
      "iteration 46849: loss: 0.215945765376091\n",
      "iteration 46850: loss: 0.21594560146331787\n",
      "iteration 46851: loss: 0.21594543755054474\n",
      "iteration 46852: loss: 0.2159452736377716\n",
      "iteration 46853: loss: 0.21594515442848206\n",
      "iteration 46854: loss: 0.21594512462615967\n",
      "iteration 46855: loss: 0.21594488620758057\n",
      "iteration 46856: loss: 0.21594476699829102\n",
      "iteration 46857: loss: 0.21594461798667908\n",
      "iteration 46858: loss: 0.21594449877738953\n",
      "iteration 46859: loss: 0.2159443348646164\n",
      "iteration 46860: loss: 0.21594417095184326\n",
      "iteration 46861: loss: 0.2159440517425537\n",
      "iteration 46862: loss: 0.21594393253326416\n",
      "iteration 46863: loss: 0.21594378352165222\n",
      "iteration 46864: loss: 0.21594366431236267\n",
      "iteration 46865: loss: 0.21594353020191193\n",
      "iteration 46866: loss: 0.2159433364868164\n",
      "iteration 46867: loss: 0.21594324707984924\n",
      "iteration 46868: loss: 0.21594305336475372\n",
      "iteration 46869: loss: 0.21594294905662537\n",
      "iteration 46870: loss: 0.21594277024269104\n",
      "iteration 46871: loss: 0.21594266593456268\n",
      "iteration 46872: loss: 0.21594257652759552\n",
      "iteration 46873: loss: 0.2159423530101776\n",
      "iteration 46874: loss: 0.21594223380088806\n",
      "iteration 46875: loss: 0.21594205498695374\n",
      "iteration 46876: loss: 0.21594193577766418\n",
      "iteration 46877: loss: 0.21594181656837463\n",
      "iteration 46878: loss: 0.2159416377544403\n",
      "iteration 46879: loss: 0.21594147384166718\n",
      "iteration 46880: loss: 0.2159414291381836\n",
      "iteration 46881: loss: 0.21594126522541046\n",
      "iteration 46882: loss: 0.21594111621379852\n",
      "iteration 46883: loss: 0.2159409523010254\n",
      "iteration 46884: loss: 0.21594086289405823\n",
      "iteration 46885: loss: 0.21594063937664032\n",
      "iteration 46886: loss: 0.21594054996967316\n",
      "iteration 46887: loss: 0.21594038605690002\n",
      "iteration 46888: loss: 0.2159401923418045\n",
      "iteration 46889: loss: 0.21594007313251495\n",
      "iteration 46890: loss: 0.21593992412090302\n",
      "iteration 46891: loss: 0.21593980491161346\n",
      "iteration 46892: loss: 0.21593967080116272\n",
      "iteration 46893: loss: 0.21593956649303436\n",
      "iteration 46894: loss: 0.21593943238258362\n",
      "iteration 46895: loss: 0.21593928337097168\n",
      "iteration 46896: loss: 0.21593911945819855\n",
      "iteration 46897: loss: 0.2159389704465866\n",
      "iteration 46898: loss: 0.21593883633613586\n",
      "iteration 46899: loss: 0.2159387171268463\n",
      "iteration 46900: loss: 0.2159385234117508\n",
      "iteration 46901: loss: 0.21593837440013885\n",
      "iteration 46902: loss: 0.2159382402896881\n",
      "iteration 46903: loss: 0.21593812108039856\n",
      "iteration 46904: loss: 0.21593794226646423\n",
      "iteration 46905: loss: 0.2159377783536911\n",
      "iteration 46906: loss: 0.21593768894672394\n",
      "iteration 46907: loss: 0.21593761444091797\n",
      "iteration 46908: loss: 0.21593740582466125\n",
      "iteration 46909: loss: 0.21593722701072693\n",
      "iteration 46910: loss: 0.21593710780143738\n",
      "iteration 46911: loss: 0.21593701839447021\n",
      "iteration 46912: loss: 0.2159368246793747\n",
      "iteration 46913: loss: 0.21593669056892395\n",
      "iteration 46914: loss: 0.2159365862607956\n",
      "iteration 46915: loss: 0.21593642234802246\n",
      "iteration 46916: loss: 0.2159363329410553\n",
      "iteration 46917: loss: 0.2159360945224762\n",
      "iteration 46918: loss: 0.2159360647201538\n",
      "iteration 46919: loss: 0.2159358561038971\n",
      "iteration 46920: loss: 0.21593570709228516\n",
      "iteration 46921: loss: 0.21593555808067322\n",
      "iteration 46922: loss: 0.21593539416790009\n",
      "iteration 46923: loss: 0.21593526005744934\n",
      "iteration 46924: loss: 0.2159351408481598\n",
      "iteration 46925: loss: 0.21593499183654785\n",
      "iteration 46926: loss: 0.2159349024295807\n",
      "iteration 46927: loss: 0.21593472361564636\n",
      "iteration 46928: loss: 0.21593455970287323\n",
      "iteration 46929: loss: 0.2159343957901001\n",
      "iteration 46930: loss: 0.21593423187732697\n",
      "iteration 46931: loss: 0.215934157371521\n",
      "iteration 46932: loss: 0.21593394875526428\n",
      "iteration 46933: loss: 0.2159338742494583\n",
      "iteration 46934: loss: 0.2159336805343628\n",
      "iteration 46935: loss: 0.21593356132507324\n",
      "iteration 46936: loss: 0.2159333974123001\n",
      "iteration 46937: loss: 0.21593324840068817\n",
      "iteration 46938: loss: 0.21593312919139862\n",
      "iteration 46939: loss: 0.21593299508094788\n",
      "iteration 46940: loss: 0.21593287587165833\n",
      "iteration 46941: loss: 0.21593275666236877\n",
      "iteration 46942: loss: 0.21593257784843445\n",
      "iteration 46943: loss: 0.21593239903450012\n",
      "iteration 46944: loss: 0.21593227982521057\n",
      "iteration 46945: loss: 0.21593213081359863\n",
      "iteration 46946: loss: 0.2159319669008255\n",
      "iteration 46947: loss: 0.21593181788921356\n",
      "iteration 46948: loss: 0.215931698679924\n",
      "iteration 46949: loss: 0.21593153476715088\n",
      "iteration 46950: loss: 0.21593138575553894\n",
      "iteration 46951: loss: 0.21593134105205536\n",
      "iteration 46952: loss: 0.21593113243579865\n",
      "iteration 46953: loss: 0.2159309834241867\n",
      "iteration 46954: loss: 0.21593086421489716\n",
      "iteration 46955: loss: 0.21593067049980164\n",
      "iteration 46956: loss: 0.21593055129051208\n",
      "iteration 46957: loss: 0.21593038737773895\n",
      "iteration 46958: loss: 0.2159302681684494\n",
      "iteration 46959: loss: 0.21593013405799866\n",
      "iteration 46960: loss: 0.21592994034290314\n",
      "iteration 46961: loss: 0.21592989563941956\n",
      "iteration 46962: loss: 0.21592974662780762\n",
      "iteration 46963: loss: 0.21592962741851807\n",
      "iteration 46964: loss: 0.21592938899993896\n",
      "iteration 46965: loss: 0.21592934429645538\n",
      "iteration 46966: loss: 0.21592918038368225\n",
      "iteration 46967: loss: 0.21592898666858673\n",
      "iteration 46968: loss: 0.21592886745929718\n",
      "iteration 46969: loss: 0.21592864394187927\n",
      "iteration 46970: loss: 0.21592852473258972\n",
      "iteration 46971: loss: 0.21592843532562256\n",
      "iteration 46972: loss: 0.2159283608198166\n",
      "iteration 46973: loss: 0.2159281224012375\n",
      "iteration 46974: loss: 0.21592795848846436\n",
      "iteration 46975: loss: 0.21592780947685242\n",
      "iteration 46976: loss: 0.21592769026756287\n",
      "iteration 46977: loss: 0.2159276008605957\n",
      "iteration 46978: loss: 0.215927392244339\n",
      "iteration 46979: loss: 0.21592727303504944\n",
      "iteration 46980: loss: 0.21592719852924347\n",
      "iteration 46981: loss: 0.21592704951763153\n",
      "iteration 46982: loss: 0.21592684090137482\n",
      "iteration 46983: loss: 0.21592673659324646\n",
      "iteration 46984: loss: 0.2159266173839569\n",
      "iteration 46985: loss: 0.21592648327350616\n",
      "iteration 46986: loss: 0.21592631936073303\n",
      "iteration 46987: loss: 0.2159261256456375\n",
      "iteration 46988: loss: 0.21592600643634796\n",
      "iteration 46989: loss: 0.21592585742473602\n",
      "iteration 46990: loss: 0.21592573821544647\n",
      "iteration 46991: loss: 0.21592554450035095\n",
      "iteration 46992: loss: 0.2159254550933838\n",
      "iteration 46993: loss: 0.21592526137828827\n",
      "iteration 46994: loss: 0.21592514216899872\n",
      "iteration 46995: loss: 0.21592497825622559\n",
      "iteration 46996: loss: 0.21592482924461365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 46997: loss: 0.2159246951341629\n",
      "iteration 46998: loss: 0.21592465043067932\n",
      "iteration 46999: loss: 0.21592441201210022\n",
      "iteration 47000: loss: 0.21592430770397186\n",
      "iteration 47001: loss: 0.21592414379119873\n",
      "iteration 47002: loss: 0.21592402458190918\n",
      "iteration 47003: loss: 0.21592386066913605\n",
      "iteration 47004: loss: 0.21592363715171814\n",
      "iteration 47005: loss: 0.21592354774475098\n",
      "iteration 47006: loss: 0.215923473238945\n",
      "iteration 47007: loss: 0.21592330932617188\n",
      "iteration 47008: loss: 0.21592316031455994\n",
      "iteration 47009: loss: 0.2159229964017868\n",
      "iteration 47010: loss: 0.21592283248901367\n",
      "iteration 47011: loss: 0.2159227430820465\n",
      "iteration 47012: loss: 0.21592256426811218\n",
      "iteration 47013: loss: 0.21592238545417786\n",
      "iteration 47014: loss: 0.21592235565185547\n",
      "iteration 47015: loss: 0.21592220664024353\n",
      "iteration 47016: loss: 0.2159220278263092\n",
      "iteration 47017: loss: 0.21592184901237488\n",
      "iteration 47018: loss: 0.21592171490192413\n",
      "iteration 47019: loss: 0.21592161059379578\n",
      "iteration 47020: loss: 0.21592144668102264\n",
      "iteration 47021: loss: 0.2159213125705719\n",
      "iteration 47022: loss: 0.21592116355895996\n",
      "iteration 47023: loss: 0.21592101454734802\n",
      "iteration 47024: loss: 0.21592089533805847\n",
      "iteration 47025: loss: 0.21592071652412415\n",
      "iteration 47026: loss: 0.21592053771018982\n",
      "iteration 47027: loss: 0.2159203737974167\n",
      "iteration 47028: loss: 0.21592029929161072\n",
      "iteration 47029: loss: 0.2159201204776764\n",
      "iteration 47030: loss: 0.21592006087303162\n",
      "iteration 47031: loss: 0.21591982245445251\n",
      "iteration 47032: loss: 0.21591968834400177\n",
      "iteration 47033: loss: 0.2159196138381958\n",
      "iteration 47034: loss: 0.21591946482658386\n",
      "iteration 47035: loss: 0.21591930091381073\n",
      "iteration 47036: loss: 0.21591916680335999\n",
      "iteration 47037: loss: 0.21591901779174805\n",
      "iteration 47038: loss: 0.21591882407665253\n",
      "iteration 47039: loss: 0.2159186601638794\n",
      "iteration 47040: loss: 0.2159186154603958\n",
      "iteration 47041: loss: 0.21591845154762268\n",
      "iteration 47042: loss: 0.21591825783252716\n",
      "iteration 47043: loss: 0.21591810882091522\n",
      "iteration 47044: loss: 0.21591803431510925\n",
      "iteration 47045: loss: 0.21591787040233612\n",
      "iteration 47046: loss: 0.2159176766872406\n",
      "iteration 47047: loss: 0.21591761708259583\n",
      "iteration 47048: loss: 0.21591734886169434\n",
      "iteration 47049: loss: 0.21591727435588837\n",
      "iteration 47050: loss: 0.21591711044311523\n",
      "iteration 47051: loss: 0.21591699123382568\n",
      "iteration 47052: loss: 0.21591679751873016\n",
      "iteration 47053: loss: 0.21591679751873016\n",
      "iteration 47054: loss: 0.21591655910015106\n",
      "iteration 47055: loss: 0.21591639518737793\n",
      "iteration 47056: loss: 0.21591632068157196\n",
      "iteration 47057: loss: 0.21591615676879883\n",
      "iteration 47058: loss: 0.2159159928560257\n",
      "iteration 47059: loss: 0.21591587364673615\n",
      "iteration 47060: loss: 0.2159157246351242\n",
      "iteration 47061: loss: 0.21591563522815704\n",
      "iteration 47062: loss: 0.2159154713153839\n",
      "iteration 47063: loss: 0.2159152776002884\n",
      "iteration 47064: loss: 0.21591512858867645\n",
      "iteration 47065: loss: 0.21591496467590332\n",
      "iteration 47066: loss: 0.21591487526893616\n",
      "iteration 47067: loss: 0.2159147560596466\n",
      "iteration 47068: loss: 0.21591457724571228\n",
      "iteration 47069: loss: 0.21591444313526154\n",
      "iteration 47070: loss: 0.21591424942016602\n",
      "iteration 47071: loss: 0.21591416001319885\n",
      "iteration 47072: loss: 0.21591396629810333\n",
      "iteration 47073: loss: 0.2159138172864914\n",
      "iteration 47074: loss: 0.21591369807720184\n",
      "iteration 47075: loss: 0.2159135341644287\n",
      "iteration 47076: loss: 0.21591345965862274\n",
      "iteration 47077: loss: 0.2159132957458496\n",
      "iteration 47078: loss: 0.2159131020307541\n",
      "iteration 47079: loss: 0.21591298282146454\n",
      "iteration 47080: loss: 0.2159128189086914\n",
      "iteration 47081: loss: 0.21591266989707947\n",
      "iteration 47082: loss: 0.21591253578662872\n",
      "iteration 47083: loss: 0.21591241657733917\n",
      "iteration 47084: loss: 0.21591229736804962\n",
      "iteration 47085: loss: 0.2159121334552765\n",
      "iteration 47086: loss: 0.21591193974018097\n",
      "iteration 47087: loss: 0.215911865234375\n",
      "iteration 47088: loss: 0.21591170132160187\n",
      "iteration 47089: loss: 0.21591147780418396\n",
      "iteration 47090: loss: 0.21591143310070038\n",
      "iteration 47091: loss: 0.21591126918792725\n",
      "iteration 47092: loss: 0.2159111499786377\n",
      "iteration 47093: loss: 0.21591106057167053\n",
      "iteration 47094: loss: 0.2159108817577362\n",
      "iteration 47095: loss: 0.21591070294380188\n",
      "iteration 47096: loss: 0.21591050922870636\n",
      "iteration 47097: loss: 0.2159103900194168\n",
      "iteration 47098: loss: 0.21591022610664368\n",
      "iteration 47099: loss: 0.21591007709503174\n",
      "iteration 47100: loss: 0.2159099280834198\n",
      "iteration 47101: loss: 0.21590988337993622\n",
      "iteration 47102: loss: 0.2159096896648407\n",
      "iteration 47103: loss: 0.21590955555438995\n",
      "iteration 47104: loss: 0.2159094363451004\n",
      "iteration 47105: loss: 0.21590924263000488\n",
      "iteration 47106: loss: 0.21590909361839294\n",
      "iteration 47107: loss: 0.21590903401374817\n",
      "iteration 47108: loss: 0.21590881049633026\n",
      "iteration 47109: loss: 0.2159086912870407\n",
      "iteration 47110: loss: 0.21590852737426758\n",
      "iteration 47111: loss: 0.21590837836265564\n",
      "iteration 47112: loss: 0.2159082442522049\n",
      "iteration 47113: loss: 0.21590816974639893\n",
      "iteration 47114: loss: 0.2159079611301422\n",
      "iteration 47115: loss: 0.21590784192085266\n",
      "iteration 47116: loss: 0.21590760350227356\n",
      "iteration 47117: loss: 0.21590757369995117\n",
      "iteration 47118: loss: 0.21590737998485565\n",
      "iteration 47119: loss: 0.21590718626976013\n",
      "iteration 47120: loss: 0.21590714156627655\n",
      "iteration 47121: loss: 0.21590697765350342\n",
      "iteration 47122: loss: 0.2159067690372467\n",
      "iteration 47123: loss: 0.21590669453144073\n",
      "iteration 47124: loss: 0.21590647101402283\n",
      "iteration 47125: loss: 0.21590638160705566\n",
      "iteration 47126: loss: 0.21590623259544373\n",
      "iteration 47127: loss: 0.21590614318847656\n",
      "iteration 47128: loss: 0.21590594947338104\n",
      "iteration 47129: loss: 0.2159058153629303\n",
      "iteration 47130: loss: 0.21590566635131836\n",
      "iteration 47131: loss: 0.21590550243854523\n",
      "iteration 47132: loss: 0.2159053534269333\n",
      "iteration 47133: loss: 0.21590518951416016\n",
      "iteration 47134: loss: 0.2159050703048706\n",
      "iteration 47135: loss: 0.21590495109558105\n",
      "iteration 47136: loss: 0.21590475738048553\n",
      "iteration 47137: loss: 0.2159046232700348\n",
      "iteration 47138: loss: 0.21590451896190643\n",
      "iteration 47139: loss: 0.21590442955493927\n",
      "iteration 47140: loss: 0.21590423583984375\n",
      "iteration 47141: loss: 0.2159040868282318\n",
      "iteration 47142: loss: 0.21590392291545868\n",
      "iteration 47143: loss: 0.2159038484096527\n",
      "iteration 47144: loss: 0.215903639793396\n",
      "iteration 47145: loss: 0.21590347588062286\n",
      "iteration 47146: loss: 0.2159033715724945\n",
      "iteration 47147: loss: 0.21590325236320496\n",
      "iteration 47148: loss: 0.21590307354927063\n",
      "iteration 47149: loss: 0.21590295433998108\n",
      "iteration 47150: loss: 0.21590283513069153\n",
      "iteration 47151: loss: 0.215902641415596\n",
      "iteration 47152: loss: 0.21590249240398407\n",
      "iteration 47153: loss: 0.21590229868888855\n",
      "iteration 47154: loss: 0.21590228378772736\n",
      "iteration 47155: loss: 0.21590206027030945\n",
      "iteration 47156: loss: 0.2159019410610199\n",
      "iteration 47157: loss: 0.21590176224708557\n",
      "iteration 47158: loss: 0.21590161323547363\n",
      "iteration 47159: loss: 0.2159014642238617\n",
      "iteration 47160: loss: 0.21590137481689453\n",
      "iteration 47161: loss: 0.2159012109041214\n",
      "iteration 47162: loss: 0.21590106189250946\n",
      "iteration 47163: loss: 0.21590092778205872\n",
      "iteration 47164: loss: 0.2159007489681244\n",
      "iteration 47165: loss: 0.21590062975883484\n",
      "iteration 47166: loss: 0.2159005105495453\n",
      "iteration 47167: loss: 0.21590037643909454\n",
      "iteration 47168: loss: 0.2159002125263214\n",
      "iteration 47169: loss: 0.21590003371238708\n",
      "iteration 47170: loss: 0.21589991450309753\n",
      "iteration 47171: loss: 0.2158997505903244\n",
      "iteration 47172: loss: 0.21589961647987366\n",
      "iteration 47173: loss: 0.21589946746826172\n",
      "iteration 47174: loss: 0.21589934825897217\n",
      "iteration 47175: loss: 0.2158992737531662\n",
      "iteration 47176: loss: 0.2158990204334259\n",
      "iteration 47177: loss: 0.21589891612529755\n",
      "iteration 47178: loss: 0.2158987820148468\n",
      "iteration 47179: loss: 0.21589866280555725\n",
      "iteration 47180: loss: 0.21589846909046173\n",
      "iteration 47181: loss: 0.2158983051776886\n",
      "iteration 47182: loss: 0.21589818596839905\n",
      "iteration 47183: loss: 0.2158980369567871\n",
      "iteration 47184: loss: 0.21589794754981995\n",
      "iteration 47185: loss: 0.21589770913124084\n",
      "iteration 47186: loss: 0.2158976048231125\n",
      "iteration 47187: loss: 0.21589744091033936\n",
      "iteration 47188: loss: 0.2158973217010498\n",
      "iteration 47189: loss: 0.21589715778827667\n",
      "iteration 47190: loss: 0.21589705348014832\n",
      "iteration 47191: loss: 0.2158968448638916\n",
      "iteration 47192: loss: 0.21589675545692444\n",
      "iteration 47193: loss: 0.21589665114879608\n",
      "iteration 47194: loss: 0.21589645743370056\n",
      "iteration 47195: loss: 0.21589629352092743\n",
      "iteration 47196: loss: 0.21589621901512146\n",
      "iteration 47197: loss: 0.2158960998058319\n",
      "iteration 47198: loss: 0.2158958464860916\n",
      "iteration 47199: loss: 0.21589577198028564\n",
      "iteration 47200: loss: 0.2158956527709961\n",
      "iteration 47201: loss: 0.21589550375938416\n",
      "iteration 47202: loss: 0.21589529514312744\n",
      "iteration 47203: loss: 0.2158951759338379\n",
      "iteration 47204: loss: 0.21589498221874237\n",
      "iteration 47205: loss: 0.21589486300945282\n",
      "iteration 47206: loss: 0.2158946692943573\n",
      "iteration 47207: loss: 0.21589457988739014\n",
      "iteration 47208: loss: 0.2158944308757782\n",
      "iteration 47209: loss: 0.21589429676532745\n",
      "iteration 47210: loss: 0.21589413285255432\n",
      "iteration 47211: loss: 0.21589402854442596\n",
      "iteration 47212: loss: 0.21589389443397522\n",
      "iteration 47213: loss: 0.2158937156200409\n",
      "iteration 47214: loss: 0.21589358150959015\n",
      "iteration 47215: loss: 0.2158934623003006\n",
      "iteration 47216: loss: 0.2158932238817215\n",
      "iteration 47217: loss: 0.21589311957359314\n",
      "iteration 47218: loss: 0.2158929854631424\n",
      "iteration 47219: loss: 0.21589282155036926\n",
      "iteration 47220: loss: 0.2158927470445633\n",
      "iteration 47221: loss: 0.21589258313179016\n",
      "iteration 47222: loss: 0.2158924639225006\n",
      "iteration 47223: loss: 0.2158922702074051\n",
      "iteration 47224: loss: 0.21589212119579315\n",
      "iteration 47225: loss: 0.2158919870853424\n",
      "iteration 47226: loss: 0.21589186787605286\n",
      "iteration 47227: loss: 0.21589171886444092\n",
      "iteration 47228: loss: 0.2158915102481842\n",
      "iteration 47229: loss: 0.21589143574237823\n",
      "iteration 47230: loss: 0.21589121222496033\n",
      "iteration 47231: loss: 0.21589116752147675\n",
      "iteration 47232: loss: 0.21589092910289764\n",
      "iteration 47233: loss: 0.21589083969593048\n",
      "iteration 47234: loss: 0.21589069068431854\n",
      "iteration 47235: loss: 0.21589060127735138\n",
      "iteration 47236: loss: 0.21589043736457825\n",
      "iteration 47237: loss: 0.2158902883529663\n",
      "iteration 47238: loss: 0.2158900946378708\n",
      "iteration 47239: loss: 0.21588996052742004\n",
      "iteration 47240: loss: 0.2158898413181305\n",
      "iteration 47241: loss: 0.21588964760303497\n",
      "iteration 47242: loss: 0.21588954329490662\n",
      "iteration 47243: loss: 0.21588942408561707\n",
      "iteration 47244: loss: 0.21588926017284393\n",
      "iteration 47245: loss: 0.2158890962600708\n",
      "iteration 47246: loss: 0.21588894724845886\n",
      "iteration 47247: loss: 0.2158888280391693\n",
      "iteration 47248: loss: 0.21588866412639618\n",
      "iteration 47249: loss: 0.21588850021362305\n",
      "iteration 47250: loss: 0.21588829159736633\n",
      "iteration 47251: loss: 0.21588823199272156\n",
      "iteration 47252: loss: 0.21588806807994843\n",
      "iteration 47253: loss: 0.21588793396949768\n",
      "iteration 47254: loss: 0.21588774025440216\n",
      "iteration 47255: loss: 0.2158876359462738\n",
      "iteration 47256: loss: 0.21588751673698425\n",
      "iteration 47257: loss: 0.2158873826265335\n",
      "iteration 47258: loss: 0.215887188911438\n",
      "iteration 47259: loss: 0.21588702499866486\n",
      "iteration 47260: loss: 0.2158869057893753\n",
      "iteration 47261: loss: 0.21588680148124695\n",
      "iteration 47262: loss: 0.21588662266731262\n",
      "iteration 47263: loss: 0.2158864438533783\n",
      "iteration 47264: loss: 0.21588626503944397\n",
      "iteration 47265: loss: 0.21588626503944397\n",
      "iteration 47266: loss: 0.21588607132434845\n",
      "iteration 47267: loss: 0.2158859521150589\n",
      "iteration 47268: loss: 0.21588578820228577\n",
      "iteration 47269: loss: 0.21588559448719025\n",
      "iteration 47270: loss: 0.2158854901790619\n",
      "iteration 47271: loss: 0.21588531136512756\n",
      "iteration 47272: loss: 0.2158852070569992\n",
      "iteration 47273: loss: 0.2158850133419037\n",
      "iteration 47274: loss: 0.21588487923145294\n",
      "iteration 47275: loss: 0.21588483452796936\n",
      "iteration 47276: loss: 0.2158845216035843\n",
      "iteration 47277: loss: 0.21588441729545593\n",
      "iteration 47278: loss: 0.21588429808616638\n",
      "iteration 47279: loss: 0.21588417887687683\n",
      "iteration 47280: loss: 0.21588408946990967\n",
      "iteration 47281: loss: 0.21588382124900818\n",
      "iteration 47282: loss: 0.2158837616443634\n",
      "iteration 47283: loss: 0.2158835381269455\n",
      "iteration 47284: loss: 0.21588349342346191\n",
      "iteration 47285: loss: 0.2158832997083664\n",
      "iteration 47286: loss: 0.21588313579559326\n",
      "iteration 47287: loss: 0.2158830612897873\n",
      "iteration 47288: loss: 0.21588286757469177\n",
      "iteration 47289: loss: 0.21588268876075745\n",
      "iteration 47290: loss: 0.21588262915611267\n",
      "iteration 47291: loss: 0.21588245034217834\n",
      "iteration 47292: loss: 0.2158823013305664\n",
      "iteration 47293: loss: 0.2158821076154709\n",
      "iteration 47294: loss: 0.21588201820850372\n",
      "iteration 47295: loss: 0.2158818542957306\n",
      "iteration 47296: loss: 0.21588170528411865\n",
      "iteration 47297: loss: 0.21588149666786194\n",
      "iteration 47298: loss: 0.21588146686553955\n",
      "iteration 47299: loss: 0.21588130295276642\n",
      "iteration 47300: loss: 0.2158811092376709\n",
      "iteration 47301: loss: 0.21588096022605896\n",
      "iteration 47302: loss: 0.2158808410167694\n",
      "iteration 47303: loss: 0.21588072180747986\n",
      "iteration 47304: loss: 0.21588055789470673\n",
      "iteration 47305: loss: 0.2158803641796112\n",
      "iteration 47306: loss: 0.21588024497032166\n",
      "iteration 47307: loss: 0.2158801257610321\n",
      "iteration 47308: loss: 0.21587996184825897\n",
      "iteration 47309: loss: 0.21587982773780823\n",
      "iteration 47310: loss: 0.21587972342967987\n",
      "iteration 47311: loss: 0.21587952971458435\n",
      "iteration 47312: loss: 0.2158793956041336\n",
      "iteration 47313: loss: 0.21587924659252167\n",
      "iteration 47314: loss: 0.21587911248207092\n",
      "iteration 47315: loss: 0.2158789187669754\n",
      "iteration 47316: loss: 0.21587876975536346\n",
      "iteration 47317: loss: 0.2158786505460739\n",
      "iteration 47318: loss: 0.21587856113910675\n",
      "iteration 47319: loss: 0.2158784419298172\n",
      "iteration 47320: loss: 0.21587829291820526\n",
      "iteration 47321: loss: 0.21587808430194855\n",
      "iteration 47322: loss: 0.21587800979614258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 47323: loss: 0.21587781608104706\n",
      "iteration 47324: loss: 0.2158776819705963\n",
      "iteration 47325: loss: 0.2158774882555008\n",
      "iteration 47326: loss: 0.21587736904621124\n",
      "iteration 47327: loss: 0.2158772200345993\n",
      "iteration 47328: loss: 0.21587705612182617\n",
      "iteration 47329: loss: 0.21587690711021423\n",
      "iteration 47330: loss: 0.2158767431974411\n",
      "iteration 47331: loss: 0.21587665379047394\n",
      "iteration 47332: loss: 0.21587646007537842\n",
      "iteration 47333: loss: 0.21587637066841125\n",
      "iteration 47334: loss: 0.21587622165679932\n",
      "iteration 47335: loss: 0.21587610244750977\n",
      "iteration 47336: loss: 0.21587593853473663\n",
      "iteration 47337: loss: 0.2158757746219635\n",
      "iteration 47338: loss: 0.21587565541267395\n",
      "iteration 47339: loss: 0.21587547659873962\n",
      "iteration 47340: loss: 0.2158753126859665\n",
      "iteration 47341: loss: 0.21587523818016052\n",
      "iteration 47342: loss: 0.21587499976158142\n",
      "iteration 47343: loss: 0.21587488055229187\n",
      "iteration 47344: loss: 0.21587474644184113\n",
      "iteration 47345: loss: 0.2158745527267456\n",
      "iteration 47346: loss: 0.21587438881397247\n",
      "iteration 47347: loss: 0.21587428450584412\n",
      "iteration 47348: loss: 0.21587419509887695\n",
      "iteration 47349: loss: 0.21587400138378143\n",
      "iteration 47350: loss: 0.2158738672733307\n",
      "iteration 47351: loss: 0.21587374806404114\n",
      "iteration 47352: loss: 0.2158735692501068\n",
      "iteration 47353: loss: 0.21587340533733368\n",
      "iteration 47354: loss: 0.21587331593036652\n",
      "iteration 47355: loss: 0.215873122215271\n",
      "iteration 47356: loss: 0.21587303280830383\n",
      "iteration 47357: loss: 0.2158728539943695\n",
      "iteration 47358: loss: 0.21587267518043518\n",
      "iteration 47359: loss: 0.21587255597114563\n",
      "iteration 47360: loss: 0.21587245166301727\n",
      "iteration 47361: loss: 0.21587221324443817\n",
      "iteration 47362: loss: 0.21587209403514862\n",
      "iteration 47363: loss: 0.21587197482585907\n",
      "iteration 47364: loss: 0.21587184071540833\n",
      "iteration 47365: loss: 0.2158716917037964\n",
      "iteration 47366: loss: 0.21587157249450684\n",
      "iteration 47367: loss: 0.2158714234828949\n",
      "iteration 47368: loss: 0.21587124466896057\n",
      "iteration 47369: loss: 0.21587112545967102\n",
      "iteration 47370: loss: 0.21587105095386505\n",
      "iteration 47371: loss: 0.21587081253528595\n",
      "iteration 47372: loss: 0.215870663523674\n",
      "iteration 47373: loss: 0.21587049961090088\n",
      "iteration 47374: loss: 0.21587033569812775\n",
      "iteration 47375: loss: 0.21587026119232178\n",
      "iteration 47376: loss: 0.21587009727954865\n",
      "iteration 47377: loss: 0.2158699482679367\n",
      "iteration 47378: loss: 0.21586985886096954\n",
      "iteration 47379: loss: 0.21586966514587402\n",
      "iteration 47380: loss: 0.21586954593658447\n",
      "iteration 47381: loss: 0.21586935222148895\n",
      "iteration 47382: loss: 0.21586915850639343\n",
      "iteration 47383: loss: 0.21586906909942627\n",
      "iteration 47384: loss: 0.21586894989013672\n",
      "iteration 47385: loss: 0.2158687561750412\n",
      "iteration 47386: loss: 0.21586866676807404\n",
      "iteration 47387: loss: 0.2158685028553009\n",
      "iteration 47388: loss: 0.21586838364601135\n",
      "iteration 47389: loss: 0.21586820483207703\n",
      "iteration 47390: loss: 0.21586807072162628\n",
      "iteration 47391: loss: 0.21586796641349792\n",
      "iteration 47392: loss: 0.21586784720420837\n",
      "iteration 47393: loss: 0.21586759388446808\n",
      "iteration 47394: loss: 0.2158675491809845\n",
      "iteration 47395: loss: 0.21586737036705017\n",
      "iteration 47396: loss: 0.21586719155311584\n",
      "iteration 47397: loss: 0.2158670723438263\n",
      "iteration 47398: loss: 0.21586689352989197\n",
      "iteration 47399: loss: 0.21586675941944122\n",
      "iteration 47400: loss: 0.21586665511131287\n",
      "iteration 47401: loss: 0.21586649119853973\n",
      "iteration 47402: loss: 0.21586628258228302\n",
      "iteration 47403: loss: 0.21586613357067108\n",
      "iteration 47404: loss: 0.21586604416370392\n",
      "iteration 47405: loss: 0.2158658802509308\n",
      "iteration 47406: loss: 0.21586573123931885\n",
      "iteration 47407: loss: 0.21586556732654572\n",
      "iteration 47408: loss: 0.21586546301841736\n",
      "iteration 47409: loss: 0.21586528420448303\n",
      "iteration 47410: loss: 0.2158651351928711\n",
      "iteration 47411: loss: 0.21586498618125916\n",
      "iteration 47412: loss: 0.21586492657661438\n",
      "iteration 47413: loss: 0.21586473286151886\n",
      "iteration 47414: loss: 0.21586456894874573\n",
      "iteration 47415: loss: 0.2158643901348114\n",
      "iteration 47416: loss: 0.21586422622203827\n",
      "iteration 47417: loss: 0.21586410701274872\n",
      "iteration 47418: loss: 0.21586401760578156\n",
      "iteration 47419: loss: 0.21586379408836365\n",
      "iteration 47420: loss: 0.2158636599779129\n",
      "iteration 47421: loss: 0.21586351096630096\n",
      "iteration 47422: loss: 0.21586346626281738\n",
      "iteration 47423: loss: 0.21586327254772186\n",
      "iteration 47424: loss: 0.21586313843727112\n",
      "iteration 47425: loss: 0.2158629447221756\n",
      "iteration 47426: loss: 0.21586279571056366\n",
      "iteration 47427: loss: 0.21586260199546814\n",
      "iteration 47428: loss: 0.21586254239082336\n",
      "iteration 47429: loss: 0.2158624231815338\n",
      "iteration 47430: loss: 0.2158622294664383\n",
      "iteration 47431: loss: 0.21586212515830994\n",
      "iteration 47432: loss: 0.2158619463443756\n",
      "iteration 47433: loss: 0.2158617526292801\n",
      "iteration 47434: loss: 0.21586163341999054\n",
      "iteration 47435: loss: 0.215861514210701\n",
      "iteration 47436: loss: 0.21586132049560547\n",
      "iteration 47437: loss: 0.21586117148399353\n",
      "iteration 47438: loss: 0.21586111187934875\n",
      "iteration 47439: loss: 0.21586088836193085\n",
      "iteration 47440: loss: 0.21586081385612488\n",
      "iteration 47441: loss: 0.21586057543754578\n",
      "iteration 47442: loss: 0.215860515832901\n",
      "iteration 47443: loss: 0.21586039662361145\n",
      "iteration 47444: loss: 0.21586021780967712\n",
      "iteration 47445: loss: 0.21585997939109802\n",
      "iteration 47446: loss: 0.21585986018180847\n",
      "iteration 47447: loss: 0.21585972607135773\n",
      "iteration 47448: loss: 0.2158595323562622\n",
      "iteration 47449: loss: 0.21585950255393982\n",
      "iteration 47450: loss: 0.21585926413536072\n",
      "iteration 47451: loss: 0.21585917472839355\n",
      "iteration 47452: loss: 0.21585902571678162\n",
      "iteration 47453: loss: 0.2158588171005249\n",
      "iteration 47454: loss: 0.2158588171005249\n",
      "iteration 47455: loss: 0.21585862338542938\n",
      "iteration 47456: loss: 0.21585841476917267\n",
      "iteration 47457: loss: 0.21585829555988312\n",
      "iteration 47458: loss: 0.21585813164710999\n",
      "iteration 47459: loss: 0.21585795283317566\n",
      "iteration 47460: loss: 0.21585789322853088\n",
      "iteration 47461: loss: 0.21585769951343536\n",
      "iteration 47462: loss: 0.21585755050182343\n",
      "iteration 47463: loss: 0.21585746109485626\n",
      "iteration 47464: loss: 0.21585723757743835\n",
      "iteration 47465: loss: 0.2158571034669876\n",
      "iteration 47466: loss: 0.21585693955421448\n",
      "iteration 47467: loss: 0.21585679054260254\n",
      "iteration 47468: loss: 0.21585671603679657\n",
      "iteration 47469: loss: 0.21585650742053986\n",
      "iteration 47470: loss: 0.2158563882112503\n",
      "iteration 47471: loss: 0.21585623919963837\n",
      "iteration 47472: loss: 0.21585611999034882\n",
      "iteration 47473: loss: 0.21585583686828613\n",
      "iteration 47474: loss: 0.21585576236248016\n",
      "iteration 47475: loss: 0.2158556431531906\n",
      "iteration 47476: loss: 0.21585550904273987\n",
      "iteration 47477: loss: 0.21585533022880554\n",
      "iteration 47478: loss: 0.2158551663160324\n",
      "iteration 47479: loss: 0.21585504710674286\n",
      "iteration 47480: loss: 0.21585488319396973\n",
      "iteration 47481: loss: 0.21585476398468018\n",
      "iteration 47482: loss: 0.21585461497306824\n",
      "iteration 47483: loss: 0.21585440635681152\n",
      "iteration 47484: loss: 0.21585431694984436\n",
      "iteration 47485: loss: 0.21585413813591003\n",
      "iteration 47486: loss: 0.21585401892662048\n",
      "iteration 47487: loss: 0.21585388481616974\n",
      "iteration 47488: loss: 0.2158537656068802\n",
      "iteration 47489: loss: 0.21585360169410706\n",
      "iteration 47490: loss: 0.2158534824848175\n",
      "iteration 47491: loss: 0.21585328876972198\n",
      "iteration 47492: loss: 0.21585321426391602\n",
      "iteration 47493: loss: 0.2158530205488205\n",
      "iteration 47494: loss: 0.21585285663604736\n",
      "iteration 47495: loss: 0.2158527374267578\n",
      "iteration 47496: loss: 0.21585257351398468\n",
      "iteration 47497: loss: 0.21585246920585632\n",
      "iteration 47498: loss: 0.2158523052930832\n",
      "iteration 47499: loss: 0.21585217118263245\n",
      "iteration 47500: loss: 0.2158520221710205\n",
      "iteration 47501: loss: 0.215851828455925\n",
      "iteration 47502: loss: 0.21585170924663544\n",
      "iteration 47503: loss: 0.2158515453338623\n",
      "iteration 47504: loss: 0.21585139632225037\n",
      "iteration 47505: loss: 0.21585126221179962\n",
      "iteration 47506: loss: 0.21585114300251007\n",
      "iteration 47507: loss: 0.21585087478160858\n",
      "iteration 47508: loss: 0.2158508002758026\n",
      "iteration 47509: loss: 0.21585068106651306\n",
      "iteration 47510: loss: 0.21585050225257874\n",
      "iteration 47511: loss: 0.21585039794445038\n",
      "iteration 47512: loss: 0.21585026383399963\n",
      "iteration 47513: loss: 0.21585004031658173\n",
      "iteration 47514: loss: 0.21584992110729218\n",
      "iteration 47515: loss: 0.215849831700325\n",
      "iteration 47516: loss: 0.21584966778755188\n",
      "iteration 47517: loss: 0.21584954857826233\n",
      "iteration 47518: loss: 0.21584932506084442\n",
      "iteration 47519: loss: 0.21584920585155487\n",
      "iteration 47520: loss: 0.21584904193878174\n",
      "iteration 47521: loss: 0.2158489227294922\n",
      "iteration 47522: loss: 0.21584877371788025\n",
      "iteration 47523: loss: 0.2158486396074295\n",
      "iteration 47524: loss: 0.21584849059581757\n",
      "iteration 47525: loss: 0.21584837138652802\n",
      "iteration 47526: loss: 0.2158481627702713\n",
      "iteration 47527: loss: 0.21584804356098175\n",
      "iteration 47528: loss: 0.2158479243516922\n",
      "iteration 47529: loss: 0.21584782004356384\n",
      "iteration 47530: loss: 0.21584753692150116\n",
      "iteration 47531: loss: 0.2158474177122116\n",
      "iteration 47532: loss: 0.21584737300872803\n",
      "iteration 47533: loss: 0.21584706008434296\n",
      "iteration 47534: loss: 0.21584701538085938\n",
      "iteration 47535: loss: 0.21584682166576385\n",
      "iteration 47536: loss: 0.2158467024564743\n",
      "iteration 47537: loss: 0.21584653854370117\n",
      "iteration 47538: loss: 0.2158464640378952\n",
      "iteration 47539: loss: 0.2158462554216385\n",
      "iteration 47540: loss: 0.21584613621234894\n",
      "iteration 47541: loss: 0.215845987200737\n",
      "iteration 47542: loss: 0.21584582328796387\n",
      "iteration 47543: loss: 0.2158457338809967\n",
      "iteration 47544: loss: 0.21584554016590118\n",
      "iteration 47545: loss: 0.21584539115428925\n",
      "iteration 47546: loss: 0.2158452570438385\n",
      "iteration 47547: loss: 0.21584510803222656\n",
      "iteration 47548: loss: 0.21584494411945343\n",
      "iteration 47549: loss: 0.21584482491016388\n",
      "iteration 47550: loss: 0.21584467589855194\n",
      "iteration 47551: loss: 0.2158445417881012\n",
      "iteration 47552: loss: 0.21584434807300568\n",
      "iteration 47553: loss: 0.21584422886371613\n",
      "iteration 47554: loss: 0.21584410965442657\n",
      "iteration 47555: loss: 0.21584394574165344\n",
      "iteration 47556: loss: 0.21584376692771912\n",
      "iteration 47557: loss: 0.21584370732307434\n",
      "iteration 47558: loss: 0.21584352850914001\n",
      "iteration 47559: loss: 0.2158433496952057\n",
      "iteration 47560: loss: 0.21584323048591614\n",
      "iteration 47561: loss: 0.2158430814743042\n",
      "iteration 47562: loss: 0.21584288775920868\n",
      "iteration 47563: loss: 0.21584276854991913\n",
      "iteration 47564: loss: 0.215842604637146\n",
      "iteration 47565: loss: 0.21584245562553406\n",
      "iteration 47566: loss: 0.21584229171276093\n",
      "iteration 47567: loss: 0.21584215760231018\n",
      "iteration 47568: loss: 0.21584203839302063\n",
      "iteration 47569: loss: 0.21584191918373108\n",
      "iteration 47570: loss: 0.21584172546863556\n",
      "iteration 47571: loss: 0.21584153175354004\n",
      "iteration 47572: loss: 0.2158414125442505\n",
      "iteration 47573: loss: 0.21584124863147736\n",
      "iteration 47574: loss: 0.2158411294221878\n",
      "iteration 47575: loss: 0.21584101021289825\n",
      "iteration 47576: loss: 0.21584086120128632\n",
      "iteration 47577: loss: 0.21584072709083557\n",
      "iteration 47578: loss: 0.21584053337574005\n",
      "iteration 47579: loss: 0.2158404290676117\n",
      "iteration 47580: loss: 0.21584025025367737\n",
      "iteration 47581: loss: 0.21584007143974304\n",
      "iteration 47582: loss: 0.21583998203277588\n",
      "iteration 47583: loss: 0.21583986282348633\n",
      "iteration 47584: loss: 0.2158396989107132\n",
      "iteration 47585: loss: 0.21583953499794006\n",
      "iteration 47586: loss: 0.2158394157886505\n",
      "iteration 47587: loss: 0.21583926677703857\n",
      "iteration 47588: loss: 0.21583910286426544\n",
      "iteration 47589: loss: 0.2158389538526535\n",
      "iteration 47590: loss: 0.21583881974220276\n",
      "iteration 47591: loss: 0.21583867073059082\n",
      "iteration 47592: loss: 0.21583858132362366\n",
      "iteration 47593: loss: 0.21583834290504456\n",
      "iteration 47594: loss: 0.21583816409111023\n",
      "iteration 47595: loss: 0.21583802998065948\n",
      "iteration 47596: loss: 0.21583795547485352\n",
      "iteration 47597: loss: 0.21583780646324158\n",
      "iteration 47598: loss: 0.21583764255046844\n",
      "iteration 47599: loss: 0.2158374786376953\n",
      "iteration 47600: loss: 0.2158372700214386\n",
      "iteration 47601: loss: 0.21583715081214905\n",
      "iteration 47602: loss: 0.2158370316028595\n",
      "iteration 47603: loss: 0.21583691239356995\n",
      "iteration 47604: loss: 0.21583668887615204\n",
      "iteration 47605: loss: 0.2158365696668625\n",
      "iteration 47606: loss: 0.21583637595176697\n",
      "iteration 47607: loss: 0.2158362865447998\n",
      "iteration 47608: loss: 0.21583612263202667\n",
      "iteration 47609: loss: 0.21583600342273712\n",
      "iteration 47610: loss: 0.21583585441112518\n",
      "iteration 47611: loss: 0.21583566069602966\n",
      "iteration 47612: loss: 0.21583549678325653\n",
      "iteration 47613: loss: 0.21583537757396698\n",
      "iteration 47614: loss: 0.21583518385887146\n",
      "iteration 47615: loss: 0.21583504974842072\n",
      "iteration 47616: loss: 0.21583493053913116\n",
      "iteration 47617: loss: 0.2158348560333252\n",
      "iteration 47618: loss: 0.21583469212055206\n",
      "iteration 47619: loss: 0.21583449840545654\n",
      "iteration 47620: loss: 0.2158343344926834\n",
      "iteration 47621: loss: 0.21583423018455505\n",
      "iteration 47622: loss: 0.21583406627178192\n",
      "iteration 47623: loss: 0.21583397686481476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 47624: loss: 0.21583375334739685\n",
      "iteration 47625: loss: 0.2158336341381073\n",
      "iteration 47626: loss: 0.21583351492881775\n",
      "iteration 47627: loss: 0.21583330631256104\n",
      "iteration 47628: loss: 0.21583321690559387\n",
      "iteration 47629: loss: 0.21583306789398193\n",
      "iteration 47630: loss: 0.2158329039812088\n",
      "iteration 47631: loss: 0.21583275496959686\n",
      "iteration 47632: loss: 0.21583262085914612\n",
      "iteration 47633: loss: 0.21583250164985657\n",
      "iteration 47634: loss: 0.21583227813243866\n",
      "iteration 47635: loss: 0.2158321887254715\n",
      "iteration 47636: loss: 0.21583203971385956\n",
      "iteration 47637: loss: 0.2158319056034088\n",
      "iteration 47638: loss: 0.21583180129528046\n",
      "iteration 47639: loss: 0.21583163738250732\n",
      "iteration 47640: loss: 0.21583148837089539\n",
      "iteration 47641: loss: 0.21583127975463867\n",
      "iteration 47642: loss: 0.21583113074302673\n",
      "iteration 47643: loss: 0.215830996632576\n",
      "iteration 47644: loss: 0.21583083271980286\n",
      "iteration 47645: loss: 0.2158307284116745\n",
      "iteration 47646: loss: 0.21583051979541779\n",
      "iteration 47647: loss: 0.21583041548728943\n",
      "iteration 47648: loss: 0.21583028137683868\n",
      "iteration 47649: loss: 0.21583005785942078\n",
      "iteration 47650: loss: 0.21582993865013123\n",
      "iteration 47651: loss: 0.21582987904548645\n",
      "iteration 47652: loss: 0.21582965552806854\n",
      "iteration 47653: loss: 0.2158295214176178\n",
      "iteration 47654: loss: 0.21582941710948944\n",
      "iteration 47655: loss: 0.21582917869091034\n",
      "iteration 47656: loss: 0.21582913398742676\n",
      "iteration 47657: loss: 0.21582894027233124\n",
      "iteration 47658: loss: 0.21582885086536407\n",
      "iteration 47659: loss: 0.21582862734794617\n",
      "iteration 47660: loss: 0.21582849323749542\n",
      "iteration 47661: loss: 0.21582834422588348\n",
      "iteration 47662: loss: 0.21582821011543274\n",
      "iteration 47663: loss: 0.2158280313014984\n",
      "iteration 47664: loss: 0.21582789719104767\n",
      "iteration 47665: loss: 0.21582770347595215\n",
      "iteration 47666: loss: 0.21582762897014618\n",
      "iteration 47667: loss: 0.21582742035388947\n",
      "iteration 47668: loss: 0.21582730114459991\n",
      "iteration 47669: loss: 0.21582713723182678\n",
      "iteration 47670: loss: 0.21582701802253723\n",
      "iteration 47671: loss: 0.2158268690109253\n",
      "iteration 47672: loss: 0.21582674980163574\n",
      "iteration 47673: loss: 0.21582655608654022\n",
      "iteration 47674: loss: 0.21582643687725067\n",
      "iteration 47675: loss: 0.21582627296447754\n",
      "iteration 47676: loss: 0.21582618355751038\n",
      "iteration 47677: loss: 0.21582596004009247\n",
      "iteration 47678: loss: 0.21582582592964172\n",
      "iteration 47679: loss: 0.21582572162151337\n",
      "iteration 47680: loss: 0.21582551300525665\n",
      "iteration 47681: loss: 0.21582536399364471\n",
      "iteration 47682: loss: 0.21582527458667755\n",
      "iteration 47683: loss: 0.21582511067390442\n",
      "iteration 47684: loss: 0.21582496166229248\n",
      "iteration 47685: loss: 0.21582479774951935\n",
      "iteration 47686: loss: 0.21582463383674622\n",
      "iteration 47687: loss: 0.21582451462745667\n",
      "iteration 47688: loss: 0.21582433581352234\n",
      "iteration 47689: loss: 0.21582429111003876\n",
      "iteration 47690: loss: 0.21582405269145966\n",
      "iteration 47691: loss: 0.21582388877868652\n",
      "iteration 47692: loss: 0.21582379937171936\n",
      "iteration 47693: loss: 0.21582357585430145\n",
      "iteration 47694: loss: 0.2158234417438507\n",
      "iteration 47695: loss: 0.21582336723804474\n",
      "iteration 47696: loss: 0.21582317352294922\n",
      "iteration 47697: loss: 0.21582302451133728\n",
      "iteration 47698: loss: 0.21582289040088654\n",
      "iteration 47699: loss: 0.2158227264881134\n",
      "iteration 47700: loss: 0.21582257747650146\n",
      "iteration 47701: loss: 0.2158224880695343\n",
      "iteration 47702: loss: 0.21582230925559998\n",
      "iteration 47703: loss: 0.21582217514514923\n",
      "iteration 47704: loss: 0.2158220261335373\n",
      "iteration 47705: loss: 0.21582181751728058\n",
      "iteration 47706: loss: 0.21582169830799103\n",
      "iteration 47707: loss: 0.2158215492963791\n",
      "iteration 47708: loss: 0.21582141518592834\n",
      "iteration 47709: loss: 0.21582123637199402\n",
      "iteration 47710: loss: 0.21582111716270447\n",
      "iteration 47711: loss: 0.21582095324993134\n",
      "iteration 47712: loss: 0.21582083404064178\n",
      "iteration 47713: loss: 0.21582069993019104\n",
      "iteration 47714: loss: 0.21582047641277313\n",
      "iteration 47715: loss: 0.2158202826976776\n",
      "iteration 47716: loss: 0.21582023799419403\n",
      "iteration 47717: loss: 0.2158200740814209\n",
      "iteration 47718: loss: 0.21581992506980896\n",
      "iteration 47719: loss: 0.21581974625587463\n",
      "iteration 47720: loss: 0.21581962704658508\n",
      "iteration 47721: loss: 0.21581947803497314\n",
      "iteration 47722: loss: 0.21581926941871643\n",
      "iteration 47723: loss: 0.21581916511058807\n",
      "iteration 47724: loss: 0.21581903100013733\n",
      "iteration 47725: loss: 0.215818852186203\n",
      "iteration 47726: loss: 0.21581880748271942\n",
      "iteration 47727: loss: 0.2158185988664627\n",
      "iteration 47728: loss: 0.21581849455833435\n",
      "iteration 47729: loss: 0.21581825613975525\n",
      "iteration 47730: loss: 0.21581816673278809\n",
      "iteration 47731: loss: 0.21581797301769257\n",
      "iteration 47732: loss: 0.21581780910491943\n",
      "iteration 47733: loss: 0.21581768989562988\n",
      "iteration 47734: loss: 0.21581754088401794\n",
      "iteration 47735: loss: 0.2158174216747284\n",
      "iteration 47736: loss: 0.21581725776195526\n",
      "iteration 47737: loss: 0.2158171683549881\n",
      "iteration 47738: loss: 0.2158169448375702\n",
      "iteration 47739: loss: 0.21581673622131348\n",
      "iteration 47740: loss: 0.2158166468143463\n",
      "iteration 47741: loss: 0.2158164083957672\n",
      "iteration 47742: loss: 0.21581633388996124\n",
      "iteration 47743: loss: 0.2158161848783493\n",
      "iteration 47744: loss: 0.2158161699771881\n",
      "iteration 47745: loss: 0.21581590175628662\n",
      "iteration 47746: loss: 0.21581582725048065\n",
      "iteration 47747: loss: 0.21581561863422394\n",
      "iteration 47748: loss: 0.2158154547214508\n",
      "iteration 47749: loss: 0.21581530570983887\n",
      "iteration 47750: loss: 0.21581518650054932\n",
      "iteration 47751: loss: 0.21581502258777618\n",
      "iteration 47752: loss: 0.21581485867500305\n",
      "iteration 47753: loss: 0.21581467986106873\n",
      "iteration 47754: loss: 0.21581462025642395\n",
      "iteration 47755: loss: 0.21581444144248962\n",
      "iteration 47756: loss: 0.21581430733203888\n",
      "iteration 47757: loss: 0.21581414341926575\n",
      "iteration 47758: loss: 0.2158139944076538\n",
      "iteration 47759: loss: 0.21581387519836426\n",
      "iteration 47760: loss: 0.21581366658210754\n",
      "iteration 47761: loss: 0.21581363677978516\n",
      "iteration 47762: loss: 0.21581339836120605\n",
      "iteration 47763: loss: 0.2158133089542389\n",
      "iteration 47764: loss: 0.21581311523914337\n",
      "iteration 47765: loss: 0.21581301093101501\n",
      "iteration 47766: loss: 0.2158128023147583\n",
      "iteration 47767: loss: 0.21581263840198517\n",
      "iteration 47768: loss: 0.21581251919269562\n",
      "iteration 47769: loss: 0.21581241488456726\n",
      "iteration 47770: loss: 0.21581216156482697\n",
      "iteration 47771: loss: 0.2158120572566986\n",
      "iteration 47772: loss: 0.21581192314624786\n",
      "iteration 47773: loss: 0.21581172943115234\n",
      "iteration 47774: loss: 0.21581165492534637\n",
      "iteration 47775: loss: 0.21581146121025085\n",
      "iteration 47776: loss: 0.2158113420009613\n",
      "iteration 47777: loss: 0.21581120789051056\n",
      "iteration 47778: loss: 0.21581105887889862\n",
      "iteration 47779: loss: 0.21581092476844788\n",
      "iteration 47780: loss: 0.21581074595451355\n",
      "iteration 47781: loss: 0.2158106565475464\n",
      "iteration 47782: loss: 0.21581041812896729\n",
      "iteration 47783: loss: 0.21581029891967773\n",
      "iteration 47784: loss: 0.21581010520458221\n",
      "iteration 47785: loss: 0.21580998599529266\n",
      "iteration 47786: loss: 0.2158098965883255\n",
      "iteration 47787: loss: 0.21580977737903595\n",
      "iteration 47788: loss: 0.21580953896045685\n",
      "iteration 47789: loss: 0.21580934524536133\n",
      "iteration 47790: loss: 0.21580930054187775\n",
      "iteration 47791: loss: 0.2158091515302658\n",
      "iteration 47792: loss: 0.2158089578151703\n",
      "iteration 47793: loss: 0.21580882370471954\n",
      "iteration 47794: loss: 0.2158087193965912\n",
      "iteration 47795: loss: 0.2158084660768509\n",
      "iteration 47796: loss: 0.2158084362745285\n",
      "iteration 47797: loss: 0.2158082276582718\n",
      "iteration 47798: loss: 0.21580806374549866\n",
      "iteration 47799: loss: 0.2158079594373703\n",
      "iteration 47800: loss: 0.2158077210187912\n",
      "iteration 47801: loss: 0.21580763161182404\n",
      "iteration 47802: loss: 0.2158074826002121\n",
      "iteration 47803: loss: 0.21580728888511658\n",
      "iteration 47804: loss: 0.21580715477466583\n",
      "iteration 47805: loss: 0.21580705046653748\n",
      "iteration 47806: loss: 0.21580684185028076\n",
      "iteration 47807: loss: 0.2158067226409912\n",
      "iteration 47808: loss: 0.21580664813518524\n",
      "iteration 47809: loss: 0.2158065140247345\n",
      "iteration 47810: loss: 0.2158062905073166\n",
      "iteration 47811: loss: 0.21580615639686584\n",
      "iteration 47812: loss: 0.2158060073852539\n",
      "iteration 47813: loss: 0.2158058136701584\n",
      "iteration 47814: loss: 0.21580572426319122\n",
      "iteration 47815: loss: 0.2158055305480957\n",
      "iteration 47816: loss: 0.21580533683300018\n",
      "iteration 47817: loss: 0.21580521762371063\n",
      "iteration 47818: loss: 0.21580509841442108\n",
      "iteration 47819: loss: 0.21580496430397034\n",
      "iteration 47820: loss: 0.21580474078655243\n",
      "iteration 47821: loss: 0.21580462157726288\n",
      "iteration 47822: loss: 0.21580450236797333\n",
      "iteration 47823: loss: 0.2158043086528778\n",
      "iteration 47824: loss: 0.21580424904823303\n",
      "iteration 47825: loss: 0.21580402553081512\n",
      "iteration 47826: loss: 0.21580389142036438\n",
      "iteration 47827: loss: 0.21580374240875244\n",
      "iteration 47828: loss: 0.2158035784959793\n",
      "iteration 47829: loss: 0.21580345928668976\n",
      "iteration 47830: loss: 0.21580329537391663\n",
      "iteration 47831: loss: 0.2158031165599823\n",
      "iteration 47832: loss: 0.21580299735069275\n",
      "iteration 47833: loss: 0.21580290794372559\n",
      "iteration 47834: loss: 0.21580271422863007\n",
      "iteration 47835: loss: 0.21580252051353455\n",
      "iteration 47836: loss: 0.21580243110656738\n",
      "iteration 47837: loss: 0.21580226719379425\n",
      "iteration 47838: loss: 0.2158021479845047\n",
      "iteration 47839: loss: 0.21580198407173157\n",
      "iteration 47840: loss: 0.21580176055431366\n",
      "iteration 47841: loss: 0.21580171585083008\n",
      "iteration 47842: loss: 0.21580156683921814\n",
      "iteration 47843: loss: 0.2158013880252838\n",
      "iteration 47844: loss: 0.21580123901367188\n",
      "iteration 47845: loss: 0.21580109000205994\n",
      "iteration 47846: loss: 0.2158009111881256\n",
      "iteration 47847: loss: 0.21580080687999725\n",
      "iteration 47848: loss: 0.21580064296722412\n",
      "iteration 47849: loss: 0.21580052375793457\n",
      "iteration 47850: loss: 0.21580035984516144\n",
      "iteration 47851: loss: 0.2158001959323883\n",
      "iteration 47852: loss: 0.21580004692077637\n",
      "iteration 47853: loss: 0.21579988300800323\n",
      "iteration 47854: loss: 0.2157997190952301\n",
      "iteration 47855: loss: 0.21579959988594055\n",
      "iteration 47856: loss: 0.2157994508743286\n",
      "iteration 47857: loss: 0.21579928696155548\n",
      "iteration 47858: loss: 0.21579913794994354\n",
      "iteration 47859: loss: 0.2157990038394928\n",
      "iteration 47860: loss: 0.21579885482788086\n",
      "iteration 47861: loss: 0.21579866111278534\n",
      "iteration 47862: loss: 0.2157985419034958\n",
      "iteration 47863: loss: 0.21579837799072266\n",
      "iteration 47864: loss: 0.21579818427562714\n",
      "iteration 47865: loss: 0.2157980501651764\n",
      "iteration 47866: loss: 0.21579799056053162\n",
      "iteration 47867: loss: 0.21579785645008087\n",
      "iteration 47868: loss: 0.21579766273498535\n",
      "iteration 47869: loss: 0.2157975435256958\n",
      "iteration 47870: loss: 0.21579737961292267\n",
      "iteration 47871: loss: 0.21579726040363312\n",
      "iteration 47872: loss: 0.2157970666885376\n",
      "iteration 47873: loss: 0.21579690277576447\n",
      "iteration 47874: loss: 0.2157968282699585\n",
      "iteration 47875: loss: 0.21579663455486298\n",
      "iteration 47876: loss: 0.21579650044441223\n",
      "iteration 47877: loss: 0.2157963067293167\n",
      "iteration 47878: loss: 0.21579615771770477\n",
      "iteration 47879: loss: 0.2157960683107376\n",
      "iteration 47880: loss: 0.21579590439796448\n",
      "iteration 47881: loss: 0.21579571068286896\n",
      "iteration 47882: loss: 0.21579554677009583\n",
      "iteration 47883: loss: 0.2157953679561615\n",
      "iteration 47884: loss: 0.21579530835151672\n",
      "iteration 47885: loss: 0.2157951295375824\n",
      "iteration 47886: loss: 0.21579501032829285\n",
      "iteration 47887: loss: 0.2157948762178421\n",
      "iteration 47888: loss: 0.21579472720623016\n",
      "iteration 47889: loss: 0.21579451858997345\n",
      "iteration 47890: loss: 0.2157944142818451\n",
      "iteration 47891: loss: 0.21579428017139435\n",
      "iteration 47892: loss: 0.21579405665397644\n",
      "iteration 47893: loss: 0.2157938927412033\n",
      "iteration 47894: loss: 0.21579377353191376\n",
      "iteration 47895: loss: 0.21579357981681824\n",
      "iteration 47896: loss: 0.21579353511333466\n",
      "iteration 47897: loss: 0.21579329669475555\n",
      "iteration 47898: loss: 0.2157931625843048\n",
      "iteration 47899: loss: 0.21579305827617645\n",
      "iteration 47900: loss: 0.21579289436340332\n",
      "iteration 47901: loss: 0.21579274535179138\n",
      "iteration 47902: loss: 0.21579261124134064\n",
      "iteration 47903: loss: 0.2157924473285675\n",
      "iteration 47904: loss: 0.2157922089099884\n",
      "iteration 47905: loss: 0.21579213440418243\n",
      "iteration 47906: loss: 0.21579205989837646\n",
      "iteration 47907: loss: 0.21579182147979736\n",
      "iteration 47908: loss: 0.21579165756702423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 47909: loss: 0.21579153835773468\n",
      "iteration 47910: loss: 0.2157914638519287\n",
      "iteration 47911: loss: 0.2157912254333496\n",
      "iteration 47912: loss: 0.21579110622406006\n",
      "iteration 47913: loss: 0.2157909870147705\n",
      "iteration 47914: loss: 0.2157907783985138\n",
      "iteration 47915: loss: 0.21579065918922424\n",
      "iteration 47916: loss: 0.21579048037528992\n",
      "iteration 47917: loss: 0.21579031646251678\n",
      "iteration 47918: loss: 0.21579022705554962\n",
      "iteration 47919: loss: 0.21579007804393768\n",
      "iteration 47920: loss: 0.21578991413116455\n",
      "iteration 47921: loss: 0.2157897651195526\n",
      "iteration 47922: loss: 0.2157895863056183\n",
      "iteration 47923: loss: 0.21578948199748993\n",
      "iteration 47924: loss: 0.2157892882823944\n",
      "iteration 47925: loss: 0.21578915417194366\n",
      "iteration 47926: loss: 0.21578900516033173\n",
      "iteration 47927: loss: 0.21578887104988098\n",
      "iteration 47928: loss: 0.21578875184059143\n",
      "iteration 47929: loss: 0.21578843891620636\n",
      "iteration 47930: loss: 0.21578839421272278\n",
      "iteration 47931: loss: 0.21578824520111084\n",
      "iteration 47932: loss: 0.2157880812883377\n",
      "iteration 47933: loss: 0.21578791737556458\n",
      "iteration 47934: loss: 0.21578781306743622\n",
      "iteration 47935: loss: 0.21578769385814667\n",
      "iteration 47936: loss: 0.21578748524188995\n",
      "iteration 47937: loss: 0.21578733623027802\n",
      "iteration 47938: loss: 0.21578717231750488\n",
      "iteration 47939: loss: 0.21578700840473175\n",
      "iteration 47940: loss: 0.21578693389892578\n",
      "iteration 47941: loss: 0.21578676998615265\n",
      "iteration 47942: loss: 0.21578660607337952\n",
      "iteration 47943: loss: 0.2157864272594452\n",
      "iteration 47944: loss: 0.21578630805015564\n",
      "iteration 47945: loss: 0.2157861441373825\n",
      "iteration 47946: loss: 0.21578598022460938\n",
      "iteration 47947: loss: 0.21578583121299744\n",
      "iteration 47948: loss: 0.21578577160835266\n",
      "iteration 47949: loss: 0.21578553318977356\n",
      "iteration 47950: loss: 0.21578538417816162\n",
      "iteration 47951: loss: 0.21578523516654968\n",
      "iteration 47952: loss: 0.21578510105609894\n",
      "iteration 47953: loss: 0.2157849818468094\n",
      "iteration 47954: loss: 0.21578483283519745\n",
      "iteration 47955: loss: 0.2157847136259079\n",
      "iteration 47956: loss: 0.21578451991081238\n",
      "iteration 47957: loss: 0.21578438580036163\n",
      "iteration 47958: loss: 0.2157842367887497\n",
      "iteration 47959: loss: 0.21578410267829895\n",
      "iteration 47960: loss: 0.21578392386436462\n",
      "iteration 47961: loss: 0.2157837599515915\n",
      "iteration 47962: loss: 0.21578368544578552\n",
      "iteration 47963: loss: 0.2157834768295288\n",
      "iteration 47964: loss: 0.21578331291675568\n",
      "iteration 47965: loss: 0.21578316390514374\n",
      "iteration 47966: loss: 0.21578307449817657\n",
      "iteration 47967: loss: 0.21578285098075867\n",
      "iteration 47968: loss: 0.21578273177146912\n",
      "iteration 47969: loss: 0.2157825231552124\n",
      "iteration 47970: loss: 0.21578244864940643\n",
      "iteration 47971: loss: 0.21578235924243927\n",
      "iteration 47972: loss: 0.21578213572502136\n",
      "iteration 47973: loss: 0.21578195691108704\n",
      "iteration 47974: loss: 0.2157818078994751\n",
      "iteration 47975: loss: 0.21578171849250793\n",
      "iteration 47976: loss: 0.21578159928321838\n",
      "iteration 47977: loss: 0.21578137576580048\n",
      "iteration 47978: loss: 0.21578118205070496\n",
      "iteration 47979: loss: 0.2157810628414154\n",
      "iteration 47980: loss: 0.21578089892864227\n",
      "iteration 47981: loss: 0.21578077971935272\n",
      "iteration 47982: loss: 0.21578064560890198\n",
      "iteration 47983: loss: 0.21578045189380646\n",
      "iteration 47984: loss: 0.2157803326845169\n",
      "iteration 47985: loss: 0.21578016877174377\n",
      "iteration 47986: loss: 0.21578001976013184\n",
      "iteration 47987: loss: 0.21577993035316467\n",
      "iteration 47988: loss: 0.21577978134155273\n",
      "iteration 47989: loss: 0.21577958762645721\n",
      "iteration 47990: loss: 0.21577942371368408\n",
      "iteration 47991: loss: 0.21577925980091095\n",
      "iteration 47992: loss: 0.2157791405916214\n",
      "iteration 47993: loss: 0.21577902138233185\n",
      "iteration 47994: loss: 0.21577882766723633\n",
      "iteration 47995: loss: 0.2157786339521408\n",
      "iteration 47996: loss: 0.21577847003936768\n",
      "iteration 47997: loss: 0.2157783955335617\n",
      "iteration 47998: loss: 0.21577823162078857\n",
      "iteration 47999: loss: 0.21577811241149902\n",
      "iteration 48000: loss: 0.2157779484987259\n",
      "iteration 48001: loss: 0.21577778458595276\n",
      "iteration 48002: loss: 0.2157776802778244\n",
      "iteration 48003: loss: 0.21577748656272888\n",
      "iteration 48004: loss: 0.21577730774879456\n",
      "iteration 48005: loss: 0.2157772034406662\n",
      "iteration 48006: loss: 0.21577703952789307\n",
      "iteration 48007: loss: 0.21577687561511993\n",
      "iteration 48008: loss: 0.21577677130699158\n",
      "iteration 48009: loss: 0.21577659249305725\n",
      "iteration 48010: loss: 0.2157764881849289\n",
      "iteration 48011: loss: 0.21577629446983337\n",
      "iteration 48012: loss: 0.21577617526054382\n",
      "iteration 48013: loss: 0.2157759666442871\n",
      "iteration 48014: loss: 0.21577592194080353\n",
      "iteration 48015: loss: 0.2157757580280304\n",
      "iteration 48016: loss: 0.2157755196094513\n",
      "iteration 48017: loss: 0.21577544510364532\n",
      "iteration 48018: loss: 0.2157752513885498\n",
      "iteration 48019: loss: 0.21577510237693787\n",
      "iteration 48020: loss: 0.21577489376068115\n",
      "iteration 48021: loss: 0.2157747745513916\n",
      "iteration 48022: loss: 0.21577465534210205\n",
      "iteration 48023: loss: 0.21577444672584534\n",
      "iteration 48024: loss: 0.21577434241771698\n",
      "iteration 48025: loss: 0.21577425301074982\n",
      "iteration 48026: loss: 0.2157740294933319\n",
      "iteration 48027: loss: 0.21577391028404236\n",
      "iteration 48028: loss: 0.21577367186546326\n",
      "iteration 48029: loss: 0.21577361226081848\n",
      "iteration 48030: loss: 0.21577343344688416\n",
      "iteration 48031: loss: 0.21577326953411102\n",
      "iteration 48032: loss: 0.21577315032482147\n",
      "iteration 48033: loss: 0.21577295660972595\n",
      "iteration 48034: loss: 0.2157728374004364\n",
      "iteration 48035: loss: 0.21577267348766327\n",
      "iteration 48036: loss: 0.21577253937721252\n",
      "iteration 48037: loss: 0.2157723605632782\n",
      "iteration 48038: loss: 0.21577215194702148\n",
      "iteration 48039: loss: 0.21577207744121552\n",
      "iteration 48040: loss: 0.21577191352844238\n",
      "iteration 48041: loss: 0.21577179431915283\n",
      "iteration 48042: loss: 0.2157716006040573\n",
      "iteration 48043: loss: 0.21577146649360657\n",
      "iteration 48044: loss: 0.21577128767967224\n",
      "iteration 48045: loss: 0.21577122807502747\n",
      "iteration 48046: loss: 0.21577100455760956\n",
      "iteration 48047: loss: 0.2157708704471588\n",
      "iteration 48048: loss: 0.21577072143554688\n",
      "iteration 48049: loss: 0.21577055752277374\n",
      "iteration 48050: loss: 0.2157703936100006\n",
      "iteration 48051: loss: 0.21577024459838867\n",
      "iteration 48052: loss: 0.21577012538909912\n",
      "iteration 48053: loss: 0.2157699316740036\n",
      "iteration 48054: loss: 0.21576988697052002\n",
      "iteration 48055: loss: 0.2157696783542633\n",
      "iteration 48056: loss: 0.21576955914497375\n",
      "iteration 48057: loss: 0.21576936542987823\n",
      "iteration 48058: loss: 0.21576924622058868\n",
      "iteration 48059: loss: 0.2157689779996872\n",
      "iteration 48060: loss: 0.2157689332962036\n",
      "iteration 48061: loss: 0.2157687246799469\n",
      "iteration 48062: loss: 0.21576862037181854\n",
      "iteration 48063: loss: 0.2157684862613678\n",
      "iteration 48064: loss: 0.21576833724975586\n",
      "iteration 48065: loss: 0.2157682478427887\n",
      "iteration 48066: loss: 0.2157680094242096\n",
      "iteration 48067: loss: 0.21576790511608124\n",
      "iteration 48068: loss: 0.21576771140098572\n",
      "iteration 48069: loss: 0.21576757729053497\n",
      "iteration 48070: loss: 0.21576745808124542\n",
      "iteration 48071: loss: 0.21576730906963348\n",
      "iteration 48072: loss: 0.21576707065105438\n",
      "iteration 48073: loss: 0.21576690673828125\n",
      "iteration 48074: loss: 0.21576686203479767\n",
      "iteration 48075: loss: 0.21576666831970215\n",
      "iteration 48076: loss: 0.21576650440692902\n",
      "iteration 48077: loss: 0.2157663106918335\n",
      "iteration 48078: loss: 0.21576623618602753\n",
      "iteration 48079: loss: 0.2157660275697708\n",
      "iteration 48080: loss: 0.21576592326164246\n",
      "iteration 48081: loss: 0.21576574444770813\n",
      "iteration 48082: loss: 0.21576566994190216\n",
      "iteration 48083: loss: 0.21576552093029022\n",
      "iteration 48084: loss: 0.21576528251171112\n",
      "iteration 48085: loss: 0.21576514840126038\n",
      "iteration 48086: loss: 0.21576502919197083\n",
      "iteration 48087: loss: 0.2157648503780365\n",
      "iteration 48088: loss: 0.21576473116874695\n",
      "iteration 48089: loss: 0.2157646119594574\n",
      "iteration 48090: loss: 0.21576443314552307\n",
      "iteration 48091: loss: 0.21576425433158875\n",
      "iteration 48092: loss: 0.21576404571533203\n",
      "iteration 48093: loss: 0.2157638967037201\n",
      "iteration 48094: loss: 0.21576377749443054\n",
      "iteration 48095: loss: 0.21576371788978577\n",
      "iteration 48096: loss: 0.21576349437236786\n",
      "iteration 48097: loss: 0.2157634049654007\n",
      "iteration 48098: loss: 0.21576324105262756\n",
      "iteration 48099: loss: 0.21576304733753204\n",
      "iteration 48100: loss: 0.2157629281282425\n",
      "iteration 48101: loss: 0.2157626897096634\n",
      "iteration 48102: loss: 0.2157626897096634\n",
      "iteration 48103: loss: 0.2157624214887619\n",
      "iteration 48104: loss: 0.21576230227947235\n",
      "iteration 48105: loss: 0.21576210856437683\n",
      "iteration 48106: loss: 0.2157619446516037\n",
      "iteration 48107: loss: 0.21576189994812012\n",
      "iteration 48108: loss: 0.21576173603534698\n",
      "iteration 48109: loss: 0.21576158702373505\n",
      "iteration 48110: loss: 0.21576134860515594\n",
      "iteration 48111: loss: 0.2157612293958664\n",
      "iteration 48112: loss: 0.21576109528541565\n",
      "iteration 48113: loss: 0.2157609462738037\n",
      "iteration 48114: loss: 0.21576079726219177\n",
      "iteration 48115: loss: 0.21576066315174103\n",
      "iteration 48116: loss: 0.2157604694366455\n",
      "iteration 48117: loss: 0.21576032042503357\n",
      "iteration 48118: loss: 0.2157602310180664\n",
      "iteration 48119: loss: 0.21576006710529327\n",
      "iteration 48120: loss: 0.21575990319252014\n",
      "iteration 48121: loss: 0.21575972437858582\n",
      "iteration 48122: loss: 0.21575960516929626\n",
      "iteration 48123: loss: 0.21575942635536194\n",
      "iteration 48124: loss: 0.21575932204723358\n",
      "iteration 48125: loss: 0.21575912833213806\n",
      "iteration 48126: loss: 0.21575896441936493\n",
      "iteration 48127: loss: 0.21575884521007538\n",
      "iteration 48128: loss: 0.21575875580310822\n",
      "iteration 48129: loss: 0.21575848758220673\n",
      "iteration 48130: loss: 0.21575841307640076\n",
      "iteration 48131: loss: 0.21575823426246643\n",
      "iteration 48132: loss: 0.2157580852508545\n",
      "iteration 48133: loss: 0.21575792133808136\n",
      "iteration 48134: loss: 0.2157578021287918\n",
      "iteration 48135: loss: 0.21575768291950226\n",
      "iteration 48136: loss: 0.21575745940208435\n",
      "iteration 48137: loss: 0.21575728058815002\n",
      "iteration 48138: loss: 0.21575720608234406\n",
      "iteration 48139: loss: 0.21575693786144257\n",
      "iteration 48140: loss: 0.2157568633556366\n",
      "iteration 48141: loss: 0.21575669944286346\n",
      "iteration 48142: loss: 0.21575656533241272\n",
      "iteration 48143: loss: 0.21575644612312317\n",
      "iteration 48144: loss: 0.21575625240802765\n",
      "iteration 48145: loss: 0.2157561331987381\n",
      "iteration 48146: loss: 0.21575596928596497\n",
      "iteration 48147: loss: 0.21575585007667542\n",
      "iteration 48148: loss: 0.2157556265592575\n",
      "iteration 48149: loss: 0.21575555205345154\n",
      "iteration 48150: loss: 0.21575531363487244\n",
      "iteration 48151: loss: 0.21575525403022766\n",
      "iteration 48152: loss: 0.21575506031513214\n",
      "iteration 48153: loss: 0.2157549113035202\n",
      "iteration 48154: loss: 0.21575477719306946\n",
      "iteration 48155: loss: 0.21575462818145752\n",
      "iteration 48156: loss: 0.21575447916984558\n",
      "iteration 48157: loss: 0.21575430035591125\n",
      "iteration 48158: loss: 0.2157541811466217\n",
      "iteration 48159: loss: 0.21575410664081573\n",
      "iteration 48160: loss: 0.21575391292572021\n",
      "iteration 48161: loss: 0.2157537192106247\n",
      "iteration 48162: loss: 0.21575355529785156\n",
      "iteration 48163: loss: 0.21575339138507843\n",
      "iteration 48164: loss: 0.21575327217578888\n",
      "iteration 48165: loss: 0.21575304865837097\n",
      "iteration 48166: loss: 0.2157529890537262\n",
      "iteration 48167: loss: 0.2157527655363083\n",
      "iteration 48168: loss: 0.21575263142585754\n",
      "iteration 48169: loss: 0.2157524824142456\n",
      "iteration 48170: loss: 0.21575233340263367\n",
      "iteration 48171: loss: 0.21575215458869934\n",
      "iteration 48172: loss: 0.21575197577476501\n",
      "iteration 48173: loss: 0.21575188636779785\n",
      "iteration 48174: loss: 0.21575172245502472\n",
      "iteration 48175: loss: 0.21575160324573517\n",
      "iteration 48176: loss: 0.21575145423412323\n",
      "iteration 48177: loss: 0.21575133502483368\n",
      "iteration 48178: loss: 0.21575117111206055\n",
      "iteration 48179: loss: 0.21575097739696503\n",
      "iteration 48180: loss: 0.2157507836818695\n",
      "iteration 48181: loss: 0.21575069427490234\n",
      "iteration 48182: loss: 0.2157505750656128\n",
      "iteration 48183: loss: 0.21575042605400085\n",
      "iteration 48184: loss: 0.21575024724006653\n",
      "iteration 48185: loss: 0.2157500684261322\n",
      "iteration 48186: loss: 0.21574993431568146\n",
      "iteration 48187: loss: 0.2157498151063919\n",
      "iteration 48188: loss: 0.21574965119361877\n",
      "iteration 48189: loss: 0.21574945747852325\n",
      "iteration 48190: loss: 0.2157493531703949\n",
      "iteration 48191: loss: 0.2157491147518158\n",
      "iteration 48192: loss: 0.21574895083904266\n",
      "iteration 48193: loss: 0.21574890613555908\n",
      "iteration 48194: loss: 0.21574869751930237\n",
      "iteration 48195: loss: 0.21574854850769043\n",
      "iteration 48196: loss: 0.2157483845949173\n",
      "iteration 48197: loss: 0.21574831008911133\n",
      "iteration 48198: loss: 0.21574810147285461\n",
      "iteration 48199: loss: 0.21574795246124268\n",
      "iteration 48200: loss: 0.21574783325195312\n",
      "iteration 48201: loss: 0.21574766933918\n",
      "iteration 48202: loss: 0.21574752032756805\n",
      "iteration 48203: loss: 0.21574735641479492\n",
      "iteration 48204: loss: 0.21574720740318298\n",
      "iteration 48205: loss: 0.21574707329273224\n",
      "iteration 48206: loss: 0.2157469093799591\n",
      "iteration 48207: loss: 0.21574676036834717\n",
      "iteration 48208: loss: 0.21574664115905762\n",
      "iteration 48209: loss: 0.2157464474439621\n",
      "iteration 48210: loss: 0.21574628353118896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 48211: loss: 0.21574616432189941\n",
      "iteration 48212: loss: 0.2157459706068039\n",
      "iteration 48213: loss: 0.21574577689170837\n",
      "iteration 48214: loss: 0.2157457172870636\n",
      "iteration 48215: loss: 0.21574552357196808\n",
      "iteration 48216: loss: 0.21574537456035614\n",
      "iteration 48217: loss: 0.2157452553510666\n",
      "iteration 48218: loss: 0.21574509143829346\n",
      "iteration 48219: loss: 0.21574494242668152\n",
      "iteration 48220: loss: 0.2157447785139084\n",
      "iteration 48221: loss: 0.21574464440345764\n",
      "iteration 48222: loss: 0.2157444953918457\n",
      "iteration 48223: loss: 0.21574434638023376\n",
      "iteration 48224: loss: 0.21574421226978302\n",
      "iteration 48225: loss: 0.21574397385120392\n",
      "iteration 48226: loss: 0.21574382483959198\n",
      "iteration 48227: loss: 0.215743750333786\n",
      "iteration 48228: loss: 0.21574358642101288\n",
      "iteration 48229: loss: 0.21574345231056213\n",
      "iteration 48230: loss: 0.2157433032989502\n",
      "iteration 48231: loss: 0.21574309468269348\n",
      "iteration 48232: loss: 0.2157430201768875\n",
      "iteration 48233: loss: 0.2157427817583084\n",
      "iteration 48234: loss: 0.21574270725250244\n",
      "iteration 48235: loss: 0.21574246883392334\n",
      "iteration 48236: loss: 0.21574237942695618\n",
      "iteration 48237: loss: 0.2157423049211502\n",
      "iteration 48238: loss: 0.2157420814037323\n",
      "iteration 48239: loss: 0.21574190258979797\n",
      "iteration 48240: loss: 0.21574178338050842\n",
      "iteration 48241: loss: 0.2157416045665741\n",
      "iteration 48242: loss: 0.21574147045612335\n",
      "iteration 48243: loss: 0.21574124693870544\n",
      "iteration 48244: loss: 0.21574115753173828\n",
      "iteration 48245: loss: 0.21574099361896515\n",
      "iteration 48246: loss: 0.2157408446073532\n",
      "iteration 48247: loss: 0.2157406061887741\n",
      "iteration 48248: loss: 0.21574056148529053\n",
      "iteration 48249: loss: 0.2157403975725174\n",
      "iteration 48250: loss: 0.21574024856090546\n",
      "iteration 48251: loss: 0.2157401591539383\n",
      "iteration 48252: loss: 0.21573996543884277\n",
      "iteration 48253: loss: 0.21573981642723083\n",
      "iteration 48254: loss: 0.2157396376132965\n",
      "iteration 48255: loss: 0.21573951840400696\n",
      "iteration 48256: loss: 0.21573933959007263\n",
      "iteration 48257: loss: 0.21573913097381592\n",
      "iteration 48258: loss: 0.21573905646800995\n",
      "iteration 48259: loss: 0.21573889255523682\n",
      "iteration 48260: loss: 0.2157386988401413\n",
      "iteration 48261: loss: 0.21573850512504578\n",
      "iteration 48262: loss: 0.215738445520401\n",
      "iteration 48263: loss: 0.21573826670646667\n",
      "iteration 48264: loss: 0.21573810279369354\n",
      "iteration 48265: loss: 0.21573801338672638\n",
      "iteration 48266: loss: 0.21573784947395325\n",
      "iteration 48267: loss: 0.21573765575885773\n",
      "iteration 48268: loss: 0.21573758125305176\n",
      "iteration 48269: loss: 0.21573734283447266\n",
      "iteration 48270: loss: 0.2157372236251831\n",
      "iteration 48271: loss: 0.21573702991008759\n",
      "iteration 48272: loss: 0.21573686599731445\n",
      "iteration 48273: loss: 0.21573682129383087\n",
      "iteration 48274: loss: 0.21573658287525177\n",
      "iteration 48275: loss: 0.2157364785671234\n",
      "iteration 48276: loss: 0.2157362997531891\n",
      "iteration 48277: loss: 0.21573612093925476\n",
      "iteration 48278: loss: 0.21573595702648163\n",
      "iteration 48279: loss: 0.21573582291603088\n",
      "iteration 48280: loss: 0.21573570370674133\n",
      "iteration 48281: loss: 0.215735524892807\n",
      "iteration 48282: loss: 0.21573539078235626\n",
      "iteration 48283: loss: 0.21573519706726074\n",
      "iteration 48284: loss: 0.2157350480556488\n",
      "iteration 48285: loss: 0.21573492884635925\n",
      "iteration 48286: loss: 0.21573476493358612\n",
      "iteration 48287: loss: 0.21573464572429657\n",
      "iteration 48288: loss: 0.21573445200920105\n",
      "iteration 48289: loss: 0.2157343178987503\n",
      "iteration 48290: loss: 0.21573419868946075\n",
      "iteration 48291: loss: 0.21573397517204285\n",
      "iteration 48292: loss: 0.2157338410615921\n",
      "iteration 48293: loss: 0.21573367714881897\n",
      "iteration 48294: loss: 0.2157335728406906\n",
      "iteration 48295: loss: 0.21573343873023987\n",
      "iteration 48296: loss: 0.21573328971862793\n",
      "iteration 48297: loss: 0.2157330960035324\n",
      "iteration 48298: loss: 0.21573297679424286\n",
      "iteration 48299: loss: 0.21573273837566376\n",
      "iteration 48300: loss: 0.2157326638698578\n",
      "iteration 48301: loss: 0.21573252975940704\n",
      "iteration 48302: loss: 0.2157324254512787\n",
      "iteration 48303: loss: 0.2157321721315384\n",
      "iteration 48304: loss: 0.21573209762573242\n",
      "iteration 48305: loss: 0.2157319039106369\n",
      "iteration 48306: loss: 0.21573171019554138\n",
      "iteration 48307: loss: 0.21573154628276825\n",
      "iteration 48308: loss: 0.21573150157928467\n",
      "iteration 48309: loss: 0.21573123335838318\n",
      "iteration 48310: loss: 0.21573114395141602\n",
      "iteration 48311: loss: 0.2157309502363205\n",
      "iteration 48312: loss: 0.21573081612586975\n",
      "iteration 48313: loss: 0.2157307118177414\n",
      "iteration 48314: loss: 0.2157304584980011\n",
      "iteration 48315: loss: 0.21573033928871155\n",
      "iteration 48316: loss: 0.215730220079422\n",
      "iteration 48317: loss: 0.21573010087013245\n",
      "iteration 48318: loss: 0.21572987735271454\n",
      "iteration 48319: loss: 0.215729758143425\n",
      "iteration 48320: loss: 0.21572968363761902\n",
      "iteration 48321: loss: 0.21572943031787872\n",
      "iteration 48322: loss: 0.2157292366027832\n",
      "iteration 48323: loss: 0.21572914719581604\n",
      "iteration 48324: loss: 0.2157290279865265\n",
      "iteration 48325: loss: 0.21572890877723694\n",
      "iteration 48326: loss: 0.21572868525981903\n",
      "iteration 48327: loss: 0.21572856605052948\n",
      "iteration 48328: loss: 0.21572843194007874\n",
      "iteration 48329: loss: 0.21572820842266083\n",
      "iteration 48330: loss: 0.21572811901569366\n",
      "iteration 48331: loss: 0.21572789549827576\n",
      "iteration 48332: loss: 0.215727761387825\n",
      "iteration 48333: loss: 0.21572761237621307\n",
      "iteration 48334: loss: 0.21572744846343994\n",
      "iteration 48335: loss: 0.215727299451828\n",
      "iteration 48336: loss: 0.21572716534137726\n",
      "iteration 48337: loss: 0.21572700142860413\n",
      "iteration 48338: loss: 0.21572689712047577\n",
      "iteration 48339: loss: 0.21572668850421906\n",
      "iteration 48340: loss: 0.21572661399841309\n",
      "iteration 48341: loss: 0.21572646498680115\n",
      "iteration 48342: loss: 0.21572630107402802\n",
      "iteration 48343: loss: 0.21572613716125488\n",
      "iteration 48344: loss: 0.21572594344615936\n",
      "iteration 48345: loss: 0.21572577953338623\n",
      "iteration 48346: loss: 0.21572566032409668\n",
      "iteration 48347: loss: 0.21572549641132355\n",
      "iteration 48348: loss: 0.2157253473997116\n",
      "iteration 48349: loss: 0.21572522819042206\n",
      "iteration 48350: loss: 0.21572503447532654\n",
      "iteration 48351: loss: 0.215724915266037\n",
      "iteration 48352: loss: 0.21572475135326385\n",
      "iteration 48353: loss: 0.21572458744049072\n",
      "iteration 48354: loss: 0.21572446823120117\n",
      "iteration 48355: loss: 0.21572427451610565\n",
      "iteration 48356: loss: 0.21572411060333252\n",
      "iteration 48357: loss: 0.21572396159172058\n",
      "iteration 48358: loss: 0.21572384238243103\n",
      "iteration 48359: loss: 0.2157236784696579\n",
      "iteration 48360: loss: 0.21572348475456238\n",
      "iteration 48361: loss: 0.21572336554527283\n",
      "iteration 48362: loss: 0.2157232016324997\n",
      "iteration 48363: loss: 0.21572306752204895\n",
      "iteration 48364: loss: 0.2157229483127594\n",
      "iteration 48365: loss: 0.21572275459766388\n",
      "iteration 48366: loss: 0.21572259068489075\n",
      "iteration 48367: loss: 0.21572251617908478\n",
      "iteration 48368: loss: 0.21572232246398926\n",
      "iteration 48369: loss: 0.21572217345237732\n",
      "iteration 48370: loss: 0.21572193503379822\n",
      "iteration 48371: loss: 0.21572184562683105\n",
      "iteration 48372: loss: 0.21572168171405792\n",
      "iteration 48373: loss: 0.21572156250476837\n",
      "iteration 48374: loss: 0.21572136878967285\n",
      "iteration 48375: loss: 0.2157212495803833\n",
      "iteration 48376: loss: 0.21572108566761017\n",
      "iteration 48377: loss: 0.21572096645832062\n",
      "iteration 48378: loss: 0.2157207429409027\n",
      "iteration 48379: loss: 0.21572062373161316\n",
      "iteration 48380: loss: 0.21572048962116241\n",
      "iteration 48381: loss: 0.21572037041187286\n",
      "iteration 48382: loss: 0.21572013199329376\n",
      "iteration 48383: loss: 0.21572008728981018\n",
      "iteration 48384: loss: 0.21571993827819824\n",
      "iteration 48385: loss: 0.21571974456310272\n",
      "iteration 48386: loss: 0.2157195806503296\n",
      "iteration 48387: loss: 0.21571941673755646\n",
      "iteration 48388: loss: 0.2157192975282669\n",
      "iteration 48389: loss: 0.21571913361549377\n",
      "iteration 48390: loss: 0.21571895480155945\n",
      "iteration 48391: loss: 0.2157188206911087\n",
      "iteration 48392: loss: 0.21571867167949677\n",
      "iteration 48393: loss: 0.21571853756904602\n",
      "iteration 48394: loss: 0.2157183587551117\n",
      "iteration 48395: loss: 0.21571822464466095\n",
      "iteration 48396: loss: 0.21571806073188782\n",
      "iteration 48397: loss: 0.21571791172027588\n",
      "iteration 48398: loss: 0.21571767330169678\n",
      "iteration 48399: loss: 0.21571752429008484\n",
      "iteration 48400: loss: 0.21571746468544006\n",
      "iteration 48401: loss: 0.21571731567382812\n",
      "iteration 48402: loss: 0.2157171070575714\n",
      "iteration 48403: loss: 0.21571692824363708\n",
      "iteration 48404: loss: 0.21571683883666992\n",
      "iteration 48405: loss: 0.2157166451215744\n",
      "iteration 48406: loss: 0.21571645140647888\n",
      "iteration 48407: loss: 0.21571633219718933\n",
      "iteration 48408: loss: 0.2157161682844162\n",
      "iteration 48409: loss: 0.21571603417396545\n",
      "iteration 48410: loss: 0.2157159298658371\n",
      "iteration 48411: loss: 0.215715691447258\n",
      "iteration 48412: loss: 0.21571555733680725\n",
      "iteration 48413: loss: 0.2157154083251953\n",
      "iteration 48414: loss: 0.21571531891822815\n",
      "iteration 48415: loss: 0.21571512520313263\n",
      "iteration 48416: loss: 0.21571500599384308\n",
      "iteration 48417: loss: 0.21571484208106995\n",
      "iteration 48418: loss: 0.2157147228717804\n",
      "iteration 48419: loss: 0.21571454405784607\n",
      "iteration 48420: loss: 0.21571438014507294\n",
      "iteration 48421: loss: 0.2157142162322998\n",
      "iteration 48422: loss: 0.21571414172649384\n",
      "iteration 48423: loss: 0.21571388840675354\n",
      "iteration 48424: loss: 0.2157137095928192\n",
      "iteration 48425: loss: 0.21571359038352966\n",
      "iteration 48426: loss: 0.21571341156959534\n",
      "iteration 48427: loss: 0.21571333706378937\n",
      "iteration 48428: loss: 0.21571311354637146\n",
      "iteration 48429: loss: 0.21571297943592072\n",
      "iteration 48430: loss: 0.21571287512779236\n",
      "iteration 48431: loss: 0.21571266651153564\n",
      "iteration 48432: loss: 0.2157125025987625\n",
      "iteration 48433: loss: 0.21571235358715057\n",
      "iteration 48434: loss: 0.2157122790813446\n",
      "iteration 48435: loss: 0.21571211516857147\n",
      "iteration 48436: loss: 0.21571187674999237\n",
      "iteration 48437: loss: 0.21571175754070282\n",
      "iteration 48438: loss: 0.21571163833141327\n",
      "iteration 48439: loss: 0.21571142971515656\n",
      "iteration 48440: loss: 0.215711310505867\n",
      "iteration 48441: loss: 0.21571116149425507\n",
      "iteration 48442: loss: 0.21571099758148193\n",
      "iteration 48443: loss: 0.21571084856987\n",
      "iteration 48444: loss: 0.2157106101512909\n",
      "iteration 48445: loss: 0.2157105654478073\n",
      "iteration 48446: loss: 0.2157103717327118\n",
      "iteration 48447: loss: 0.21571019291877747\n",
      "iteration 48448: loss: 0.21571004390716553\n",
      "iteration 48449: loss: 0.21570995450019836\n",
      "iteration 48450: loss: 0.21570976078510284\n",
      "iteration 48451: loss: 0.2157096415758133\n",
      "iteration 48452: loss: 0.21570947766304016\n",
      "iteration 48453: loss: 0.21570925414562225\n",
      "iteration 48454: loss: 0.21570917963981628\n",
      "iteration 48455: loss: 0.21570901572704315\n",
      "iteration 48456: loss: 0.21570882201194763\n",
      "iteration 48457: loss: 0.2157086580991745\n",
      "iteration 48458: loss: 0.21570844948291779\n",
      "iteration 48459: loss: 0.21570837497711182\n",
      "iteration 48460: loss: 0.2157081812620163\n",
      "iteration 48461: loss: 0.21570809185504913\n",
      "iteration 48462: loss: 0.2157078981399536\n",
      "iteration 48463: loss: 0.21570782363414764\n",
      "iteration 48464: loss: 0.21570751070976257\n",
      "iteration 48465: loss: 0.21570739150047302\n",
      "iteration 48466: loss: 0.21570730209350586\n",
      "iteration 48467: loss: 0.21570715308189392\n",
      "iteration 48468: loss: 0.21570703387260437\n",
      "iteration 48469: loss: 0.21570682525634766\n",
      "iteration 48470: loss: 0.2157067358493805\n",
      "iteration 48471: loss: 0.21570654213428497\n",
      "iteration 48472: loss: 0.21570634841918945\n",
      "iteration 48473: loss: 0.2157062292098999\n",
      "iteration 48474: loss: 0.21570608019828796\n",
      "iteration 48475: loss: 0.21570594608783722\n",
      "iteration 48476: loss: 0.21570579707622528\n",
      "iteration 48477: loss: 0.21570563316345215\n",
      "iteration 48478: loss: 0.21570554375648499\n",
      "iteration 48479: loss: 0.21570535004138947\n",
      "iteration 48480: loss: 0.21570518612861633\n",
      "iteration 48481: loss: 0.215705007314682\n",
      "iteration 48482: loss: 0.21570487320423126\n",
      "iteration 48483: loss: 0.21570470929145813\n",
      "iteration 48484: loss: 0.21570459008216858\n",
      "iteration 48485: loss: 0.21570444107055664\n",
      "iteration 48486: loss: 0.2157042771577835\n",
      "iteration 48487: loss: 0.21570412814617157\n",
      "iteration 48488: loss: 0.21570388972759247\n",
      "iteration 48489: loss: 0.2157038152217865\n",
      "iteration 48490: loss: 0.21570363640785217\n",
      "iteration 48491: loss: 0.21570345759391785\n",
      "iteration 48492: loss: 0.2157033234834671\n",
      "iteration 48493: loss: 0.21570315957069397\n",
      "iteration 48494: loss: 0.21570301055908203\n",
      "iteration 48495: loss: 0.21570289134979248\n",
      "iteration 48496: loss: 0.21570272743701935\n",
      "iteration 48497: loss: 0.2157025784254074\n",
      "iteration 48498: loss: 0.2157023847103119\n",
      "iteration 48499: loss: 0.21570225059986115\n",
      "iteration 48500: loss: 0.21570205688476562\n",
      "iteration 48501: loss: 0.2157019078731537\n",
      "iteration 48502: loss: 0.21570181846618652\n",
      "iteration 48503: loss: 0.2157016545534134\n",
      "iteration 48504: loss: 0.21570143103599548\n",
      "iteration 48505: loss: 0.21570131182670593\n",
      "iteration 48506: loss: 0.2157011777162552\n",
      "iteration 48507: loss: 0.21570102870464325\n",
      "iteration 48508: loss: 0.2157008945941925\n",
      "iteration 48509: loss: 0.21570071578025818\n",
      "iteration 48510: loss: 0.21570058166980743\n",
      "iteration 48511: loss: 0.2157004326581955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 48512: loss: 0.21570029854774475\n",
      "iteration 48513: loss: 0.21570010483264923\n",
      "iteration 48514: loss: 0.2156999558210373\n",
      "iteration 48515: loss: 0.21569983661174774\n",
      "iteration 48516: loss: 0.2156997174024582\n",
      "iteration 48517: loss: 0.2156994789838791\n",
      "iteration 48518: loss: 0.21569935977458954\n",
      "iteration 48519: loss: 0.21569912135601044\n",
      "iteration 48520: loss: 0.21569903194904327\n",
      "iteration 48521: loss: 0.21569891273975372\n",
      "iteration 48522: loss: 0.2156987190246582\n",
      "iteration 48523: loss: 0.21569852530956268\n",
      "iteration 48524: loss: 0.21569840610027313\n",
      "iteration 48525: loss: 0.2156982719898224\n",
      "iteration 48526: loss: 0.21569807827472687\n",
      "iteration 48527: loss: 0.2156980037689209\n",
      "iteration 48528: loss: 0.21569783985614777\n",
      "iteration 48529: loss: 0.21569772064685822\n",
      "iteration 48530: loss: 0.2156974822282791\n",
      "iteration 48531: loss: 0.2156972885131836\n",
      "iteration 48532: loss: 0.21569719910621643\n",
      "iteration 48533: loss: 0.21569707989692688\n",
      "iteration 48534: loss: 0.21569688618183136\n",
      "iteration 48535: loss: 0.21569672226905823\n",
      "iteration 48536: loss: 0.2156965285539627\n",
      "iteration 48537: loss: 0.21569645404815674\n",
      "iteration 48538: loss: 0.2156962901353836\n",
      "iteration 48539: loss: 0.21569609642028809\n",
      "iteration 48540: loss: 0.21569600701332092\n",
      "iteration 48541: loss: 0.2156958281993866\n",
      "iteration 48542: loss: 0.21569569408893585\n",
      "iteration 48543: loss: 0.21569553017616272\n",
      "iteration 48544: loss: 0.2156953364610672\n",
      "iteration 48545: loss: 0.21569514274597168\n",
      "iteration 48546: loss: 0.21569505333900452\n",
      "iteration 48547: loss: 0.21569490432739258\n",
      "iteration 48548: loss: 0.21569471061229706\n",
      "iteration 48549: loss: 0.2156945765018463\n",
      "iteration 48550: loss: 0.21569445729255676\n",
      "iteration 48551: loss: 0.21569423377513885\n",
      "iteration 48552: loss: 0.2156941443681717\n",
      "iteration 48553: loss: 0.21569395065307617\n",
      "iteration 48554: loss: 0.215693861246109\n",
      "iteration 48555: loss: 0.2156936377286911\n",
      "iteration 48556: loss: 0.2156934291124344\n",
      "iteration 48557: loss: 0.2156933844089508\n",
      "iteration 48558: loss: 0.21569319069385529\n",
      "iteration 48559: loss: 0.21569299697875977\n",
      "iteration 48560: loss: 0.21569284796714783\n",
      "iteration 48561: loss: 0.21569280326366425\n",
      "iteration 48562: loss: 0.21569254994392395\n",
      "iteration 48563: loss: 0.2156924307346344\n",
      "iteration 48564: loss: 0.21569235622882843\n",
      "iteration 48565: loss: 0.21569211781024933\n",
      "iteration 48566: loss: 0.2156919538974762\n",
      "iteration 48567: loss: 0.21569180488586426\n",
      "iteration 48568: loss: 0.21569165587425232\n",
      "iteration 48569: loss: 0.2156914919614792\n",
      "iteration 48570: loss: 0.21569137275218964\n",
      "iteration 48571: loss: 0.21569116413593292\n",
      "iteration 48572: loss: 0.2156910002231598\n",
      "iteration 48573: loss: 0.21569089591503143\n",
      "iteration 48574: loss: 0.21569068729877472\n",
      "iteration 48575: loss: 0.21569061279296875\n",
      "iteration 48576: loss: 0.21569044888019562\n",
      "iteration 48577: loss: 0.21569034457206726\n",
      "iteration 48578: loss: 0.21569013595581055\n",
      "iteration 48579: loss: 0.2156899869441986\n",
      "iteration 48580: loss: 0.21568980813026428\n",
      "iteration 48581: loss: 0.21568968892097473\n",
      "iteration 48582: loss: 0.21568946540355682\n",
      "iteration 48583: loss: 0.21568933129310608\n",
      "iteration 48584: loss: 0.21568918228149414\n",
      "iteration 48585: loss: 0.2156890332698822\n",
      "iteration 48586: loss: 0.21568886935710907\n",
      "iteration 48587: loss: 0.21568870544433594\n",
      "iteration 48588: loss: 0.215688556432724\n",
      "iteration 48589: loss: 0.21568843722343445\n",
      "iteration 48590: loss: 0.21568827331066132\n",
      "iteration 48591: loss: 0.21568818390369415\n",
      "iteration 48592: loss: 0.21568801999092102\n",
      "iteration 48593: loss: 0.2156878262758255\n",
      "iteration 48594: loss: 0.21568767726421356\n",
      "iteration 48595: loss: 0.215687558054924\n",
      "iteration 48596: loss: 0.2156873196363449\n",
      "iteration 48597: loss: 0.21568720042705536\n",
      "iteration 48598: loss: 0.21568700671195984\n",
      "iteration 48599: loss: 0.21568694710731506\n",
      "iteration 48600: loss: 0.21568672358989716\n",
      "iteration 48601: loss: 0.2156865894794464\n",
      "iteration 48602: loss: 0.21568648517131805\n",
      "iteration 48603: loss: 0.21568629145622253\n",
      "iteration 48604: loss: 0.2156861275434494\n",
      "iteration 48605: loss: 0.21568596363067627\n",
      "iteration 48606: loss: 0.21568581461906433\n",
      "iteration 48607: loss: 0.21568563580513\n",
      "iteration 48608: loss: 0.21568551659584045\n",
      "iteration 48609: loss: 0.2156853973865509\n",
      "iteration 48610: loss: 0.2156851589679718\n",
      "iteration 48611: loss: 0.21568505465984344\n",
      "iteration 48612: loss: 0.21568486094474792\n",
      "iteration 48613: loss: 0.2156846523284912\n",
      "iteration 48614: loss: 0.21568456292152405\n",
      "iteration 48615: loss: 0.2156844586133957\n",
      "iteration 48616: loss: 0.21568429470062256\n",
      "iteration 48617: loss: 0.21568408608436584\n",
      "iteration 48618: loss: 0.2156839370727539\n",
      "iteration 48619: loss: 0.21568381786346436\n",
      "iteration 48620: loss: 0.21568360924720764\n",
      "iteration 48621: loss: 0.2156834900379181\n",
      "iteration 48622: loss: 0.21568331122398376\n",
      "iteration 48623: loss: 0.2156831920146942\n",
      "iteration 48624: loss: 0.21568305790424347\n",
      "iteration 48625: loss: 0.21568286418914795\n",
      "iteration 48626: loss: 0.2156827747821808\n",
      "iteration 48627: loss: 0.2156825065612793\n",
      "iteration 48628: loss: 0.21568241715431213\n",
      "iteration 48629: loss: 0.21568234264850616\n",
      "iteration 48630: loss: 0.21568210422992706\n",
      "iteration 48631: loss: 0.21568195521831512\n",
      "iteration 48632: loss: 0.21568183600902557\n",
      "iteration 48633: loss: 0.21568164229393005\n",
      "iteration 48634: loss: 0.2156815081834793\n",
      "iteration 48635: loss: 0.21568135917186737\n",
      "iteration 48636: loss: 0.21568115055561066\n",
      "iteration 48637: loss: 0.21568100154399872\n",
      "iteration 48638: loss: 0.21568091213703156\n",
      "iteration 48639: loss: 0.21568074822425842\n",
      "iteration 48640: loss: 0.21568048000335693\n",
      "iteration 48641: loss: 0.21568040549755096\n",
      "iteration 48642: loss: 0.21568027138710022\n",
      "iteration 48643: loss: 0.21568015217781067\n",
      "iteration 48644: loss: 0.21567992866039276\n",
      "iteration 48645: loss: 0.21567976474761963\n",
      "iteration 48646: loss: 0.2156796157360077\n",
      "iteration 48647: loss: 0.21567949652671814\n",
      "iteration 48648: loss: 0.2156793177127838\n",
      "iteration 48649: loss: 0.2156791388988495\n",
      "iteration 48650: loss: 0.21567901968955994\n",
      "iteration 48651: loss: 0.2156788408756256\n",
      "iteration 48652: loss: 0.21567873656749725\n",
      "iteration 48653: loss: 0.21567854285240173\n",
      "iteration 48654: loss: 0.21567845344543457\n",
      "iteration 48655: loss: 0.21567821502685547\n",
      "iteration 48656: loss: 0.2156781405210495\n",
      "iteration 48657: loss: 0.21567797660827637\n",
      "iteration 48658: loss: 0.21567781269550323\n",
      "iteration 48659: loss: 0.2156776636838913\n",
      "iteration 48660: loss: 0.21567746996879578\n",
      "iteration 48661: loss: 0.21567735075950623\n",
      "iteration 48662: loss: 0.21567721664905548\n",
      "iteration 48663: loss: 0.21567702293395996\n",
      "iteration 48664: loss: 0.21567678451538086\n",
      "iteration 48665: loss: 0.2156767100095749\n",
      "iteration 48666: loss: 0.21567654609680176\n",
      "iteration 48667: loss: 0.21567639708518982\n",
      "iteration 48668: loss: 0.2156762182712555\n",
      "iteration 48669: loss: 0.21567602455615997\n",
      "iteration 48670: loss: 0.215675950050354\n",
      "iteration 48671: loss: 0.21567578613758087\n",
      "iteration 48672: loss: 0.21567563712596893\n",
      "iteration 48673: loss: 0.2156755030155182\n",
      "iteration 48674: loss: 0.21567530930042267\n",
      "iteration 48675: loss: 0.21567514538764954\n",
      "iteration 48676: loss: 0.21567504107952118\n",
      "iteration 48677: loss: 0.21567487716674805\n",
      "iteration 48678: loss: 0.2156747579574585\n",
      "iteration 48679: loss: 0.21567456424236298\n",
      "iteration 48680: loss: 0.21567437052726746\n",
      "iteration 48681: loss: 0.21567420661449432\n",
      "iteration 48682: loss: 0.2156740427017212\n",
      "iteration 48683: loss: 0.21567392349243164\n",
      "iteration 48684: loss: 0.2156738042831421\n",
      "iteration 48685: loss: 0.21567364037036896\n",
      "iteration 48686: loss: 0.21567344665527344\n",
      "iteration 48687: loss: 0.21567335724830627\n",
      "iteration 48688: loss: 0.21567311882972717\n",
      "iteration 48689: loss: 0.21567299962043762\n",
      "iteration 48690: loss: 0.21567292511463165\n",
      "iteration 48691: loss: 0.21567273139953613\n",
      "iteration 48692: loss: 0.2156725823879242\n",
      "iteration 48693: loss: 0.21567237377166748\n",
      "iteration 48694: loss: 0.21567220985889435\n",
      "iteration 48695: loss: 0.2156720906496048\n",
      "iteration 48696: loss: 0.21567189693450928\n",
      "iteration 48697: loss: 0.21567177772521973\n",
      "iteration 48698: loss: 0.2156716287136078\n",
      "iteration 48699: loss: 0.21567144989967346\n",
      "iteration 48700: loss: 0.2156713306903839\n",
      "iteration 48701: loss: 0.21567115187644958\n",
      "iteration 48702: loss: 0.21567098796367645\n",
      "iteration 48703: loss: 0.21567079424858093\n",
      "iteration 48704: loss: 0.21567067503929138\n",
      "iteration 48705: loss: 0.21567049622535706\n",
      "iteration 48706: loss: 0.21567034721374512\n",
      "iteration 48707: loss: 0.21567022800445557\n",
      "iteration 48708: loss: 0.21567001938819885\n",
      "iteration 48709: loss: 0.21566994488239288\n",
      "iteration 48710: loss: 0.21566978096961975\n",
      "iteration 48711: loss: 0.2156696319580078\n",
      "iteration 48712: loss: 0.21566948294639587\n",
      "iteration 48713: loss: 0.21566924452781677\n",
      "iteration 48714: loss: 0.21566911041736603\n",
      "iteration 48715: loss: 0.2156689614057541\n",
      "iteration 48716: loss: 0.21566882729530334\n",
      "iteration 48717: loss: 0.21566863358020782\n",
      "iteration 48718: loss: 0.2156684845685959\n",
      "iteration 48719: loss: 0.21566836535930634\n",
      "iteration 48720: loss: 0.2156682014465332\n",
      "iteration 48721: loss: 0.21566803753376007\n",
      "iteration 48722: loss: 0.21566791832447052\n",
      "iteration 48723: loss: 0.2156677544116974\n",
      "iteration 48724: loss: 0.21566753089427948\n",
      "iteration 48725: loss: 0.2156674861907959\n",
      "iteration 48726: loss: 0.21566727757453918\n",
      "iteration 48727: loss: 0.21566712856292725\n",
      "iteration 48728: loss: 0.2156669646501541\n",
      "iteration 48729: loss: 0.21566680073738098\n",
      "iteration 48730: loss: 0.21566668152809143\n",
      "iteration 48731: loss: 0.2156664878129959\n",
      "iteration 48732: loss: 0.21566641330718994\n",
      "iteration 48733: loss: 0.21566620469093323\n",
      "iteration 48734: loss: 0.2156660109758377\n",
      "iteration 48735: loss: 0.21566586196422577\n",
      "iteration 48736: loss: 0.2156657874584198\n",
      "iteration 48737: loss: 0.21566560864448547\n",
      "iteration 48738: loss: 0.21566534042358398\n",
      "iteration 48739: loss: 0.21566525101661682\n",
      "iteration 48740: loss: 0.21566514670848846\n",
      "iteration 48741: loss: 0.21566490828990936\n",
      "iteration 48742: loss: 0.2156648188829422\n",
      "iteration 48743: loss: 0.21566469967365265\n",
      "iteration 48744: loss: 0.21566450595855713\n",
      "iteration 48745: loss: 0.21566429734230042\n",
      "iteration 48746: loss: 0.21566417813301086\n",
      "iteration 48747: loss: 0.2156640589237213\n",
      "iteration 48748: loss: 0.2156638354063034\n",
      "iteration 48749: loss: 0.21566371619701385\n",
      "iteration 48750: loss: 0.21566350758075714\n",
      "iteration 48751: loss: 0.21566347777843475\n",
      "iteration 48752: loss: 0.21566328406333923\n",
      "iteration 48753: loss: 0.2156631052494049\n",
      "iteration 48754: loss: 0.21566295623779297\n",
      "iteration 48755: loss: 0.21566276252269745\n",
      "iteration 48756: loss: 0.2156626433134079\n",
      "iteration 48757: loss: 0.21566250920295715\n",
      "iteration 48758: loss: 0.21566233038902283\n",
      "iteration 48759: loss: 0.2156621515750885\n",
      "iteration 48760: loss: 0.21566197276115417\n",
      "iteration 48761: loss: 0.2156619131565094\n",
      "iteration 48762: loss: 0.21566173434257507\n",
      "iteration 48763: loss: 0.21566149592399597\n",
      "iteration 48764: loss: 0.2156614363193512\n",
      "iteration 48765: loss: 0.2156611979007721\n",
      "iteration 48766: loss: 0.21566107869148254\n",
      "iteration 48767: loss: 0.2156609743833542\n",
      "iteration 48768: loss: 0.21566073596477509\n",
      "iteration 48769: loss: 0.21566061675548553\n",
      "iteration 48770: loss: 0.2156604826450348\n",
      "iteration 48771: loss: 0.21566030383110046\n",
      "iteration 48772: loss: 0.21566012501716614\n",
      "iteration 48773: loss: 0.21566002070903778\n",
      "iteration 48774: loss: 0.21565982699394226\n",
      "iteration 48775: loss: 0.21565964818000793\n",
      "iteration 48776: loss: 0.21565957367420197\n",
      "iteration 48777: loss: 0.21565940976142883\n",
      "iteration 48778: loss: 0.2156592309474945\n",
      "iteration 48779: loss: 0.2156589776277542\n",
      "iteration 48780: loss: 0.21565890312194824\n",
      "iteration 48781: loss: 0.2156587839126587\n",
      "iteration 48782: loss: 0.21565861999988556\n",
      "iteration 48783: loss: 0.21565847098827362\n",
      "iteration 48784: loss: 0.2156583070755005\n",
      "iteration 48785: loss: 0.21565814316272736\n",
      "iteration 48786: loss: 0.21565794944763184\n",
      "iteration 48787: loss: 0.21565787494182587\n",
      "iteration 48788: loss: 0.21565766632556915\n",
      "iteration 48789: loss: 0.21565747261047363\n",
      "iteration 48790: loss: 0.21565738320350647\n",
      "iteration 48791: loss: 0.21565718948841095\n",
      "iteration 48792: loss: 0.21565699577331543\n",
      "iteration 48793: loss: 0.2156568318605423\n",
      "iteration 48794: loss: 0.21565678715705872\n",
      "iteration 48795: loss: 0.21565647423267365\n",
      "iteration 48796: loss: 0.21565639972686768\n",
      "iteration 48797: loss: 0.21565628051757812\n",
      "iteration 48798: loss: 0.2156560719013214\n",
      "iteration 48799: loss: 0.21565592288970947\n",
      "iteration 48800: loss: 0.21565575897693634\n",
      "iteration 48801: loss: 0.2156556397676468\n",
      "iteration 48802: loss: 0.21565547585487366\n",
      "iteration 48803: loss: 0.21565528213977814\n",
      "iteration 48804: loss: 0.2156551331281662\n",
      "iteration 48805: loss: 0.21565504372119904\n",
      "iteration 48806: loss: 0.21565485000610352\n",
      "iteration 48807: loss: 0.215654656291008\n",
      "iteration 48808: loss: 0.21565453708171844\n",
      "iteration 48809: loss: 0.21565434336662292\n",
      "iteration 48810: loss: 0.21565420925617218\n",
      "iteration 48811: loss: 0.21565409004688263\n",
      "iteration 48812: loss: 0.21565397083759308\n",
      "iteration 48813: loss: 0.21565377712249756\n",
      "iteration 48814: loss: 0.21565358340740204\n",
      "iteration 48815: loss: 0.2156534641981125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 48816: loss: 0.21565330028533936\n",
      "iteration 48817: loss: 0.21565310657024384\n",
      "iteration 48818: loss: 0.2156529426574707\n",
      "iteration 48819: loss: 0.21565289795398712\n",
      "iteration 48820: loss: 0.215652734041214\n",
      "iteration 48821: loss: 0.21565254032611847\n",
      "iteration 48822: loss: 0.21565234661102295\n",
      "iteration 48823: loss: 0.2156522572040558\n",
      "iteration 48824: loss: 0.21565203368663788\n",
      "iteration 48825: loss: 0.21565186977386475\n",
      "iteration 48826: loss: 0.21565178036689758\n",
      "iteration 48827: loss: 0.21565160155296326\n",
      "iteration 48828: loss: 0.21565143764019012\n",
      "iteration 48829: loss: 0.2156512290239334\n",
      "iteration 48830: loss: 0.21565106511116028\n",
      "iteration 48831: loss: 0.2156510055065155\n",
      "iteration 48832: loss: 0.21565082669258118\n",
      "iteration 48833: loss: 0.21565064787864685\n",
      "iteration 48834: loss: 0.21565046906471252\n",
      "iteration 48835: loss: 0.21565039455890656\n",
      "iteration 48836: loss: 0.21565024554729462\n",
      "iteration 48837: loss: 0.2156500518321991\n",
      "iteration 48838: loss: 0.2156497687101364\n",
      "iteration 48839: loss: 0.21564969420433044\n",
      "iteration 48840: loss: 0.2156495600938797\n",
      "iteration 48841: loss: 0.21564941108226776\n",
      "iteration 48842: loss: 0.21564927697181702\n",
      "iteration 48843: loss: 0.2156490534543991\n",
      "iteration 48844: loss: 0.21564888954162598\n",
      "iteration 48845: loss: 0.21564877033233643\n",
      "iteration 48846: loss: 0.21564869582653046\n",
      "iteration 48847: loss: 0.21564844250679016\n",
      "iteration 48848: loss: 0.2156483232975006\n",
      "iteration 48849: loss: 0.21564817428588867\n",
      "iteration 48850: loss: 0.21564802527427673\n",
      "iteration 48851: loss: 0.2156478464603424\n",
      "iteration 48852: loss: 0.21564769744873047\n",
      "iteration 48853: loss: 0.21564757823944092\n",
      "iteration 48854: loss: 0.21564745903015137\n",
      "iteration 48855: loss: 0.21564719080924988\n",
      "iteration 48856: loss: 0.21564707159996033\n",
      "iteration 48857: loss: 0.21564695239067078\n",
      "iteration 48858: loss: 0.21564683318138123\n",
      "iteration 48859: loss: 0.21564659476280212\n",
      "iteration 48860: loss: 0.21564647555351257\n",
      "iteration 48861: loss: 0.21564629673957825\n",
      "iteration 48862: loss: 0.2156461775302887\n",
      "iteration 48863: loss: 0.21564599871635437\n",
      "iteration 48864: loss: 0.21564581990242004\n",
      "iteration 48865: loss: 0.2156457006931305\n",
      "iteration 48866: loss: 0.21564550697803497\n",
      "iteration 48867: loss: 0.21564534306526184\n",
      "iteration 48868: loss: 0.2156452238559723\n",
      "iteration 48869: loss: 0.2156449854373932\n",
      "iteration 48870: loss: 0.2156449556350708\n",
      "iteration 48871: loss: 0.2156447470188141\n",
      "iteration 48872: loss: 0.21564455330371857\n",
      "iteration 48873: loss: 0.21564443409442902\n",
      "iteration 48874: loss: 0.21564432978630066\n",
      "iteration 48875: loss: 0.21564416587352753\n",
      "iteration 48876: loss: 0.21564404666423798\n",
      "iteration 48877: loss: 0.2156437635421753\n",
      "iteration 48878: loss: 0.21564368903636932\n",
      "iteration 48879: loss: 0.2156434953212738\n",
      "iteration 48880: loss: 0.21564336121082306\n",
      "iteration 48881: loss: 0.21564319729804993\n",
      "iteration 48882: loss: 0.2156430184841156\n",
      "iteration 48883: loss: 0.21564288437366486\n",
      "iteration 48884: loss: 0.21564272046089172\n",
      "iteration 48885: loss: 0.21564260125160217\n",
      "iteration 48886: loss: 0.21564245223999023\n",
      "iteration 48887: loss: 0.21564224362373352\n",
      "iteration 48888: loss: 0.21564212441444397\n",
      "iteration 48889: loss: 0.21564188599586487\n",
      "iteration 48890: loss: 0.2156418263912201\n",
      "iteration 48891: loss: 0.215641587972641\n",
      "iteration 48892: loss: 0.21564149856567383\n",
      "iteration 48893: loss: 0.2156413495540619\n",
      "iteration 48894: loss: 0.21564114093780518\n",
      "iteration 48895: loss: 0.21564099192619324\n",
      "iteration 48896: loss: 0.2156408578157425\n",
      "iteration 48897: loss: 0.21564070880413055\n",
      "iteration 48898: loss: 0.21564054489135742\n",
      "iteration 48899: loss: 0.2156403809785843\n",
      "iteration 48900: loss: 0.21564026176929474\n",
      "iteration 48901: loss: 0.2156401127576828\n",
      "iteration 48902: loss: 0.2156399041414261\n",
      "iteration 48903: loss: 0.21563978493213654\n",
      "iteration 48904: loss: 0.21563959121704102\n",
      "iteration 48905: loss: 0.21563942730426788\n",
      "iteration 48906: loss: 0.21563923358917236\n",
      "iteration 48907: loss: 0.2156391590833664\n",
      "iteration 48908: loss: 0.2156389057636261\n",
      "iteration 48909: loss: 0.2156388759613037\n",
      "iteration 48910: loss: 0.215638667345047\n",
      "iteration 48911: loss: 0.21563848853111267\n",
      "iteration 48912: loss: 0.21563835442066193\n",
      "iteration 48913: loss: 0.21563820540905\n",
      "iteration 48914: loss: 0.2156379669904709\n",
      "iteration 48915: loss: 0.21563789248466492\n",
      "iteration 48916: loss: 0.21563772857189178\n",
      "iteration 48917: loss: 0.21563759446144104\n",
      "iteration 48918: loss: 0.21563740074634552\n",
      "iteration 48919: loss: 0.21563728153705597\n",
      "iteration 48920: loss: 0.21563705801963806\n",
      "iteration 48921: loss: 0.21563692390918732\n",
      "iteration 48922: loss: 0.21563680469989777\n",
      "iteration 48923: loss: 0.21563664078712463\n",
      "iteration 48924: loss: 0.2156364470720291\n",
      "iteration 48925: loss: 0.21563634276390076\n",
      "iteration 48926: loss: 0.21563616394996643\n",
      "iteration 48927: loss: 0.2156359702348709\n",
      "iteration 48928: loss: 0.21563580632209778\n",
      "iteration 48929: loss: 0.2156357318162918\n",
      "iteration 48930: loss: 0.2156355381011963\n",
      "iteration 48931: loss: 0.21563534438610077\n",
      "iteration 48932: loss: 0.21563521027565002\n",
      "iteration 48933: loss: 0.21563509106636047\n",
      "iteration 48934: loss: 0.21563485264778137\n",
      "iteration 48935: loss: 0.21563470363616943\n",
      "iteration 48936: loss: 0.21563461422920227\n",
      "iteration 48937: loss: 0.21563443541526794\n",
      "iteration 48938: loss: 0.21563425660133362\n",
      "iteration 48939: loss: 0.21563418209552765\n",
      "iteration 48940: loss: 0.21563398838043213\n",
      "iteration 48941: loss: 0.21563386917114258\n",
      "iteration 48942: loss: 0.21563363075256348\n",
      "iteration 48943: loss: 0.21563351154327393\n",
      "iteration 48944: loss: 0.2156333029270172\n",
      "iteration 48945: loss: 0.21563322842121124\n",
      "iteration 48946: loss: 0.21563303470611572\n",
      "iteration 48947: loss: 0.2156328707933426\n",
      "iteration 48948: loss: 0.21563276648521423\n",
      "iteration 48949: loss: 0.2156325876712799\n",
      "iteration 48950: loss: 0.21563240885734558\n",
      "iteration 48951: loss: 0.21563223004341125\n",
      "iteration 48952: loss: 0.2156321257352829\n",
      "iteration 48953: loss: 0.21563191711902618\n",
      "iteration 48954: loss: 0.21563176810741425\n",
      "iteration 48955: loss: 0.2156316339969635\n",
      "iteration 48956: loss: 0.21563144028186798\n",
      "iteration 48957: loss: 0.21563127636909485\n",
      "iteration 48958: loss: 0.2156311571598053\n",
      "iteration 48959: loss: 0.21563100814819336\n",
      "iteration 48960: loss: 0.2156309187412262\n",
      "iteration 48961: loss: 0.2156306952238083\n",
      "iteration 48962: loss: 0.21563053131103516\n",
      "iteration 48963: loss: 0.215630441904068\n",
      "iteration 48964: loss: 0.21563021838665009\n",
      "iteration 48965: loss: 0.21563000977039337\n",
      "iteration 48966: loss: 0.2156299650669098\n",
      "iteration 48967: loss: 0.21562984585762024\n",
      "iteration 48968: loss: 0.21562960743904114\n",
      "iteration 48969: loss: 0.21562938392162323\n",
      "iteration 48970: loss: 0.21562926471233368\n",
      "iteration 48971: loss: 0.21562913060188293\n",
      "iteration 48972: loss: 0.2156289517879486\n",
      "iteration 48973: loss: 0.21562881767749786\n",
      "iteration 48974: loss: 0.21562862396240234\n",
      "iteration 48975: loss: 0.2156284749507904\n",
      "iteration 48976: loss: 0.21562834084033966\n",
      "iteration 48977: loss: 0.21562817692756653\n",
      "iteration 48978: loss: 0.21562805771827698\n",
      "iteration 48979: loss: 0.21562786400318146\n",
      "iteration 48980: loss: 0.21562771499156952\n",
      "iteration 48981: loss: 0.21562758088111877\n",
      "iteration 48982: loss: 0.21562738716602325\n",
      "iteration 48983: loss: 0.21562719345092773\n",
      "iteration 48984: loss: 0.21562710404396057\n",
      "iteration 48985: loss: 0.21562691032886505\n",
      "iteration 48986: loss: 0.2156267613172531\n",
      "iteration 48987: loss: 0.21562659740447998\n",
      "iteration 48988: loss: 0.21562647819519043\n",
      "iteration 48989: loss: 0.2156262844800949\n",
      "iteration 48990: loss: 0.2156260758638382\n",
      "iteration 48991: loss: 0.21562600135803223\n",
      "iteration 48992: loss: 0.2156258523464203\n",
      "iteration 48993: loss: 0.21562571823596954\n",
      "iteration 48994: loss: 0.21562552452087402\n",
      "iteration 48995: loss: 0.21562528610229492\n",
      "iteration 48996: loss: 0.21562519669532776\n",
      "iteration 48997: loss: 0.21562504768371582\n",
      "iteration 48998: loss: 0.2156248837709427\n",
      "iteration 48999: loss: 0.21562473475933075\n",
      "iteration 49000: loss: 0.2156246453523636\n",
      "iteration 49001: loss: 0.21562440693378448\n",
      "iteration 49002: loss: 0.21562421321868896\n",
      "iteration 49003: loss: 0.2156241238117218\n",
      "iteration 49004: loss: 0.21562394499778748\n",
      "iteration 49005: loss: 0.21562381088733673\n",
      "iteration 49006: loss: 0.2156236618757248\n",
      "iteration 49007: loss: 0.2156234234571457\n",
      "iteration 49008: loss: 0.21562328934669495\n",
      "iteration 49009: loss: 0.21562322974205017\n",
      "iteration 49010: loss: 0.21562299132347107\n",
      "iteration 49011: loss: 0.21562281250953674\n",
      "iteration 49012: loss: 0.2156227082014084\n",
      "iteration 49013: loss: 0.21562251448631287\n",
      "iteration 49014: loss: 0.21562233567237854\n",
      "iteration 49015: loss: 0.21562226116657257\n",
      "iteration 49016: loss: 0.21562202274799347\n",
      "iteration 49017: loss: 0.2156219482421875\n",
      "iteration 49018: loss: 0.2156217396259308\n",
      "iteration 49019: loss: 0.21562154591083527\n",
      "iteration 49020: loss: 0.2156214416027069\n",
      "iteration 49021: loss: 0.21562127768993378\n",
      "iteration 49022: loss: 0.21562115848064423\n",
      "iteration 49023: loss: 0.21562092006206512\n",
      "iteration 49024: loss: 0.21562080085277557\n",
      "iteration 49025: loss: 0.21562066674232483\n",
      "iteration 49026: loss: 0.2156205177307129\n",
      "iteration 49027: loss: 0.2156202495098114\n",
      "iteration 49028: loss: 0.21562020480632782\n",
      "iteration 49029: loss: 0.2156199961900711\n",
      "iteration 49030: loss: 0.21561989188194275\n",
      "iteration 49031: loss: 0.215619757771492\n",
      "iteration 49032: loss: 0.2156195342540741\n",
      "iteration 49033: loss: 0.21561932563781738\n",
      "iteration 49034: loss: 0.21561920642852783\n",
      "iteration 49035: loss: 0.2156190425157547\n",
      "iteration 49036: loss: 0.21561884880065918\n",
      "iteration 49037: loss: 0.21561875939369202\n",
      "iteration 49038: loss: 0.21561861038208008\n",
      "iteration 49039: loss: 0.21561849117279053\n",
      "iteration 49040: loss: 0.2156182825565338\n",
      "iteration 49041: loss: 0.21561816334724426\n",
      "iteration 49042: loss: 0.21561798453330994\n",
      "iteration 49043: loss: 0.2156178206205368\n",
      "iteration 49044: loss: 0.21561770141124725\n",
      "iteration 49045: loss: 0.2156176120042801\n",
      "iteration 49046: loss: 0.215617373585701\n",
      "iteration 49047: loss: 0.21561715006828308\n",
      "iteration 49048: loss: 0.21561698615550995\n",
      "iteration 49049: loss: 0.21561689674854279\n",
      "iteration 49050: loss: 0.21561674773693085\n",
      "iteration 49051: loss: 0.21561658382415771\n",
      "iteration 49052: loss: 0.21561646461486816\n",
      "iteration 49053: loss: 0.21561627089977264\n",
      "iteration 49054: loss: 0.21561607718467712\n",
      "iteration 49055: loss: 0.21561598777770996\n",
      "iteration 49056: loss: 0.21561583876609802\n",
      "iteration 49057: loss: 0.2156156599521637\n",
      "iteration 49058: loss: 0.21561551094055176\n",
      "iteration 49059: loss: 0.21561531722545624\n",
      "iteration 49060: loss: 0.2156151980161667\n",
      "iteration 49061: loss: 0.21561500430107117\n",
      "iteration 49062: loss: 0.21561482548713684\n",
      "iteration 49063: loss: 0.2156146764755249\n",
      "iteration 49064: loss: 0.21561460196971893\n",
      "iteration 49065: loss: 0.2156144380569458\n",
      "iteration 49066: loss: 0.2156141698360443\n",
      "iteration 49067: loss: 0.21561400592327118\n",
      "iteration 49068: loss: 0.21561384201049805\n",
      "iteration 49069: loss: 0.21561376750469208\n",
      "iteration 49070: loss: 0.21561355888843536\n",
      "iteration 49071: loss: 0.2156134396791458\n",
      "iteration 49072: loss: 0.21561332046985626\n",
      "iteration 49073: loss: 0.21561312675476074\n",
      "iteration 49074: loss: 0.2156129628419876\n",
      "iteration 49075: loss: 0.2156127393245697\n",
      "iteration 49076: loss: 0.21561267971992493\n",
      "iteration 49077: loss: 0.2156125009059906\n",
      "iteration 49078: loss: 0.21561233699321747\n",
      "iteration 49079: loss: 0.2156122624874115\n",
      "iteration 49080: loss: 0.21561197936534882\n",
      "iteration 49081: loss: 0.2156117856502533\n",
      "iteration 49082: loss: 0.21561172604560852\n",
      "iteration 49083: loss: 0.2156115025281906\n",
      "iteration 49084: loss: 0.21561136841773987\n",
      "iteration 49085: loss: 0.2156112641096115\n",
      "iteration 49086: loss: 0.215611070394516\n",
      "iteration 49087: loss: 0.21561093628406525\n",
      "iteration 49088: loss: 0.21561074256896973\n",
      "iteration 49089: loss: 0.2156105935573578\n",
      "iteration 49090: loss: 0.21561045944690704\n",
      "iteration 49091: loss: 0.21561026573181152\n",
      "iteration 49092: loss: 0.21561014652252197\n",
      "iteration 49093: loss: 0.21560995280742645\n",
      "iteration 49094: loss: 0.2156098186969757\n",
      "iteration 49095: loss: 0.21560963988304138\n",
      "iteration 49096: loss: 0.21560955047607422\n",
      "iteration 49097: loss: 0.21560931205749512\n",
      "iteration 49098: loss: 0.21560916304588318\n",
      "iteration 49099: loss: 0.21560907363891602\n",
      "iteration 49100: loss: 0.2156088650226593\n",
      "iteration 49101: loss: 0.21560874581336975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 49102: loss: 0.21560856699943542\n",
      "iteration 49103: loss: 0.21560844779014587\n",
      "iteration 49104: loss: 0.21560828387737274\n",
      "iteration 49105: loss: 0.21560804545879364\n",
      "iteration 49106: loss: 0.2156079113483429\n",
      "iteration 49107: loss: 0.21560779213905334\n",
      "iteration 49108: loss: 0.21560756862163544\n",
      "iteration 49109: loss: 0.2156074494123459\n",
      "iteration 49110: loss: 0.21560733020305634\n",
      "iteration 49111: loss: 0.2156071662902832\n",
      "iteration 49112: loss: 0.21560701727867126\n",
      "iteration 49113: loss: 0.21560683846473694\n",
      "iteration 49114: loss: 0.2156067192554474\n",
      "iteration 49115: loss: 0.21560649573802948\n",
      "iteration 49116: loss: 0.21560636162757874\n",
      "iteration 49117: loss: 0.2156061828136444\n",
      "iteration 49118: loss: 0.21560609340667725\n",
      "iteration 49119: loss: 0.2156059294939041\n",
      "iteration 49120: loss: 0.2156057059764862\n",
      "iteration 49121: loss: 0.21560554206371307\n",
      "iteration 49122: loss: 0.21560542285442352\n",
      "iteration 49123: loss: 0.2156052589416504\n",
      "iteration 49124: loss: 0.21560506522655487\n",
      "iteration 49125: loss: 0.21560493111610413\n",
      "iteration 49126: loss: 0.2156047374010086\n",
      "iteration 49127: loss: 0.21560463309288025\n",
      "iteration 49128: loss: 0.21560446918010712\n",
      "iteration 49129: loss: 0.21560430526733398\n",
      "iteration 49130: loss: 0.21560409665107727\n",
      "iteration 49131: loss: 0.21560394763946533\n",
      "iteration 49132: loss: 0.21560387313365936\n",
      "iteration 49133: loss: 0.21560366451740265\n",
      "iteration 49134: loss: 0.2156035453081131\n",
      "iteration 49135: loss: 0.2156033217906952\n",
      "iteration 49136: loss: 0.21560323238372803\n",
      "iteration 49137: loss: 0.2156030237674713\n",
      "iteration 49138: loss: 0.21560287475585938\n",
      "iteration 49139: loss: 0.21560272574424744\n",
      "iteration 49140: loss: 0.2156025469303131\n",
      "iteration 49141: loss: 0.21560236811637878\n",
      "iteration 49142: loss: 0.21560224890708923\n",
      "iteration 49143: loss: 0.21560212969779968\n",
      "iteration 49144: loss: 0.21560201048851013\n",
      "iteration 49145: loss: 0.21560177206993103\n",
      "iteration 49146: loss: 0.21560163795948029\n",
      "iteration 49147: loss: 0.21560144424438477\n",
      "iteration 49148: loss: 0.21560129523277283\n",
      "iteration 49149: loss: 0.21560116112232208\n",
      "iteration 49150: loss: 0.21560099720954895\n",
      "iteration 49151: loss: 0.21560081839561462\n",
      "iteration 49152: loss: 0.21560068428516388\n",
      "iteration 49153: loss: 0.21560052037239075\n",
      "iteration 49154: loss: 0.21560032665729523\n",
      "iteration 49155: loss: 0.2156001776456833\n",
      "iteration 49156: loss: 0.21560001373291016\n",
      "iteration 49157: loss: 0.2155998945236206\n",
      "iteration 49158: loss: 0.21559973061084747\n",
      "iteration 49159: loss: 0.21559958159923553\n",
      "iteration 49160: loss: 0.21559938788414001\n",
      "iteration 49161: loss: 0.2155992090702057\n",
      "iteration 49162: loss: 0.21559908986091614\n",
      "iteration 49163: loss: 0.2155989110469818\n",
      "iteration 49164: loss: 0.21559877693653107\n",
      "iteration 49165: loss: 0.21559865772724152\n",
      "iteration 49166: loss: 0.21559849381446838\n",
      "iteration 49167: loss: 0.21559830009937286\n",
      "iteration 49168: loss: 0.21559815108776093\n",
      "iteration 49169: loss: 0.2155979573726654\n",
      "iteration 49170: loss: 0.21559782326221466\n",
      "iteration 49171: loss: 0.2155977189540863\n",
      "iteration 49172: loss: 0.2155975103378296\n",
      "iteration 49173: loss: 0.21559739112854004\n",
      "iteration 49174: loss: 0.21559715270996094\n",
      "iteration 49175: loss: 0.2155970335006714\n",
      "iteration 49176: loss: 0.21559683978557587\n",
      "iteration 49177: loss: 0.21559672057628632\n",
      "iteration 49178: loss: 0.21559660136699677\n",
      "iteration 49179: loss: 0.21559643745422363\n",
      "iteration 49180: loss: 0.2155962437391281\n",
      "iteration 49181: loss: 0.21559610962867737\n",
      "iteration 49182: loss: 0.21559596061706543\n",
      "iteration 49183: loss: 0.21559575200080872\n",
      "iteration 49184: loss: 0.21559564769268036\n",
      "iteration 49185: loss: 0.21559548377990723\n",
      "iteration 49186: loss: 0.2155953198671341\n",
      "iteration 49187: loss: 0.21559515595436096\n",
      "iteration 49188: loss: 0.21559497714042664\n",
      "iteration 49189: loss: 0.21559488773345947\n",
      "iteration 49190: loss: 0.21559467911720276\n",
      "iteration 49191: loss: 0.21559453010559082\n",
      "iteration 49192: loss: 0.2155943363904953\n",
      "iteration 49193: loss: 0.21559420228004456\n",
      "iteration 49194: loss: 0.215594083070755\n",
      "iteration 49195: loss: 0.21559388935565948\n",
      "iteration 49196: loss: 0.21559372544288635\n",
      "iteration 49197: loss: 0.21559353172779083\n",
      "iteration 49198: loss: 0.2155933827161789\n",
      "iteration 49199: loss: 0.21559329330921173\n",
      "iteration 49200: loss: 0.2155930995941162\n",
      "iteration 49201: loss: 0.21559295058250427\n",
      "iteration 49202: loss: 0.21559281647205353\n",
      "iteration 49203: loss: 0.215592622756958\n",
      "iteration 49204: loss: 0.2155924290418625\n",
      "iteration 49205: loss: 0.21559229493141174\n",
      "iteration 49206: loss: 0.2155921459197998\n",
      "iteration 49207: loss: 0.21559205651283264\n",
      "iteration 49208: loss: 0.21559183299541473\n",
      "iteration 49209: loss: 0.2155916690826416\n",
      "iteration 49210: loss: 0.21559152007102966\n",
      "iteration 49211: loss: 0.21559131145477295\n",
      "iteration 49212: loss: 0.2155911922454834\n",
      "iteration 49213: loss: 0.21559107303619385\n",
      "iteration 49214: loss: 0.21559086441993713\n",
      "iteration 49215: loss: 0.2155906856060028\n",
      "iteration 49216: loss: 0.21559052169322968\n",
      "iteration 49217: loss: 0.2155904769897461\n",
      "iteration 49218: loss: 0.21559028327465057\n",
      "iteration 49219: loss: 0.21559008955955505\n",
      "iteration 49220: loss: 0.2155899703502655\n",
      "iteration 49221: loss: 0.21558980643749237\n",
      "iteration 49222: loss: 0.21558961272239685\n",
      "iteration 49223: loss: 0.21558944880962372\n",
      "iteration 49224: loss: 0.21558932960033417\n",
      "iteration 49225: loss: 0.21558916568756104\n",
      "iteration 49226: loss: 0.21558897197246552\n",
      "iteration 49227: loss: 0.21558883786201477\n",
      "iteration 49228: loss: 0.21558865904808044\n",
      "iteration 49229: loss: 0.2155885398387909\n",
      "iteration 49230: loss: 0.21558836102485657\n",
      "iteration 49231: loss: 0.21558818221092224\n",
      "iteration 49232: loss: 0.2155880481004715\n",
      "iteration 49233: loss: 0.21558792889118195\n",
      "iteration 49234: loss: 0.21558770537376404\n",
      "iteration 49235: loss: 0.2155875414609909\n",
      "iteration 49236: loss: 0.21558745205402374\n",
      "iteration 49237: loss: 0.21558725833892822\n",
      "iteration 49238: loss: 0.2155870944261551\n",
      "iteration 49239: loss: 0.21558694541454315\n",
      "iteration 49240: loss: 0.21558678150177002\n",
      "iteration 49241: loss: 0.2155865728855133\n",
      "iteration 49242: loss: 0.21558642387390137\n",
      "iteration 49243: loss: 0.21558627486228943\n",
      "iteration 49244: loss: 0.21558614075183868\n",
      "iteration 49245: loss: 0.21558603644371033\n",
      "iteration 49246: loss: 0.215585857629776\n",
      "iteration 49247: loss: 0.2155856341123581\n",
      "iteration 49248: loss: 0.21558554470539093\n",
      "iteration 49249: loss: 0.21558532118797302\n",
      "iteration 49250: loss: 0.21558523178100586\n",
      "iteration 49251: loss: 0.21558499336242676\n",
      "iteration 49252: loss: 0.21558484435081482\n",
      "iteration 49253: loss: 0.21558475494384766\n",
      "iteration 49254: loss: 0.21558454632759094\n",
      "iteration 49255: loss: 0.215584397315979\n",
      "iteration 49256: loss: 0.21558424830436707\n",
      "iteration 49257: loss: 0.21558408439159393\n",
      "iteration 49258: loss: 0.21558387577533722\n",
      "iteration 49259: loss: 0.21558372676372528\n",
      "iteration 49260: loss: 0.21558359265327454\n",
      "iteration 49261: loss: 0.2155834138393402\n",
      "iteration 49262: loss: 0.21558327972888947\n",
      "iteration 49263: loss: 0.21558313071727753\n",
      "iteration 49264: loss: 0.2155829221010208\n",
      "iteration 49265: loss: 0.21558280289173126\n",
      "iteration 49266: loss: 0.21558263897895813\n",
      "iteration 49267: loss: 0.21558251976966858\n",
      "iteration 49268: loss: 0.21558234095573425\n",
      "iteration 49269: loss: 0.2155822217464447\n",
      "iteration 49270: loss: 0.21558193862438202\n",
      "iteration 49271: loss: 0.21558184921741486\n",
      "iteration 49272: loss: 0.21558161079883575\n",
      "iteration 49273: loss: 0.21558158099651337\n",
      "iteration 49274: loss: 0.21558138728141785\n",
      "iteration 49275: loss: 0.2155812680721283\n",
      "iteration 49276: loss: 0.21558105945587158\n",
      "iteration 49277: loss: 0.21558089554309845\n",
      "iteration 49278: loss: 0.21558073163032532\n",
      "iteration 49279: loss: 0.21558065712451935\n",
      "iteration 49280: loss: 0.21558043360710144\n",
      "iteration 49281: loss: 0.2155802994966507\n",
      "iteration 49282: loss: 0.21558015048503876\n",
      "iteration 49283: loss: 0.2155800312757492\n",
      "iteration 49284: loss: 0.2155797928571701\n",
      "iteration 49285: loss: 0.21557970345020294\n",
      "iteration 49286: loss: 0.21557946503162384\n",
      "iteration 49287: loss: 0.2155793160200119\n",
      "iteration 49288: loss: 0.21557918190956116\n",
      "iteration 49289: loss: 0.21557903289794922\n",
      "iteration 49290: loss: 0.2155788689851761\n",
      "iteration 49291: loss: 0.21557867527008057\n",
      "iteration 49292: loss: 0.21557855606079102\n",
      "iteration 49293: loss: 0.2155783623456955\n",
      "iteration 49294: loss: 0.21557822823524475\n",
      "iteration 49295: loss: 0.2155781090259552\n",
      "iteration 49296: loss: 0.21557791531085968\n",
      "iteration 49297: loss: 0.21557769179344177\n",
      "iteration 49298: loss: 0.21557757258415222\n",
      "iteration 49299: loss: 0.21557751297950745\n",
      "iteration 49300: loss: 0.21557721495628357\n",
      "iteration 49301: loss: 0.21557708084583282\n",
      "iteration 49302: loss: 0.2155769318342209\n",
      "iteration 49303: loss: 0.21557679772377014\n",
      "iteration 49304: loss: 0.2155766785144806\n",
      "iteration 49305: loss: 0.2155764400959015\n",
      "iteration 49306: loss: 0.21557629108428955\n",
      "iteration 49307: loss: 0.21557612717151642\n",
      "iteration 49308: loss: 0.21557600796222687\n",
      "iteration 49309: loss: 0.21557581424713135\n",
      "iteration 49310: loss: 0.21557565033435822\n",
      "iteration 49311: loss: 0.21557548642158508\n",
      "iteration 49312: loss: 0.21557536721229553\n",
      "iteration 49313: loss: 0.21557526290416718\n",
      "iteration 49314: loss: 0.2155749797821045\n",
      "iteration 49315: loss: 0.21557490527629852\n",
      "iteration 49316: loss: 0.21557465195655823\n",
      "iteration 49317: loss: 0.21557457745075226\n",
      "iteration 49318: loss: 0.21557441353797913\n",
      "iteration 49319: loss: 0.2155742347240448\n",
      "iteration 49320: loss: 0.21557405591011047\n",
      "iteration 49321: loss: 0.21557390689849854\n",
      "iteration 49322: loss: 0.21557378768920898\n",
      "iteration 49323: loss: 0.21557359397411346\n",
      "iteration 49324: loss: 0.2155734747648239\n",
      "iteration 49325: loss: 0.21557331085205078\n",
      "iteration 49326: loss: 0.21557311713695526\n",
      "iteration 49327: loss: 0.2155730277299881\n",
      "iteration 49328: loss: 0.21557286381721497\n",
      "iteration 49329: loss: 0.21557264029979706\n",
      "iteration 49330: loss: 0.2155725061893463\n",
      "iteration 49331: loss: 0.21557240188121796\n",
      "iteration 49332: loss: 0.21557216346263885\n",
      "iteration 49333: loss: 0.2155720293521881\n",
      "iteration 49334: loss: 0.21557191014289856\n",
      "iteration 49335: loss: 0.21557171642780304\n",
      "iteration 49336: loss: 0.21557149291038513\n",
      "iteration 49337: loss: 0.2155713587999344\n",
      "iteration 49338: loss: 0.21557125449180603\n",
      "iteration 49339: loss: 0.2155710905790329\n",
      "iteration 49340: loss: 0.21557092666625977\n",
      "iteration 49341: loss: 0.21557071805000305\n",
      "iteration 49342: loss: 0.21557053923606873\n",
      "iteration 49343: loss: 0.21557044982910156\n",
      "iteration 49344: loss: 0.21557025611400604\n",
      "iteration 49345: loss: 0.2155701220035553\n",
      "iteration 49346: loss: 0.21556994318962097\n",
      "iteration 49347: loss: 0.21556980907917023\n",
      "iteration 49348: loss: 0.2155696451663971\n",
      "iteration 49349: loss: 0.21556949615478516\n",
      "iteration 49350: loss: 0.21556925773620605\n",
      "iteration 49351: loss: 0.2155691683292389\n",
      "iteration 49352: loss: 0.21556898951530457\n",
      "iteration 49353: loss: 0.21556885540485382\n",
      "iteration 49354: loss: 0.21556861698627472\n",
      "iteration 49355: loss: 0.21556849777698517\n",
      "iteration 49356: loss: 0.21556830406188965\n",
      "iteration 49357: loss: 0.21556821465492249\n",
      "iteration 49358: loss: 0.21556803584098816\n",
      "iteration 49359: loss: 0.21556787192821503\n",
      "iteration 49360: loss: 0.2155677080154419\n",
      "iteration 49361: loss: 0.21556761860847473\n",
      "iteration 49362: loss: 0.2155674397945404\n",
      "iteration 49363: loss: 0.21556727588176727\n",
      "iteration 49364: loss: 0.21556706726551056\n",
      "iteration 49365: loss: 0.2155669629573822\n",
      "iteration 49366: loss: 0.2155667245388031\n",
      "iteration 49367: loss: 0.21556659042835236\n",
      "iteration 49368: loss: 0.2155664712190628\n",
      "iteration 49369: loss: 0.21556630730628967\n",
      "iteration 49370: loss: 0.21556608378887177\n",
      "iteration 49371: loss: 0.21556594967842102\n",
      "iteration 49372: loss: 0.21556580066680908\n",
      "iteration 49373: loss: 0.21556565165519714\n",
      "iteration 49374: loss: 0.215565487742424\n",
      "iteration 49375: loss: 0.21556532382965088\n",
      "iteration 49376: loss: 0.21556515991687775\n",
      "iteration 49377: loss: 0.21556499600410461\n",
      "iteration 49378: loss: 0.2155648171901703\n",
      "iteration 49379: loss: 0.21556472778320312\n",
      "iteration 49380: loss: 0.2155645191669464\n",
      "iteration 49381: loss: 0.21556444466114044\n",
      "iteration 49382: loss: 0.21556422114372253\n",
      "iteration 49383: loss: 0.2155640572309494\n",
      "iteration 49384: loss: 0.21556396782398224\n",
      "iteration 49385: loss: 0.21556374430656433\n",
      "iteration 49386: loss: 0.2155636101961136\n",
      "iteration 49387: loss: 0.21556346118450165\n",
      "iteration 49388: loss: 0.21556320786476135\n",
      "iteration 49389: loss: 0.21556313335895538\n",
      "iteration 49390: loss: 0.21556298434734344\n",
      "iteration 49391: loss: 0.21556279063224792\n",
      "iteration 49392: loss: 0.21556265652179718\n",
      "iteration 49393: loss: 0.21556243300437927\n",
      "iteration 49394: loss: 0.21556225419044495\n",
      "iteration 49395: loss: 0.21556219458580017\n",
      "iteration 49396: loss: 0.21556206047534943\n",
      "iteration 49397: loss: 0.21556179225444794\n",
      "iteration 49398: loss: 0.2155616283416748\n",
      "iteration 49399: loss: 0.21556155383586884\n",
      "iteration 49400: loss: 0.21556134521961212\n",
      "iteration 49401: loss: 0.215561181306839\n",
      "iteration 49402: loss: 0.21556106209754944\n",
      "iteration 49403: loss: 0.21556086838245392\n",
      "iteration 49404: loss: 0.21556074917316437\n",
      "iteration 49405: loss: 0.21556052565574646\n",
      "iteration 49406: loss: 0.21556036174297333\n",
      "iteration 49407: loss: 0.21556028723716736\n",
      "iteration 49408: loss: 0.21556010842323303\n",
      "iteration 49409: loss: 0.21555987000465393\n",
      "iteration 49410: loss: 0.21555975079536438\n",
      "iteration 49411: loss: 0.21555960178375244\n",
      "iteration 49412: loss: 0.2155594825744629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 49413: loss: 0.21555928885936737\n",
      "iteration 49414: loss: 0.2155591994524002\n",
      "iteration 49415: loss: 0.21555893123149872\n",
      "iteration 49416: loss: 0.21555879712104797\n",
      "iteration 49417: loss: 0.21555864810943604\n",
      "iteration 49418: loss: 0.2155584841966629\n",
      "iteration 49419: loss: 0.21555829048156738\n",
      "iteration 49420: loss: 0.21555817127227783\n",
      "iteration 49421: loss: 0.21555796265602112\n",
      "iteration 49422: loss: 0.21555784344673157\n",
      "iteration 49423: loss: 0.21555766463279724\n",
      "iteration 49424: loss: 0.21555745601654053\n",
      "iteration 49425: loss: 0.21555733680725098\n",
      "iteration 49426: loss: 0.21555721759796143\n",
      "iteration 49427: loss: 0.2155570089817047\n",
      "iteration 49428: loss: 0.21555693447589874\n",
      "iteration 49429: loss: 0.21555666625499725\n",
      "iteration 49430: loss: 0.2155565321445465\n",
      "iteration 49431: loss: 0.21555647253990173\n",
      "iteration 49432: loss: 0.2155562937259674\n",
      "iteration 49433: loss: 0.21555602550506592\n",
      "iteration 49434: loss: 0.21555595099925995\n",
      "iteration 49435: loss: 0.21555575728416443\n",
      "iteration 49436: loss: 0.21555563807487488\n",
      "iteration 49437: loss: 0.21555547416210175\n",
      "iteration 49438: loss: 0.2155553102493286\n",
      "iteration 49439: loss: 0.21555514633655548\n",
      "iteration 49440: loss: 0.21555498242378235\n",
      "iteration 49441: loss: 0.21555475890636444\n",
      "iteration 49442: loss: 0.21555466949939728\n",
      "iteration 49443: loss: 0.21555452048778534\n",
      "iteration 49444: loss: 0.21555432677268982\n",
      "iteration 49445: loss: 0.2155541628599167\n",
      "iteration 49446: loss: 0.21555404365062714\n",
      "iteration 49447: loss: 0.215553879737854\n",
      "iteration 49448: loss: 0.21555368602275848\n",
      "iteration 49449: loss: 0.21555349230766296\n",
      "iteration 49450: loss: 0.2155533730983734\n",
      "iteration 49451: loss: 0.21555320918560028\n",
      "iteration 49452: loss: 0.21555308997631073\n",
      "iteration 49453: loss: 0.21555295586585999\n",
      "iteration 49454: loss: 0.21555273234844208\n",
      "iteration 49455: loss: 0.21555261313915253\n",
      "iteration 49456: loss: 0.215552419424057\n",
      "iteration 49457: loss: 0.21555225551128387\n",
      "iteration 49458: loss: 0.21555209159851074\n",
      "iteration 49459: loss: 0.2155519276857376\n",
      "iteration 49460: loss: 0.21555176377296448\n",
      "iteration 49461: loss: 0.2155516892671585\n",
      "iteration 49462: loss: 0.215551495552063\n",
      "iteration 49463: loss: 0.21555128693580627\n",
      "iteration 49464: loss: 0.2155512571334839\n",
      "iteration 49465: loss: 0.2155509889125824\n",
      "iteration 49466: loss: 0.21555082499980927\n",
      "iteration 49467: loss: 0.21555063128471375\n",
      "iteration 49468: loss: 0.2155504673719406\n",
      "iteration 49469: loss: 0.21555039286613464\n",
      "iteration 49470: loss: 0.21555021405220032\n",
      "iteration 49471: loss: 0.215550035238266\n",
      "iteration 49472: loss: 0.21554982662200928\n",
      "iteration 49473: loss: 0.21554966270923615\n",
      "iteration 49474: loss: 0.2155495434999466\n",
      "iteration 49475: loss: 0.21554939448833466\n",
      "iteration 49476: loss: 0.21554920077323914\n",
      "iteration 49477: loss: 0.2155490666627884\n",
      "iteration 49478: loss: 0.21554884314537048\n",
      "iteration 49479: loss: 0.21554875373840332\n",
      "iteration 49480: loss: 0.21554851531982422\n",
      "iteration 49481: loss: 0.21554844081401825\n",
      "iteration 49482: loss: 0.2155483067035675\n",
      "iteration 49483: loss: 0.21554811298847198\n",
      "iteration 49484: loss: 0.21554799377918243\n",
      "iteration 49485: loss: 0.2155478298664093\n",
      "iteration 49486: loss: 0.2155475914478302\n",
      "iteration 49487: loss: 0.21554744243621826\n",
      "iteration 49488: loss: 0.21554724872112274\n",
      "iteration 49489: loss: 0.2155471295118332\n",
      "iteration 49490: loss: 0.21554699540138245\n",
      "iteration 49491: loss: 0.2155468910932541\n",
      "iteration 49492: loss: 0.215546652674675\n",
      "iteration 49493: loss: 0.21554645895957947\n",
      "iteration 49494: loss: 0.2155463993549347\n",
      "iteration 49495: loss: 0.21554617583751678\n",
      "iteration 49496: loss: 0.21554605662822723\n",
      "iteration 49497: loss: 0.2155458629131317\n",
      "iteration 49498: loss: 0.21554572880268097\n",
      "iteration 49499: loss: 0.21554556488990784\n",
      "iteration 49500: loss: 0.21554537117481232\n",
      "iteration 49501: loss: 0.21554526686668396\n",
      "iteration 49502: loss: 0.21554513275623322\n",
      "iteration 49503: loss: 0.2155448943376541\n",
      "iteration 49504: loss: 0.2155447006225586\n",
      "iteration 49505: loss: 0.21554461121559143\n",
      "iteration 49506: loss: 0.21554438769817352\n",
      "iteration 49507: loss: 0.21554426848888397\n",
      "iteration 49508: loss: 0.21554413437843323\n",
      "iteration 49509: loss: 0.2155439406633377\n",
      "iteration 49510: loss: 0.21554379165172577\n",
      "iteration 49511: loss: 0.21554359793663025\n",
      "iteration 49512: loss: 0.21554353833198547\n",
      "iteration 49513: loss: 0.2155432403087616\n",
      "iteration 49514: loss: 0.21554315090179443\n",
      "iteration 49515: loss: 0.2155429869890213\n",
      "iteration 49516: loss: 0.21554282307624817\n",
      "iteration 49517: loss: 0.21554270386695862\n",
      "iteration 49518: loss: 0.2155425101518631\n",
      "iteration 49519: loss: 0.21554231643676758\n",
      "iteration 49520: loss: 0.21554216742515564\n",
      "iteration 49521: loss: 0.2155420035123825\n",
      "iteration 49522: loss: 0.21554183959960938\n",
      "iteration 49523: loss: 0.2155417501926422\n",
      "iteration 49524: loss: 0.21554160118103027\n",
      "iteration 49525: loss: 0.21554140746593475\n",
      "iteration 49526: loss: 0.21554124355316162\n",
      "iteration 49527: loss: 0.21554109454154968\n",
      "iteration 49528: loss: 0.21554096043109894\n",
      "iteration 49529: loss: 0.2155407965183258\n",
      "iteration 49530: loss: 0.2155405730009079\n",
      "iteration 49531: loss: 0.21554045379161835\n",
      "iteration 49532: loss: 0.21554026007652283\n",
      "iteration 49533: loss: 0.21554014086723328\n",
      "iteration 49534: loss: 0.21553997695446014\n",
      "iteration 49535: loss: 0.215539813041687\n",
      "iteration 49536: loss: 0.21553964912891388\n",
      "iteration 49537: loss: 0.21553945541381836\n",
      "iteration 49538: loss: 0.21553930640220642\n",
      "iteration 49539: loss: 0.2155391275882721\n",
      "iteration 49540: loss: 0.21553893387317657\n",
      "iteration 49541: loss: 0.2155388593673706\n",
      "iteration 49542: loss: 0.21553859114646912\n",
      "iteration 49543: loss: 0.21553847193717957\n",
      "iteration 49544: loss: 0.21553835272789001\n",
      "iteration 49545: loss: 0.2155381739139557\n",
      "iteration 49546: loss: 0.21553805470466614\n",
      "iteration 49547: loss: 0.21553783118724823\n",
      "iteration 49548: loss: 0.21553771197795868\n",
      "iteration 49549: loss: 0.21553757786750793\n",
      "iteration 49550: loss: 0.21553733944892883\n",
      "iteration 49551: loss: 0.2155371606349945\n",
      "iteration 49552: loss: 0.21553702652454376\n",
      "iteration 49553: loss: 0.21553687751293182\n",
      "iteration 49554: loss: 0.2155367136001587\n",
      "iteration 49555: loss: 0.21553662419319153\n",
      "iteration 49556: loss: 0.215536430478096\n",
      "iteration 49557: loss: 0.21553628146648407\n",
      "iteration 49558: loss: 0.21553614735603333\n",
      "iteration 49559: loss: 0.21553587913513184\n",
      "iteration 49560: loss: 0.21553578972816467\n",
      "iteration 49561: loss: 0.21553555130958557\n",
      "iteration 49562: loss: 0.21553544700145721\n",
      "iteration 49563: loss: 0.21553535759449005\n",
      "iteration 49564: loss: 0.21553513407707214\n",
      "iteration 49565: loss: 0.2155350148677826\n",
      "iteration 49566: loss: 0.21553480625152588\n",
      "iteration 49567: loss: 0.21553459763526917\n",
      "iteration 49568: loss: 0.21553456783294678\n",
      "iteration 49569: loss: 0.21553435921669006\n",
      "iteration 49570: loss: 0.21553421020507812\n",
      "iteration 49571: loss: 0.2155340015888214\n",
      "iteration 49572: loss: 0.21553388237953186\n",
      "iteration 49573: loss: 0.21553368866443634\n",
      "iteration 49574: loss: 0.21553346514701843\n",
      "iteration 49575: loss: 0.21553340554237366\n",
      "iteration 49576: loss: 0.21553325653076172\n",
      "iteration 49577: loss: 0.2155330628156662\n",
      "iteration 49578: loss: 0.21553286910057068\n",
      "iteration 49579: loss: 0.21553273499011993\n",
      "iteration 49580: loss: 0.215532585978508\n",
      "iteration 49581: loss: 0.21553237736225128\n",
      "iteration 49582: loss: 0.21553225815296173\n",
      "iteration 49583: loss: 0.2155321091413498\n",
      "iteration 49584: loss: 0.2155318707227707\n",
      "iteration 49585: loss: 0.2155318707227707\n",
      "iteration 49586: loss: 0.21553166210651398\n",
      "iteration 49587: loss: 0.21553149819374084\n",
      "iteration 49588: loss: 0.21553125977516174\n",
      "iteration 49589: loss: 0.2155311405658722\n",
      "iteration 49590: loss: 0.21553096175193787\n",
      "iteration 49591: loss: 0.21553082764148712\n",
      "iteration 49592: loss: 0.2155306339263916\n",
      "iteration 49593: loss: 0.2155304253101349\n",
      "iteration 49594: loss: 0.21553035080432892\n",
      "iteration 49595: loss: 0.2155301868915558\n",
      "iteration 49596: loss: 0.21552999317646027\n",
      "iteration 49597: loss: 0.21552979946136475\n",
      "iteration 49598: loss: 0.21552971005439758\n",
      "iteration 49599: loss: 0.21552956104278564\n",
      "iteration 49600: loss: 0.21552935242652893\n",
      "iteration 49601: loss: 0.215529203414917\n",
      "iteration 49602: loss: 0.21552905440330505\n",
      "iteration 49603: loss: 0.21552887558937073\n",
      "iteration 49604: loss: 0.21552875638008118\n",
      "iteration 49605: loss: 0.21552853286266327\n",
      "iteration 49606: loss: 0.21552839875221252\n",
      "iteration 49607: loss: 0.21552817523479462\n",
      "iteration 49608: loss: 0.21552810072898865\n",
      "iteration 49609: loss: 0.21552793681621552\n",
      "iteration 49610: loss: 0.21552777290344238\n",
      "iteration 49611: loss: 0.21552753448486328\n",
      "iteration 49612: loss: 0.2155274897813797\n",
      "iteration 49613: loss: 0.2155272513628006\n",
      "iteration 49614: loss: 0.21552713215351105\n",
      "iteration 49615: loss: 0.21552696824073792\n",
      "iteration 49616: loss: 0.2155267298221588\n",
      "iteration 49617: loss: 0.21552667021751404\n",
      "iteration 49618: loss: 0.2155264914035797\n",
      "iteration 49619: loss: 0.21552631258964539\n",
      "iteration 49620: loss: 0.21552613377571106\n",
      "iteration 49621: loss: 0.2155260145664215\n",
      "iteration 49622: loss: 0.215525820851326\n",
      "iteration 49623: loss: 0.21552565693855286\n",
      "iteration 49624: loss: 0.21552547812461853\n",
      "iteration 49625: loss: 0.2155252993106842\n",
      "iteration 49626: loss: 0.21552523970603943\n",
      "iteration 49627: loss: 0.2155250757932663\n",
      "iteration 49628: loss: 0.2155248373746872\n",
      "iteration 49629: loss: 0.21552462875843048\n",
      "iteration 49630: loss: 0.21552443504333496\n",
      "iteration 49631: loss: 0.21552439033985138\n",
      "iteration 49632: loss: 0.21552422642707825\n",
      "iteration 49633: loss: 0.21552404761314392\n",
      "iteration 49634: loss: 0.2155238687992096\n",
      "iteration 49635: loss: 0.21552376449108124\n",
      "iteration 49636: loss: 0.21552357077598572\n",
      "iteration 49637: loss: 0.2155233919620514\n",
      "iteration 49638: loss: 0.21552327275276184\n",
      "iteration 49639: loss: 0.21552309393882751\n",
      "iteration 49640: loss: 0.2155229151248932\n",
      "iteration 49641: loss: 0.21552272140979767\n",
      "iteration 49642: loss: 0.2155226171016693\n",
      "iteration 49643: loss: 0.21552245318889618\n",
      "iteration 49644: loss: 0.21552233397960663\n",
      "iteration 49645: loss: 0.2155221402645111\n",
      "iteration 49646: loss: 0.215521901845932\n",
      "iteration 49647: loss: 0.21552181243896484\n",
      "iteration 49648: loss: 0.2155216634273529\n",
      "iteration 49649: loss: 0.21552148461341858\n",
      "iteration 49650: loss: 0.21552138030529022\n",
      "iteration 49651: loss: 0.2155212163925171\n",
      "iteration 49652: loss: 0.215520977973938\n",
      "iteration 49653: loss: 0.21552085876464844\n",
      "iteration 49654: loss: 0.21552066504955292\n",
      "iteration 49655: loss: 0.21552053093910217\n",
      "iteration 49656: loss: 0.21552035212516785\n",
      "iteration 49657: loss: 0.21552017331123352\n",
      "iteration 49658: loss: 0.21552005410194397\n",
      "iteration 49659: loss: 0.21551983058452606\n",
      "iteration 49660: loss: 0.21551963686943054\n",
      "iteration 49661: loss: 0.21551959216594696\n",
      "iteration 49662: loss: 0.2155194729566574\n",
      "iteration 49663: loss: 0.2155192345380783\n",
      "iteration 49664: loss: 0.215518981218338\n",
      "iteration 49665: loss: 0.21551890671253204\n",
      "iteration 49666: loss: 0.2155187577009201\n",
      "iteration 49667: loss: 0.21551862359046936\n",
      "iteration 49668: loss: 0.21551842987537384\n",
      "iteration 49669: loss: 0.21551832556724548\n",
      "iteration 49670: loss: 0.21551808714866638\n",
      "iteration 49671: loss: 0.21551796793937683\n",
      "iteration 49672: loss: 0.2155178040266037\n",
      "iteration 49673: loss: 0.21551759541034698\n",
      "iteration 49674: loss: 0.21551747620105743\n",
      "iteration 49675: loss: 0.21551728248596191\n",
      "iteration 49676: loss: 0.21551713347434998\n",
      "iteration 49677: loss: 0.21551696956157684\n",
      "iteration 49678: loss: 0.21551677584648132\n",
      "iteration 49679: loss: 0.21551664173603058\n",
      "iteration 49680: loss: 0.21551652252674103\n",
      "iteration 49681: loss: 0.21551629900932312\n",
      "iteration 49682: loss: 0.21551613509655\n",
      "iteration 49683: loss: 0.21551597118377686\n",
      "iteration 49684: loss: 0.2155158519744873\n",
      "iteration 49685: loss: 0.21551573276519775\n",
      "iteration 49686: loss: 0.21551553905010223\n",
      "iteration 49687: loss: 0.21551530063152313\n",
      "iteration 49688: loss: 0.21551518142223358\n",
      "iteration 49689: loss: 0.21551504731178284\n",
      "iteration 49690: loss: 0.2155148684978485\n",
      "iteration 49691: loss: 0.21551468968391418\n",
      "iteration 49692: loss: 0.21551457047462463\n",
      "iteration 49693: loss: 0.21551434695720673\n",
      "iteration 49694: loss: 0.21551422774791718\n",
      "iteration 49695: loss: 0.21551409363746643\n",
      "iteration 49696: loss: 0.2155139148235321\n",
      "iteration 49697: loss: 0.2155137062072754\n",
      "iteration 49698: loss: 0.21551363170146942\n",
      "iteration 49699: loss: 0.2155134230852127\n",
      "iteration 49700: loss: 0.21551325917243958\n",
      "iteration 49701: loss: 0.21551311016082764\n",
      "iteration 49702: loss: 0.21551290154457092\n",
      "iteration 49703: loss: 0.21551279723644257\n",
      "iteration 49704: loss: 0.21551260352134705\n",
      "iteration 49705: loss: 0.2155124396085739\n",
      "iteration 49706: loss: 0.21551235020160675\n",
      "iteration 49707: loss: 0.21551211178302765\n",
      "iteration 49708: loss: 0.21551191806793213\n",
      "iteration 49709: loss: 0.21551184356212616\n",
      "iteration 49710: loss: 0.21551163494586945\n",
      "iteration 49711: loss: 0.2155115306377411\n",
      "iteration 49712: loss: 0.21551132202148438\n",
      "iteration 49713: loss: 0.21551117300987244\n",
      "iteration 49714: loss: 0.2155110090970993\n",
      "iteration 49715: loss: 0.21551081538200378\n",
      "iteration 49716: loss: 0.21551068127155304\n",
      "iteration 49717: loss: 0.21551048755645752\n",
      "iteration 49718: loss: 0.21551036834716797\n",
      "iteration 49719: loss: 0.21551021933555603\n",
      "iteration 49720: loss: 0.21551001071929932\n",
      "iteration 49721: loss: 0.21550986170768738\n",
      "iteration 49722: loss: 0.21550972759723663\n",
      "iteration 49723: loss: 0.2155095636844635\n",
      "iteration 49724: loss: 0.21550936996936798\n",
      "iteration 49725: loss: 0.21550925076007843\n",
      "iteration 49726: loss: 0.2155091017484665\n",
      "iteration 49727: loss: 0.21550893783569336\n",
      "iteration 49728: loss: 0.21550877392292023\n",
      "iteration 49729: loss: 0.2155086100101471\n",
      "iteration 49730: loss: 0.21550846099853516\n",
      "iteration 49731: loss: 0.21550826728343964\n",
      "iteration 49732: loss: 0.21550807356834412\n",
      "iteration 49733: loss: 0.21550790965557098\n",
      "iteration 49734: loss: 0.21550779044628143\n",
      "iteration 49735: loss: 0.2155075967311859\n",
      "iteration 49736: loss: 0.21550746262073517\n",
      "iteration 49737: loss: 0.21550726890563965\n",
      "iteration 49738: loss: 0.21550710499286652\n",
      "iteration 49739: loss: 0.21550700068473816\n",
      "iteration 49740: loss: 0.21550682187080383\n",
      "iteration 49741: loss: 0.2155066728591919\n",
      "iteration 49742: loss: 0.21550650894641876\n",
      "iteration 49743: loss: 0.21550631523132324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 49744: loss: 0.2155061513185501\n",
      "iteration 49745: loss: 0.2155059576034546\n",
      "iteration 49746: loss: 0.21550588309764862\n",
      "iteration 49747: loss: 0.2155056893825531\n",
      "iteration 49748: loss: 0.2155054807662964\n",
      "iteration 49749: loss: 0.21550531685352325\n",
      "iteration 49750: loss: 0.21550524234771729\n",
      "iteration 49751: loss: 0.21550504863262177\n",
      "iteration 49752: loss: 0.21550485491752625\n",
      "iteration 49753: loss: 0.2155047357082367\n",
      "iteration 49754: loss: 0.2155044972896576\n",
      "iteration 49755: loss: 0.21550437808036804\n",
      "iteration 49756: loss: 0.2155042588710785\n",
      "iteration 49757: loss: 0.21550408005714417\n",
      "iteration 49758: loss: 0.21550388634204865\n",
      "iteration 49759: loss: 0.2155037820339203\n",
      "iteration 49760: loss: 0.21550364792346954\n",
      "iteration 49761: loss: 0.21550342440605164\n",
      "iteration 49762: loss: 0.2155032455921173\n",
      "iteration 49763: loss: 0.21550309658050537\n",
      "iteration 49764: loss: 0.21550294756889343\n",
      "iteration 49765: loss: 0.21550285816192627\n",
      "iteration 49766: loss: 0.21550261974334717\n",
      "iteration 49767: loss: 0.21550245583057404\n",
      "iteration 49768: loss: 0.21550223231315613\n",
      "iteration 49769: loss: 0.21550214290618896\n",
      "iteration 49770: loss: 0.21550193428993225\n",
      "iteration 49771: loss: 0.2155017852783203\n",
      "iteration 49772: loss: 0.21550166606903076\n",
      "iteration 49773: loss: 0.21550150215625763\n",
      "iteration 49774: loss: 0.2155013084411621\n",
      "iteration 49775: loss: 0.2155010998249054\n",
      "iteration 49776: loss: 0.21550099551677704\n",
      "iteration 49777: loss: 0.21550078690052032\n",
      "iteration 49778: loss: 0.2155006229877472\n",
      "iteration 49779: loss: 0.21550056338310242\n",
      "iteration 49780: loss: 0.21550031006336212\n",
      "iteration 49781: loss: 0.215500146150589\n",
      "iteration 49782: loss: 0.21550004184246063\n",
      "iteration 49783: loss: 0.21549983322620392\n",
      "iteration 49784: loss: 0.21549972891807556\n",
      "iteration 49785: loss: 0.21549952030181885\n",
      "iteration 49786: loss: 0.21549932658672333\n",
      "iteration 49787: loss: 0.2154991328716278\n",
      "iteration 49788: loss: 0.21549901366233826\n",
      "iteration 49789: loss: 0.21549883484840393\n",
      "iteration 49790: loss: 0.215498685836792\n",
      "iteration 49791: loss: 0.21549853682518005\n",
      "iteration 49792: loss: 0.2154984027147293\n",
      "iteration 49793: loss: 0.21549825370311737\n",
      "iteration 49794: loss: 0.21549808979034424\n",
      "iteration 49795: loss: 0.21549789607524872\n",
      "iteration 49796: loss: 0.21549773216247559\n",
      "iteration 49797: loss: 0.21549761295318604\n",
      "iteration 49798: loss: 0.2154974490404129\n",
      "iteration 49799: loss: 0.21549728512763977\n",
      "iteration 49800: loss: 0.21549710631370544\n",
      "iteration 49801: loss: 0.2154969722032547\n",
      "iteration 49802: loss: 0.21549677848815918\n",
      "iteration 49803: loss: 0.21549661457538605\n",
      "iteration 49804: loss: 0.2154964953660965\n",
      "iteration 49805: loss: 0.2154962569475174\n",
      "iteration 49806: loss: 0.21549610793590546\n",
      "iteration 49807: loss: 0.21549594402313232\n",
      "iteration 49808: loss: 0.21549582481384277\n",
      "iteration 49809: loss: 0.21549567580223083\n",
      "iteration 49810: loss: 0.21549543738365173\n",
      "iteration 49811: loss: 0.21549531817436218\n",
      "iteration 49812: loss: 0.21549513936042786\n",
      "iteration 49813: loss: 0.2154950201511383\n",
      "iteration 49814: loss: 0.21549482643604279\n",
      "iteration 49815: loss: 0.21549467742443085\n",
      "iteration 49816: loss: 0.21549458801746368\n",
      "iteration 49817: loss: 0.21549436450004578\n",
      "iteration 49818: loss: 0.21549418568611145\n",
      "iteration 49819: loss: 0.21549400687217712\n",
      "iteration 49820: loss: 0.21549387276172638\n",
      "iteration 49821: loss: 0.21549372375011444\n",
      "iteration 49822: loss: 0.2154935598373413\n",
      "iteration 49823: loss: 0.21549341082572937\n",
      "iteration 49824: loss: 0.21549320220947266\n",
      "iteration 49825: loss: 0.21549305319786072\n",
      "iteration 49826: loss: 0.2154928743839264\n",
      "iteration 49827: loss: 0.21549268066883087\n",
      "iteration 49828: loss: 0.21549256145954132\n",
      "iteration 49829: loss: 0.2154923975467682\n",
      "iteration 49830: loss: 0.21549227833747864\n",
      "iteration 49831: loss: 0.2154920995235443\n",
      "iteration 49832: loss: 0.21549193561077118\n",
      "iteration 49833: loss: 0.21549177169799805\n",
      "iteration 49834: loss: 0.2154916226863861\n",
      "iteration 49835: loss: 0.21549148857593536\n",
      "iteration 49836: loss: 0.21549129486083984\n",
      "iteration 49837: loss: 0.21549110114574432\n",
      "iteration 49838: loss: 0.21549096703529358\n",
      "iteration 49839: loss: 0.21549078822135925\n",
      "iteration 49840: loss: 0.21549062430858612\n",
      "iteration 49841: loss: 0.2154904305934906\n",
      "iteration 49842: loss: 0.21549034118652344\n",
      "iteration 49843: loss: 0.21549013257026672\n",
      "iteration 49844: loss: 0.21549002826213837\n",
      "iteration 49845: loss: 0.21548978984355927\n",
      "iteration 49846: loss: 0.21548965573310852\n",
      "iteration 49847: loss: 0.2154894769191742\n",
      "iteration 49848: loss: 0.21548934280872345\n",
      "iteration 49849: loss: 0.21548907458782196\n",
      "iteration 49850: loss: 0.21548905968666077\n",
      "iteration 49851: loss: 0.21548882126808167\n",
      "iteration 49852: loss: 0.21548867225646973\n",
      "iteration 49853: loss: 0.2154885232448578\n",
      "iteration 49854: loss: 0.21548835933208466\n",
      "iteration 49855: loss: 0.2154882401227951\n",
      "iteration 49856: loss: 0.21548807621002197\n",
      "iteration 49857: loss: 0.21548783779144287\n",
      "iteration 49858: loss: 0.21548771858215332\n",
      "iteration 49859: loss: 0.2154875248670578\n",
      "iteration 49860: loss: 0.21548736095428467\n",
      "iteration 49861: loss: 0.21548721194267273\n",
      "iteration 49862: loss: 0.21548700332641602\n",
      "iteration 49863: loss: 0.21548691391944885\n",
      "iteration 49864: loss: 0.21548672020435333\n",
      "iteration 49865: loss: 0.2154865711927414\n",
      "iteration 49866: loss: 0.21548643708229065\n",
      "iteration 49867: loss: 0.21548621356487274\n",
      "iteration 49868: loss: 0.215486079454422\n",
      "iteration 49869: loss: 0.21548588573932648\n",
      "iteration 49870: loss: 0.21548573672771454\n",
      "iteration 49871: loss: 0.21548566222190857\n",
      "iteration 49872: loss: 0.21548542380332947\n",
      "iteration 49873: loss: 0.21548521518707275\n",
      "iteration 49874: loss: 0.21548517048358917\n",
      "iteration 49875: loss: 0.21548490226268768\n",
      "iteration 49876: loss: 0.21548478305339813\n",
      "iteration 49877: loss: 0.2154845893383026\n",
      "iteration 49878: loss: 0.21548452973365784\n",
      "iteration 49879: loss: 0.21548430621623993\n",
      "iteration 49880: loss: 0.2154841423034668\n",
      "iteration 49881: loss: 0.21548399329185486\n",
      "iteration 49882: loss: 0.21548378467559814\n",
      "iteration 49883: loss: 0.2154836356639862\n",
      "iteration 49884: loss: 0.21548350155353546\n",
      "iteration 49885: loss: 0.21548330783843994\n",
      "iteration 49886: loss: 0.2154831886291504\n",
      "iteration 49887: loss: 0.21548298001289368\n",
      "iteration 49888: loss: 0.21548283100128174\n",
      "iteration 49889: loss: 0.21548262238502502\n",
      "iteration 49890: loss: 0.21548250317573547\n",
      "iteration 49891: loss: 0.21548232436180115\n",
      "iteration 49892: loss: 0.21548223495483398\n",
      "iteration 49893: loss: 0.21548207104206085\n",
      "iteration 49894: loss: 0.21548190712928772\n",
      "iteration 49895: loss: 0.2154817134141922\n",
      "iteration 49896: loss: 0.21548151969909668\n",
      "iteration 49897: loss: 0.21548137068748474\n",
      "iteration 49898: loss: 0.215481236577034\n",
      "iteration 49899: loss: 0.2154810130596161\n",
      "iteration 49900: loss: 0.2154809534549713\n",
      "iteration 49901: loss: 0.2154807597398758\n",
      "iteration 49902: loss: 0.2154805213212967\n",
      "iteration 49903: loss: 0.21548040211200714\n",
      "iteration 49904: loss: 0.21548029780387878\n",
      "iteration 49905: loss: 0.21548011898994446\n",
      "iteration 49906: loss: 0.21547996997833252\n",
      "iteration 49907: loss: 0.2154797613620758\n",
      "iteration 49908: loss: 0.21547961235046387\n",
      "iteration 49909: loss: 0.21547946333885193\n",
      "iteration 49910: loss: 0.21547922492027283\n",
      "iteration 49911: loss: 0.21547909080982208\n",
      "iteration 49912: loss: 0.21547897160053253\n",
      "iteration 49913: loss: 0.21547874808311462\n",
      "iteration 49914: loss: 0.2154785841703415\n",
      "iteration 49915: loss: 0.21547846496105194\n",
      "iteration 49916: loss: 0.21547827124595642\n",
      "iteration 49917: loss: 0.21547813713550568\n",
      "iteration 49918: loss: 0.21547801792621613\n",
      "iteration 49919: loss: 0.21547777950763702\n",
      "iteration 49920: loss: 0.2154776155948639\n",
      "iteration 49921: loss: 0.21547755599021912\n",
      "iteration 49922: loss: 0.21547730267047882\n",
      "iteration 49923: loss: 0.2154771387577057\n",
      "iteration 49924: loss: 0.21547694504261017\n",
      "iteration 49925: loss: 0.2154768407344818\n",
      "iteration 49926: loss: 0.21547672152519226\n",
      "iteration 49927: loss: 0.21547646820545197\n",
      "iteration 49928: loss: 0.21547630429267883\n",
      "iteration 49929: loss: 0.2154761254787445\n",
      "iteration 49930: loss: 0.21547603607177734\n",
      "iteration 49931: loss: 0.2154758870601654\n",
      "iteration 49932: loss: 0.2154756784439087\n",
      "iteration 49933: loss: 0.21547551453113556\n",
      "iteration 49934: loss: 0.215475395321846\n",
      "iteration 49935: loss: 0.21547512710094452\n",
      "iteration 49936: loss: 0.21547503769397736\n",
      "iteration 49937: loss: 0.21547487378120422\n",
      "iteration 49938: loss: 0.21547472476959229\n",
      "iteration 49939: loss: 0.21547456085681915\n",
      "iteration 49940: loss: 0.21547436714172363\n",
      "iteration 49941: loss: 0.21547424793243408\n",
      "iteration 49942: loss: 0.21547408401966095\n",
      "iteration 49943: loss: 0.215473935008049\n",
      "iteration 49944: loss: 0.21547380089759827\n",
      "iteration 49945: loss: 0.21547357738018036\n",
      "iteration 49946: loss: 0.21547341346740723\n",
      "iteration 49947: loss: 0.2154732644557953\n",
      "iteration 49948: loss: 0.21547310054302216\n",
      "iteration 49949: loss: 0.2154729813337326\n",
      "iteration 49950: loss: 0.2154727727174759\n",
      "iteration 49951: loss: 0.21547260880470276\n",
      "iteration 49952: loss: 0.2154725044965744\n",
      "iteration 49953: loss: 0.21547222137451172\n",
      "iteration 49954: loss: 0.21547210216522217\n",
      "iteration 49955: loss: 0.21547195315361023\n",
      "iteration 49956: loss: 0.21547181904315948\n",
      "iteration 49957: loss: 0.21547159552574158\n",
      "iteration 49958: loss: 0.21547146141529083\n",
      "iteration 49959: loss: 0.21547122299671173\n",
      "iteration 49960: loss: 0.21547110378742218\n",
      "iteration 49961: loss: 0.21547095477581024\n",
      "iteration 49962: loss: 0.2154707908630371\n",
      "iteration 49963: loss: 0.21547064185142517\n",
      "iteration 49964: loss: 0.21547047793865204\n",
      "iteration 49965: loss: 0.2154703587293625\n",
      "iteration 49966: loss: 0.2154701203107834\n",
      "iteration 49967: loss: 0.21547003090381622\n",
      "iteration 49968: loss: 0.21546980738639832\n",
      "iteration 49969: loss: 0.21546968817710876\n",
      "iteration 49970: loss: 0.21546950936317444\n",
      "iteration 49971: loss: 0.2154693603515625\n",
      "iteration 49972: loss: 0.21546919643878937\n",
      "iteration 49973: loss: 0.21546903252601624\n",
      "iteration 49974: loss: 0.2154688537120819\n",
      "iteration 49975: loss: 0.21546871960163116\n",
      "iteration 49976: loss: 0.21546845138072968\n",
      "iteration 49977: loss: 0.2154683619737625\n",
      "iteration 49978: loss: 0.2154681235551834\n",
      "iteration 49979: loss: 0.21546800434589386\n",
      "iteration 49980: loss: 0.21546784043312073\n",
      "iteration 49981: loss: 0.21546772122383118\n",
      "iteration 49982: loss: 0.21546760201454163\n",
      "iteration 49983: loss: 0.2154674232006073\n",
      "iteration 49984: loss: 0.21546721458435059\n",
      "iteration 49985: loss: 0.21546705067157745\n",
      "iteration 49986: loss: 0.21546688675880432\n",
      "iteration 49987: loss: 0.21546670794487\n",
      "iteration 49988: loss: 0.2154664546251297\n",
      "iteration 49989: loss: 0.2154664546251297\n",
      "iteration 49990: loss: 0.2154662162065506\n",
      "iteration 49991: loss: 0.21546606719493866\n",
      "iteration 49992: loss: 0.21546593308448792\n",
      "iteration 49993: loss: 0.21546581387519836\n",
      "iteration 49994: loss: 0.21546563506126404\n",
      "iteration 49995: loss: 0.21546539664268494\n",
      "iteration 49996: loss: 0.2154652625322342\n",
      "iteration 49997: loss: 0.2154650241136551\n",
      "iteration 49998: loss: 0.21546494960784912\n",
      "iteration 49999: loss: 0.2154647558927536\n",
      "iteration 50000: loss: 0.21546459197998047\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.02\n",
    "EPOCHS = 50000\n",
    "\n",
    "# Initializing a model\n",
    "model = Logreg(X_train.shape[1], 1)\n",
    "\n",
    "# Optimizing criterion and optimizing algorithm\n",
    "# ignore talking about momentum and nesterov\n",
    "criterion = BCELoss(weight=torch.from_numpy(weights).float())\n",
    "optimizer = SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, nesterov=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model.fit(X_train_t, Y_train_t, EPOCHS, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "024b97d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8620689655172413\n",
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sn # used for creating a heatmap for displaying confusion matrix\n",
    "\n",
    "# testing data torch.tensors\n",
    "X_test_t = torch.from_numpy(X_test).float()\n",
    "Y_test_t = torch.from_numpy(Y_test).float()\n",
    "\n",
    "# Eval mode -- Explain that it is used to \n",
    "# set the model into evaluation mode, which \n",
    "# turns off certain layers used during training\n",
    "model.eval()\n",
    "\n",
    "# prediction\n",
    "Y_pred_t = model(X_test_t)\n",
    "# convert probabilities to 0 or 1\n",
    "Y_pred = (Y_pred_t.detach().numpy() > 0.5).astype(np.int32)\n",
    "\n",
    "F1 = f1_score(Y_test, Y_pred)\n",
    "accuracy = np.sum(Y_pred == Y_test)/Y_test.size\n",
    "\n",
    "print(f\"F1 Score: {F1}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "295c7dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_conf_mat(Y_pred, Y_gt):\n",
    "    \"\"\"\n",
    "    Plots Confusion Matrix\n",
    "    Args:\n",
    "        Y_pred: predictions array (np.array)\n",
    "        Y_gt: ground truth array (np.array)\n",
    "    \"\"\"\n",
    "    conf_mat = pd.DataFrame(confusion_matrix(Y_gt, Y_pred, labels=[1,0]))\n",
    "    sn.heatmap(conf_mat, annot=True)\n",
    "    \n",
    "plot_conf_mat(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c7c7c",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "After training our Logistic Regression Classifier, we observe a very high Accuracy and F1 Score, which is awesome! Lets keep track of our progress:\n",
    "\n",
    "| Model | Observations | Accuracy | F1 | \n",
    "|:- |:- |:- | :- |\n",
    "| Logistic Regression | High accuracy, very few false positives and false negatives | 88% | 88% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57d2328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save state dictionary of weights and bias\n",
    "if __name__ == \"__main__\":\n",
    "    torch.save(model.state_dict(), \"suv_predictor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b085e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
