{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "992d9986",
   "metadata": {},
   "source": [
    "# Topic 2 -- Logistic Regression\n",
    "\n",
    "Welcome to topic 2 of the Beginner AI Course! In this notebook, we are going to explore the **classification** problem in supervised learning, how **logistic regression** triumphs over **linear regression** in this area, as well as see logistic regression in action predicting the types of Pokemon!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f0d0da",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Installing Dependencies](#installing)\n",
    "\n",
    "\n",
    "2. [Linear Regression vs Classification](#linregfail)\n",
    "\n",
    "\n",
    "3. [Logistic Regression](#logreg)\n",
    "    - [Sigmoid Activation Function](#sigmoid)\n",
    "    - [Cost Function](#cost)\n",
    "    - [Putting it all together](#putting)\n",
    "   \n",
    "   \n",
    "4. [Binary Classification](#binary)\n",
    "    - [Pokemon Classification Problem](#pokemon)\n",
    "    - [Choosing our Features](#choosing)\n",
    "    - [Imbalanced Classes](#imbalanced)\n",
    "    - [Training our Logistic Regression Model](#training1)\n",
    "    - [Confusion Matrix](#conf)\n",
    "    - [Observations](#obs1)\n",
    "    \n",
    "    \n",
    "5. [Multi-Class Classification](#multi)\n",
    "    - [One vs All](#onevall)\n",
    "    - [One-Hot Encoding](#onehot)\n",
    "    - [Choosing our Features](#choosingf)\n",
    "    - [Choosing our Labels](#choosingl)\n",
    "    - [Training a Multi-Class Classifier](#train_m)\n",
    "    - [Observations](#obs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab1ef81",
   "metadata": {},
   "source": [
    "## Installing Dependencies <a name=\"installing\">\n",
    "\n",
    "Lets first install the dependencies needed for this demo. Here are a list of the dependencies and descriptions on what they are for:\n",
    "\n",
    "- **Numpy**: Powerful linear regression library\n",
    "- **Pandas**: Used for data visualization and organization\n",
    "- **SciKitLearn**: Maching learning library containing the `LogisticRegression` class\n",
    "- **Bokeh**, **MatPlotLib**, and  **SeaBorn**: Plots and Graphs for visualizing the data\n",
    "- **disp_utils**: A custom module for displaying the demo visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae25d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from disp_utils import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fcece7",
   "metadata": {},
   "source": [
    "## Linear Regression vs Classification <a name=\"linregfail\">\n",
    "\n",
    "Lets set the scene: you are trying to train a machine learning model that will predict whether you get admitted into the **NBA** based on your **height**. Notice that this is a classification problem, as there are two discrete values that the model is allowed to output: Either **yes**, you get admitted, or **no**, you are rejected. We will denote $\\hat{y}$ as the output, and it could only take on **1 or 0**, 1 being yes and 0 being no. Lets display a plot of what a hypothetical dataset would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30791b76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_height_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf33c83f",
   "metadata": {},
   "source": [
    "Here, we can see that at around 1.8m, you will be admitted into the NBA. After running linear regression, we are able to fit a linear hypothesis function to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda2e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_height_line1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc8a52d",
   "metadata": {},
   "source": [
    "One thing to note is that the linear hypothesis function is not discrete, which is okay, because we can assign the output value of the function as a **probability** that you will be admitted given your height. For example, if you are 1.6 metres tall, there is roughly a 10% probability that you will be admitted, whereas if you are 2 meters tall, there is a 90% probability you will be admitted. We will use this probability metric to make a cut-off: **all probabilities greater or equal to 50% we will predict $\\hat{y} = 1$, and all probabilities less than 50% we will predict $\\hat{y} = 0$.** \n",
    "\n",
    "However, there are a few problems to discuss: if your height is less than 1.55 metres, then your probability of getting admitted is less than 0%, and if your height is over 2.05 metres, your probability is greater than 100%. Furthermore, lets say we add some **outliers**; lets add the height of 1.0 metres to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7eba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_height_line2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5071b4dd",
   "metadata": {},
   "source": [
    "Here, we can see that adding an outlier has significantly shifted the **decision boundary**. Before, a height of 1.8 meters and above will have a high likelyhood of getting admitted, whereas now, with the introduction of one datapoint, the acceptable height has decreased to 1.67m.\n",
    "\n",
    "**Therefore, Linear Regression cannot be used for classification problems.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679291e",
   "metadata": {},
   "source": [
    "## Logistic Regression <a name=\"logreg\">\n",
    "\n",
    "Despite the name \"regression\", logistic regression is an algorithm used for classification problems. The base concept is **very similar to linear regression**; you still have a linear feed forward step. This time I will use the letter $z$ to denote the linear output:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <div>\n",
    "    &nbsp;\n",
    "    </div>\n",
    " $z = wx + b$\n",
    "</div>\n",
    "\n",
    "### Sigmoid Activation Function <a name=\"sigmoid\">\n",
    "Now, however, we input the linear results into the **sigmoid function**. The sigmoid function is one type of **activation function** that bounds the output between 0 and 1, which eliminates probabilities of greater than 1 or less than 0. Also, using the sigmoid activation function ensures that the **decision boundary** would not get shifted due to **outliers**. \n",
    "    \n",
    "The sigmoid function is represented by the formula down below:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <div>\n",
    "    &nbsp;\n",
    "    </div>\n",
    " $\\begin{align}\\sigma(z) = {1\\over 1 + e^{-z}}\\end{align}$\n",
    "</div>\n",
    "    \n",
    "To visualize the sigmoid function, lets create a sigmoid function down below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e31621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7363b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_func(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849d61ff",
   "metadata": {},
   "source": [
    "### Cost Function <a name=\"cost\">\n",
    "    \n",
    "Previously, we learned that Linear Regression uses the **Mean Squared Error (MSE)** cost function to measure the error in the fit. While MSE works great for linear regression, it does not work very well for logistic regression. This is because due to the **sigmoid function** used in logistic regression, the resulting cost function is **non-convex**. Take a look at the figures below showing the non-convex nature of MSE used with the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ad3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_convex()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b49d7f",
   "metadata": {},
   "source": [
    "Non-convex functions can be problematic to **gradient descent**. To make the cost convex, we need a different cost function:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <div>\n",
    "    &nbsp;\n",
    "    </div>\n",
    " $\\begin{align}C(w, b) = {1\\over m} \\sum\\limits_{i=1}^m -y^{(i)}\\log(a^{(i)})-(1-y^{(i)})\\log(1-a^{(i)})\\end{align}$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6535363",
   "metadata": {},
   "source": [
    "This cost function is called **Binary Cross Entropy (BCE)**, and is the cost function of choice for logistic regression. This cost function is guaranteed to be convex when used with the **sigmoid activation function**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefb2e9b",
   "metadata": {},
   "source": [
    "### Putting it all together <a name=\"putting\">\n",
    "\n",
    "All in all, how logistic regression learns is very similar to linear regression, other than the activation function and the cost function. Here's a summary of how it all works, shown in **pseudo-code**:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <div>\n",
    "    &nbsp;\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "```python\n",
    "# Linear Regression:\n",
    "for i in range(iterations):\n",
    "    prediction = w*x + b\n",
    "    \n",
    "    cost = MSE_cost_function(prediction, label)\n",
    "    slopes = cost.get_slopes()\n",
    "    update(w, b).with(slopes)\n",
    "    \n",
    "# Logistic Regression:\n",
    "for i in range(iterations):\n",
    "    z = w*x + b\n",
    "    prediction = sigmoid(z)\n",
    "    \n",
    "    cost = BCE_cost_function(prediction, label)\n",
    "    slopes = cost.get_slopes()\n",
    "    update(w, b).with(slopes)\n",
    "```\n",
    "    \n",
    "\n",
    "[https://www.desmos.com/calculator/z4petl2oxa](https://www.desmos.com/calculator/z4petl2oxa)\n",
    "    \n",
    "[https://www.geogebra.org/3d/yku5x5tf](https://www.geogebra.org/3d/yku5x5tf)\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc4262f",
   "metadata": {},
   "source": [
    "## Binary Classification <a name=\"binary\">\n",
    "    \n",
    "Binary Classification, as the name suggests, predicts on an output space of **two** states. An example of which is NBA Admittance, where the two possible states are either **yes** or **no**. This is the most basic form of logistic regression, and is the basis of more advanced forms of logistic regression.\n",
    "    \n",
    "### Pokemon Classification Problem <a name=\"pokemon\">\n",
    "Lets say we were to build a classifier to classify whether or not a Pokemon is **legendary** or not. This is a perfect example of binary classification. Let's load in the `pokemon.csv` file, which we will use for the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee7d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('datasets/pokemon.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1d5bf",
   "metadata": {},
   "source": [
    "Taking a look at the dataset, notice that the rows are **truncated**. We can see the first few examples, as well as the last. Looking at the `is_legendary` column, we notice that all of the legendary pokemons are arranged at the bottom of our dataset. In order get a **uniform distribution** of either class in the training and testing sets, we need to **shuffle** our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1609031",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd50d96",
   "metadata": {},
   "source": [
    "### Choosing our Features <a name=\"choosing\">\n",
    "    \n",
    "Taking a look at the dataset, there are quite a few features that we can include into our training and testing sets. However, we probably do not need to include all of the features, as many features are redundant. Since we are training a classifier to classify whether or not a Pokemon is **legendary**, we can possibly choose the features `sp_attack`, `sp_defense`, and `weight_kg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9467791",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset[['sp_attack', 'sp_defense', 'weight_kg']].to_numpy()\n",
    "labels = dataset['is_legendary'].to_numpy()\n",
    "\n",
    "print(f\"features shape: {features.shape}\")\n",
    "print(f\"labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79b56e",
   "metadata": {},
   "source": [
    "### Imbalanced Classes <a name=\"imbalanced\">\n",
    "\n",
    "#### *Intuition*\n",
    "\n",
    "> *Suppose you were training a classifier for predicting whether a person owns a mansion or not. You gather together a dataset of 100 examples, containing features such as income, occupation, etc. In your dataset, 99 people do not own mansions, and 1 person owns a mansion. You train a classifier, and is impressed by the __98.5%__ accuracy that you obtained! However, I come in and present a model that performs better than yours! I achieved 99% accuracy by __predicting all zeros__. It might get that one instance wrong, but the other 99 instances it is correct.*\n",
    "\n",
    "Sometimes in the real world, it is **not feasible** to get data with **perfectly balanced classes**. When the classes are not balanced, you will often train models that will be **biased** towards the more frequent class and simply not predict on the less frequent class. Here, we print the number of legendary Pokemons and the number of non-legendary Pokemons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9922ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the number of legendary pokemon\n",
    "num_legendaries = np.sum(dataset['is_legendary'].to_numpy())\n",
    "# total pokemon\n",
    "total_pokemon = dataset.shape[0]\n",
    "\n",
    "print(f\"number of legendary pokemon: {num_legendaries}\")\n",
    "print(f\"number of non-legendary pokemon: {total_pokemon - num_legendaries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd7b5b",
   "metadata": {},
   "source": [
    "The classes in this case are quite **skewed** towards the non-legendary Pokemons in this case. We will resolve that when setting up the logistic regression model.\n",
    "\n",
    "### Training our Logistic Regression Model <a name=\"training1\">\n",
    "\n",
    "Training the Logistic Regression model is quite similar to training a Linear Regression model as we have seen before. This time, we are going to use the `LogisticRegressionCV`class instead of the `LinearRegression` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1baa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# converts all NaN to 0\n",
    "features = np.nan_to_num(features, nan=0)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc5c63d",
   "metadata": {},
   "source": [
    "Remember the unbalanced classes mentioned above? Here, we are going to pass in the parameter `class_weight='balanced'` into the `LogisticRegressionCV` class, which would automatically balance out the classes by **emphasizing the importance of the less frequent class**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1987dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "logreg = LogisticRegressionCV(max_iter=3000, class_weight='balanced', verbose=5, n_jobs=-1)\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "\n",
    "\n",
    "X_train_p = poly.fit_transform(X_train)\n",
    "X_test_p = poly.fit_transform(X_test)\n",
    "Y_train_p = Y_train\n",
    "Y_test_p = Y_test\n",
    "\n",
    "print(f\"X_train_p shape: {X_train_p.shape}\")\n",
    "print(f\"Y_train_p shape: {Y_train_p.shape}\")\n",
    "print(f\"X_test_p shape: {X_test_p.shape}\")\n",
    "print(f\"Y_test_p shape: {Y_test_p.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade254f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train_p, Y_train_p)\n",
    "\n",
    "print(f\"Accuracy: {logreg.score(X_test_p, Y_test_p)}\")\n",
    "\n",
    "Y_pred_p = logreg.predict(X_test_p)\n",
    "\n",
    "print(f\"F1 Score: {f1_score(Y_test_p, Y_pred_p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522cd435",
   "metadata": {},
   "source": [
    "### Confusion Matrix <a name=\"conf\">\n",
    "\n",
    "Often times in classification scenarios, we will evaluate our models by running on the test set and plotting a **confusion matrix**. The **confusion matrix** shows the **ground truth** values on the **horizontal axis** as well as the **predicted values** on the **vertical axis**. The resulting grid then displays frequency of the outcomes of prediction -- whether the label was `True` and the model predicted `False`, or any combination of label values vs prediction values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be74232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_pred = logreg.predict(X_test_p)\n",
    "Y_true = Y_test_p\n",
    "\n",
    "conf_mat = pd.DataFrame(confusion_matrix(Y_true, Y_pred, labels=[0, 1]))\n",
    "sn.heatmap(conf_mat, annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12021d",
   "metadata": {},
   "source": [
    "### Observations <a name=\"obs1\">\n",
    "\n",
    "Here we see that we obtained an **Accuracy** of around 90%, and an F1 Score of 60%. Notice how we are using these two metrics instead of $R^2$, which was used in linear regression. The accuracy metric measures the number of correct predictions vs the total predictions, and the F1 Score is a combination of precision and recall. Furthermore, the confusion matrix gives us a sense of what actually happened with all the predictions. Let's fill out the observations table down below:\n",
    "\n",
    "\n",
    "| Model | Observations | Accuracy | F1 |\n",
    "|:----- |:------------| :--------| :--- |\n",
    "|Logistic Regression (Binary) | Fairly high accuracy, many false positives in the confusion matrix | 90% | 60% |\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33af2c14",
   "metadata": {},
   "source": [
    "## Multi-Class Classification <a name=\"multi\">\n",
    "\n",
    "Most of the classification problems nowadays deal with predicting across multiple classes. For example, *is this a picture of a cat, dog, bird, or horse?* One naive way of performing multi-class classification is to assign integer labels to each class:\n",
    "    \n",
    "    - cat: 0\n",
    "    - dog: 1\n",
    "    - bird: 2\n",
    "    - horse: 3\n",
    "\n",
    "This however becomes a **regression problem** -- rather, a **poorly performing** one at best, and since these integers are comparable, there is a chance that the model can learn a `cat`< `dog` < `bird` < `horse` relation. It is worth saying that sklearn allows you to label classes like this, however behind the scenes, it is converting this label encoding into **One-Hot Encoding**\n",
    "    \n",
    "### One vs All <a name=\"onevall\">\n",
    "\n",
    "Consider training 4 Binary Classifiers: the first one predicts on whether it is a picture of a cat or not, the second one predicts whether it is a picture of a dog or not, and so on. This is called **One vs All** classification -- for each classifier, you are setting one class as the output $\\hat{y} = 1$, and the rest of the classes as $\\hat{y} = 0$.\n",
    "    \n",
    "This affirms the idea mentioned above that *Binary Classification forms the basis of more advanced classification methods [multi-class]*. Each class is effectively governed by its own binary classifier, and the final prediction is decided by which ever classifier outputs the **largest value**.\n",
    "    \n",
    "### One-Hot Encoding <a name=\"onehot\">\n",
    "    \n",
    "When training a multi-class model, since you are effectively training multiple classifiers, each classifier needs an output label. These labels can be stored into an `np.array` as shown:\n",
    "    \n",
    "```python\n",
    "    \n",
    "# Individual examples\n",
    "cat = np.array([[1, 0, 0, 0]])\n",
    "dog = np.array([[0, 1, 0, 0]])\n",
    "bird = np.array([[0, 0, 1, 0]])\n",
    "horse = np.array([[0, 0, 0, 1]])\n",
    "    \n",
    "# Labels set\n",
    "Y_train = np.array([\n",
    "    [1 ,0, 0, 0],  # cat\n",
    "    [0, 0, 1, 0],  # bird\n",
    "    [0, 1, 0, 0]   # dog\n",
    "    ...\n",
    "])\n",
    "```\n",
    "    \n",
    "This is called **One-Hot Encoding**, where the true class is labeled with a 1, and everywhere else is labeled with a 0.\n",
    "    \n",
    "### Choosing our Features <a name=\"choosingf\">\n",
    "    \n",
    "We are going to train a Multi-class Logistic Regression Classifier to predict whether the pokemon is **Fire, Water, Grass, or None of the Above (NA)**. First, we will print out the `dataset` to give ourselves a little refresher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686f484b",
   "metadata": {},
   "source": [
    "One thing to ask ourselves when choosing features is *What features are possibly related to the problem?* After all, using `weight_kg` as a feature to predict the type of pokemon is probably not the best idea. Here, we will choose the features that tell us how well a Pokemon of a certain type does **against another type**. For example, an **Ice** type Pokemon might do very well against **Fire**, but not so well against other **Ice** types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = list(dataset.columns[1:19])\n",
    "print(input_features, '\\n')\n",
    "features_m = dataset[input_features].to_numpy()\n",
    "\n",
    "print(f\"features_m shape: {features_m.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed522f",
   "metadata": {},
   "source": [
    "### Choosing our Labels <a name=\"choosingl\">\n",
    "    \n",
    "For this classification problem, we want to make **One-Hot encodings** of the types `\"Fire\", \"Water\", \"Grass\", \"NA\"`. Looking at the `dataset`, we can see the `type1` column containing the primary types of the Pokemon. These are currently stored as datatype `string`, however we want to convert them to integers. `pd.get_dummies()` is a function that will create One-Hot encoding of all the different values of strings it sees in a column and store it as a spearate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38d5dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "type1_dummies = pd.get_dummies(dataset[['type1']])\n",
    "type1_dummies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e405b",
   "metadata": {},
   "source": [
    "From the 18 columns in `type1_dummies`, we will only need 3 columns. These three columns (`\"type1_fire, type1_water, type1_grass\"`) are stored back into `dataset` as new columns. The `\"NA\"` column is created via the logic of `not(\"Fire\" or \"Water\" or \"Grass\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ffdd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['fire'] = type1_dummies['type1_fire']\n",
    "dataset['water'] = type1_dummies['type1_water']\n",
    "dataset['grass'] = type1_dummies['type1_grass']\n",
    "dataset['NA'] = ~(type1_dummies['type1_fire'] | type1_dummies['type1_water'] | type1_dummies['type1_grass']) - 254\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98591fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_m = dataset[['fire', 'water', 'grass', 'NA']].to_numpy()\n",
    "print(f\"labels_m shape: {labels_m.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6890bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_m, X_test_m, Y_train_m, Y_test_m = train_test_split(features_m, labels_m, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf8a7d4",
   "metadata": {},
   "source": [
    "### Training the Multi-Class Classifier <a name=\"train_m\">\n",
    "\n",
    "Remember how training a multi-class classifier is like training a bunch of binary classifiers based on how many classes you have? When it comes to implementing a multi-class classifier in SKLearn, we will fit a `LogisticRegressionCV()` object **within** a `OneVsRestClassifier()` object, which effectively **creates multiple** `LogisticRegressionCV()` classifiers for binary classification of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e012147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "logreg_m = OneVsRestClassifier(LogisticRegressionCV(max_iter=2000, class_weight='balanced'), n_jobs=-1)\n",
    "\n",
    "logreg_m.fit(X_train_m, Y_train_m)\n",
    "accuracy = logreg_m.score(X_test_m, Y_test_m)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "Y_pred_raw = logreg_m.predict(X_test_m)\n",
    "\n",
    "Y_pred = np.argmax(Y_pred_raw, axis=1)\n",
    "Y_true = np.argmax(Y_test_m, axis=1)\n",
    "\n",
    "print(f\"F1 Score: {f1_score(Y_true, Y_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02611c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = pd.DataFrame(confusion_matrix(Y_true, Y_pred),\n",
    "                        index=[\"Fire\", \"Water\", \"Grass\", \"NA\"],\n",
    "                        columns=[\"Fire\", \"Water\", \"Grass\", \"NA\"])\n",
    "sn.heatmap(conf_mat, annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed358060",
   "metadata": {},
   "source": [
    "### Observations <a name=\"obs2\">\n",
    "\n",
    "We were able to obtain an accuracy of about 95% as well as an F1 Score of 97%, which is fantastic. Looking at the confusion matrix, we can see very good testing accuracy with select few misclassifications. Let's popoulate the observations table:\n",
    "    \n",
    "| Model | Observations | Accuracy | F1 |\n",
    "|:----- |:------------| :--------| :--- |\n",
    "|Logistic Regression (Binary) | Fairly high accuracy, many false positives in the confusion matrix | 90% |60% |\n",
    "| Logistic Regression (Multi-class)| Very high accuracy, very few misclassifications |95%| 97% |\n",
    "     \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9fc986",
   "metadata": {},
   "source": [
    "## $\\mathcal{Fin}$\n",
    "\n",
    "Congrats on making it to the end of this Notebook! Our next activity is to create a Logistic Regression Classifier in **Pytorch** to predict whether or not you should buy an SUV.\n",
    "<img src=\"images/umaru.png\" alt=\"cannot display image\" style=\"width=700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407b5b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
