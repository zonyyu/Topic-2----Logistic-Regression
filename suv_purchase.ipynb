{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "name": "suv_purchase.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96f33ffb"
      },
      "source": [
        "# SUV Purchase Predictor\n",
        "\n",
        "In this notebook, we will train a logistic regression model to predict whether or not one should buy an SUV given their age, gender, and annual salary. The purpose of this programming exercise is to expose students to the **PyTorch** framework, as well as diving deeper into the architecture of learning algorithms.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Suppose you are thinking about buying an SUV, however you are not sure whether or not you should drop the money on a new car. You decide to make a classifier that learns people's decisions in the past to aid you in your own decision. You are given a dataset with the features `UserID`, `Gender`, `Age`, `EstimatedSalary`, as well as your label `Purchased`.\n",
        "\n",
        "## Unpacking the dataset"
      ],
      "id": "96f33ffb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aa5aaf4",
        "outputId": "b9eee032-7061-4cad-fc3d-bccd844fee77"
      },
      "source": [
        "!unzip colab\n",
        "!pip install numpy pandas torch=1.8.1 sklearn matplotlib seaborn\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)"
      ],
      "id": "6aa5aaf4",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  colab.zip\n",
            "   creating: datasets/\n",
            "  inflating: datasets/pokemon.csv    \n",
            "  inflating: datasets/SUV_Purchase.csv  \n",
            "   creating: images/\n",
            "  inflating: images/umaru.png        \n",
            "  inflating: disp_utils.py           \n",
            "  inflating: requirements.txt        \n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "aa706a1b",
        "outputId": "aa5fd4a9-9bca-41f5-89ba-f12159071ddb"
      },
      "source": [
        "data = pd.read_csv('datasets/SUV_Purchase.csv')\n",
        "data"
      ],
      "id": "aa706a1b",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User ID</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>Purchased</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15624510</td>\n",
              "      <td>Male</td>\n",
              "      <td>19</td>\n",
              "      <td>19000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15810944</td>\n",
              "      <td>Male</td>\n",
              "      <td>35</td>\n",
              "      <td>20000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15668575</td>\n",
              "      <td>Female</td>\n",
              "      <td>26</td>\n",
              "      <td>43000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>15603246</td>\n",
              "      <td>Female</td>\n",
              "      <td>27</td>\n",
              "      <td>57000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15804002</td>\n",
              "      <td>Male</td>\n",
              "      <td>19</td>\n",
              "      <td>76000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>15691863</td>\n",
              "      <td>Female</td>\n",
              "      <td>46</td>\n",
              "      <td>41000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>15706071</td>\n",
              "      <td>Male</td>\n",
              "      <td>51</td>\n",
              "      <td>23000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>15654296</td>\n",
              "      <td>Female</td>\n",
              "      <td>50</td>\n",
              "      <td>20000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>15755018</td>\n",
              "      <td>Male</td>\n",
              "      <td>36</td>\n",
              "      <td>33000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>15594041</td>\n",
              "      <td>Female</td>\n",
              "      <td>49</td>\n",
              "      <td>36000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      User ID  Gender  Age  EstimatedSalary  Purchased\n",
              "0    15624510    Male   19            19000          0\n",
              "1    15810944    Male   35            20000          0\n",
              "2    15668575  Female   26            43000          0\n",
              "3    15603246  Female   27            57000          0\n",
              "4    15804002    Male   19            76000          0\n",
              "..        ...     ...  ...              ...        ...\n",
              "395  15691863  Female   46            41000          1\n",
              "396  15706071    Male   51            23000          1\n",
              "397  15654296  Female   50            20000          1\n",
              "398  15755018    Male   36            33000          0\n",
              "399  15594041  Female   49            36000          1\n",
              "\n",
              "[400 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "0e9c7f8a",
        "outputId": "673ef3e2-49b3-45b4-8e05-43cfa2752ac2"
      },
      "source": [
        "# Converting Gender into categorical values\n",
        "gender_dummies = pd.get_dummies(data['Gender'])\n",
        "gender_dummies"
      ],
      "id": "0e9c7f8a",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Female</th>\n",
              "      <th>Male</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Female  Male\n",
              "0         0     1\n",
              "1         0     1\n",
              "2         1     0\n",
              "3         1     0\n",
              "4         0     1\n",
              "..      ...   ...\n",
              "395       1     0\n",
              "396       0     1\n",
              "397       1     0\n",
              "398       0     1\n",
              "399       1     0\n",
              "\n",
              "[400 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "6903c155",
        "outputId": "d501fa45-f25c-4b91-9ebc-cc90a507380b"
      },
      "source": [
        "# let 0 = male, 1 = female\n",
        "data['gender_num'] = gender_dummies['Female']\n",
        "data"
      ],
      "id": "6903c155",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User ID</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>Purchased</th>\n",
              "      <th>gender_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15624510</td>\n",
              "      <td>Male</td>\n",
              "      <td>19</td>\n",
              "      <td>19000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15810944</td>\n",
              "      <td>Male</td>\n",
              "      <td>35</td>\n",
              "      <td>20000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15668575</td>\n",
              "      <td>Female</td>\n",
              "      <td>26</td>\n",
              "      <td>43000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>15603246</td>\n",
              "      <td>Female</td>\n",
              "      <td>27</td>\n",
              "      <td>57000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15804002</td>\n",
              "      <td>Male</td>\n",
              "      <td>19</td>\n",
              "      <td>76000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>15691863</td>\n",
              "      <td>Female</td>\n",
              "      <td>46</td>\n",
              "      <td>41000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>15706071</td>\n",
              "      <td>Male</td>\n",
              "      <td>51</td>\n",
              "      <td>23000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>15654296</td>\n",
              "      <td>Female</td>\n",
              "      <td>50</td>\n",
              "      <td>20000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>15755018</td>\n",
              "      <td>Male</td>\n",
              "      <td>36</td>\n",
              "      <td>33000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>15594041</td>\n",
              "      <td>Female</td>\n",
              "      <td>49</td>\n",
              "      <td>36000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      User ID  Gender  Age  EstimatedSalary  Purchased  gender_num\n",
              "0    15624510    Male   19            19000          0           0\n",
              "1    15810944    Male   35            20000          0           0\n",
              "2    15668575  Female   26            43000          0           1\n",
              "3    15603246  Female   27            57000          0           1\n",
              "4    15804002    Male   19            76000          0           0\n",
              "..        ...     ...  ...              ...        ...         ...\n",
              "395  15691863  Female   46            41000          1           1\n",
              "396  15706071    Male   51            23000          1           0\n",
              "397  15654296  Female   50            20000          1           1\n",
              "398  15755018    Male   36            33000          0           0\n",
              "399  15594041  Female   49            36000          1           1\n",
              "\n",
              "[400 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8939ee7a",
        "outputId": "0debe96c-5343-4bdc-e335-df2476ea4a6b"
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Data selection\n",
        "features = data[['Age', 'EstimatedSalary', 'gender_num']].to_numpy()\n",
        "labels = data[['Purchased']].to_numpy()\n",
        "\n",
        "# Train test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size = 0.2)\n",
        "poly= PolynomialFeatures(degree=3)\n",
        "\n",
        "# Create Polynomial Features\n",
        "X_train = poly.fit_transform(X_train)\n",
        "X_test = poly.fit_transform(X_test)\n",
        "\n",
        "# Display shapes\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)\n",
        "\n",
        "# Convert into pytorch tensors \n",
        "X_train_t = torch.from_numpy(X_train).float()\n",
        "Y_train_t = torch.from_numpy(Y_train).float()"
      ],
      "id": "8939ee7a",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(320, 20)\n",
            "(320, 1)\n",
            "(80, 20)\n",
            "(80, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11d507b2",
        "outputId": "e189e7bc-d79e-4d55-efc2-97df5ef43ee7"
      },
      "source": [
        "# Number of positive and negative (females and males)\n",
        "positives = np.sum(data['Purchased'].to_numpy())\n",
        "negatives = int(len(data.index) - positives)\n",
        "\n",
        "print(f'Num of positives: {positives}')\n",
        "print(f'Num of negatives: {negatives}\\n')\n",
        "\n",
        "# Calculate Class Weights to balance classes\n",
        "pos_weight = len(data.index)/(2 * positives)\n",
        "neg_weight = len(data.index)/(2 * negatives)\n",
        "\n",
        "print(f\"positive weight: {pos_weight}\")\n",
        "print(f\"negative weight: {neg_weight}\")\n",
        "\n",
        "\n",
        "def class_weights(Y_train, pos_weight, neg_weight):\n",
        "    \"\"\"\n",
        "    calculates the weights for each and every training example\n",
        "    Args:\n",
        "        Y_train: training labels\n",
        "        pos_weight: Weight of positive labels\n",
        "        neg_weight: Weight of negative labels\n",
        "    Returns:\n",
        "        weights: an np.array where its shape is identical to Y_train\n",
        "    \"\"\"\n",
        "    \n",
        "    pos_mask = Y_train.astype(bool)\n",
        "    neg_mask = ~Y_train.astype(bool)\n",
        "\n",
        "    pos_mask = pos_mask.astype(np.float32) * pos_weight\n",
        "\n",
        "    neg_mask = neg_mask.astype(np.float32) * neg_weight\n",
        "    return pos_mask + neg_mask\n",
        "    \n",
        "\n",
        "weights = class_weights(Y_train, pos_weight, neg_weight)\n",
        "print(weights.shape)"
      ],
      "id": "11d507b2",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num of positives: 143\n",
            "Num of negatives: 257\n",
            "\n",
            "positive weight: 1.3986013986013985\n",
            "negative weight: 0.7782101167315175\n",
            "(320, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be9b3d45"
      },
      "source": [
        "## Training the Logistic Regression Model\n",
        "\n",
        "Recall from the Topic 2 Notebook, Logistic Regression differs from Linear Regression in terms of the Activation function (sigmoid) and the Cost function (BCE). Most other aspects are similar. Here we will use the `BatchNorm1d` layer to normalize the inputs."
      ],
      "id": "be9b3d45"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c974717d"
      },
      "source": [
        "from torch.nn import Module, Linear, BCELoss, BatchNorm1d\n",
        "from torch.optim import SGD\n",
        "\n",
        "class Logreg(Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.linear  = Linear(input_dim, output_dim)\n",
        "        self.bn = BatchNorm1d(input_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Batch norm is used solely to normalize the inputs\n",
        "        x = self.bn(x)\n",
        "        z = self.linear(x)\n",
        "        y_pred = torch.sigmoid(z)\n",
        "        return y_pred\n",
        "    \n",
        "    def fit(self, X_train, Y_train, epochs, loss_func, opt):\n",
        "        \"\"\"\n",
        "        Trains the model.\n",
        "        Args:\n",
        "            X_train, Y_train: training set and labels. Must be \n",
        "                            torch.tensor\n",
        "            epochs: The number of passes over your training dataset\n",
        "            loss_func: optimizing criterion\n",
        "            opt: optimizing algorithm\n",
        "        \"\"\"\n",
        "        for i in range(epochs):\n",
        "            self.train()\n",
        "            opt.zero_grad()\n",
        "        \n",
        "            Y_pred = self(X_train)\n",
        "            loss = loss_func(Y_pred, Y_train)\n",
        "            \n",
        "            print(f\"iteration {i+1}: loss: {loss.item()}\")\n",
        "        \n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            del Y_pred\n",
        "            del loss"
      ],
      "id": "c974717d",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2584aecd",
        "outputId": "b2ecfad2-1db4-4cb4-d0b4-22cc92cc2044"
      },
      "source": [
        "LEARNING_RATE = 0.02\n",
        "EPOCHS = 50000\n",
        "\n",
        "# Initializing a model\n",
        "model = Logreg(X_train.shape[1], 1).cpu()\n",
        "\n",
        "# Optimizing criterion and optimizing algorithm\n",
        "# ignore talking about momentum and nesterov\n",
        "criterion = BCELoss(weight=torch.from_numpy(weights).float())\n",
        "optimizer = SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "model.fit(X_train_t, Y_train_t, EPOCHS, criterion, optimizer)"
      ],
      "id": "2584aecd",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iteration 45001: loss: 0.22629141807556152\n",
            "iteration 45002: loss: 0.22629137337207794\n",
            "iteration 45003: loss: 0.22629134356975555\n",
            "iteration 45004: loss: 0.22629132866859436\n",
            "iteration 45005: loss: 0.22629129886627197\n",
            "iteration 45006: loss: 0.22629129886627197\n",
            "iteration 45007: loss: 0.22629126906394958\n",
            "iteration 45008: loss: 0.226291224360466\n",
            "iteration 45009: loss: 0.226291224360466\n",
            "iteration 45010: loss: 0.226291224360466\n",
            "iteration 45011: loss: 0.2262912094593048\n",
            "iteration 45012: loss: 0.22629117965698242\n",
            "iteration 45013: loss: 0.22629117965698242\n",
            "iteration 45014: loss: 0.22629114985466003\n",
            "iteration 45015: loss: 0.22629110515117645\n",
            "iteration 45016: loss: 0.22629110515117645\n",
            "iteration 45017: loss: 0.22629103064537048\n",
            "iteration 45018: loss: 0.22629103064537048\n",
            "iteration 45019: loss: 0.2262910157442093\n",
            "iteration 45020: loss: 0.2262910157442093\n",
            "iteration 45021: loss: 0.2262909859418869\n",
            "iteration 45022: loss: 0.2262909859418869\n",
            "iteration 45023: loss: 0.22629094123840332\n",
            "iteration 45024: loss: 0.22629094123840332\n",
            "iteration 45025: loss: 0.22629091143608093\n",
            "iteration 45026: loss: 0.22629086673259735\n",
            "iteration 45027: loss: 0.22629086673259735\n",
            "iteration 45028: loss: 0.22629085183143616\n",
            "iteration 45029: loss: 0.22629079222679138\n",
            "iteration 45030: loss: 0.22629079222679138\n",
            "iteration 45031: loss: 0.2262907773256302\n",
            "iteration 45032: loss: 0.2262907773256302\n",
            "iteration 45033: loss: 0.2262907773256302\n",
            "iteration 45034: loss: 0.22629070281982422\n",
            "iteration 45035: loss: 0.22629067301750183\n",
            "iteration 45036: loss: 0.22629065811634064\n",
            "iteration 45037: loss: 0.22629065811634064\n",
            "iteration 45038: loss: 0.22629065811634064\n",
            "iteration 45039: loss: 0.22629062831401825\n",
            "iteration 45040: loss: 0.22629061341285706\n",
            "iteration 45041: loss: 0.22629055380821228\n",
            "iteration 45042: loss: 0.22629055380821228\n",
            "iteration 45043: loss: 0.2262905091047287\n",
            "iteration 45044: loss: 0.2262905091047287\n",
            "iteration 45045: loss: 0.2262904942035675\n",
            "iteration 45046: loss: 0.22629046440124512\n",
            "iteration 45047: loss: 0.22629043459892273\n",
            "iteration 45048: loss: 0.22629043459892273\n",
            "iteration 45049: loss: 0.22629037499427795\n",
            "iteration 45050: loss: 0.22629037499427795\n",
            "iteration 45051: loss: 0.22629034519195557\n",
            "iteration 45052: loss: 0.22629034519195557\n",
            "iteration 45053: loss: 0.22629031538963318\n",
            "iteration 45054: loss: 0.2262902706861496\n",
            "iteration 45055: loss: 0.22629030048847198\n",
            "iteration 45056: loss: 0.2262902706861496\n",
            "iteration 45057: loss: 0.2262902557849884\n",
            "iteration 45058: loss: 0.22629022598266602\n",
            "iteration 45059: loss: 0.22629018127918243\n",
            "iteration 45060: loss: 0.22629013657569885\n",
            "iteration 45061: loss: 0.22629015147686005\n",
            "iteration 45062: loss: 0.22629015147686005\n",
            "iteration 45063: loss: 0.22629007697105408\n",
            "iteration 45064: loss: 0.22629010677337646\n",
            "iteration 45065: loss: 0.2262900322675705\n",
            "iteration 45066: loss: 0.22629006206989288\n",
            "iteration 45067: loss: 0.2262900322675705\n",
            "iteration 45068: loss: 0.22628998756408691\n",
            "iteration 45069: loss: 0.22628995776176453\n",
            "iteration 45070: loss: 0.22628998756408691\n",
            "iteration 45071: loss: 0.22628995776176453\n",
            "iteration 45072: loss: 0.22628991305828094\n",
            "iteration 45073: loss: 0.22628991305828094\n",
            "iteration 45074: loss: 0.22628991305828094\n",
            "iteration 45075: loss: 0.22628989815711975\n",
            "iteration 45076: loss: 0.22628983855247498\n",
            "iteration 45077: loss: 0.22628983855247498\n",
            "iteration 45078: loss: 0.2262897938489914\n",
            "iteration 45079: loss: 0.2262897938489914\n",
            "iteration 45080: loss: 0.2262897491455078\n",
            "iteration 45081: loss: 0.2262897789478302\n",
            "iteration 45082: loss: 0.22628970444202423\n",
            "iteration 45083: loss: 0.22628967463970184\n",
            "iteration 45084: loss: 0.22628965973854065\n",
            "iteration 45085: loss: 0.22628965973854065\n",
            "iteration 45086: loss: 0.22628962993621826\n",
            "iteration 45087: loss: 0.22628965973854065\n",
            "iteration 45088: loss: 0.22628960013389587\n",
            "iteration 45089: loss: 0.22628960013389587\n",
            "iteration 45090: loss: 0.2262895554304123\n",
            "iteration 45091: loss: 0.22628958523273468\n",
            "iteration 45092: loss: 0.2262895107269287\n",
            "iteration 45093: loss: 0.2262895107269287\n",
            "iteration 45094: loss: 0.22628942131996155\n",
            "iteration 45095: loss: 0.22628943622112274\n",
            "iteration 45096: loss: 0.22628943622112274\n",
            "iteration 45097: loss: 0.22628946602344513\n",
            "iteration 45098: loss: 0.22628939151763916\n",
            "iteration 45099: loss: 0.22628936171531677\n",
            "iteration 45100: loss: 0.22628936171531677\n",
            "iteration 45101: loss: 0.2262893170118332\n",
            "iteration 45102: loss: 0.2262893170118332\n",
            "iteration 45103: loss: 0.22628922760486603\n",
            "iteration 45104: loss: 0.226289302110672\n",
            "iteration 45105: loss: 0.22628922760486603\n",
            "iteration 45106: loss: 0.22628924250602722\n",
            "iteration 45107: loss: 0.22628922760486603\n",
            "iteration 45108: loss: 0.22628919780254364\n",
            "iteration 45109: loss: 0.22628915309906006\n",
            "iteration 45110: loss: 0.22628912329673767\n",
            "iteration 45111: loss: 0.22628912329673767\n",
            "iteration 45112: loss: 0.2262890636920929\n",
            "iteration 45113: loss: 0.2262890636920929\n",
            "iteration 45114: loss: 0.2262890338897705\n",
            "iteration 45115: loss: 0.2262890636920929\n",
            "iteration 45116: loss: 0.2262890338897705\n",
            "iteration 45117: loss: 0.2262890338897705\n",
            "iteration 45118: loss: 0.22628898918628693\n",
            "iteration 45119: loss: 0.22628898918628693\n",
            "iteration 45120: loss: 0.22628894448280334\n",
            "iteration 45121: loss: 0.22628888487815857\n",
            "iteration 45122: loss: 0.22628888487815857\n",
            "iteration 45123: loss: 0.22628886997699738\n",
            "iteration 45124: loss: 0.226288840174675\n",
            "iteration 45125: loss: 0.2262888252735138\n",
            "iteration 45126: loss: 0.2262887954711914\n",
            "iteration 45127: loss: 0.2262887954711914\n",
            "iteration 45128: loss: 0.22628876566886902\n",
            "iteration 45129: loss: 0.2262887954711914\n",
            "iteration 45130: loss: 0.22628875076770782\n",
            "iteration 45131: loss: 0.22628870606422424\n",
            "iteration 45132: loss: 0.22628872096538544\n",
            "iteration 45133: loss: 0.22628864645957947\n",
            "iteration 45134: loss: 0.22628864645957947\n",
            "iteration 45135: loss: 0.22628863155841827\n",
            "iteration 45136: loss: 0.22628863155841827\n",
            "iteration 45137: loss: 0.2262885868549347\n",
            "iteration 45138: loss: 0.2262885570526123\n",
            "iteration 45139: loss: 0.2262885570526123\n",
            "iteration 45140: loss: 0.22628851234912872\n",
            "iteration 45141: loss: 0.22628851234912872\n",
            "iteration 45142: loss: 0.22628852725028992\n",
            "iteration 45143: loss: 0.22628840804100037\n",
            "iteration 45144: loss: 0.22628840804100037\n",
            "iteration 45145: loss: 0.22628840804100037\n",
            "iteration 45146: loss: 0.22628840804100037\n",
            "iteration 45147: loss: 0.22628839313983917\n",
            "iteration 45148: loss: 0.2262883484363556\n",
            "iteration 45149: loss: 0.2262883186340332\n",
            "iteration 45150: loss: 0.2262883186340332\n",
            "iteration 45151: loss: 0.22628828883171082\n",
            "iteration 45152: loss: 0.22628828883171082\n",
            "iteration 45153: loss: 0.22628827393054962\n",
            "iteration 45154: loss: 0.22628819942474365\n",
            "iteration 45155: loss: 0.22628819942474365\n",
            "iteration 45156: loss: 0.22628815472126007\n",
            "iteration 45157: loss: 0.22628816962242126\n",
            "iteration 45158: loss: 0.22628816962242126\n",
            "iteration 45159: loss: 0.22628812491893768\n",
            "iteration 45160: loss: 0.22628812491893768\n",
            "iteration 45161: loss: 0.2262880802154541\n",
            "iteration 45162: loss: 0.22628803551197052\n",
            "iteration 45163: loss: 0.22628799080848694\n",
            "iteration 45164: loss: 0.2262880504131317\n",
            "iteration 45165: loss: 0.22628799080848694\n",
            "iteration 45166: loss: 0.22628799080848694\n",
            "iteration 45167: loss: 0.22628793120384216\n",
            "iteration 45168: loss: 0.22628793120384216\n",
            "iteration 45169: loss: 0.22628791630268097\n",
            "iteration 45170: loss: 0.22628793120384216\n",
            "iteration 45171: loss: 0.22628788650035858\n",
            "iteration 45172: loss: 0.226287841796875\n",
            "iteration 45173: loss: 0.2262878119945526\n",
            "iteration 45174: loss: 0.22628779709339142\n",
            "iteration 45175: loss: 0.2262878119945526\n",
            "iteration 45176: loss: 0.22628775238990784\n",
            "iteration 45177: loss: 0.22628772258758545\n",
            "iteration 45178: loss: 0.22628775238990784\n",
            "iteration 45179: loss: 0.22628772258758545\n",
            "iteration 45180: loss: 0.22628767788410187\n",
            "iteration 45181: loss: 0.22628769278526306\n",
            "iteration 45182: loss: 0.2262876331806183\n",
            "iteration 45183: loss: 0.2262876033782959\n",
            "iteration 45184: loss: 0.2262876331806183\n",
            "iteration 45185: loss: 0.2262876033782959\n",
            "iteration 45186: loss: 0.22628755867481232\n",
            "iteration 45187: loss: 0.22628755867481232\n",
            "iteration 45188: loss: 0.22628751397132874\n",
            "iteration 45189: loss: 0.22628745436668396\n",
            "iteration 45190: loss: 0.22628751397132874\n",
            "iteration 45191: loss: 0.22628751397132874\n",
            "iteration 45192: loss: 0.22628743946552277\n",
            "iteration 45193: loss: 0.22628743946552277\n",
            "iteration 45194: loss: 0.22628739476203918\n",
            "iteration 45195: loss: 0.22628732025623322\n",
            "iteration 45196: loss: 0.2262873649597168\n",
            "iteration 45197: loss: 0.22628739476203918\n",
            "iteration 45198: loss: 0.22628732025623322\n",
            "iteration 45199: loss: 0.22628732025623322\n",
            "iteration 45200: loss: 0.22628727555274963\n",
            "iteration 45201: loss: 0.22628724575042725\n",
            "iteration 45202: loss: 0.22628721594810486\n",
            "iteration 45203: loss: 0.22628724575042725\n",
            "iteration 45204: loss: 0.22628715634346008\n",
            "iteration 45205: loss: 0.22628717124462128\n",
            "iteration 45206: loss: 0.22628717124462128\n",
            "iteration 45207: loss: 0.2262871265411377\n",
            "iteration 45208: loss: 0.2262871265411377\n",
            "iteration 45209: loss: 0.2262870818376541\n",
            "iteration 45210: loss: 0.2262870818376541\n",
            "iteration 45211: loss: 0.2262870818376541\n",
            "iteration 45212: loss: 0.22628700733184814\n",
            "iteration 45213: loss: 0.22628705203533173\n",
            "iteration 45214: loss: 0.22628700733184814\n",
            "iteration 45215: loss: 0.22628696262836456\n",
            "iteration 45216: loss: 0.22628693282604218\n",
            "iteration 45217: loss: 0.22628691792488098\n",
            "iteration 45218: loss: 0.2262868881225586\n",
            "iteration 45219: loss: 0.2262868881225586\n",
            "iteration 45220: loss: 0.226286843419075\n",
            "iteration 45221: loss: 0.226286843419075\n",
            "iteration 45222: loss: 0.22628681361675262\n",
            "iteration 45223: loss: 0.22628679871559143\n",
            "iteration 45224: loss: 0.22628676891326904\n",
            "iteration 45225: loss: 0.22628676891326904\n",
            "iteration 45226: loss: 0.22628673911094666\n",
            "iteration 45227: loss: 0.22628669440746307\n",
            "iteration 45228: loss: 0.22628669440746307\n",
            "iteration 45229: loss: 0.22628667950630188\n",
            "iteration 45230: loss: 0.2262866497039795\n",
            "iteration 45231: loss: 0.22628667950630188\n",
            "iteration 45232: loss: 0.2262866497039795\n",
            "iteration 45233: loss: 0.22628656029701233\n",
            "iteration 45234: loss: 0.22628657519817352\n",
            "iteration 45235: loss: 0.22628656029701233\n",
            "iteration 45236: loss: 0.22628653049468994\n",
            "iteration 45237: loss: 0.22628650069236755\n",
            "iteration 45238: loss: 0.22628648579120636\n",
            "iteration 45239: loss: 0.22628645598888397\n",
            "iteration 45240: loss: 0.22628645598888397\n",
            "iteration 45241: loss: 0.226286381483078\n",
            "iteration 45242: loss: 0.226286381483078\n",
            "iteration 45243: loss: 0.2262864112854004\n",
            "iteration 45244: loss: 0.226286381483078\n",
            "iteration 45245: loss: 0.2262863665819168\n",
            "iteration 45246: loss: 0.22628632187843323\n",
            "iteration 45247: loss: 0.22628633677959442\n",
            "iteration 45248: loss: 0.22628626227378845\n",
            "iteration 45249: loss: 0.22628626227378845\n",
            "iteration 45250: loss: 0.22628626227378845\n",
            "iteration 45251: loss: 0.22628624737262726\n",
            "iteration 45252: loss: 0.22628620266914368\n",
            "iteration 45253: loss: 0.22628621757030487\n",
            "iteration 45254: loss: 0.22628620266914368\n",
            "iteration 45255: loss: 0.2262861430644989\n",
            "iteration 45256: loss: 0.22628608345985413\n",
            "iteration 45257: loss: 0.22628609836101532\n",
            "iteration 45258: loss: 0.22628608345985413\n",
            "iteration 45259: loss: 0.22628609836101532\n",
            "iteration 45260: loss: 0.22628602385520935\n",
            "iteration 45261: loss: 0.22628600895404816\n",
            "iteration 45262: loss: 0.22628600895404816\n",
            "iteration 45263: loss: 0.22628597915172577\n",
            "iteration 45264: loss: 0.22628600895404816\n",
            "iteration 45265: loss: 0.2262859344482422\n",
            "iteration 45266: loss: 0.2262858897447586\n",
            "iteration 45267: loss: 0.2262859344482422\n",
            "iteration 45268: loss: 0.22628584504127502\n",
            "iteration 45269: loss: 0.22628581523895264\n",
            "iteration 45270: loss: 0.22628584504127502\n",
            "iteration 45271: loss: 0.22628581523895264\n",
            "iteration 45272: loss: 0.22628578543663025\n",
            "iteration 45273: loss: 0.22628577053546906\n",
            "iteration 45274: loss: 0.22628574073314667\n",
            "iteration 45275: loss: 0.22628577053546906\n",
            "iteration 45276: loss: 0.22628572583198547\n",
            "iteration 45277: loss: 0.2262856513261795\n",
            "iteration 45278: loss: 0.2262856513261795\n",
            "iteration 45279: loss: 0.22628560662269592\n",
            "iteration 45280: loss: 0.22628560662269592\n",
            "iteration 45281: loss: 0.22628557682037354\n",
            "iteration 45282: loss: 0.22628557682037354\n",
            "iteration 45283: loss: 0.22628557682037354\n",
            "iteration 45284: loss: 0.22628557682037354\n",
            "iteration 45285: loss: 0.22628554701805115\n",
            "iteration 45286: loss: 0.22628553211688995\n",
            "iteration 45287: loss: 0.22628545761108398\n",
            "iteration 45288: loss: 0.22628545761108398\n",
            "iteration 45289: loss: 0.22628545761108398\n",
            "iteration 45290: loss: 0.22628545761108398\n",
            "iteration 45291: loss: 0.22628538310527802\n",
            "iteration 45292: loss: 0.22628536820411682\n",
            "iteration 45293: loss: 0.22628533840179443\n",
            "iteration 45294: loss: 0.22628533840179443\n",
            "iteration 45295: loss: 0.22628536820411682\n",
            "iteration 45296: loss: 0.22628530859947205\n",
            "iteration 45297: loss: 0.22628529369831085\n",
            "iteration 45298: loss: 0.22628526389598846\n",
            "iteration 45299: loss: 0.22628521919250488\n",
            "iteration 45300: loss: 0.22628521919250488\n",
            "iteration 45301: loss: 0.2262851893901825\n",
            "iteration 45302: loss: 0.2262851744890213\n",
            "iteration 45303: loss: 0.2262851446866989\n",
            "iteration 45304: loss: 0.2262851446866989\n",
            "iteration 45305: loss: 0.22628507018089294\n",
            "iteration 45306: loss: 0.22628512978553772\n",
            "iteration 45307: loss: 0.22628507018089294\n",
            "iteration 45308: loss: 0.22628505527973175\n",
            "iteration 45309: loss: 0.22628502547740936\n",
            "iteration 45310: loss: 0.22628501057624817\n",
            "iteration 45311: loss: 0.22628501057624817\n",
            "iteration 45312: loss: 0.22628498077392578\n",
            "iteration 45313: loss: 0.2262849062681198\n",
            "iteration 45314: loss: 0.2262849062681198\n",
            "iteration 45315: loss: 0.22628486156463623\n",
            "iteration 45316: loss: 0.22628486156463623\n",
            "iteration 45317: loss: 0.22628486156463623\n",
            "iteration 45318: loss: 0.22628483176231384\n",
            "iteration 45319: loss: 0.22628483176231384\n",
            "iteration 45320: loss: 0.22628481686115265\n",
            "iteration 45321: loss: 0.22628477215766907\n",
            "iteration 45322: loss: 0.22628474235534668\n",
            "iteration 45323: loss: 0.2262846976518631\n",
            "iteration 45324: loss: 0.22628474235534668\n",
            "iteration 45325: loss: 0.2262846976518631\n",
            "iteration 45326: loss: 0.2262846678495407\n",
            "iteration 45327: loss: 0.2262846976518631\n",
            "iteration 45328: loss: 0.22628459334373474\n",
            "iteration 45329: loss: 0.22628465294837952\n",
            "iteration 45330: loss: 0.22628457844257355\n",
            "iteration 45331: loss: 0.22628457844257355\n",
            "iteration 45332: loss: 0.22628459334373474\n",
            "iteration 45333: loss: 0.22628453373908997\n",
            "iteration 45334: loss: 0.22628453373908997\n",
            "iteration 45335: loss: 0.22628450393676758\n",
            "iteration 45336: loss: 0.2262844741344452\n",
            "iteration 45337: loss: 0.2262844741344452\n",
            "iteration 45338: loss: 0.226284459233284\n",
            "iteration 45339: loss: 0.22628441452980042\n",
            "iteration 45340: loss: 0.2262844294309616\n",
            "iteration 45341: loss: 0.22628438472747803\n",
            "iteration 45342: loss: 0.22628435492515564\n",
            "iteration 45343: loss: 0.22628435492515564\n",
            "iteration 45344: loss: 0.22628434002399445\n",
            "iteration 45345: loss: 0.22628431022167206\n",
            "iteration 45346: loss: 0.22628429532051086\n",
            "iteration 45347: loss: 0.22628426551818848\n",
            "iteration 45348: loss: 0.22628426551818848\n",
            "iteration 45349: loss: 0.2262841910123825\n",
            "iteration 45350: loss: 0.2262841910123825\n",
            "iteration 45351: loss: 0.22628411650657654\n",
            "iteration 45352: loss: 0.22628414630889893\n",
            "iteration 45353: loss: 0.22628414630889893\n",
            "iteration 45354: loss: 0.22628407180309296\n",
            "iteration 45355: loss: 0.22628407180309296\n",
            "iteration 45356: loss: 0.22628407180309296\n",
            "iteration 45357: loss: 0.2262839823961258\n",
            "iteration 45358: loss: 0.22628405690193176\n",
            "iteration 45359: loss: 0.2262839823961258\n",
            "iteration 45360: loss: 0.226283997297287\n",
            "iteration 45361: loss: 0.2262839525938034\n",
            "iteration 45362: loss: 0.2262839823961258\n",
            "iteration 45363: loss: 0.22628387808799744\n",
            "iteration 45364: loss: 0.22628386318683624\n",
            "iteration 45365: loss: 0.22628387808799744\n",
            "iteration 45366: loss: 0.22628386318683624\n",
            "iteration 45367: loss: 0.22628383338451385\n",
            "iteration 45368: loss: 0.22628383338451385\n",
            "iteration 45369: loss: 0.22628383338451385\n",
            "iteration 45370: loss: 0.2262837439775467\n",
            "iteration 45371: loss: 0.22628375887870789\n",
            "iteration 45372: loss: 0.22628375887870789\n",
            "iteration 45373: loss: 0.2262836992740631\n",
            "iteration 45374: loss: 0.2262836992740631\n",
            "iteration 45375: loss: 0.2262836992740631\n",
            "iteration 45376: loss: 0.22628363966941833\n",
            "iteration 45377: loss: 0.22628363966941833\n",
            "iteration 45378: loss: 0.22628359496593475\n",
            "iteration 45379: loss: 0.22628363966941833\n",
            "iteration 45380: loss: 0.22628359496593475\n",
            "iteration 45381: loss: 0.22628358006477356\n",
            "iteration 45382: loss: 0.22628352046012878\n",
            "iteration 45383: loss: 0.2262835055589676\n",
            "iteration 45384: loss: 0.2262835055589676\n",
            "iteration 45385: loss: 0.226283460855484\n",
            "iteration 45386: loss: 0.2262834757566452\n",
            "iteration 45387: loss: 0.22628343105316162\n",
            "iteration 45388: loss: 0.22628340125083923\n",
            "iteration 45389: loss: 0.22628340125083923\n",
            "iteration 45390: loss: 0.22628334164619446\n",
            "iteration 45391: loss: 0.22628334164619446\n",
            "iteration 45392: loss: 0.22628331184387207\n",
            "iteration 45393: loss: 0.2262832671403885\n",
            "iteration 45394: loss: 0.22628331184387207\n",
            "iteration 45395: loss: 0.2262832373380661\n",
            "iteration 45396: loss: 0.22628328204154968\n",
            "iteration 45397: loss: 0.2262832373380661\n",
            "iteration 45398: loss: 0.22628319263458252\n",
            "iteration 45399: loss: 0.2262832224369049\n",
            "iteration 45400: loss: 0.22628316283226013\n",
            "iteration 45401: loss: 0.22628316283226013\n",
            "iteration 45402: loss: 0.22628311812877655\n",
            "iteration 45403: loss: 0.22628311812877655\n",
            "iteration 45404: loss: 0.22628307342529297\n",
            "iteration 45405: loss: 0.22628310322761536\n",
            "iteration 45406: loss: 0.22628304362297058\n",
            "iteration 45407: loss: 0.22628295421600342\n",
            "iteration 45408: loss: 0.226282998919487\n",
            "iteration 45409: loss: 0.226282998919487\n",
            "iteration 45410: loss: 0.22628292441368103\n",
            "iteration 45411: loss: 0.22628287971019745\n",
            "iteration 45412: loss: 0.22628292441368103\n",
            "iteration 45413: loss: 0.22628287971019745\n",
            "iteration 45414: loss: 0.22628290951251984\n",
            "iteration 45415: loss: 0.22628283500671387\n",
            "iteration 45416: loss: 0.22628283500671387\n",
            "iteration 45417: loss: 0.22628280520439148\n",
            "iteration 45418: loss: 0.22628279030323029\n",
            "iteration 45419: loss: 0.22628279030323029\n",
            "iteration 45420: loss: 0.2262827605009079\n",
            "iteration 45421: loss: 0.2262827605009079\n",
            "iteration 45422: loss: 0.22628268599510193\n",
            "iteration 45423: loss: 0.22628267109394073\n",
            "iteration 45424: loss: 0.22628267109394073\n",
            "iteration 45425: loss: 0.22628264129161835\n",
            "iteration 45426: loss: 0.22628262639045715\n",
            "iteration 45427: loss: 0.22628259658813477\n",
            "iteration 45428: loss: 0.22628259658813477\n",
            "iteration 45429: loss: 0.2262825220823288\n",
            "iteration 45430: loss: 0.22628255188465118\n",
            "iteration 45431: loss: 0.2262825220823288\n",
            "iteration 45432: loss: 0.22628247737884521\n",
            "iteration 45433: loss: 0.22628244757652283\n",
            "iteration 45434: loss: 0.22628247737884521\n",
            "iteration 45435: loss: 0.22628243267536163\n",
            "iteration 45436: loss: 0.22628240287303925\n",
            "iteration 45437: loss: 0.22628240287303925\n",
            "iteration 45438: loss: 0.22628235816955566\n",
            "iteration 45439: loss: 0.22628238797187805\n",
            "iteration 45440: loss: 0.22628235816955566\n",
            "iteration 45441: loss: 0.22628235816955566\n",
            "iteration 45442: loss: 0.22628231346607208\n",
            "iteration 45443: loss: 0.22628231346607208\n",
            "iteration 45444: loss: 0.2262822687625885\n",
            "iteration 45445: loss: 0.22628220915794373\n",
            "iteration 45446: loss: 0.22628219425678253\n",
            "iteration 45447: loss: 0.22628220915794373\n",
            "iteration 45448: loss: 0.22628219425678253\n",
            "iteration 45449: loss: 0.22628219425678253\n",
            "iteration 45450: loss: 0.22628214955329895\n",
            "iteration 45451: loss: 0.22628214955329895\n",
            "iteration 45452: loss: 0.2262820452451706\n",
            "iteration 45453: loss: 0.22628207504749298\n",
            "iteration 45454: loss: 0.22628207504749298\n",
            "iteration 45455: loss: 0.2262820303440094\n",
            "iteration 45456: loss: 0.226282000541687\n",
            "iteration 45457: loss: 0.2262820303440094\n",
            "iteration 45458: loss: 0.22628195583820343\n",
            "iteration 45459: loss: 0.22628197073936462\n",
            "iteration 45460: loss: 0.22628195583820343\n",
            "iteration 45461: loss: 0.22628192603588104\n",
            "iteration 45462: loss: 0.22628188133239746\n",
            "iteration 45463: loss: 0.22628188133239746\n",
            "iteration 45464: loss: 0.22628185153007507\n",
            "iteration 45465: loss: 0.2262818068265915\n",
            "iteration 45466: loss: 0.2262818068265915\n",
            "iteration 45467: loss: 0.2262817919254303\n",
            "iteration 45468: loss: 0.22628173232078552\n",
            "iteration 45469: loss: 0.2262817621231079\n",
            "iteration 45470: loss: 0.22628171741962433\n",
            "iteration 45471: loss: 0.22628173232078552\n",
            "iteration 45472: loss: 0.22628171741962433\n",
            "iteration 45473: loss: 0.22628168761730194\n",
            "iteration 45474: loss: 0.22628167271614075\n",
            "iteration 45475: loss: 0.22628161311149597\n",
            "iteration 45476: loss: 0.22628161311149597\n",
            "iteration 45477: loss: 0.2262815684080124\n",
            "iteration 45478: loss: 0.2262815684080124\n",
            "iteration 45479: loss: 0.2262815684080124\n",
            "iteration 45480: loss: 0.2262815535068512\n",
            "iteration 45481: loss: 0.22628149390220642\n",
            "iteration 45482: loss: 0.22628147900104523\n",
            "iteration 45483: loss: 0.22628147900104523\n",
            "iteration 45484: loss: 0.22628143429756165\n",
            "iteration 45485: loss: 0.22628140449523926\n",
            "iteration 45486: loss: 0.22628137469291687\n",
            "iteration 45487: loss: 0.22628140449523926\n",
            "iteration 45488: loss: 0.22628140449523926\n",
            "iteration 45489: loss: 0.22628135979175568\n",
            "iteration 45490: loss: 0.2262813150882721\n",
            "iteration 45491: loss: 0.2262813150882721\n",
            "iteration 45492: loss: 0.2262812852859497\n",
            "iteration 45493: loss: 0.22628125548362732\n",
            "iteration 45494: loss: 0.22628124058246613\n",
            "iteration 45495: loss: 0.22628121078014374\n",
            "iteration 45496: loss: 0.22628121078014374\n",
            "iteration 45497: loss: 0.22628121078014374\n",
            "iteration 45498: loss: 0.22628113627433777\n",
            "iteration 45499: loss: 0.22628112137317657\n",
            "iteration 45500: loss: 0.22628112137317657\n",
            "iteration 45501: loss: 0.2262810915708542\n",
            "iteration 45502: loss: 0.226281076669693\n",
            "iteration 45503: loss: 0.22628101706504822\n",
            "iteration 45504: loss: 0.22628101706504822\n",
            "iteration 45505: loss: 0.22628097236156464\n",
            "iteration 45506: loss: 0.22628100216388702\n",
            "iteration 45507: loss: 0.22628100216388702\n",
            "iteration 45508: loss: 0.22628092765808105\n",
            "iteration 45509: loss: 0.22628092765808105\n",
            "iteration 45510: loss: 0.22628089785575867\n",
            "iteration 45511: loss: 0.22628088295459747\n",
            "iteration 45512: loss: 0.22628088295459747\n",
            "iteration 45513: loss: 0.22628088295459747\n",
            "iteration 45514: loss: 0.22628088295459747\n",
            "iteration 45515: loss: 0.2262808084487915\n",
            "iteration 45516: loss: 0.22628076374530792\n",
            "iteration 45517: loss: 0.22628076374530792\n",
            "iteration 45518: loss: 0.22628076374530792\n",
            "iteration 45519: loss: 0.22628068923950195\n",
            "iteration 45520: loss: 0.22628071904182434\n",
            "iteration 45521: loss: 0.22628064453601837\n",
            "iteration 45522: loss: 0.22628064453601837\n",
            "iteration 45523: loss: 0.22628064453601837\n",
            "iteration 45524: loss: 0.22628065943717957\n",
            "iteration 45525: loss: 0.2262805998325348\n",
            "iteration 45526: loss: 0.2262805998325348\n",
            "iteration 45527: loss: 0.22628054022789001\n",
            "iteration 45528: loss: 0.22628054022789001\n",
            "iteration 45529: loss: 0.22628052532672882\n",
            "iteration 45530: loss: 0.22628052532672882\n",
            "iteration 45531: loss: 0.22628045082092285\n",
            "iteration 45532: loss: 0.22628045082092285\n",
            "iteration 45533: loss: 0.22628045082092285\n",
            "iteration 45534: loss: 0.22628042101860046\n",
            "iteration 45535: loss: 0.22628040611743927\n",
            "iteration 45536: loss: 0.22628040611743927\n",
            "iteration 45537: loss: 0.2262803018093109\n",
            "iteration 45538: loss: 0.22628037631511688\n",
            "iteration 45539: loss: 0.2262803316116333\n",
            "iteration 45540: loss: 0.2262803018093109\n",
            "iteration 45541: loss: 0.2262803018093109\n",
            "iteration 45542: loss: 0.22628025710582733\n",
            "iteration 45543: loss: 0.22628024220466614\n",
            "iteration 45544: loss: 0.22628021240234375\n",
            "iteration 45545: loss: 0.22628018260002136\n",
            "iteration 45546: loss: 0.22628016769886017\n",
            "iteration 45547: loss: 0.22628016769886017\n",
            "iteration 45548: loss: 0.2262801229953766\n",
            "iteration 45549: loss: 0.22628013789653778\n",
            "iteration 45550: loss: 0.2262800633907318\n",
            "iteration 45551: loss: 0.2262800931930542\n",
            "iteration 45552: loss: 0.2262800633907318\n",
            "iteration 45553: loss: 0.22628004848957062\n",
            "iteration 45554: loss: 0.22628000378608704\n",
            "iteration 45555: loss: 0.22627997398376465\n",
            "iteration 45556: loss: 0.22627997398376465\n",
            "iteration 45557: loss: 0.22627994418144226\n",
            "iteration 45558: loss: 0.22627992928028107\n",
            "iteration 45559: loss: 0.22627989947795868\n",
            "iteration 45560: loss: 0.22627988457679749\n",
            "iteration 45561: loss: 0.22627989947795868\n",
            "iteration 45562: loss: 0.2262798547744751\n",
            "iteration 45563: loss: 0.2262798547744751\n",
            "iteration 45564: loss: 0.2262798547744751\n",
            "iteration 45565: loss: 0.22627978026866913\n",
            "iteration 45566: loss: 0.22627976536750793\n",
            "iteration 45567: loss: 0.22627978026866913\n",
            "iteration 45568: loss: 0.22627973556518555\n",
            "iteration 45569: loss: 0.22627970576286316\n",
            "iteration 45570: loss: 0.22627973556518555\n",
            "iteration 45571: loss: 0.22627970576286316\n",
            "iteration 45572: loss: 0.2262795865535736\n",
            "iteration 45573: loss: 0.22627966105937958\n",
            "iteration 45574: loss: 0.226279616355896\n",
            "iteration 45575: loss: 0.2262795865535736\n",
            "iteration 45576: loss: 0.22627954185009003\n",
            "iteration 45577: loss: 0.22627954185009003\n",
            "iteration 45578: loss: 0.22627952694892883\n",
            "iteration 45579: loss: 0.22627954185009003\n",
            "iteration 45580: loss: 0.22627949714660645\n",
            "iteration 45581: loss: 0.22627946734428406\n",
            "iteration 45582: loss: 0.22627945244312286\n",
            "iteration 45583: loss: 0.22627942264080048\n",
            "iteration 45584: loss: 0.2262793779373169\n",
            "iteration 45585: loss: 0.22627940773963928\n",
            "iteration 45586: loss: 0.2262793779373169\n",
            "iteration 45587: loss: 0.2262793481349945\n",
            "iteration 45588: loss: 0.2262793332338333\n",
            "iteration 45589: loss: 0.22627928853034973\n",
            "iteration 45590: loss: 0.22627925872802734\n",
            "iteration 45591: loss: 0.22627925872802734\n",
            "iteration 45592: loss: 0.22627921402454376\n",
            "iteration 45593: loss: 0.22627916932106018\n",
            "iteration 45594: loss: 0.22627921402454376\n",
            "iteration 45595: loss: 0.22627918422222137\n",
            "iteration 45596: loss: 0.2262791395187378\n",
            "iteration 45597: loss: 0.22627916932106018\n",
            "iteration 45598: loss: 0.2262791097164154\n",
            "iteration 45599: loss: 0.2262791097164154\n",
            "iteration 45600: loss: 0.2262790948152542\n",
            "iteration 45601: loss: 0.22627905011177063\n",
            "iteration 45602: loss: 0.22627902030944824\n",
            "iteration 45603: loss: 0.22627902030944824\n",
            "iteration 45604: loss: 0.22627902030944824\n",
            "iteration 45605: loss: 0.22627905011177063\n",
            "iteration 45606: loss: 0.22627902030944824\n",
            "iteration 45607: loss: 0.22627893090248108\n",
            "iteration 45608: loss: 0.22627894580364227\n",
            "iteration 45609: loss: 0.22627893090248108\n",
            "iteration 45610: loss: 0.2262788563966751\n",
            "iteration 45611: loss: 0.2262788712978363\n",
            "iteration 45612: loss: 0.22627882659435272\n",
            "iteration 45613: loss: 0.22627882659435272\n",
            "iteration 45614: loss: 0.22627881169319153\n",
            "iteration 45615: loss: 0.22627875208854675\n",
            "iteration 45616: loss: 0.22627878189086914\n",
            "iteration 45617: loss: 0.22627870738506317\n",
            "iteration 45618: loss: 0.22627869248390198\n",
            "iteration 45619: loss: 0.2262786626815796\n",
            "iteration 45620: loss: 0.22627873718738556\n",
            "iteration 45621: loss: 0.22627869248390198\n",
            "iteration 45622: loss: 0.226278617978096\n",
            "iteration 45623: loss: 0.226278617978096\n",
            "iteration 45624: loss: 0.226278617978096\n",
            "iteration 45625: loss: 0.22627854347229004\n",
            "iteration 45626: loss: 0.22627851366996765\n",
            "iteration 45627: loss: 0.22627849876880646\n",
            "iteration 45628: loss: 0.22627849876880646\n",
            "iteration 45629: loss: 0.22627849876880646\n",
            "iteration 45630: loss: 0.22627845406532288\n",
            "iteration 45631: loss: 0.22627849876880646\n",
            "iteration 45632: loss: 0.22627845406532288\n",
            "iteration 45633: loss: 0.2262783944606781\n",
            "iteration 45634: loss: 0.2262783944606781\n",
            "iteration 45635: loss: 0.2262784242630005\n",
            "iteration 45636: loss: 0.22627833485603333\n",
            "iteration 45637: loss: 0.22627833485603333\n",
            "iteration 45638: loss: 0.22627833485603333\n",
            "iteration 45639: loss: 0.22627830505371094\n",
            "iteration 45640: loss: 0.22627827525138855\n",
            "iteration 45641: loss: 0.22627827525138855\n",
            "iteration 45642: loss: 0.22627823054790497\n",
            "iteration 45643: loss: 0.226278156042099\n",
            "iteration 45644: loss: 0.226278156042099\n",
            "iteration 45645: loss: 0.226278156042099\n",
            "iteration 45646: loss: 0.226278156042099\n",
            "iteration 45647: loss: 0.226278156042099\n",
            "iteration 45648: loss: 0.22627806663513184\n",
            "iteration 45649: loss: 0.22627809643745422\n",
            "iteration 45650: loss: 0.22627803683280945\n",
            "iteration 45651: loss: 0.22627806663513184\n",
            "iteration 45652: loss: 0.22627799212932587\n",
            "iteration 45653: loss: 0.22627802193164825\n",
            "iteration 45654: loss: 0.22627799212932587\n",
            "iteration 45655: loss: 0.22627794742584229\n",
            "iteration 45656: loss: 0.22627794742584229\n",
            "iteration 45657: loss: 0.22627797722816467\n",
            "iteration 45658: loss: 0.22627787292003632\n",
            "iteration 45659: loss: 0.22627785801887512\n",
            "iteration 45660: loss: 0.22627785801887512\n",
            "iteration 45661: loss: 0.22627782821655273\n",
            "iteration 45662: loss: 0.22627785801887512\n",
            "iteration 45663: loss: 0.22627778351306915\n",
            "iteration 45664: loss: 0.22627778351306915\n",
            "iteration 45665: loss: 0.22627775371074677\n",
            "iteration 45666: loss: 0.22627773880958557\n",
            "iteration 45667: loss: 0.22627770900726318\n",
            "iteration 45668: loss: 0.2262776792049408\n",
            "iteration 45669: loss: 0.2262776792049408\n",
            "iteration 45670: loss: 0.22627763450145721\n",
            "iteration 45671: loss: 0.22627763450145721\n",
            "iteration 45672: loss: 0.22627763450145721\n",
            "iteration 45673: loss: 0.22627761960029602\n",
            "iteration 45674: loss: 0.22627758979797363\n",
            "iteration 45675: loss: 0.22627755999565125\n",
            "iteration 45676: loss: 0.22627754509449005\n",
            "iteration 45677: loss: 0.22627755999565125\n",
            "iteration 45678: loss: 0.22627750039100647\n",
            "iteration 45679: loss: 0.22627747058868408\n",
            "iteration 45680: loss: 0.2262774258852005\n",
            "iteration 45681: loss: 0.2262774407863617\n",
            "iteration 45682: loss: 0.2262773960828781\n",
            "iteration 45683: loss: 0.2262773960828781\n",
            "iteration 45684: loss: 0.22627732157707214\n",
            "iteration 45685: loss: 0.2262773960828781\n",
            "iteration 45686: loss: 0.22627732157707214\n",
            "iteration 45687: loss: 0.22627732157707214\n",
            "iteration 45688: loss: 0.22627730667591095\n",
            "iteration 45689: loss: 0.22627727687358856\n",
            "iteration 45690: loss: 0.22627726197242737\n",
            "iteration 45691: loss: 0.22627723217010498\n",
            "iteration 45692: loss: 0.2262772023677826\n",
            "iteration 45693: loss: 0.2262771874666214\n",
            "iteration 45694: loss: 0.2262771874666214\n",
            "iteration 45695: loss: 0.22627711296081543\n",
            "iteration 45696: loss: 0.22627711296081543\n",
            "iteration 45697: loss: 0.22627706825733185\n",
            "iteration 45698: loss: 0.22627711296081543\n",
            "iteration 45699: loss: 0.22627708315849304\n",
            "iteration 45700: loss: 0.22627708315849304\n",
            "iteration 45701: loss: 0.22627702355384827\n",
            "iteration 45702: loss: 0.22627702355384827\n",
            "iteration 45703: loss: 0.22627699375152588\n",
            "iteration 45704: loss: 0.2262769639492035\n",
            "iteration 45705: loss: 0.2262769639492035\n",
            "iteration 45706: loss: 0.2262769192457199\n",
            "iteration 45707: loss: 0.22627690434455872\n",
            "iteration 45708: loss: 0.22627687454223633\n",
            "iteration 45709: loss: 0.22627687454223633\n",
            "iteration 45710: loss: 0.22627684473991394\n",
            "iteration 45711: loss: 0.22627690434455872\n",
            "iteration 45712: loss: 0.22627682983875275\n",
            "iteration 45713: loss: 0.22627680003643036\n",
            "iteration 45714: loss: 0.22627680003643036\n",
            "iteration 45715: loss: 0.2262767106294632\n",
            "iteration 45716: loss: 0.2262767255306244\n",
            "iteration 45717: loss: 0.2262767255306244\n",
            "iteration 45718: loss: 0.2262766808271408\n",
            "iteration 45719: loss: 0.2262766808271408\n",
            "iteration 45720: loss: 0.22627666592597961\n",
            "iteration 45721: loss: 0.22627663612365723\n",
            "iteration 45722: loss: 0.22627660632133484\n",
            "iteration 45723: loss: 0.22627659142017365\n",
            "iteration 45724: loss: 0.22627659142017365\n",
            "iteration 45725: loss: 0.22627659142017365\n",
            "iteration 45726: loss: 0.22627651691436768\n",
            "iteration 45727: loss: 0.2262764871120453\n",
            "iteration 45728: loss: 0.2262764871120453\n",
            "iteration 45729: loss: 0.2262764722108841\n",
            "iteration 45730: loss: 0.2262764424085617\n",
            "iteration 45731: loss: 0.22627639770507812\n",
            "iteration 45732: loss: 0.22627639770507812\n",
            "iteration 45733: loss: 0.22627639770507812\n",
            "iteration 45734: loss: 0.22627636790275574\n",
            "iteration 45735: loss: 0.22627635300159454\n",
            "iteration 45736: loss: 0.22627639770507812\n",
            "iteration 45737: loss: 0.22627630829811096\n",
            "iteration 45738: loss: 0.226276233792305\n",
            "iteration 45739: loss: 0.2262762486934662\n",
            "iteration 45740: loss: 0.226276233792305\n",
            "iteration 45741: loss: 0.226276233792305\n",
            "iteration 45742: loss: 0.226276233792305\n",
            "iteration 45743: loss: 0.2262761890888214\n",
            "iteration 45744: loss: 0.22627615928649902\n",
            "iteration 45745: loss: 0.22627615928649902\n",
            "iteration 45746: loss: 0.22627612948417664\n",
            "iteration 45747: loss: 0.22627611458301544\n",
            "iteration 45748: loss: 0.22627606987953186\n",
            "iteration 45749: loss: 0.22627608478069305\n",
            "iteration 45750: loss: 0.22627604007720947\n",
            "iteration 45751: loss: 0.22627601027488708\n",
            "iteration 45752: loss: 0.22627604007720947\n",
            "iteration 45753: loss: 0.2262759953737259\n",
            "iteration 45754: loss: 0.2262759655714035\n",
            "iteration 45755: loss: 0.22627592086791992\n",
            "iteration 45756: loss: 0.22627589106559753\n",
            "iteration 45757: loss: 0.22627589106559753\n",
            "iteration 45758: loss: 0.22627592086791992\n",
            "iteration 45759: loss: 0.22627587616443634\n",
            "iteration 45760: loss: 0.22627583146095276\n",
            "iteration 45761: loss: 0.22627577185630798\n",
            "iteration 45762: loss: 0.22627583146095276\n",
            "iteration 45763: loss: 0.22627577185630798\n",
            "iteration 45764: loss: 0.22627577185630798\n",
            "iteration 45765: loss: 0.22627568244934082\n",
            "iteration 45766: loss: 0.2262757569551468\n",
            "iteration 45767: loss: 0.2262757271528244\n",
            "iteration 45768: loss: 0.22627568244934082\n",
            "iteration 45769: loss: 0.22627565264701843\n",
            "iteration 45770: loss: 0.22627563774585724\n",
            "iteration 45771: loss: 0.22627560794353485\n",
            "iteration 45772: loss: 0.22627559304237366\n",
            "iteration 45773: loss: 0.22627553343772888\n",
            "iteration 45774: loss: 0.22627553343772888\n",
            "iteration 45775: loss: 0.22627556324005127\n",
            "iteration 45776: loss: 0.2262755185365677\n",
            "iteration 45777: loss: 0.2262754887342453\n",
            "iteration 45778: loss: 0.2262754738330841\n",
            "iteration 45779: loss: 0.22627544403076172\n",
            "iteration 45780: loss: 0.22627544403076172\n",
            "iteration 45781: loss: 0.22627541422843933\n",
            "iteration 45782: loss: 0.22627536952495575\n",
            "iteration 45783: loss: 0.22627541422843933\n",
            "iteration 45784: loss: 0.22627532482147217\n",
            "iteration 45785: loss: 0.22627532482147217\n",
            "iteration 45786: loss: 0.22627535462379456\n",
            "iteration 45787: loss: 0.2262752801179886\n",
            "iteration 45788: loss: 0.2262752801179886\n",
            "iteration 45789: loss: 0.2262752503156662\n",
            "iteration 45790: loss: 0.226275235414505\n",
            "iteration 45791: loss: 0.226275235414505\n",
            "iteration 45792: loss: 0.22627520561218262\n",
            "iteration 45793: loss: 0.22627513110637665\n",
            "iteration 45794: loss: 0.22627516090869904\n",
            "iteration 45795: loss: 0.22627516090869904\n",
            "iteration 45796: loss: 0.22627511620521545\n",
            "iteration 45797: loss: 0.22627508640289307\n",
            "iteration 45798: loss: 0.22627508640289307\n",
            "iteration 45799: loss: 0.22627508640289307\n",
            "iteration 45800: loss: 0.22627505660057068\n",
            "iteration 45801: loss: 0.2262750118970871\n",
            "iteration 45802: loss: 0.2262750118970871\n",
            "iteration 45803: loss: 0.22627496719360352\n",
            "iteration 45804: loss: 0.22627493739128113\n",
            "iteration 45805: loss: 0.22627493739128113\n",
            "iteration 45806: loss: 0.22627492249011993\n",
            "iteration 45807: loss: 0.22627487778663635\n",
            "iteration 45808: loss: 0.22627492249011993\n",
            "iteration 45809: loss: 0.22627481818199158\n",
            "iteration 45810: loss: 0.22627487778663635\n",
            "iteration 45811: loss: 0.22627480328083038\n",
            "iteration 45812: loss: 0.226274773478508\n",
            "iteration 45813: loss: 0.22627481818199158\n",
            "iteration 45814: loss: 0.226274773478508\n",
            "iteration 45815: loss: 0.22627469897270203\n",
            "iteration 45816: loss: 0.22627472877502441\n",
            "iteration 45817: loss: 0.22627469897270203\n",
            "iteration 45818: loss: 0.22627469897270203\n",
            "iteration 45819: loss: 0.22627468407154083\n",
            "iteration 45820: loss: 0.22627457976341248\n",
            "iteration 45821: loss: 0.22627457976341248\n",
            "iteration 45822: loss: 0.22627457976341248\n",
            "iteration 45823: loss: 0.22627456486225128\n",
            "iteration 45824: loss: 0.22627456486225128\n",
            "iteration 45825: loss: 0.2262745350599289\n",
            "iteration 45826: loss: 0.2262745201587677\n",
            "iteration 45827: loss: 0.2262744903564453\n",
            "iteration 45828: loss: 0.22627444565296173\n",
            "iteration 45829: loss: 0.22627444565296173\n",
            "iteration 45830: loss: 0.22627437114715576\n",
            "iteration 45831: loss: 0.22627440094947815\n",
            "iteration 45832: loss: 0.22627440094947815\n",
            "iteration 45833: loss: 0.22627441585063934\n",
            "iteration 45834: loss: 0.22627434134483337\n",
            "iteration 45835: loss: 0.22627432644367218\n",
            "iteration 45836: loss: 0.2262742966413498\n",
            "iteration 45837: loss: 0.2262742966413498\n",
            "iteration 45838: loss: 0.2262742519378662\n",
            "iteration 45839: loss: 0.2262742966413498\n",
            "iteration 45840: loss: 0.2262742519378662\n",
            "iteration 45841: loss: 0.2262742519378662\n",
            "iteration 45842: loss: 0.2262742519378662\n",
            "iteration 45843: loss: 0.22627416253089905\n",
            "iteration 45844: loss: 0.22627413272857666\n",
            "iteration 45845: loss: 0.22627416253089905\n",
            "iteration 45846: loss: 0.22627410292625427\n",
            "iteration 45847: loss: 0.22627410292625427\n",
            "iteration 45848: loss: 0.22627408802509308\n",
            "iteration 45849: loss: 0.2262740433216095\n",
            "iteration 45850: loss: 0.2262740433216095\n",
            "iteration 45851: loss: 0.2262740135192871\n",
            "iteration 45852: loss: 0.22627396881580353\n",
            "iteration 45853: loss: 0.22627392411231995\n",
            "iteration 45854: loss: 0.22627396881580353\n",
            "iteration 45855: loss: 0.22627396881580353\n",
            "iteration 45856: loss: 0.22627392411231995\n",
            "iteration 45857: loss: 0.22627392411231995\n",
            "iteration 45858: loss: 0.22627386450767517\n",
            "iteration 45859: loss: 0.22627384960651398\n",
            "iteration 45860: loss: 0.22627384960651398\n",
            "iteration 45861: loss: 0.2262738198041916\n",
            "iteration 45862: loss: 0.226273775100708\n",
            "iteration 45863: loss: 0.22627374529838562\n",
            "iteration 45864: loss: 0.22627374529838562\n",
            "iteration 45865: loss: 0.22627374529838562\n",
            "iteration 45866: loss: 0.22627370059490204\n",
            "iteration 45867: loss: 0.22627370059490204\n",
            "iteration 45868: loss: 0.22627368569374084\n",
            "iteration 45869: loss: 0.22627368569374084\n",
            "iteration 45870: loss: 0.22627361118793488\n",
            "iteration 45871: loss: 0.22627362608909607\n",
            "iteration 45872: loss: 0.2262735664844513\n",
            "iteration 45873: loss: 0.2262735664844513\n",
            "iteration 45874: loss: 0.2262735366821289\n",
            "iteration 45875: loss: 0.2262735664844513\n",
            "iteration 45876: loss: 0.22627350687980652\n",
            "iteration 45877: loss: 0.22627349197864532\n",
            "iteration 45878: loss: 0.22627349197864532\n",
            "iteration 45879: loss: 0.22627346217632294\n",
            "iteration 45880: loss: 0.22627344727516174\n",
            "iteration 45881: loss: 0.22627341747283936\n",
            "iteration 45882: loss: 0.22627338767051697\n",
            "iteration 45883: loss: 0.22627337276935577\n",
            "iteration 45884: loss: 0.2262733429670334\n",
            "iteration 45885: loss: 0.22627326846122742\n",
            "iteration 45886: loss: 0.2262733280658722\n",
            "iteration 45887: loss: 0.2262733280658722\n",
            "iteration 45888: loss: 0.22627325356006622\n",
            "iteration 45889: loss: 0.22627325356006622\n",
            "iteration 45890: loss: 0.22627320885658264\n",
            "iteration 45891: loss: 0.22627325356006622\n",
            "iteration 45892: loss: 0.22627314925193787\n",
            "iteration 45893: loss: 0.22627314925193787\n",
            "iteration 45894: loss: 0.22627310454845428\n",
            "iteration 45895: loss: 0.22627310454845428\n",
            "iteration 45896: loss: 0.22627310454845428\n",
            "iteration 45897: loss: 0.22627310454845428\n",
            "iteration 45898: loss: 0.2262730896472931\n",
            "iteration 45899: loss: 0.22627301514148712\n",
            "iteration 45900: loss: 0.22627301514148712\n",
            "iteration 45901: loss: 0.22627301514148712\n",
            "iteration 45902: loss: 0.22627298533916473\n",
            "iteration 45903: loss: 0.22627298533916473\n",
            "iteration 45904: loss: 0.22627297043800354\n",
            "iteration 45905: loss: 0.22627289593219757\n",
            "iteration 45906: loss: 0.22627289593219757\n",
            "iteration 45907: loss: 0.22627289593219757\n",
            "iteration 45908: loss: 0.226272851228714\n",
            "iteration 45909: loss: 0.22627286612987518\n",
            "iteration 45910: loss: 0.226272851228714\n",
            "iteration 45911: loss: 0.2262727916240692\n",
            "iteration 45912: loss: 0.2262727916240692\n",
            "iteration 45913: loss: 0.22627277672290802\n",
            "iteration 45914: loss: 0.22627277672290802\n",
            "iteration 45915: loss: 0.22627273201942444\n",
            "iteration 45916: loss: 0.22627273201942444\n",
            "iteration 45917: loss: 0.22627273201942444\n",
            "iteration 45918: loss: 0.22627270221710205\n",
            "iteration 45919: loss: 0.22627262771129608\n",
            "iteration 45920: loss: 0.2262726128101349\n",
            "iteration 45921: loss: 0.2262725830078125\n",
            "iteration 45922: loss: 0.2262725830078125\n",
            "iteration 45923: loss: 0.2262725830078125\n",
            "iteration 45924: loss: 0.22627253830432892\n",
            "iteration 45925: loss: 0.22627250850200653\n",
            "iteration 45926: loss: 0.22627250850200653\n",
            "iteration 45927: loss: 0.22627253830432892\n",
            "iteration 45928: loss: 0.22627243399620056\n",
            "iteration 45929: loss: 0.22627249360084534\n",
            "iteration 45930: loss: 0.22627243399620056\n",
            "iteration 45931: loss: 0.22627243399620056\n",
            "iteration 45932: loss: 0.2262723743915558\n",
            "iteration 45933: loss: 0.22627238929271698\n",
            "iteration 45934: loss: 0.2262723743915558\n",
            "iteration 45935: loss: 0.2262723445892334\n",
            "iteration 45936: loss: 0.226272314786911\n",
            "iteration 45937: loss: 0.22627229988574982\n",
            "iteration 45938: loss: 0.22627229988574982\n",
            "iteration 45939: loss: 0.22627222537994385\n",
            "iteration 45940: loss: 0.22627225518226624\n",
            "iteration 45941: loss: 0.22627229988574982\n",
            "iteration 45942: loss: 0.22627218067646027\n",
            "iteration 45943: loss: 0.22627218067646027\n",
            "iteration 45944: loss: 0.22627218067646027\n",
            "iteration 45945: loss: 0.22627215087413788\n",
            "iteration 45946: loss: 0.2262721061706543\n",
            "iteration 45947: loss: 0.2262720763683319\n",
            "iteration 45948: loss: 0.22627206146717072\n",
            "iteration 45949: loss: 0.2262720763683319\n",
            "iteration 45950: loss: 0.22627203166484833\n",
            "iteration 45951: loss: 0.22627203166484833\n",
            "iteration 45952: loss: 0.22627206146717072\n",
            "iteration 45953: loss: 0.22627195715904236\n",
            "iteration 45954: loss: 0.22627194225788116\n",
            "iteration 45955: loss: 0.22627195715904236\n",
            "iteration 45956: loss: 0.22627189755439758\n",
            "iteration 45957: loss: 0.2262718379497528\n",
            "iteration 45958: loss: 0.2262718677520752\n",
            "iteration 45959: loss: 0.2262718379497528\n",
            "iteration 45960: loss: 0.2262718677520752\n",
            "iteration 45961: loss: 0.2262718379497528\n",
            "iteration 45962: loss: 0.2262718230485916\n",
            "iteration 45963: loss: 0.22627177834510803\n",
            "iteration 45964: loss: 0.22627174854278564\n",
            "iteration 45965: loss: 0.22627170383930206\n",
            "iteration 45966: loss: 0.22627174854278564\n",
            "iteration 45967: loss: 0.22627170383930206\n",
            "iteration 45968: loss: 0.22627167403697968\n",
            "iteration 45969: loss: 0.22627167403697968\n",
            "iteration 45970: loss: 0.2262716293334961\n",
            "iteration 45971: loss: 0.2262715995311737\n",
            "iteration 45972: loss: 0.2262715846300125\n",
            "iteration 45973: loss: 0.2262715846300125\n",
            "iteration 45974: loss: 0.22627148032188416\n",
            "iteration 45975: loss: 0.22627153992652893\n",
            "iteration 45976: loss: 0.22627151012420654\n",
            "iteration 45977: loss: 0.22627148032188416\n",
            "iteration 45978: loss: 0.22627146542072296\n",
            "iteration 45979: loss: 0.22627146542072296\n",
            "iteration 45980: loss: 0.226271390914917\n",
            "iteration 45981: loss: 0.22627143561840057\n",
            "iteration 45982: loss: 0.226271390914917\n",
            "iteration 45983: loss: 0.22627142071723938\n",
            "iteration 45984: loss: 0.2262713462114334\n",
            "iteration 45985: loss: 0.2262713462114334\n",
            "iteration 45986: loss: 0.22627131640911102\n",
            "iteration 45987: loss: 0.22627130150794983\n",
            "iteration 45988: loss: 0.22627127170562744\n",
            "iteration 45989: loss: 0.22627124190330505\n",
            "iteration 45990: loss: 0.22627124190330505\n",
            "iteration 45991: loss: 0.22627119719982147\n",
            "iteration 45992: loss: 0.22627119719982147\n",
            "iteration 45993: loss: 0.22627118229866028\n",
            "iteration 45994: loss: 0.2262711226940155\n",
            "iteration 45995: loss: 0.2262711524963379\n",
            "iteration 45996: loss: 0.2262711226940155\n",
            "iteration 45997: loss: 0.22627107799053192\n",
            "iteration 45998: loss: 0.2262711077928543\n",
            "iteration 45999: loss: 0.22627107799053192\n",
            "iteration 46000: loss: 0.22627103328704834\n",
            "iteration 46001: loss: 0.22627100348472595\n",
            "iteration 46002: loss: 0.22627098858356476\n",
            "iteration 46003: loss: 0.22627094388008118\n",
            "iteration 46004: loss: 0.22627095878124237\n",
            "iteration 46005: loss: 0.22627094388008118\n",
            "iteration 46006: loss: 0.22627094388008118\n",
            "iteration 46007: loss: 0.2262708842754364\n",
            "iteration 46008: loss: 0.2262709140777588\n",
            "iteration 46009: loss: 0.2262708693742752\n",
            "iteration 46010: loss: 0.22627083957195282\n",
            "iteration 46011: loss: 0.22627082467079163\n",
            "iteration 46012: loss: 0.22627079486846924\n",
            "iteration 46013: loss: 0.22627076506614685\n",
            "iteration 46014: loss: 0.22627072036266327\n",
            "iteration 46015: loss: 0.22627075016498566\n",
            "iteration 46016: loss: 0.22627072036266327\n",
            "iteration 46017: loss: 0.2262706756591797\n",
            "iteration 46018: loss: 0.22627070546150208\n",
            "iteration 46019: loss: 0.22627070546150208\n",
            "iteration 46020: loss: 0.22627060115337372\n",
            "iteration 46021: loss: 0.22627058625221252\n",
            "iteration 46022: loss: 0.22627060115337372\n",
            "iteration 46023: loss: 0.22627055644989014\n",
            "iteration 46024: loss: 0.22627055644989014\n",
            "iteration 46025: loss: 0.22627052664756775\n",
            "iteration 46026: loss: 0.22627051174640656\n",
            "iteration 46027: loss: 0.22627051174640656\n",
            "iteration 46028: loss: 0.22627048194408417\n",
            "iteration 46029: loss: 0.22627043724060059\n",
            "iteration 46030: loss: 0.2262704074382782\n",
            "iteration 46031: loss: 0.2262704074382782\n",
            "iteration 46032: loss: 0.226270392537117\n",
            "iteration 46033: loss: 0.22627036273479462\n",
            "iteration 46034: loss: 0.22627031803131104\n",
            "iteration 46035: loss: 0.22627034783363342\n",
            "iteration 46036: loss: 0.22627034783363342\n",
            "iteration 46037: loss: 0.22627028822898865\n",
            "iteration 46038: loss: 0.22627024352550507\n",
            "iteration 46039: loss: 0.22627027332782745\n",
            "iteration 46040: loss: 0.22627024352550507\n",
            "iteration 46041: loss: 0.22627022862434387\n",
            "iteration 46042: loss: 0.22627019882202148\n",
            "iteration 46043: loss: 0.2262701690196991\n",
            "iteration 46044: loss: 0.22627019882202148\n",
            "iteration 46045: loss: 0.22627010941505432\n",
            "iteration 46046: loss: 0.22627007961273193\n",
            "iteration 46047: loss: 0.22627007961273193\n",
            "iteration 46048: loss: 0.22627010941505432\n",
            "iteration 46049: loss: 0.22627003490924835\n",
            "iteration 46050: loss: 0.22627000510692596\n",
            "iteration 46051: loss: 0.22627000510692596\n",
            "iteration 46052: loss: 0.22627003490924835\n",
            "iteration 46053: loss: 0.22626996040344238\n",
            "iteration 46054: loss: 0.22626996040344238\n",
            "iteration 46055: loss: 0.2262699156999588\n",
            "iteration 46056: loss: 0.2262699156999588\n",
            "iteration 46057: loss: 0.2262698858976364\n",
            "iteration 46058: loss: 0.22626987099647522\n",
            "iteration 46059: loss: 0.22626984119415283\n",
            "iteration 46060: loss: 0.22626987099647522\n",
            "iteration 46061: loss: 0.22626976668834686\n",
            "iteration 46062: loss: 0.22626981139183044\n",
            "iteration 46063: loss: 0.22626979649066925\n",
            "iteration 46064: loss: 0.22626975178718567\n",
            "iteration 46065: loss: 0.22626975178718567\n",
            "iteration 46066: loss: 0.22626975178718567\n",
            "iteration 46067: loss: 0.22626972198486328\n",
            "iteration 46068: loss: 0.2262696921825409\n",
            "iteration 46069: loss: 0.22626963257789612\n",
            "iteration 46070: loss: 0.2262696474790573\n",
            "iteration 46071: loss: 0.22626963257789612\n",
            "iteration 46072: loss: 0.22626960277557373\n",
            "iteration 46073: loss: 0.22626955807209015\n",
            "iteration 46074: loss: 0.22626957297325134\n",
            "iteration 46075: loss: 0.22626955807209015\n",
            "iteration 46076: loss: 0.22626952826976776\n",
            "iteration 46077: loss: 0.22626948356628418\n",
            "iteration 46078: loss: 0.2262694537639618\n",
            "iteration 46079: loss: 0.2262694388628006\n",
            "iteration 46080: loss: 0.22626948356628418\n",
            "iteration 46081: loss: 0.2262694388628006\n",
            "iteration 46082: loss: 0.22626939415931702\n",
            "iteration 46083: loss: 0.22626933455467224\n",
            "iteration 46084: loss: 0.22626936435699463\n",
            "iteration 46085: loss: 0.22626933455467224\n",
            "iteration 46086: loss: 0.22626931965351105\n",
            "iteration 46087: loss: 0.22626933455467224\n",
            "iteration 46088: loss: 0.22626927495002747\n",
            "iteration 46089: loss: 0.22626927495002747\n",
            "iteration 46090: loss: 0.2262692153453827\n",
            "iteration 46091: loss: 0.2262692153453827\n",
            "iteration 46092: loss: 0.2262692004442215\n",
            "iteration 46093: loss: 0.2262692153453827\n",
            "iteration 46094: loss: 0.22626915574073792\n",
            "iteration 46095: loss: 0.22626915574073792\n",
            "iteration 46096: loss: 0.22626912593841553\n",
            "iteration 46097: loss: 0.22626905143260956\n",
            "iteration 46098: loss: 0.22626909613609314\n",
            "iteration 46099: loss: 0.22626908123493195\n",
            "iteration 46100: loss: 0.22626905143260956\n",
            "iteration 46101: loss: 0.22626903653144836\n",
            "iteration 46102: loss: 0.22626900672912598\n",
            "iteration 46103: loss: 0.2262689620256424\n",
            "iteration 46104: loss: 0.22626893222332\n",
            "iteration 46105: loss: 0.22626893222332\n",
            "iteration 46106: loss: 0.2262689620256424\n",
            "iteration 46107: loss: 0.22626888751983643\n",
            "iteration 46108: loss: 0.2262689173221588\n",
            "iteration 46109: loss: 0.22626888751983643\n",
            "iteration 46110: loss: 0.22626884281635284\n",
            "iteration 46111: loss: 0.22626881301403046\n",
            "iteration 46112: loss: 0.22626884281635284\n",
            "iteration 46113: loss: 0.22626879811286926\n",
            "iteration 46114: loss: 0.2262687236070633\n",
            "iteration 46115: loss: 0.2262687385082245\n",
            "iteration 46116: loss: 0.2262687385082245\n",
            "iteration 46117: loss: 0.2262687385082245\n",
            "iteration 46118: loss: 0.2262686789035797\n",
            "iteration 46119: loss: 0.2262686938047409\n",
            "iteration 46120: loss: 0.22626864910125732\n",
            "iteration 46121: loss: 0.22626861929893494\n",
            "iteration 46122: loss: 0.22626857459545135\n",
            "iteration 46123: loss: 0.22626855969429016\n",
            "iteration 46124: loss: 0.22626857459545135\n",
            "iteration 46125: loss: 0.22626852989196777\n",
            "iteration 46126: loss: 0.22626852989196777\n",
            "iteration 46127: loss: 0.22626850008964539\n",
            "iteration 46128: loss: 0.22626850008964539\n",
            "iteration 46129: loss: 0.22626852989196777\n",
            "iteration 46130: loss: 0.2262684553861618\n",
            "iteration 46131: loss: 0.2262684404850006\n",
            "iteration 46132: loss: 0.22626838088035583\n",
            "iteration 46133: loss: 0.22626833617687225\n",
            "iteration 46134: loss: 0.22626833617687225\n",
            "iteration 46135: loss: 0.22626836597919464\n",
            "iteration 46136: loss: 0.22626833617687225\n",
            "iteration 46137: loss: 0.22626832127571106\n",
            "iteration 46138: loss: 0.22626826167106628\n",
            "iteration 46139: loss: 0.22626826167106628\n",
            "iteration 46140: loss: 0.22626826167106628\n",
            "iteration 46141: loss: 0.2262682467699051\n",
            "iteration 46142: loss: 0.2262682020664215\n",
            "iteration 46143: loss: 0.2262682020664215\n",
            "iteration 46144: loss: 0.22626817226409912\n",
            "iteration 46145: loss: 0.22626814246177673\n",
            "iteration 46146: loss: 0.22626812756061554\n",
            "iteration 46147: loss: 0.22626812756061554\n",
            "iteration 46148: loss: 0.22626808285713196\n",
            "iteration 46149: loss: 0.22626808285713196\n",
            "iteration 46150: loss: 0.22626805305480957\n",
            "iteration 46151: loss: 0.22626802325248718\n",
            "iteration 46152: loss: 0.2262679785490036\n",
            "iteration 46153: loss: 0.226268008351326\n",
            "iteration 46154: loss: 0.2262679785490036\n",
            "iteration 46155: loss: 0.22626793384552002\n",
            "iteration 46156: loss: 0.22626790404319763\n",
            "iteration 46157: loss: 0.22626788914203644\n",
            "iteration 46158: loss: 0.22626785933971405\n",
            "iteration 46159: loss: 0.22626788914203644\n",
            "iteration 46160: loss: 0.22626785933971405\n",
            "iteration 46161: loss: 0.22626781463623047\n",
            "iteration 46162: loss: 0.22626781463623047\n",
            "iteration 46163: loss: 0.2262677699327469\n",
            "iteration 46164: loss: 0.22626778483390808\n",
            "iteration 46165: loss: 0.2262677252292633\n",
            "iteration 46166: loss: 0.2262677252292633\n",
            "iteration 46167: loss: 0.22626766562461853\n",
            "iteration 46168: loss: 0.2262677252292633\n",
            "iteration 46169: loss: 0.22626769542694092\n",
            "iteration 46170: loss: 0.22626765072345734\n",
            "iteration 46171: loss: 0.22626766562461853\n",
            "iteration 46172: loss: 0.22626760601997375\n",
            "iteration 46173: loss: 0.22626760601997375\n",
            "iteration 46174: loss: 0.22626757621765137\n",
            "iteration 46175: loss: 0.22626754641532898\n",
            "iteration 46176: loss: 0.22626753151416779\n",
            "iteration 46177: loss: 0.22626753151416779\n",
            "iteration 46178: loss: 0.2262674868106842\n",
            "iteration 46179: loss: 0.22626753151416779\n",
            "iteration 46180: loss: 0.2262674868106842\n",
            "iteration 46181: loss: 0.22626741230487823\n",
            "iteration 46182: loss: 0.22626742720603943\n",
            "iteration 46183: loss: 0.22626738250255585\n",
            "iteration 46184: loss: 0.22626741230487823\n",
            "iteration 46185: loss: 0.22626738250255585\n",
            "iteration 46186: loss: 0.22626733779907227\n",
            "iteration 46187: loss: 0.22626733779907227\n",
            "iteration 46188: loss: 0.22626730799674988\n",
            "iteration 46189: loss: 0.2262672632932663\n",
            "iteration 46190: loss: 0.2262672483921051\n",
            "iteration 46191: loss: 0.2262672632932663\n",
            "iteration 46192: loss: 0.22626718878746033\n",
            "iteration 46193: loss: 0.22626717388629913\n",
            "iteration 46194: loss: 0.22626717388629913\n",
            "iteration 46195: loss: 0.22626714408397675\n",
            "iteration 46196: loss: 0.22626714408397675\n",
            "iteration 46197: loss: 0.22626712918281555\n",
            "iteration 46198: loss: 0.22626706957817078\n",
            "iteration 46199: loss: 0.22626709938049316\n",
            "iteration 46200: loss: 0.22626709938049316\n",
            "iteration 46201: loss: 0.2262670248746872\n",
            "iteration 46202: loss: 0.226267009973526\n",
            "iteration 46203: loss: 0.226267009973526\n",
            "iteration 46204: loss: 0.2262669801712036\n",
            "iteration 46205: loss: 0.22626690566539764\n",
            "iteration 46206: loss: 0.2262669801712036\n",
            "iteration 46207: loss: 0.22626693546772003\n",
            "iteration 46208: loss: 0.22626695036888123\n",
            "iteration 46209: loss: 0.22626689076423645\n",
            "iteration 46210: loss: 0.22626686096191406\n",
            "iteration 46211: loss: 0.22626683115959167\n",
            "iteration 46212: loss: 0.2262667864561081\n",
            "iteration 46213: loss: 0.22626681625843048\n",
            "iteration 46214: loss: 0.2262667715549469\n",
            "iteration 46215: loss: 0.2262667715549469\n",
            "iteration 46216: loss: 0.22626671195030212\n",
            "iteration 46217: loss: 0.2262667417526245\n",
            "iteration 46218: loss: 0.22626671195030212\n",
            "iteration 46219: loss: 0.22626669704914093\n",
            "iteration 46220: loss: 0.22626671195030212\n",
            "iteration 46221: loss: 0.22626665234565735\n",
            "iteration 46222: loss: 0.22626659274101257\n",
            "iteration 46223: loss: 0.22626657783985138\n",
            "iteration 46224: loss: 0.22626659274101257\n",
            "iteration 46225: loss: 0.2262665331363678\n",
            "iteration 46226: loss: 0.2262665331363678\n",
            "iteration 46227: loss: 0.2262665331363678\n",
            "iteration 46228: loss: 0.2262665033340454\n",
            "iteration 46229: loss: 0.2262665033340454\n",
            "iteration 46230: loss: 0.22626645863056183\n",
            "iteration 46231: loss: 0.22626642882823944\n",
            "iteration 46232: loss: 0.22626641392707825\n",
            "iteration 46233: loss: 0.22626638412475586\n",
            "iteration 46234: loss: 0.22626641392707825\n",
            "iteration 46235: loss: 0.22626633942127228\n",
            "iteration 46236: loss: 0.22626633942127228\n",
            "iteration 46237: loss: 0.22626633942127228\n",
            "iteration 46238: loss: 0.22626633942127228\n",
            "iteration 46239: loss: 0.22626623511314392\n",
            "iteration 46240: loss: 0.22626623511314392\n",
            "iteration 46241: loss: 0.2262662649154663\n",
            "iteration 46242: loss: 0.22626623511314392\n",
            "iteration 46243: loss: 0.22626619040966034\n",
            "iteration 46244: loss: 0.22626619040966034\n",
            "iteration 46245: loss: 0.22626617550849915\n",
            "iteration 46246: loss: 0.22626614570617676\n",
            "iteration 46247: loss: 0.22626610100269318\n",
            "iteration 46248: loss: 0.22626610100269318\n",
            "iteration 46249: loss: 0.2262660712003708\n",
            "iteration 46250: loss: 0.2262660712003708\n",
            "iteration 46251: loss: 0.2262660264968872\n",
            "iteration 46252: loss: 0.2262660264968872\n",
            "iteration 46253: loss: 0.22626599669456482\n",
            "iteration 46254: loss: 0.22626598179340363\n",
            "iteration 46255: loss: 0.22626598179340363\n",
            "iteration 46256: loss: 0.22626593708992004\n",
            "iteration 46257: loss: 0.22626593708992004\n",
            "iteration 46258: loss: 0.22626590728759766\n",
            "iteration 46259: loss: 0.22626590728759766\n",
            "iteration 46260: loss: 0.22626587748527527\n",
            "iteration 46261: loss: 0.22626587748527527\n",
            "iteration 46262: loss: 0.2262658327817917\n",
            "iteration 46263: loss: 0.2262657880783081\n",
            "iteration 46264: loss: 0.2262657880783081\n",
            "iteration 46265: loss: 0.22626575827598572\n",
            "iteration 46266: loss: 0.22626574337482452\n",
            "iteration 46267: loss: 0.22626571357250214\n",
            "iteration 46268: loss: 0.22626571357250214\n",
            "iteration 46269: loss: 0.22626566886901855\n",
            "iteration 46270: loss: 0.22626569867134094\n",
            "iteration 46271: loss: 0.22626562416553497\n",
            "iteration 46272: loss: 0.22626563906669617\n",
            "iteration 46273: loss: 0.22626562416553497\n",
            "iteration 46274: loss: 0.2262655794620514\n",
            "iteration 46275: loss: 0.2262655794620514\n",
            "iteration 46276: loss: 0.22626550495624542\n",
            "iteration 46277: loss: 0.22626551985740662\n",
            "iteration 46278: loss: 0.22626550495624542\n",
            "iteration 46279: loss: 0.22626547515392303\n",
            "iteration 46280: loss: 0.22626543045043945\n",
            "iteration 46281: loss: 0.22626543045043945\n",
            "iteration 46282: loss: 0.22626546025276184\n",
            "iteration 46283: loss: 0.22626538574695587\n",
            "iteration 46284: loss: 0.22626540064811707\n",
            "iteration 46285: loss: 0.2262653410434723\n",
            "iteration 46286: loss: 0.22626535594463348\n",
            "iteration 46287: loss: 0.2262653112411499\n",
            "iteration 46288: loss: 0.2262653410434723\n",
            "iteration 46289: loss: 0.2262653112411499\n",
            "iteration 46290: loss: 0.22626526653766632\n",
            "iteration 46291: loss: 0.22626528143882751\n",
            "iteration 46292: loss: 0.22626522183418274\n",
            "iteration 46293: loss: 0.22626522183418274\n",
            "iteration 46294: loss: 0.22626519203186035\n",
            "iteration 46295: loss: 0.22626516222953796\n",
            "iteration 46296: loss: 0.22626516222953796\n",
            "iteration 46297: loss: 0.22626511752605438\n",
            "iteration 46298: loss: 0.22626511752605438\n",
            "iteration 46299: loss: 0.22626511752605438\n",
            "iteration 46300: loss: 0.2262651026248932\n",
            "iteration 46301: loss: 0.2262650430202484\n",
            "iteration 46302: loss: 0.2262650430202484\n",
            "iteration 46303: loss: 0.2262650430202484\n",
            "iteration 46304: loss: 0.22626499831676483\n",
            "iteration 46305: loss: 0.22626498341560364\n",
            "iteration 46306: loss: 0.22626495361328125\n",
            "iteration 46307: loss: 0.22626495361328125\n",
            "iteration 46308: loss: 0.22626495361328125\n",
            "iteration 46309: loss: 0.2262648642063141\n",
            "iteration 46310: loss: 0.2262648642063141\n",
            "iteration 46311: loss: 0.2262648642063141\n",
            "iteration 46312: loss: 0.22626487910747528\n",
            "iteration 46313: loss: 0.2262648642063141\n",
            "iteration 46314: loss: 0.2262648046016693\n",
            "iteration 46315: loss: 0.22626475989818573\n",
            "iteration 46316: loss: 0.2262648046016693\n",
            "iteration 46317: loss: 0.22626475989818573\n",
            "iteration 46318: loss: 0.22626471519470215\n",
            "iteration 46319: loss: 0.22626471519470215\n",
            "iteration 46320: loss: 0.22626467049121857\n",
            "iteration 46321: loss: 0.22626467049121857\n",
            "iteration 46322: loss: 0.22626464068889618\n",
            "iteration 46323: loss: 0.22626462578773499\n",
            "iteration 46324: loss: 0.22626462578773499\n",
            "iteration 46325: loss: 0.22626462578773499\n",
            "iteration 46326: loss: 0.2262645959854126\n",
            "iteration 46327: loss: 0.22626455128192902\n",
            "iteration 46328: loss: 0.22626455128192902\n",
            "iteration 46329: loss: 0.22626455128192902\n",
            "iteration 46330: loss: 0.22626447677612305\n",
            "iteration 46331: loss: 0.22626444697380066\n",
            "iteration 46332: loss: 0.22626440227031708\n",
            "iteration 46333: loss: 0.22626444697380066\n",
            "iteration 46334: loss: 0.22626443207263947\n",
            "iteration 46335: loss: 0.22626440227031708\n",
            "iteration 46336: loss: 0.22626443207263947\n",
            "iteration 46337: loss: 0.2262643575668335\n",
            "iteration 46338: loss: 0.22626431286334991\n",
            "iteration 46339: loss: 0.2262643277645111\n",
            "iteration 46340: loss: 0.22626428306102753\n",
            "iteration 46341: loss: 0.22626426815986633\n",
            "iteration 46342: loss: 0.22626431286334991\n",
            "iteration 46343: loss: 0.22626420855522156\n",
            "iteration 46344: loss: 0.22626419365406036\n",
            "iteration 46345: loss: 0.22626420855522156\n",
            "iteration 46346: loss: 0.22626416385173798\n",
            "iteration 46347: loss: 0.22626419365406036\n",
            "iteration 46348: loss: 0.2262641191482544\n",
            "iteration 46349: loss: 0.226264089345932\n",
            "iteration 46350: loss: 0.226264089345932\n",
            "iteration 46351: loss: 0.22626404464244843\n",
            "iteration 46352: loss: 0.22626404464244843\n",
            "iteration 46353: loss: 0.22626404464244843\n",
            "iteration 46354: loss: 0.22626404464244843\n",
            "iteration 46355: loss: 0.22626399993896484\n",
            "iteration 46356: loss: 0.22626397013664246\n",
            "iteration 46357: loss: 0.22626395523548126\n",
            "iteration 46358: loss: 0.22626391053199768\n",
            "iteration 46359: loss: 0.22626392543315887\n",
            "iteration 46360: loss: 0.2262638509273529\n",
            "iteration 46361: loss: 0.22626391053199768\n",
            "iteration 46362: loss: 0.2262638509273529\n",
            "iteration 46363: loss: 0.2262638509273529\n",
            "iteration 46364: loss: 0.22626380622386932\n",
            "iteration 46365: loss: 0.2262638509273529\n",
            "iteration 46366: loss: 0.22626380622386932\n",
            "iteration 46367: loss: 0.22626376152038574\n",
            "iteration 46368: loss: 0.22626376152038574\n",
            "iteration 46369: loss: 0.22626371681690216\n",
            "iteration 46370: loss: 0.22626371681690216\n",
            "iteration 46371: loss: 0.22626367211341858\n",
            "iteration 46372: loss: 0.22626368701457977\n",
            "iteration 46373: loss: 0.2262636125087738\n",
            "iteration 46374: loss: 0.2262635976076126\n",
            "iteration 46375: loss: 0.22626356780529022\n",
            "iteration 46376: loss: 0.22626356780529022\n",
            "iteration 46377: loss: 0.22626356780529022\n",
            "iteration 46378: loss: 0.22626356780529022\n",
            "iteration 46379: loss: 0.22626352310180664\n",
            "iteration 46380: loss: 0.22626352310180664\n",
            "iteration 46381: loss: 0.22626349329948425\n",
            "iteration 46382: loss: 0.22626347839832306\n",
            "iteration 46383: loss: 0.22626344859600067\n",
            "iteration 46384: loss: 0.22626343369483948\n",
            "iteration 46385: loss: 0.22626343369483948\n",
            "iteration 46386: loss: 0.2262634038925171\n",
            "iteration 46387: loss: 0.2262634038925171\n",
            "iteration 46388: loss: 0.2262633740901947\n",
            "iteration 46389: loss: 0.22626332938671112\n",
            "iteration 46390: loss: 0.22626332938671112\n",
            "iteration 46391: loss: 0.22626332938671112\n",
            "iteration 46392: loss: 0.22626328468322754\n",
            "iteration 46393: loss: 0.22626323997974396\n",
            "iteration 46394: loss: 0.22626321017742157\n",
            "iteration 46395: loss: 0.22626321017742157\n",
            "iteration 46396: loss: 0.22626319527626038\n",
            "iteration 46397: loss: 0.226263165473938\n",
            "iteration 46398: loss: 0.2262631356716156\n",
            "iteration 46399: loss: 0.2262631356716156\n",
            "iteration 46400: loss: 0.2262631207704544\n",
            "iteration 46401: loss: 0.22626309096813202\n",
            "iteration 46402: loss: 0.22626307606697083\n",
            "iteration 46403: loss: 0.22626304626464844\n",
            "iteration 46404: loss: 0.22626307606697083\n",
            "iteration 46405: loss: 0.22626304626464844\n",
            "iteration 46406: loss: 0.22626295685768127\n",
            "iteration 46407: loss: 0.22626295685768127\n",
            "iteration 46408: loss: 0.22626301646232605\n",
            "iteration 46409: loss: 0.2262629270553589\n",
            "iteration 46410: loss: 0.22626297175884247\n",
            "iteration 46411: loss: 0.2262628972530365\n",
            "iteration 46412: loss: 0.2262628823518753\n",
            "iteration 46413: loss: 0.2262628823518753\n",
            "iteration 46414: loss: 0.22626283764839172\n",
            "iteration 46415: loss: 0.22626280784606934\n",
            "iteration 46416: loss: 0.22626277804374695\n",
            "iteration 46417: loss: 0.22626273334026337\n",
            "iteration 46418: loss: 0.22626276314258575\n",
            "iteration 46419: loss: 0.22626273334026337\n",
            "iteration 46420: loss: 0.22626268863677979\n",
            "iteration 46421: loss: 0.2262626588344574\n",
            "iteration 46422: loss: 0.2262626588344574\n",
            "iteration 46423: loss: 0.2262626588344574\n",
            "iteration 46424: loss: 0.2262626588344574\n",
            "iteration 46425: loss: 0.22626259922981262\n",
            "iteration 46426: loss: 0.2262626439332962\n",
            "iteration 46427: loss: 0.22626261413097382\n",
            "iteration 46428: loss: 0.22626256942749023\n",
            "iteration 46429: loss: 0.22626253962516785\n",
            "iteration 46430: loss: 0.22626252472400665\n",
            "iteration 46431: loss: 0.22626248002052307\n",
            "iteration 46432: loss: 0.22626249492168427\n",
            "iteration 46433: loss: 0.22626249492168427\n",
            "iteration 46434: loss: 0.22626245021820068\n",
            "iteration 46435: loss: 0.22626245021820068\n",
            "iteration 46436: loss: 0.2262624204158783\n",
            "iteration 46437: loss: 0.2262624204158783\n",
            "iteration 46438: loss: 0.22626237571239471\n",
            "iteration 46439: loss: 0.22626237571239471\n",
            "iteration 46440: loss: 0.22626233100891113\n",
            "iteration 46441: loss: 0.22626230120658875\n",
            "iteration 46442: loss: 0.22626225650310516\n",
            "iteration 46443: loss: 0.22626228630542755\n",
            "iteration 46444: loss: 0.22626225650310516\n",
            "iteration 46445: loss: 0.22626225650310516\n",
            "iteration 46446: loss: 0.22626221179962158\n",
            "iteration 46447: loss: 0.22626221179962158\n",
            "iteration 46448: loss: 0.2262621819972992\n",
            "iteration 46449: loss: 0.2262621372938156\n",
            "iteration 46450: loss: 0.2262621372938156\n",
            "iteration 46451: loss: 0.22626209259033203\n",
            "iteration 46452: loss: 0.22626212239265442\n",
            "iteration 46453: loss: 0.22626212239265442\n",
            "iteration 46454: loss: 0.22626212239265442\n",
            "iteration 46455: loss: 0.22626201808452606\n",
            "iteration 46456: loss: 0.22626204788684845\n",
            "iteration 46457: loss: 0.2262619286775589\n",
            "iteration 46458: loss: 0.2262619435787201\n",
            "iteration 46459: loss: 0.2262619435787201\n",
            "iteration 46460: loss: 0.2262618988752365\n",
            "iteration 46461: loss: 0.2262619286775589\n",
            "iteration 46462: loss: 0.2262618988752365\n",
            "iteration 46463: loss: 0.2262619286775589\n",
            "iteration 46464: loss: 0.22626185417175293\n",
            "iteration 46465: loss: 0.22626185417175293\n",
            "iteration 46466: loss: 0.22626180946826935\n",
            "iteration 46467: loss: 0.22626185417175293\n",
            "iteration 46468: loss: 0.22626177966594696\n",
            "iteration 46469: loss: 0.22626173496246338\n",
            "iteration 46470: loss: 0.22626176476478577\n",
            "iteration 46471: loss: 0.22626173496246338\n",
            "iteration 46472: loss: 0.226261705160141\n",
            "iteration 46473: loss: 0.226261705160141\n",
            "iteration 46474: loss: 0.2262616604566574\n",
            "iteration 46475: loss: 0.2262616902589798\n",
            "iteration 46476: loss: 0.22626161575317383\n",
            "iteration 46477: loss: 0.22626161575317383\n",
            "iteration 46478: loss: 0.22626158595085144\n",
            "iteration 46479: loss: 0.22626158595085144\n",
            "iteration 46480: loss: 0.22626152634620667\n",
            "iteration 46481: loss: 0.22626152634620667\n",
            "iteration 46482: loss: 0.22626149654388428\n",
            "iteration 46483: loss: 0.22626154124736786\n",
            "iteration 46484: loss: 0.2262614667415619\n",
            "iteration 46485: loss: 0.2262614220380783\n",
            "iteration 46486: loss: 0.2262614220380783\n",
            "iteration 46487: loss: 0.22626137733459473\n",
            "iteration 46488: loss: 0.22626140713691711\n",
            "iteration 46489: loss: 0.22626134753227234\n",
            "iteration 46490: loss: 0.22626133263111115\n",
            "iteration 46491: loss: 0.22626133263111115\n",
            "iteration 46492: loss: 0.22626128792762756\n",
            "iteration 46493: loss: 0.22626130282878876\n",
            "iteration 46494: loss: 0.22626128792762756\n",
            "iteration 46495: loss: 0.22626125812530518\n",
            "iteration 46496: loss: 0.2262612283229828\n",
            "iteration 46497: loss: 0.2262611836194992\n",
            "iteration 46498: loss: 0.2262611836194992\n",
            "iteration 46499: loss: 0.226261168718338\n",
            "iteration 46500: loss: 0.2262611836194992\n",
            "iteration 46501: loss: 0.22626113891601562\n",
            "iteration 46502: loss: 0.22626109421253204\n",
            "iteration 46503: loss: 0.22626109421253204\n",
            "iteration 46504: loss: 0.22626104950904846\n",
            "iteration 46505: loss: 0.22626109421253204\n",
            "iteration 46506: loss: 0.2262609899044037\n",
            "iteration 46507: loss: 0.2262609899044037\n",
            "iteration 46508: loss: 0.22626101970672607\n",
            "iteration 46509: loss: 0.2262609899044037\n",
            "iteration 46510: loss: 0.2262609750032425\n",
            "iteration 46511: loss: 0.2262609750032425\n",
            "iteration 46512: loss: 0.22626085579395294\n",
            "iteration 46513: loss: 0.22626090049743652\n",
            "iteration 46514: loss: 0.22626085579395294\n",
            "iteration 46515: loss: 0.22626085579395294\n",
            "iteration 46516: loss: 0.22626082599163055\n",
            "iteration 46517: loss: 0.22626082599163055\n",
            "iteration 46518: loss: 0.22626085579395294\n",
            "iteration 46519: loss: 0.22626078128814697\n",
            "iteration 46520: loss: 0.22626081109046936\n",
            "iteration 46521: loss: 0.22626075148582458\n",
            "iteration 46522: loss: 0.226260706782341\n",
            "iteration 46523: loss: 0.226260706782341\n",
            "iteration 46524: loss: 0.2262606918811798\n",
            "iteration 46525: loss: 0.2262606918811798\n",
            "iteration 46526: loss: 0.22626061737537384\n",
            "iteration 46527: loss: 0.22626063227653503\n",
            "iteration 46528: loss: 0.22626061737537384\n",
            "iteration 46529: loss: 0.22626058757305145\n",
            "iteration 46530: loss: 0.22626057267189026\n",
            "iteration 46531: loss: 0.22626051306724548\n",
            "iteration 46532: loss: 0.22626057267189026\n",
            "iteration 46533: loss: 0.22626054286956787\n",
            "iteration 46534: loss: 0.2262604683637619\n",
            "iteration 46535: loss: 0.2262604683637619\n",
            "iteration 46536: loss: 0.2262604534626007\n",
            "iteration 46537: loss: 0.22626037895679474\n",
            "iteration 46538: loss: 0.22626039385795593\n",
            "iteration 46539: loss: 0.22626037895679474\n",
            "iteration 46540: loss: 0.22626037895679474\n",
            "iteration 46541: loss: 0.22626033425331116\n",
            "iteration 46542: loss: 0.22626037895679474\n",
            "iteration 46543: loss: 0.22626033425331116\n",
            "iteration 46544: loss: 0.22626033425331116\n",
            "iteration 46545: loss: 0.2262602597475052\n",
            "iteration 46546: loss: 0.2262602150440216\n",
            "iteration 46547: loss: 0.22626027464866638\n",
            "iteration 46548: loss: 0.2262602299451828\n",
            "iteration 46549: loss: 0.22626018524169922\n",
            "iteration 46550: loss: 0.22626015543937683\n",
            "iteration 46551: loss: 0.22626015543937683\n",
            "iteration 46552: loss: 0.22626014053821564\n",
            "iteration 46553: loss: 0.22626011073589325\n",
            "iteration 46554: loss: 0.22626015543937683\n",
            "iteration 46555: loss: 0.22626009583473206\n",
            "iteration 46556: loss: 0.22626006603240967\n",
            "iteration 46557: loss: 0.2262600213289261\n",
            "iteration 46558: loss: 0.22626003623008728\n",
            "iteration 46559: loss: 0.2262599915266037\n",
            "iteration 46560: loss: 0.2262599915266037\n",
            "iteration 46561: loss: 0.22625994682312012\n",
            "iteration 46562: loss: 0.22625991702079773\n",
            "iteration 46563: loss: 0.22625994682312012\n",
            "iteration 46564: loss: 0.22625994682312012\n",
            "iteration 46565: loss: 0.22625985741615295\n",
            "iteration 46566: loss: 0.22625985741615295\n",
            "iteration 46567: loss: 0.22625982761383057\n",
            "iteration 46568: loss: 0.22625982761383057\n",
            "iteration 46569: loss: 0.22625979781150818\n",
            "iteration 46570: loss: 0.22625978291034698\n",
            "iteration 46571: loss: 0.22625978291034698\n",
            "iteration 46572: loss: 0.2262597531080246\n",
            "iteration 46573: loss: 0.2262597382068634\n",
            "iteration 46574: loss: 0.22625970840454102\n",
            "iteration 46575: loss: 0.22625967860221863\n",
            "iteration 46576: loss: 0.22625967860221863\n",
            "iteration 46577: loss: 0.22625963389873505\n",
            "iteration 46578: loss: 0.22625958919525146\n",
            "iteration 46579: loss: 0.22625961899757385\n",
            "iteration 46580: loss: 0.22625958919525146\n",
            "iteration 46581: loss: 0.22625955939292908\n",
            "iteration 46582: loss: 0.22625955939292908\n",
            "iteration 46583: loss: 0.2262595146894455\n",
            "iteration 46584: loss: 0.22625955939292908\n",
            "iteration 46585: loss: 0.22625954449176788\n",
            "iteration 46586: loss: 0.2262594997882843\n",
            "iteration 46587: loss: 0.22625944018363953\n",
            "iteration 46588: loss: 0.22625944018363953\n",
            "iteration 46589: loss: 0.22625938057899475\n",
            "iteration 46590: loss: 0.22625942528247833\n",
            "iteration 46591: loss: 0.22625938057899475\n",
            "iteration 46592: loss: 0.22625938057899475\n",
            "iteration 46593: loss: 0.22625935077667236\n",
            "iteration 46594: loss: 0.22625932097434998\n",
            "iteration 46595: loss: 0.2262592762708664\n",
            "iteration 46596: loss: 0.22625930607318878\n",
            "iteration 46597: loss: 0.2262592762708664\n",
            "iteration 46598: loss: 0.2262592315673828\n",
            "iteration 46599: loss: 0.22625918686389923\n",
            "iteration 46600: loss: 0.22625918686389923\n",
            "iteration 46601: loss: 0.2262592315673828\n",
            "iteration 46602: loss: 0.22625915706157684\n",
            "iteration 46603: loss: 0.22625914216041565\n",
            "iteration 46604: loss: 0.22625914216041565\n",
            "iteration 46605: loss: 0.22625911235809326\n",
            "iteration 46606: loss: 0.22625911235809326\n",
            "iteration 46607: loss: 0.22625908255577087\n",
            "iteration 46608: loss: 0.22625906765460968\n",
            "iteration 46609: loss: 0.2262589931488037\n",
            "iteration 46610: loss: 0.2262589931488037\n",
            "iteration 46611: loss: 0.2262589931488037\n",
            "iteration 46612: loss: 0.22625896334648132\n",
            "iteration 46613: loss: 0.2262589931488037\n",
            "iteration 46614: loss: 0.22625891864299774\n",
            "iteration 46615: loss: 0.22625890374183655\n",
            "iteration 46616: loss: 0.22625891864299774\n",
            "iteration 46617: loss: 0.22625887393951416\n",
            "iteration 46618: loss: 0.22625890374183655\n",
            "iteration 46619: loss: 0.22625884413719177\n",
            "iteration 46620: loss: 0.22625884413719177\n",
            "iteration 46621: loss: 0.2262587547302246\n",
            "iteration 46622: loss: 0.22625872492790222\n",
            "iteration 46623: loss: 0.226258784532547\n",
            "iteration 46624: loss: 0.22625871002674103\n",
            "iteration 46625: loss: 0.22625871002674103\n",
            "iteration 46626: loss: 0.22625871002674103\n",
            "iteration 46627: loss: 0.22625866532325745\n",
            "iteration 46628: loss: 0.22625868022441864\n",
            "iteration 46629: loss: 0.22625860571861267\n",
            "iteration 46630: loss: 0.22625866532325745\n",
            "iteration 46631: loss: 0.22625859081745148\n",
            "iteration 46632: loss: 0.2262585610151291\n",
            "iteration 46633: loss: 0.2262585610151291\n",
            "iteration 46634: loss: 0.22625860571861267\n",
            "iteration 46635: loss: 0.2262585163116455\n",
            "iteration 46636: loss: 0.2262585163116455\n",
            "iteration 46637: loss: 0.22625848650932312\n",
            "iteration 46638: loss: 0.22625842690467834\n",
            "iteration 46639: loss: 0.22625847160816193\n",
            "iteration 46640: loss: 0.22625842690467834\n",
            "iteration 46641: loss: 0.22625836730003357\n",
            "iteration 46642: loss: 0.22625839710235596\n",
            "iteration 46643: loss: 0.22625832259655\n",
            "iteration 46644: loss: 0.22625835239887238\n",
            "iteration 46645: loss: 0.22625836730003357\n",
            "iteration 46646: loss: 0.22625835239887238\n",
            "iteration 46647: loss: 0.2262583076953888\n",
            "iteration 46648: loss: 0.22625824809074402\n",
            "iteration 46649: loss: 0.22625823318958282\n",
            "iteration 46650: loss: 0.22625824809074402\n",
            "iteration 46651: loss: 0.22625823318958282\n",
            "iteration 46652: loss: 0.22625820338726044\n",
            "iteration 46653: loss: 0.22625818848609924\n",
            "iteration 46654: loss: 0.22625818848609924\n",
            "iteration 46655: loss: 0.22625812888145447\n",
            "iteration 46656: loss: 0.22625811398029327\n",
            "iteration 46657: loss: 0.2262580841779709\n",
            "iteration 46658: loss: 0.2262580841779709\n",
            "iteration 46659: loss: 0.2262580692768097\n",
            "iteration 46660: loss: 0.2262580394744873\n",
            "iteration 46661: loss: 0.2262580692768097\n",
            "iteration 46662: loss: 0.22625800967216492\n",
            "iteration 46663: loss: 0.22625796496868134\n",
            "iteration 46664: loss: 0.22625799477100372\n",
            "iteration 46665: loss: 0.22625789046287537\n",
            "iteration 46666: loss: 0.22625789046287537\n",
            "iteration 46667: loss: 0.22625789046287537\n",
            "iteration 46668: loss: 0.22625789046287537\n",
            "iteration 46669: loss: 0.22625787556171417\n",
            "iteration 46670: loss: 0.22625789046287537\n",
            "iteration 46671: loss: 0.2262578308582306\n",
            "iteration 46672: loss: 0.2262578308582306\n",
            "iteration 46673: loss: 0.2262578010559082\n",
            "iteration 46674: loss: 0.22625777125358582\n",
            "iteration 46675: loss: 0.22625775635242462\n",
            "iteration 46676: loss: 0.22625771164894104\n",
            "iteration 46677: loss: 0.22625765204429626\n",
            "iteration 46678: loss: 0.22625765204429626\n",
            "iteration 46679: loss: 0.22625768184661865\n",
            "iteration 46680: loss: 0.22625765204429626\n",
            "iteration 46681: loss: 0.22625763714313507\n",
            "iteration 46682: loss: 0.22625763714313507\n",
            "iteration 46683: loss: 0.2262575924396515\n",
            "iteration 46684: loss: 0.22625760734081268\n",
            "iteration 46685: loss: 0.2262575626373291\n",
            "iteration 46686: loss: 0.2262575328350067\n",
            "iteration 46687: loss: 0.22625747323036194\n",
            "iteration 46688: loss: 0.22625748813152313\n",
            "iteration 46689: loss: 0.22625747323036194\n",
            "iteration 46690: loss: 0.22625741362571716\n",
            "iteration 46691: loss: 0.22625741362571716\n",
            "iteration 46692: loss: 0.22625736892223358\n",
            "iteration 46693: loss: 0.22625741362571716\n",
            "iteration 46694: loss: 0.22625739872455597\n",
            "iteration 46695: loss: 0.22625736892223358\n",
            "iteration 46696: loss: 0.2262573540210724\n",
            "iteration 46697: loss: 0.2262573540210724\n",
            "iteration 46698: loss: 0.22625724971294403\n",
            "iteration 46699: loss: 0.22625724971294403\n",
            "iteration 46700: loss: 0.22625724971294403\n",
            "iteration 46701: loss: 0.22625724971294403\n",
            "iteration 46702: loss: 0.22625723481178284\n",
            "iteration 46703: loss: 0.22625720500946045\n",
            "iteration 46704: loss: 0.22625720500946045\n",
            "iteration 46705: loss: 0.22625713050365448\n",
            "iteration 46706: loss: 0.2262571156024933\n",
            "iteration 46707: loss: 0.22625713050365448\n",
            "iteration 46708: loss: 0.2262570858001709\n",
            "iteration 46709: loss: 0.2262571156024933\n",
            "iteration 46710: loss: 0.2262570858001709\n",
            "iteration 46711: loss: 0.2262570559978485\n",
            "iteration 46712: loss: 0.22625704109668732\n",
            "iteration 46713: loss: 0.2262570559978485\n",
            "iteration 46714: loss: 0.22625693678855896\n",
            "iteration 46715: loss: 0.22625696659088135\n",
            "iteration 46716: loss: 0.22625699639320374\n",
            "iteration 46717: loss: 0.22625689208507538\n",
            "iteration 46718: loss: 0.22625692188739777\n",
            "iteration 46719: loss: 0.22625689208507538\n",
            "iteration 46720: loss: 0.2262568473815918\n",
            "iteration 46721: loss: 0.2262568473815918\n",
            "iteration 46722: loss: 0.2262568175792694\n",
            "iteration 46723: loss: 0.2262568175792694\n",
            "iteration 46724: loss: 0.22625677287578583\n",
            "iteration 46725: loss: 0.22625677287578583\n",
            "iteration 46726: loss: 0.22625677287578583\n",
            "iteration 46727: loss: 0.22625675797462463\n",
            "iteration 46728: loss: 0.22625669836997986\n",
            "iteration 46729: loss: 0.22625668346881866\n",
            "iteration 46730: loss: 0.22625668346881866\n",
            "iteration 46731: loss: 0.22625665366649628\n",
            "iteration 46732: loss: 0.2262566089630127\n",
            "iteration 46733: loss: 0.22625663876533508\n",
            "iteration 46734: loss: 0.2262566089630127\n",
            "iteration 46735: loss: 0.2262565791606903\n",
            "iteration 46736: loss: 0.2262565642595291\n",
            "iteration 46737: loss: 0.22625653445720673\n",
            "iteration 46738: loss: 0.22625653445720673\n",
            "iteration 46739: loss: 0.22625648975372314\n",
            "iteration 46740: loss: 0.22625651955604553\n",
            "iteration 46741: loss: 0.22625648975372314\n",
            "iteration 46742: loss: 0.22625645995140076\n",
            "iteration 46743: loss: 0.22625640034675598\n",
            "iteration 46744: loss: 0.22625644505023956\n",
            "iteration 46745: loss: 0.22625641524791718\n",
            "iteration 46746: loss: 0.22625641524791718\n",
            "iteration 46747: loss: 0.2262563407421112\n",
            "iteration 46748: loss: 0.22625632584095\n",
            "iteration 46749: loss: 0.2262563407421112\n",
            "iteration 46750: loss: 0.22625632584095\n",
            "iteration 46751: loss: 0.22625628113746643\n",
            "iteration 46752: loss: 0.22625622153282166\n",
            "iteration 46753: loss: 0.22625625133514404\n",
            "iteration 46754: loss: 0.22625620663166046\n",
            "iteration 46755: loss: 0.22625616192817688\n",
            "iteration 46756: loss: 0.22625617682933807\n",
            "iteration 46757: loss: 0.2262561321258545\n",
            "iteration 46758: loss: 0.22625616192817688\n",
            "iteration 46759: loss: 0.2262561321258545\n",
            "iteration 46760: loss: 0.2262560874223709\n",
            "iteration 46761: loss: 0.2262561023235321\n",
            "iteration 46762: loss: 0.2262560874223709\n",
            "iteration 46763: loss: 0.22625601291656494\n",
            "iteration 46764: loss: 0.22625605762004852\n",
            "iteration 46765: loss: 0.22625601291656494\n",
            "iteration 46766: loss: 0.22625601291656494\n",
            "iteration 46767: loss: 0.22625593841075897\n",
            "iteration 46768: loss: 0.22625593841075897\n",
            "iteration 46769: loss: 0.22625592350959778\n",
            "iteration 46770: loss: 0.2262558937072754\n",
            "iteration 46771: loss: 0.2262558490037918\n",
            "iteration 46772: loss: 0.226255863904953\n",
            "iteration 46773: loss: 0.22625580430030823\n",
            "iteration 46774: loss: 0.22625580430030823\n",
            "iteration 46775: loss: 0.22625581920146942\n",
            "iteration 46776: loss: 0.22625580430030823\n",
            "iteration 46777: loss: 0.22625577449798584\n",
            "iteration 46778: loss: 0.22625577449798584\n",
            "iteration 46779: loss: 0.22625572979450226\n",
            "iteration 46780: loss: 0.22625572979450226\n",
            "iteration 46781: loss: 0.22625568509101868\n",
            "iteration 46782: loss: 0.22625568509101868\n",
            "iteration 46783: loss: 0.22625569999217987\n",
            "iteration 46784: loss: 0.2262556254863739\n",
            "iteration 46785: loss: 0.2262556105852127\n",
            "iteration 46786: loss: 0.22625558078289032\n",
            "iteration 46787: loss: 0.2262556105852127\n",
            "iteration 46788: loss: 0.22625553607940674\n",
            "iteration 46789: loss: 0.22625553607940674\n",
            "iteration 46790: loss: 0.22625556588172913\n",
            "iteration 46791: loss: 0.22625553607940674\n",
            "iteration 46792: loss: 0.22625546157360077\n",
            "iteration 46793: loss: 0.22625544667243958\n",
            "iteration 46794: loss: 0.22625544667243958\n",
            "iteration 46795: loss: 0.22625544667243958\n",
            "iteration 46796: loss: 0.2262554168701172\n",
            "iteration 46797: loss: 0.2262553721666336\n",
            "iteration 46798: loss: 0.2262553721666336\n",
            "iteration 46799: loss: 0.2262554168701172\n",
            "iteration 46800: loss: 0.22625532746315002\n",
            "iteration 46801: loss: 0.22625529766082764\n",
            "iteration 46802: loss: 0.22625529766082764\n",
            "iteration 46803: loss: 0.22625522315502167\n",
            "iteration 46804: loss: 0.22625522315502167\n",
            "iteration 46805: loss: 0.22625522315502167\n",
            "iteration 46806: loss: 0.22625520825386047\n",
            "iteration 46807: loss: 0.22625520825386047\n",
            "iteration 46808: loss: 0.2262551486492157\n",
            "iteration 46809: loss: 0.22625517845153809\n",
            "iteration 46810: loss: 0.2262551337480545\n",
            "iteration 46811: loss: 0.2262551337480545\n",
            "iteration 46812: loss: 0.22625508904457092\n",
            "iteration 46813: loss: 0.22625505924224854\n",
            "iteration 46814: loss: 0.22625505924224854\n",
            "iteration 46815: loss: 0.22625510394573212\n",
            "iteration 46816: loss: 0.22625502943992615\n",
            "iteration 46817: loss: 0.22625498473644257\n",
            "iteration 46818: loss: 0.22625498473644257\n",
            "iteration 46819: loss: 0.2262549102306366\n",
            "iteration 46820: loss: 0.22625498473644257\n",
            "iteration 46821: loss: 0.2262548953294754\n",
            "iteration 46822: loss: 0.2262549102306366\n",
            "iteration 46823: loss: 0.22625486552715302\n",
            "iteration 46824: loss: 0.22625486552715302\n",
            "iteration 46825: loss: 0.22625485062599182\n",
            "iteration 46826: loss: 0.22625485062599182\n",
            "iteration 46827: loss: 0.22625479102134705\n",
            "iteration 46828: loss: 0.22625474631786346\n",
            "iteration 46829: loss: 0.22625477612018585\n",
            "iteration 46830: loss: 0.22625474631786346\n",
            "iteration 46831: loss: 0.22625474631786346\n",
            "iteration 46832: loss: 0.22625477612018585\n",
            "iteration 46833: loss: 0.2262546718120575\n",
            "iteration 46834: loss: 0.2262546569108963\n",
            "iteration 46835: loss: 0.22625461220741272\n",
            "iteration 46836: loss: 0.22625461220741272\n",
            "iteration 46837: loss: 0.22625461220741272\n",
            "iteration 46838: loss: 0.22625455260276794\n",
            "iteration 46839: loss: 0.22625453770160675\n",
            "iteration 46840: loss: 0.22625453770160675\n",
            "iteration 46841: loss: 0.22625455260276794\n",
            "iteration 46842: loss: 0.22625453770160675\n",
            "iteration 46843: loss: 0.22625446319580078\n",
            "iteration 46844: loss: 0.22625449299812317\n",
            "iteration 46845: loss: 0.22625449299812317\n",
            "iteration 46846: loss: 0.2262544333934784\n",
            "iteration 46847: loss: 0.2262543886899948\n",
            "iteration 46848: loss: 0.2262543886899948\n",
            "iteration 46849: loss: 0.2262543886899948\n",
            "iteration 46850: loss: 0.22625431418418884\n",
            "iteration 46851: loss: 0.22625431418418884\n",
            "iteration 46852: loss: 0.22625434398651123\n",
            "iteration 46853: loss: 0.22625434398651123\n",
            "iteration 46854: loss: 0.22625426948070526\n",
            "iteration 46855: loss: 0.22625426948070526\n",
            "iteration 46856: loss: 0.22625426948070526\n",
            "iteration 46857: loss: 0.22625422477722168\n",
            "iteration 46858: loss: 0.22625422477722168\n",
            "iteration 46859: loss: 0.2262541949748993\n",
            "iteration 46860: loss: 0.2262541502714157\n",
            "iteration 46861: loss: 0.2262541800737381\n",
            "iteration 46862: loss: 0.22625413537025452\n",
            "iteration 46863: loss: 0.22625413537025452\n",
            "iteration 46864: loss: 0.2262541502714157\n",
            "iteration 46865: loss: 0.22625403106212616\n",
            "iteration 46866: loss: 0.22625407576560974\n",
            "iteration 46867: loss: 0.22625401616096497\n",
            "iteration 46868: loss: 0.22625398635864258\n",
            "iteration 46869: loss: 0.2262539565563202\n",
            "iteration 46870: loss: 0.226253941655159\n",
            "iteration 46871: loss: 0.22625398635864258\n",
            "iteration 46872: loss: 0.22625398635864258\n",
            "iteration 46873: loss: 0.2262539118528366\n",
            "iteration 46874: loss: 0.22625389695167542\n",
            "iteration 46875: loss: 0.22625389695167542\n",
            "iteration 46876: loss: 0.22625383734703064\n",
            "iteration 46877: loss: 0.22625383734703064\n",
            "iteration 46878: loss: 0.22625383734703064\n",
            "iteration 46879: loss: 0.22625379264354706\n",
            "iteration 46880: loss: 0.22625379264354706\n",
            "iteration 46881: loss: 0.2262537181377411\n",
            "iteration 46882: loss: 0.22625374794006348\n",
            "iteration 46883: loss: 0.2262537181377411\n",
            "iteration 46884: loss: 0.2262537032365799\n",
            "iteration 46885: loss: 0.2262537181377411\n",
            "iteration 46886: loss: 0.2262536585330963\n",
            "iteration 46887: loss: 0.22625362873077393\n",
            "iteration 46888: loss: 0.22625362873077393\n",
            "iteration 46889: loss: 0.22625362873077393\n",
            "iteration 46890: loss: 0.22625358402729034\n",
            "iteration 46891: loss: 0.22625355422496796\n",
            "iteration 46892: loss: 0.22625350952148438\n",
            "iteration 46893: loss: 0.22625350952148438\n",
            "iteration 46894: loss: 0.226253479719162\n",
            "iteration 46895: loss: 0.22625353932380676\n",
            "iteration 46896: loss: 0.2262534648180008\n",
            "iteration 46897: loss: 0.2262534648180008\n",
            "iteration 46898: loss: 0.2262534350156784\n",
            "iteration 46899: loss: 0.22625339031219482\n",
            "iteration 46900: loss: 0.2262534201145172\n",
            "iteration 46901: loss: 0.2262534201145172\n",
            "iteration 46902: loss: 0.22625336050987244\n",
            "iteration 46903: loss: 0.22625334560871124\n",
            "iteration 46904: loss: 0.22625331580638885\n",
            "iteration 46905: loss: 0.22625330090522766\n",
            "iteration 46906: loss: 0.22625330090522766\n",
            "iteration 46907: loss: 0.22625331580638885\n",
            "iteration 46908: loss: 0.2262532263994217\n",
            "iteration 46909: loss: 0.22625324130058289\n",
            "iteration 46910: loss: 0.2262532263994217\n",
            "iteration 46911: loss: 0.2262531816959381\n",
            "iteration 46912: loss: 0.2262531816959381\n",
            "iteration 46913: loss: 0.22625315189361572\n",
            "iteration 46914: loss: 0.22625315189361572\n",
            "iteration 46915: loss: 0.22625310719013214\n",
            "iteration 46916: loss: 0.22625310719013214\n",
            "iteration 46917: loss: 0.22625303268432617\n",
            "iteration 46918: loss: 0.22625303268432617\n",
            "iteration 46919: loss: 0.22625300288200378\n",
            "iteration 46920: loss: 0.22625300288200378\n",
            "iteration 46921: loss: 0.2262529879808426\n",
            "iteration 46922: loss: 0.2262529879808426\n",
            "iteration 46923: loss: 0.2262529581785202\n",
            "iteration 46924: loss: 0.226252943277359\n",
            "iteration 46925: loss: 0.2262529879808426\n",
            "iteration 46926: loss: 0.22625288367271423\n",
            "iteration 46927: loss: 0.22625288367271423\n",
            "iteration 46928: loss: 0.22625286877155304\n",
            "iteration 46929: loss: 0.22625282406806946\n",
            "iteration 46930: loss: 0.22625279426574707\n",
            "iteration 46931: loss: 0.22625283896923065\n",
            "iteration 46932: loss: 0.22625282406806946\n",
            "iteration 46933: loss: 0.2262527495622635\n",
            "iteration 46934: loss: 0.22625276446342468\n",
            "iteration 46935: loss: 0.22625276446342468\n",
            "iteration 46936: loss: 0.2262527048587799\n",
            "iteration 46937: loss: 0.2262527048587799\n",
            "iteration 46938: loss: 0.2262527048587799\n",
            "iteration 46939: loss: 0.22625267505645752\n",
            "iteration 46940: loss: 0.22625263035297394\n",
            "iteration 46941: loss: 0.22625263035297394\n",
            "iteration 46942: loss: 0.22625263035297394\n",
            "iteration 46943: loss: 0.22625258564949036\n",
            "iteration 46944: loss: 0.22625252604484558\n",
            "iteration 46945: loss: 0.22625252604484558\n",
            "iteration 46946: loss: 0.2262525111436844\n",
            "iteration 46947: loss: 0.22625252604484558\n",
            "iteration 46948: loss: 0.226252481341362\n",
            "iteration 46949: loss: 0.22625243663787842\n",
            "iteration 46950: loss: 0.22625240683555603\n",
            "iteration 46951: loss: 0.22625240683555603\n",
            "iteration 46952: loss: 0.22625243663787842\n",
            "iteration 46953: loss: 0.22625239193439484\n",
            "iteration 46954: loss: 0.22625236213207245\n",
            "iteration 46955: loss: 0.22625239193439484\n",
            "iteration 46956: loss: 0.22625228762626648\n",
            "iteration 46957: loss: 0.22625228762626648\n",
            "iteration 46958: loss: 0.22625227272510529\n",
            "iteration 46959: loss: 0.22625227272510529\n",
            "iteration 46960: loss: 0.2262522280216217\n",
            "iteration 46961: loss: 0.2262522429227829\n",
            "iteration 46962: loss: 0.22625219821929932\n",
            "iteration 46963: loss: 0.22625219821929932\n",
            "iteration 46964: loss: 0.22625219821929932\n",
            "iteration 46965: loss: 0.22625219821929932\n",
            "iteration 46966: loss: 0.22625215351581573\n",
            "iteration 46967: loss: 0.22625212371349335\n",
            "iteration 46968: loss: 0.22625204920768738\n",
            "iteration 46969: loss: 0.22625215351581573\n",
            "iteration 46970: loss: 0.22625207901000977\n",
            "iteration 46971: loss: 0.2262520045042038\n",
            "iteration 46972: loss: 0.2262520045042038\n",
            "iteration 46973: loss: 0.2262520045042038\n",
            "iteration 46974: loss: 0.22625195980072021\n",
            "iteration 46975: loss: 0.22625195980072021\n",
            "iteration 46976: loss: 0.22625192999839783\n",
            "iteration 46977: loss: 0.22625195980072021\n",
            "iteration 46978: loss: 0.22625191509723663\n",
            "iteration 46979: loss: 0.22625184059143066\n",
            "iteration 46980: loss: 0.22625181078910828\n",
            "iteration 46981: loss: 0.22625184059143066\n",
            "iteration 46982: loss: 0.22625187039375305\n",
            "iteration 46983: loss: 0.2262517660856247\n",
            "iteration 46984: loss: 0.22625181078910828\n",
            "iteration 46985: loss: 0.2262517660856247\n",
            "iteration 46986: loss: 0.2262517213821411\n",
            "iteration 46987: loss: 0.22625167667865753\n",
            "iteration 46988: loss: 0.22625167667865753\n",
            "iteration 46989: loss: 0.2262517213821411\n",
            "iteration 46990: loss: 0.22625167667865753\n",
            "iteration 46991: loss: 0.22625167667865753\n",
            "iteration 46992: loss: 0.22625164687633514\n",
            "iteration 46993: loss: 0.22625160217285156\n",
            "iteration 46994: loss: 0.22625163197517395\n",
            "iteration 46995: loss: 0.22625155746936798\n",
            "iteration 46996: loss: 0.2262515276670456\n",
            "iteration 46997: loss: 0.2262515127658844\n",
            "iteration 46998: loss: 0.2262515127658844\n",
            "iteration 46999: loss: 0.2262515127658844\n",
            "iteration 47000: loss: 0.2262515276670456\n",
            "iteration 47001: loss: 0.226251482963562\n",
            "iteration 47002: loss: 0.22625140845775604\n",
            "iteration 47003: loss: 0.22625140845775604\n",
            "iteration 47004: loss: 0.22625145316123962\n",
            "iteration 47005: loss: 0.22625140845775604\n",
            "iteration 47006: loss: 0.22625140845775604\n",
            "iteration 47007: loss: 0.22625139355659485\n",
            "iteration 47008: loss: 0.22625131905078888\n",
            "iteration 47009: loss: 0.2262512743473053\n",
            "iteration 47010: loss: 0.2262512743473053\n",
            "iteration 47011: loss: 0.2262512445449829\n",
            "iteration 47012: loss: 0.2262512743473053\n",
            "iteration 47013: loss: 0.22625121474266052\n",
            "iteration 47014: loss: 0.2262512445449829\n",
            "iteration 47015: loss: 0.22625119984149933\n",
            "iteration 47016: loss: 0.22625117003917694\n",
            "iteration 47017: loss: 0.22625115513801575\n",
            "iteration 47018: loss: 0.22625109553337097\n",
            "iteration 47019: loss: 0.22625112533569336\n",
            "iteration 47020: loss: 0.22625112533569336\n",
            "iteration 47021: loss: 0.22625108063220978\n",
            "iteration 47022: loss: 0.22625115513801575\n",
            "iteration 47023: loss: 0.2262510508298874\n",
            "iteration 47024: loss: 0.22625097632408142\n",
            "iteration 47025: loss: 0.2262510359287262\n",
            "iteration 47026: loss: 0.22625097632408142\n",
            "iteration 47027: loss: 0.2262510061264038\n",
            "iteration 47028: loss: 0.22625096142292023\n",
            "iteration 47029: loss: 0.22625091671943665\n",
            "iteration 47030: loss: 0.22625093162059784\n",
            "iteration 47031: loss: 0.22625088691711426\n",
            "iteration 47032: loss: 0.22625084221363068\n",
            "iteration 47033: loss: 0.2262508124113083\n",
            "iteration 47034: loss: 0.22625084221363068\n",
            "iteration 47035: loss: 0.2262508124113083\n",
            "iteration 47036: loss: 0.2262507975101471\n",
            "iteration 47037: loss: 0.2262507975101471\n",
            "iteration 47038: loss: 0.2262507677078247\n",
            "iteration 47039: loss: 0.2262507677078247\n",
            "iteration 47040: loss: 0.22625072300434113\n",
            "iteration 47041: loss: 0.22625069320201874\n",
            "iteration 47042: loss: 0.22625069320201874\n",
            "iteration 47043: loss: 0.22625060379505157\n",
            "iteration 47044: loss: 0.22625061869621277\n",
            "iteration 47045: loss: 0.22625064849853516\n",
            "iteration 47046: loss: 0.22625060379505157\n",
            "iteration 47047: loss: 0.226250559091568\n",
            "iteration 47048: loss: 0.226250559091568\n",
            "iteration 47049: loss: 0.2262505292892456\n",
            "iteration 47050: loss: 0.22625049948692322\n",
            "iteration 47051: loss: 0.22625049948692322\n",
            "iteration 47052: loss: 0.22625049948692322\n",
            "iteration 47053: loss: 0.22625048458576202\n",
            "iteration 47054: loss: 0.22625045478343964\n",
            "iteration 47055: loss: 0.22625041007995605\n",
            "iteration 47056: loss: 0.22625038027763367\n",
            "iteration 47057: loss: 0.22625041007995605\n",
            "iteration 47058: loss: 0.22625036537647247\n",
            "iteration 47059: loss: 0.22625036537647247\n",
            "iteration 47060: loss: 0.22625036537647247\n",
            "iteration 47061: loss: 0.22625033557415009\n",
            "iteration 47062: loss: 0.2262502908706665\n",
            "iteration 47063: loss: 0.22625026106834412\n",
            "iteration 47064: loss: 0.22625026106834412\n",
            "iteration 47065: loss: 0.22625024616718292\n",
            "iteration 47066: loss: 0.22625021636486053\n",
            "iteration 47067: loss: 0.22625017166137695\n",
            "iteration 47068: loss: 0.22625017166137695\n",
            "iteration 47069: loss: 0.22625017166137695\n",
            "iteration 47070: loss: 0.22625014185905457\n",
            "iteration 47071: loss: 0.22625009715557098\n",
            "iteration 47072: loss: 0.22625009715557098\n",
            "iteration 47073: loss: 0.22625009715557098\n",
            "iteration 47074: loss: 0.22625009715557098\n",
            "iteration 47075: loss: 0.22625002264976501\n",
            "iteration 47076: loss: 0.22625002264976501\n",
            "iteration 47077: loss: 0.2262500822544098\n",
            "iteration 47078: loss: 0.22624997794628143\n",
            "iteration 47079: loss: 0.22624997794628143\n",
            "iteration 47080: loss: 0.22624990344047546\n",
            "iteration 47081: loss: 0.22624993324279785\n",
            "iteration 47082: loss: 0.22624996304512024\n",
            "iteration 47083: loss: 0.22624988853931427\n",
            "iteration 47084: loss: 0.22624988853931427\n",
            "iteration 47085: loss: 0.22624990344047546\n",
            "iteration 47086: loss: 0.2262498438358307\n",
            "iteration 47087: loss: 0.2262498438358307\n",
            "iteration 47088: loss: 0.2262497842311859\n",
            "iteration 47089: loss: 0.2262497842311859\n",
            "iteration 47090: loss: 0.22624976933002472\n",
            "iteration 47091: loss: 0.22624973952770233\n",
            "iteration 47092: loss: 0.22624973952770233\n",
            "iteration 47093: loss: 0.22624969482421875\n",
            "iteration 47094: loss: 0.22624969482421875\n",
            "iteration 47095: loss: 0.22624962031841278\n",
            "iteration 47096: loss: 0.22624962031841278\n",
            "iteration 47097: loss: 0.2262496054172516\n",
            "iteration 47098: loss: 0.2262496054172516\n",
            "iteration 47099: loss: 0.2262496054172516\n",
            "iteration 47100: loss: 0.2262495756149292\n",
            "iteration 47101: loss: 0.22624950110912323\n",
            "iteration 47102: loss: 0.22624953091144562\n",
            "iteration 47103: loss: 0.22624953091144562\n",
            "iteration 47104: loss: 0.22624950110912323\n",
            "iteration 47105: loss: 0.22624945640563965\n",
            "iteration 47106: loss: 0.22624945640563965\n",
            "iteration 47107: loss: 0.22624941170215607\n",
            "iteration 47108: loss: 0.22624942660331726\n",
            "iteration 47109: loss: 0.22624942660331726\n",
            "iteration 47110: loss: 0.22624941170215607\n",
            "iteration 47111: loss: 0.22624936699867249\n",
            "iteration 47112: loss: 0.2262493371963501\n",
            "iteration 47113: loss: 0.2262493073940277\n",
            "iteration 47114: loss: 0.2262493073940277\n",
            "iteration 47115: loss: 0.2262493371963501\n",
            "iteration 47116: loss: 0.22624929249286652\n",
            "iteration 47117: loss: 0.22624921798706055\n",
            "iteration 47118: loss: 0.22624918818473816\n",
            "iteration 47119: loss: 0.22624918818473816\n",
            "iteration 47120: loss: 0.22624918818473816\n",
            "iteration 47121: loss: 0.22624917328357697\n",
            "iteration 47122: loss: 0.22624914348125458\n",
            "iteration 47123: loss: 0.22624914348125458\n",
            "iteration 47124: loss: 0.22624912858009338\n",
            "iteration 47125: loss: 0.22624912858009338\n",
            "iteration 47126: loss: 0.22624905407428741\n",
            "iteration 47127: loss: 0.2262490689754486\n",
            "iteration 47128: loss: 0.22624900937080383\n",
            "iteration 47129: loss: 0.22624902427196503\n",
            "iteration 47130: loss: 0.22624902427196503\n",
            "iteration 47131: loss: 0.22624902427196503\n",
            "iteration 47132: loss: 0.22624894976615906\n",
            "iteration 47133: loss: 0.22624897956848145\n",
            "iteration 47134: loss: 0.22624890506267548\n",
            "iteration 47135: loss: 0.22624890506267548\n",
            "iteration 47136: loss: 0.22624889016151428\n",
            "iteration 47137: loss: 0.2262488603591919\n",
            "iteration 47138: loss: 0.2262488156557083\n",
            "iteration 47139: loss: 0.2262488305568695\n",
            "iteration 47140: loss: 0.2262488156557083\n",
            "iteration 47141: loss: 0.2262488156557083\n",
            "iteration 47142: loss: 0.22624877095222473\n",
            "iteration 47143: loss: 0.22624877095222473\n",
            "iteration 47144: loss: 0.22624871134757996\n",
            "iteration 47145: loss: 0.22624874114990234\n",
            "iteration 47146: loss: 0.22624871134757996\n",
            "iteration 47147: loss: 0.22624869644641876\n",
            "iteration 47148: loss: 0.22624869644641876\n",
            "iteration 47149: loss: 0.22624865174293518\n",
            "iteration 47150: loss: 0.2262485921382904\n",
            "iteration 47151: loss: 0.2262485921382904\n",
            "iteration 47152: loss: 0.2262485772371292\n",
            "iteration 47153: loss: 0.2262485772371292\n",
            "iteration 47154: loss: 0.22624853253364563\n",
            "iteration 47155: loss: 0.22624850273132324\n",
            "iteration 47156: loss: 0.22624850273132324\n",
            "iteration 47157: loss: 0.22624853253364563\n",
            "iteration 47158: loss: 0.22624842822551727\n",
            "iteration 47159: loss: 0.22624845802783966\n",
            "iteration 47160: loss: 0.22624845802783966\n",
            "iteration 47161: loss: 0.22624841332435608\n",
            "iteration 47162: loss: 0.22624842822551727\n",
            "iteration 47163: loss: 0.2262483537197113\n",
            "iteration 47164: loss: 0.2262483537197113\n",
            "iteration 47165: loss: 0.2262483537197113\n",
            "iteration 47166: loss: 0.22624830901622772\n",
            "iteration 47167: loss: 0.22624829411506653\n",
            "iteration 47168: loss: 0.22624830901622772\n",
            "iteration 47169: loss: 0.22624826431274414\n",
            "iteration 47170: loss: 0.22624823451042175\n",
            "iteration 47171: loss: 0.22624818980693817\n",
            "iteration 47172: loss: 0.22624818980693817\n",
            "iteration 47173: loss: 0.22624823451042175\n",
            "iteration 47174: loss: 0.22624817490577698\n",
            "iteration 47175: loss: 0.22624817490577698\n",
            "iteration 47176: loss: 0.2262481153011322\n",
            "iteration 47177: loss: 0.226248100399971\n",
            "iteration 47178: loss: 0.22624807059764862\n",
            "iteration 47179: loss: 0.22624807059764862\n",
            "iteration 47180: loss: 0.22624807059764862\n",
            "iteration 47181: loss: 0.22624802589416504\n",
            "iteration 47182: loss: 0.22624799609184265\n",
            "iteration 47183: loss: 0.22624799609184265\n",
            "iteration 47184: loss: 0.22624798119068146\n",
            "iteration 47185: loss: 0.22624793648719788\n",
            "iteration 47186: loss: 0.22624795138835907\n",
            "iteration 47187: loss: 0.22624793648719788\n",
            "iteration 47188: loss: 0.2262478768825531\n",
            "iteration 47189: loss: 0.22624793648719788\n",
            "iteration 47190: loss: 0.2262479066848755\n",
            "iteration 47191: loss: 0.2262478768825531\n",
            "iteration 47192: loss: 0.22624783217906952\n",
            "iteration 47193: loss: 0.22624781727790833\n",
            "iteration 47194: loss: 0.22624775767326355\n",
            "iteration 47195: loss: 0.22624778747558594\n",
            "iteration 47196: loss: 0.22624774277210236\n",
            "iteration 47197: loss: 0.22624774277210236\n",
            "iteration 47198: loss: 0.22624771296977997\n",
            "iteration 47199: loss: 0.226247638463974\n",
            "iteration 47200: loss: 0.226247638463974\n",
            "iteration 47201: loss: 0.226247638463974\n",
            "iteration 47202: loss: 0.226247638463974\n",
            "iteration 47203: loss: 0.226247638463974\n",
            "iteration 47204: loss: 0.22624757885932922\n",
            "iteration 47205: loss: 0.22624759376049042\n",
            "iteration 47206: loss: 0.22624754905700684\n",
            "iteration 47207: loss: 0.22624751925468445\n",
            "iteration 47208: loss: 0.22624750435352325\n",
            "iteration 47209: loss: 0.22624745965003967\n",
            "iteration 47210: loss: 0.22624745965003967\n",
            "iteration 47211: loss: 0.22624747455120087\n",
            "iteration 47212: loss: 0.22624745965003967\n",
            "iteration 47213: loss: 0.22624745965003967\n",
            "iteration 47214: loss: 0.22624745965003967\n",
            "iteration 47215: loss: 0.2262473851442337\n",
            "iteration 47216: loss: 0.22624735534191132\n",
            "iteration 47217: loss: 0.22624735534191132\n",
            "iteration 47218: loss: 0.22624734044075012\n",
            "iteration 47219: loss: 0.22624731063842773\n",
            "iteration 47220: loss: 0.22624731063842773\n",
            "iteration 47221: loss: 0.22624728083610535\n",
            "iteration 47222: loss: 0.22624726593494415\n",
            "iteration 47223: loss: 0.22624723613262177\n",
            "iteration 47224: loss: 0.22624722123146057\n",
            "iteration 47225: loss: 0.22624719142913818\n",
            "iteration 47226: loss: 0.22624719142913818\n",
            "iteration 47227: loss: 0.2262471467256546\n",
            "iteration 47228: loss: 0.2262471616268158\n",
            "iteration 47229: loss: 0.2262471467256546\n",
            "iteration 47230: loss: 0.22624710202217102\n",
            "iteration 47231: loss: 0.22624711692333221\n",
            "iteration 47232: loss: 0.2262471467256546\n",
            "iteration 47233: loss: 0.22624704241752625\n",
            "iteration 47234: loss: 0.22624699771404266\n",
            "iteration 47235: loss: 0.22624699771404266\n",
            "iteration 47236: loss: 0.22624695301055908\n",
            "iteration 47237: loss: 0.22624699771404266\n",
            "iteration 47238: loss: 0.2262469232082367\n",
            "iteration 47239: loss: 0.2262469232082367\n",
            "iteration 47240: loss: 0.2262469232082367\n",
            "iteration 47241: loss: 0.2262469083070755\n",
            "iteration 47242: loss: 0.2262469083070755\n",
            "iteration 47243: loss: 0.2262469083070755\n",
            "iteration 47244: loss: 0.22624683380126953\n",
            "iteration 47245: loss: 0.22624678909778595\n",
            "iteration 47246: loss: 0.22624680399894714\n",
            "iteration 47247: loss: 0.22624674439430237\n",
            "iteration 47248: loss: 0.22624675929546356\n",
            "iteration 47249: loss: 0.22624675929546356\n",
            "iteration 47250: loss: 0.22624674439430237\n",
            "iteration 47251: loss: 0.22624671459197998\n",
            "iteration 47252: loss: 0.2262466847896576\n",
            "iteration 47253: loss: 0.226246640086174\n",
            "iteration 47254: loss: 0.2262466698884964\n",
            "iteration 47255: loss: 0.22624662518501282\n",
            "iteration 47256: loss: 0.22624662518501282\n",
            "iteration 47257: loss: 0.22624659538269043\n",
            "iteration 47258: loss: 0.22624659538269043\n",
            "iteration 47259: loss: 0.22624659538269043\n",
            "iteration 47260: loss: 0.22624655067920685\n",
            "iteration 47261: loss: 0.22624650597572327\n",
            "iteration 47262: loss: 0.22624647617340088\n",
            "iteration 47263: loss: 0.22624647617340088\n",
            "iteration 47264: loss: 0.2262464463710785\n",
            "iteration 47265: loss: 0.2262464463710785\n",
            "iteration 47266: loss: 0.2262464314699173\n",
            "iteration 47267: loss: 0.2262464016675949\n",
            "iteration 47268: loss: 0.22624638676643372\n",
            "iteration 47269: loss: 0.22624631226062775\n",
            "iteration 47270: loss: 0.22624635696411133\n",
            "iteration 47271: loss: 0.22624635696411133\n",
            "iteration 47272: loss: 0.22624632716178894\n",
            "iteration 47273: loss: 0.22624628245830536\n",
            "iteration 47274: loss: 0.22624631226062775\n",
            "iteration 47275: loss: 0.22624623775482178\n",
            "iteration 47276: loss: 0.2262462079524994\n",
            "iteration 47277: loss: 0.2262462079524994\n",
            "iteration 47278: loss: 0.2262462079524994\n",
            "iteration 47279: loss: 0.22624614834785461\n",
            "iteration 47280: loss: 0.22624614834785461\n",
            "iteration 47281: loss: 0.22624614834785461\n",
            "iteration 47282: loss: 0.22624611854553223\n",
            "iteration 47283: loss: 0.22624611854553223\n",
            "iteration 47284: loss: 0.22624607384204865\n",
            "iteration 47285: loss: 0.22624607384204865\n",
            "iteration 47286: loss: 0.22624602913856506\n",
            "iteration 47287: loss: 0.2262459695339203\n",
            "iteration 47288: loss: 0.22624602913856506\n",
            "iteration 47289: loss: 0.2262459695339203\n",
            "iteration 47290: loss: 0.2262459546327591\n",
            "iteration 47291: loss: 0.2262459695339203\n",
            "iteration 47292: loss: 0.2262459248304367\n",
            "iteration 47293: loss: 0.2262459248304367\n",
            "iteration 47294: loss: 0.22624588012695312\n",
            "iteration 47295: loss: 0.22624585032463074\n",
            "iteration 47296: loss: 0.22624588012695312\n",
            "iteration 47297: loss: 0.22624585032463074\n",
            "iteration 47298: loss: 0.22624580562114716\n",
            "iteration 47299: loss: 0.22624576091766357\n",
            "iteration 47300: loss: 0.2262457311153412\n",
            "iteration 47301: loss: 0.22624576091766357\n",
            "iteration 47302: loss: 0.22624576091766357\n",
            "iteration 47303: loss: 0.2262457311153412\n",
            "iteration 47304: loss: 0.2262456864118576\n",
            "iteration 47305: loss: 0.2262456864118576\n",
            "iteration 47306: loss: 0.2262456715106964\n",
            "iteration 47307: loss: 0.22624564170837402\n",
            "iteration 47308: loss: 0.22624561190605164\n",
            "iteration 47309: loss: 0.22624559700489044\n",
            "iteration 47310: loss: 0.22624559700489044\n",
            "iteration 47311: loss: 0.22624556720256805\n",
            "iteration 47312: loss: 0.22624555230140686\n",
            "iteration 47313: loss: 0.22624552249908447\n",
            "iteration 47314: loss: 0.22624549269676208\n",
            "iteration 47315: loss: 0.22624552249908447\n",
            "iteration 47316: loss: 0.22624549269676208\n",
            "iteration 47317: loss: 0.2262454479932785\n",
            "iteration 47318: loss: 0.2262454777956009\n",
            "iteration 47319: loss: 0.2262454330921173\n",
            "iteration 47320: loss: 0.22624540328979492\n",
            "iteration 47321: loss: 0.22624537348747253\n",
            "iteration 47322: loss: 0.22624540328979492\n",
            "iteration 47323: loss: 0.22624535858631134\n",
            "iteration 47324: loss: 0.22624532878398895\n",
            "iteration 47325: loss: 0.22624531388282776\n",
            "iteration 47326: loss: 0.22624531388282776\n",
            "iteration 47327: loss: 0.22624528408050537\n",
            "iteration 47328: loss: 0.22624525427818298\n",
            "iteration 47329: loss: 0.2262452393770218\n",
            "iteration 47330: loss: 0.2262452393770218\n",
            "iteration 47331: loss: 0.2262451946735382\n",
            "iteration 47332: loss: 0.2262452095746994\n",
            "iteration 47333: loss: 0.22624516487121582\n",
            "iteration 47334: loss: 0.22624516487121582\n",
            "iteration 47335: loss: 0.22624512016773224\n",
            "iteration 47336: loss: 0.22624509036540985\n",
            "iteration 47337: loss: 0.22624507546424866\n",
            "iteration 47338: loss: 0.22624507546424866\n",
            "iteration 47339: loss: 0.22624504566192627\n",
            "iteration 47340: loss: 0.22624501585960388\n",
            "iteration 47341: loss: 0.2262449711561203\n",
            "iteration 47342: loss: 0.22624501585960388\n",
            "iteration 47343: loss: 0.2262450009584427\n",
            "iteration 47344: loss: 0.2262449711561203\n",
            "iteration 47345: loss: 0.22624492645263672\n",
            "iteration 47346: loss: 0.2262449562549591\n",
            "iteration 47347: loss: 0.22624488174915314\n",
            "iteration 47348: loss: 0.22624489665031433\n",
            "iteration 47349: loss: 0.22624485194683075\n",
            "iteration 47350: loss: 0.22624483704566956\n",
            "iteration 47351: loss: 0.22624480724334717\n",
            "iteration 47352: loss: 0.22624477744102478\n",
            "iteration 47353: loss: 0.2262447625398636\n",
            "iteration 47354: loss: 0.2262447327375412\n",
            "iteration 47355: loss: 0.2262447625398636\n",
            "iteration 47356: loss: 0.2262447625398636\n",
            "iteration 47357: loss: 0.2262447327375412\n",
            "iteration 47358: loss: 0.22624468803405762\n",
            "iteration 47359: loss: 0.22624468803405762\n",
            "iteration 47360: loss: 0.22624464333057404\n",
            "iteration 47361: loss: 0.22624456882476807\n",
            "iteration 47362: loss: 0.22624459862709045\n",
            "iteration 47363: loss: 0.22624459862709045\n",
            "iteration 47364: loss: 0.22624459862709045\n",
            "iteration 47365: loss: 0.22624456882476807\n",
            "iteration 47366: loss: 0.22624452412128448\n",
            "iteration 47367: loss: 0.22624452412128448\n",
            "iteration 47368: loss: 0.2262444794178009\n",
            "iteration 47369: loss: 0.2262444943189621\n",
            "iteration 47370: loss: 0.2262444794178009\n",
            "iteration 47371: loss: 0.22624444961547852\n",
            "iteration 47372: loss: 0.22624441981315613\n",
            "iteration 47373: loss: 0.22624440491199493\n",
            "iteration 47374: loss: 0.22624437510967255\n",
            "iteration 47375: loss: 0.22624436020851135\n",
            "iteration 47376: loss: 0.22624436020851135\n",
            "iteration 47377: loss: 0.22624433040618896\n",
            "iteration 47378: loss: 0.22624428570270538\n",
            "iteration 47379: loss: 0.22624428570270538\n",
            "iteration 47380: loss: 0.22624428570270538\n",
            "iteration 47381: loss: 0.22624430060386658\n",
            "iteration 47382: loss: 0.2262442409992218\n",
            "iteration 47383: loss: 0.22624421119689941\n",
            "iteration 47384: loss: 0.226244255900383\n",
            "iteration 47385: loss: 0.22624412178993225\n",
            "iteration 47386: loss: 0.22624413669109344\n",
            "iteration 47387: loss: 0.22624416649341583\n",
            "iteration 47388: loss: 0.22624412178993225\n",
            "iteration 47389: loss: 0.22624409198760986\n",
            "iteration 47390: loss: 0.22624406218528748\n",
            "iteration 47391: loss: 0.22624406218528748\n",
            "iteration 47392: loss: 0.22624404728412628\n",
            "iteration 47393: loss: 0.2262440174818039\n",
            "iteration 47394: loss: 0.2262440174818039\n",
            "iteration 47395: loss: 0.2262440174818039\n",
            "iteration 47396: loss: 0.2262439727783203\n",
            "iteration 47397: loss: 0.2262439727783203\n",
            "iteration 47398: loss: 0.22624389827251434\n",
            "iteration 47399: loss: 0.22624389827251434\n",
            "iteration 47400: loss: 0.22624389827251434\n",
            "iteration 47401: loss: 0.22624388337135315\n",
            "iteration 47402: loss: 0.22624388337135315\n",
            "iteration 47403: loss: 0.22624380886554718\n",
            "iteration 47404: loss: 0.22624382376670837\n",
            "iteration 47405: loss: 0.22624380886554718\n",
            "iteration 47406: loss: 0.2262437641620636\n",
            "iteration 47407: loss: 0.2262437790632248\n",
            "iteration 47408: loss: 0.22624380886554718\n",
            "iteration 47409: loss: 0.2262437343597412\n",
            "iteration 47410: loss: 0.22624370455741882\n",
            "iteration 47411: loss: 0.22624370455741882\n",
            "iteration 47412: loss: 0.22624365985393524\n",
            "iteration 47413: loss: 0.22624364495277405\n",
            "iteration 47414: loss: 0.22624364495277405\n",
            "iteration 47415: loss: 0.22624358534812927\n",
            "iteration 47416: loss: 0.22624361515045166\n",
            "iteration 47417: loss: 0.22624358534812927\n",
            "iteration 47418: loss: 0.22624358534812927\n",
            "iteration 47419: loss: 0.22624357044696808\n",
            "iteration 47420: loss: 0.2262435257434845\n",
            "iteration 47421: loss: 0.2262435257434845\n",
            "iteration 47422: loss: 0.22624345123767853\n",
            "iteration 47423: loss: 0.22624345123767853\n",
            "iteration 47424: loss: 0.22624345123767853\n",
            "iteration 47425: loss: 0.22624345123767853\n",
            "iteration 47426: loss: 0.22624340653419495\n",
            "iteration 47427: loss: 0.22624340653419495\n",
            "iteration 47428: loss: 0.22624337673187256\n",
            "iteration 47429: loss: 0.22624337673187256\n",
            "iteration 47430: loss: 0.2262432873249054\n",
            "iteration 47431: loss: 0.2262433022260666\n",
            "iteration 47432: loss: 0.22624333202838898\n",
            "iteration 47433: loss: 0.22624333202838898\n",
            "iteration 47434: loss: 0.2262432873249054\n",
            "iteration 47435: loss: 0.226243257522583\n",
            "iteration 47436: loss: 0.22624321281909943\n",
            "iteration 47437: loss: 0.22624321281909943\n",
            "iteration 47438: loss: 0.22624316811561584\n",
            "iteration 47439: loss: 0.22624321281909943\n",
            "iteration 47440: loss: 0.22624316811561584\n",
            "iteration 47441: loss: 0.22624316811561584\n",
            "iteration 47442: loss: 0.22624310851097107\n",
            "iteration 47443: loss: 0.22624310851097107\n",
            "iteration 47444: loss: 0.2262430638074875\n",
            "iteration 47445: loss: 0.2262430191040039\n",
            "iteration 47446: loss: 0.2262430489063263\n",
            "iteration 47447: loss: 0.2262430191040039\n",
            "iteration 47448: loss: 0.2262430191040039\n",
            "iteration 47449: loss: 0.22624297440052032\n",
            "iteration 47450: loss: 0.22624297440052032\n",
            "iteration 47451: loss: 0.22624292969703674\n",
            "iteration 47452: loss: 0.22624289989471436\n",
            "iteration 47453: loss: 0.22624292969703674\n",
            "iteration 47454: loss: 0.22624287009239197\n",
            "iteration 47455: loss: 0.22624285519123077\n",
            "iteration 47456: loss: 0.2262428104877472\n",
            "iteration 47457: loss: 0.2262428253889084\n",
            "iteration 47458: loss: 0.2262428104877472\n",
            "iteration 47459: loss: 0.2262427806854248\n",
            "iteration 47460: loss: 0.2262428104877472\n",
            "iteration 47461: loss: 0.22624273598194122\n",
            "iteration 47462: loss: 0.22624273598194122\n",
            "iteration 47463: loss: 0.22624270617961884\n",
            "iteration 47464: loss: 0.22624273598194122\n",
            "iteration 47465: loss: 0.22624269127845764\n",
            "iteration 47466: loss: 0.22624269127845764\n",
            "iteration 47467: loss: 0.22624261677265167\n",
            "iteration 47468: loss: 0.22624263167381287\n",
            "iteration 47469: loss: 0.22624263167381287\n",
            "iteration 47470: loss: 0.2262425720691681\n",
            "iteration 47471: loss: 0.2262425720691681\n",
            "iteration 47472: loss: 0.2262425422668457\n",
            "iteration 47473: loss: 0.22624249756336212\n",
            "iteration 47474: loss: 0.22624246776103973\n",
            "iteration 47475: loss: 0.22624249756336212\n",
            "iteration 47476: loss: 0.22624249756336212\n",
            "iteration 47477: loss: 0.22624245285987854\n",
            "iteration 47478: loss: 0.22624245285987854\n",
            "iteration 47479: loss: 0.22624239325523376\n",
            "iteration 47480: loss: 0.22624239325523376\n",
            "iteration 47481: loss: 0.22624242305755615\n",
            "iteration 47482: loss: 0.22624234855175018\n",
            "iteration 47483: loss: 0.22624234855175018\n",
            "iteration 47484: loss: 0.226242333650589\n",
            "iteration 47485: loss: 0.22624225914478302\n",
            "iteration 47486: loss: 0.2262423038482666\n",
            "iteration 47487: loss: 0.2262422740459442\n",
            "iteration 47488: loss: 0.22624222934246063\n",
            "iteration 47489: loss: 0.22624225914478302\n",
            "iteration 47490: loss: 0.22624225914478302\n",
            "iteration 47491: loss: 0.22624221444129944\n",
            "iteration 47492: loss: 0.22624218463897705\n",
            "iteration 47493: loss: 0.22624215483665466\n",
            "iteration 47494: loss: 0.22624211013317108\n",
            "iteration 47495: loss: 0.2262420952320099\n",
            "iteration 47496: loss: 0.22624211013317108\n",
            "iteration 47497: loss: 0.2262420654296875\n",
            "iteration 47498: loss: 0.2262420654296875\n",
            "iteration 47499: loss: 0.2262420654296875\n",
            "iteration 47500: loss: 0.22624202072620392\n",
            "iteration 47501: loss: 0.22624202072620392\n",
            "iteration 47502: loss: 0.22624194622039795\n",
            "iteration 47503: loss: 0.22624194622039795\n",
            "iteration 47504: loss: 0.22624197602272034\n",
            "iteration 47505: loss: 0.22624191641807556\n",
            "iteration 47506: loss: 0.22624197602272034\n",
            "iteration 47507: loss: 0.22624191641807556\n",
            "iteration 47508: loss: 0.22624187171459198\n",
            "iteration 47509: loss: 0.22624187171459198\n",
            "iteration 47510: loss: 0.2262418270111084\n",
            "iteration 47511: loss: 0.226241797208786\n",
            "iteration 47512: loss: 0.22624178230762482\n",
            "iteration 47513: loss: 0.22624178230762482\n",
            "iteration 47514: loss: 0.22624175250530243\n",
            "iteration 47515: loss: 0.22624175250530243\n",
            "iteration 47516: loss: 0.22624175250530243\n",
            "iteration 47517: loss: 0.22624173760414124\n",
            "iteration 47518: loss: 0.22624167799949646\n",
            "iteration 47519: loss: 0.22624167799949646\n",
            "iteration 47520: loss: 0.22624166309833527\n",
            "iteration 47521: loss: 0.22624163329601288\n",
            "iteration 47522: loss: 0.2262415885925293\n",
            "iteration 47523: loss: 0.2262415885925293\n",
            "iteration 47524: loss: 0.22624154388904572\n",
            "iteration 47525: loss: 0.22624154388904572\n",
            "iteration 47526: loss: 0.22624154388904572\n",
            "iteration 47527: loss: 0.22624154388904572\n",
            "iteration 47528: loss: 0.22624151408672333\n",
            "iteration 47529: loss: 0.22624146938323975\n",
            "iteration 47530: loss: 0.22624146938323975\n",
            "iteration 47531: loss: 0.22624143958091736\n",
            "iteration 47532: loss: 0.22624142467975616\n",
            "iteration 47533: loss: 0.22624139487743378\n",
            "iteration 47534: loss: 0.22624137997627258\n",
            "iteration 47535: loss: 0.22624139487743378\n",
            "iteration 47536: loss: 0.2262413203716278\n",
            "iteration 47537: loss: 0.2262413203716278\n",
            "iteration 47538: loss: 0.2262413054704666\n",
            "iteration 47539: loss: 0.2262413054704666\n",
            "iteration 47540: loss: 0.22624126076698303\n",
            "iteration 47541: loss: 0.22624123096466064\n",
            "iteration 47542: loss: 0.22624118626117706\n",
            "iteration 47543: loss: 0.22624120116233826\n",
            "iteration 47544: loss: 0.22624120116233826\n",
            "iteration 47545: loss: 0.22624115645885468\n",
            "iteration 47546: loss: 0.22624120116233826\n",
            "iteration 47547: loss: 0.22624114155769348\n",
            "iteration 47548: loss: 0.22624114155769348\n",
            "iteration 47549: loss: 0.2262411117553711\n",
            "iteration 47550: loss: 0.2262410670518875\n",
            "iteration 47551: loss: 0.22624102234840393\n",
            "iteration 47552: loss: 0.2262410670518875\n",
            "iteration 47553: loss: 0.22624102234840393\n",
            "iteration 47554: loss: 0.22624102234840393\n",
            "iteration 47555: loss: 0.22624099254608154\n",
            "iteration 47556: loss: 0.22624096274375916\n",
            "iteration 47557: loss: 0.22624099254608154\n",
            "iteration 47558: loss: 0.22624094784259796\n",
            "iteration 47559: loss: 0.22624090313911438\n",
            "iteration 47560: loss: 0.226240873336792\n",
            "iteration 47561: loss: 0.22624090313911438\n",
            "iteration 47562: loss: 0.226240873336792\n",
            "iteration 47563: loss: 0.226240873336792\n",
            "iteration 47564: loss: 0.2262408435344696\n",
            "iteration 47565: loss: 0.22624079883098602\n",
            "iteration 47566: loss: 0.22624075412750244\n",
            "iteration 47567: loss: 0.22624072432518005\n",
            "iteration 47568: loss: 0.22624072432518005\n",
            "iteration 47569: loss: 0.22624078392982483\n",
            "iteration 47570: loss: 0.22624072432518005\n",
            "iteration 47571: loss: 0.22624070942401886\n",
            "iteration 47572: loss: 0.22624067962169647\n",
            "iteration 47573: loss: 0.22624067962169647\n",
            "iteration 47574: loss: 0.2262406051158905\n",
            "iteration 47575: loss: 0.2262406349182129\n",
            "iteration 47576: loss: 0.2262405902147293\n",
            "iteration 47577: loss: 0.2262406051158905\n",
            "iteration 47578: loss: 0.22624056041240692\n",
            "iteration 47579: loss: 0.22624054551124573\n",
            "iteration 47580: loss: 0.22624054551124573\n",
            "iteration 47581: loss: 0.22624056041240692\n",
            "iteration 47582: loss: 0.22624051570892334\n",
            "iteration 47583: loss: 0.22624048590660095\n",
            "iteration 47584: loss: 0.22624047100543976\n",
            "iteration 47585: loss: 0.22624042630195618\n",
            "iteration 47586: loss: 0.2262403964996338\n",
            "iteration 47587: loss: 0.22624042630195618\n",
            "iteration 47588: loss: 0.2262403666973114\n",
            "iteration 47589: loss: 0.22624032199382782\n",
            "iteration 47590: loss: 0.2262403517961502\n",
            "iteration 47591: loss: 0.2262403517961502\n",
            "iteration 47592: loss: 0.22624027729034424\n",
            "iteration 47593: loss: 0.22624027729034424\n",
            "iteration 47594: loss: 0.22624027729034424\n",
            "iteration 47595: loss: 0.22624024748802185\n",
            "iteration 47596: loss: 0.22624024748802185\n",
            "iteration 47597: loss: 0.22624023258686066\n",
            "iteration 47598: loss: 0.22624018788337708\n",
            "iteration 47599: loss: 0.22624018788337708\n",
            "iteration 47600: loss: 0.2262401580810547\n",
            "iteration 47601: loss: 0.2262401282787323\n",
            "iteration 47602: loss: 0.22624008357524872\n",
            "iteration 47603: loss: 0.2262401282787323\n",
            "iteration 47604: loss: 0.22624003887176514\n",
            "iteration 47605: loss: 0.22624006867408752\n",
            "iteration 47606: loss: 0.22624000906944275\n",
            "iteration 47607: loss: 0.22624006867408752\n",
            "iteration 47608: loss: 0.22624006867408752\n",
            "iteration 47609: loss: 0.22623996436595917\n",
            "iteration 47610: loss: 0.22624000906944275\n",
            "iteration 47611: loss: 0.22623994946479797\n",
            "iteration 47612: loss: 0.226239874958992\n",
            "iteration 47613: loss: 0.2262398898601532\n",
            "iteration 47614: loss: 0.226239874958992\n",
            "iteration 47615: loss: 0.2262398898601532\n",
            "iteration 47616: loss: 0.22623983025550842\n",
            "iteration 47617: loss: 0.22623980045318604\n",
            "iteration 47618: loss: 0.22623983025550842\n",
            "iteration 47619: loss: 0.22623983025550842\n",
            "iteration 47620: loss: 0.22623980045318604\n",
            "iteration 47621: loss: 0.22623975574970245\n",
            "iteration 47622: loss: 0.22623972594738007\n",
            "iteration 47623: loss: 0.22623972594738007\n",
            "iteration 47624: loss: 0.22623971104621887\n",
            "iteration 47625: loss: 0.22623971104621887\n",
            "iteration 47626: loss: 0.22623968124389648\n",
            "iteration 47627: loss: 0.2262396514415741\n",
            "iteration 47628: loss: 0.2262396365404129\n",
            "iteration 47629: loss: 0.22623959183692932\n",
            "iteration 47630: loss: 0.22623960673809052\n",
            "iteration 47631: loss: 0.22623956203460693\n",
            "iteration 47632: loss: 0.22623953223228455\n",
            "iteration 47633: loss: 0.22623956203460693\n",
            "iteration 47634: loss: 0.22623951733112335\n",
            "iteration 47635: loss: 0.22623948752880096\n",
            "iteration 47636: loss: 0.22623948752880096\n",
            "iteration 47637: loss: 0.226239413022995\n",
            "iteration 47638: loss: 0.22623944282531738\n",
            "iteration 47639: loss: 0.22623947262763977\n",
            "iteration 47640: loss: 0.22623944282531738\n",
            "iteration 47641: loss: 0.2262393981218338\n",
            "iteration 47642: loss: 0.2262393683195114\n",
            "iteration 47643: loss: 0.2262393981218338\n",
            "iteration 47644: loss: 0.2262393683195114\n",
            "iteration 47645: loss: 0.22623935341835022\n",
            "iteration 47646: loss: 0.22623932361602783\n",
            "iteration 47647: loss: 0.22623929381370544\n",
            "iteration 47648: loss: 0.22623924911022186\n",
            "iteration 47649: loss: 0.22623924911022186\n",
            "iteration 47650: loss: 0.22623920440673828\n",
            "iteration 47651: loss: 0.22623920440673828\n",
            "iteration 47652: loss: 0.2262391746044159\n",
            "iteration 47653: loss: 0.22623911499977112\n",
            "iteration 47654: loss: 0.2262391597032547\n",
            "iteration 47655: loss: 0.2262391597032547\n",
            "iteration 47656: loss: 0.22623908519744873\n",
            "iteration 47657: loss: 0.22623908519744873\n",
            "iteration 47658: loss: 0.22623904049396515\n",
            "iteration 47659: loss: 0.22623904049396515\n",
            "iteration 47660: loss: 0.22623905539512634\n",
            "iteration 47661: loss: 0.22623901069164276\n",
            "iteration 47662: loss: 0.22623901069164276\n",
            "iteration 47663: loss: 0.22623901069164276\n",
            "iteration 47664: loss: 0.22623901069164276\n",
            "iteration 47665: loss: 0.22623896598815918\n",
            "iteration 47666: loss: 0.2262388914823532\n",
            "iteration 47667: loss: 0.2262388914823532\n",
            "iteration 47668: loss: 0.2262388914823532\n",
            "iteration 47669: loss: 0.22623887658119202\n",
            "iteration 47670: loss: 0.22623884677886963\n",
            "iteration 47671: loss: 0.22623884677886963\n",
            "iteration 47672: loss: 0.22623880207538605\n",
            "iteration 47673: loss: 0.22623880207538605\n",
            "iteration 47674: loss: 0.22623877227306366\n",
            "iteration 47675: loss: 0.22623877227306366\n",
            "iteration 47676: loss: 0.22623875737190247\n",
            "iteration 47677: loss: 0.2262386977672577\n",
            "iteration 47678: loss: 0.22623872756958008\n",
            "iteration 47679: loss: 0.22623872756958008\n",
            "iteration 47680: loss: 0.2262386828660965\n",
            "iteration 47681: loss: 0.22623860836029053\n",
            "iteration 47682: loss: 0.22623860836029053\n",
            "iteration 47683: loss: 0.22623857855796814\n",
            "iteration 47684: loss: 0.22623857855796814\n",
            "iteration 47685: loss: 0.22623857855796814\n",
            "iteration 47686: loss: 0.22623853385448456\n",
            "iteration 47687: loss: 0.22623853385448456\n",
            "iteration 47688: loss: 0.22623853385448456\n",
            "iteration 47689: loss: 0.22623848915100098\n",
            "iteration 47690: loss: 0.22623848915100098\n",
            "iteration 47691: loss: 0.2262384444475174\n",
            "iteration 47692: loss: 0.226238414645195\n",
            "iteration 47693: loss: 0.2262384444475174\n",
            "iteration 47694: loss: 0.226238414645195\n",
            "iteration 47695: loss: 0.22623836994171143\n",
            "iteration 47696: loss: 0.22623832523822784\n",
            "iteration 47697: loss: 0.22623832523822784\n",
            "iteration 47698: loss: 0.22623829543590546\n",
            "iteration 47699: loss: 0.22623832523822784\n",
            "iteration 47700: loss: 0.22623828053474426\n",
            "iteration 47701: loss: 0.22623832523822784\n",
            "iteration 47702: loss: 0.22623825073242188\n",
            "iteration 47703: loss: 0.22623825073242188\n",
            "iteration 47704: loss: 0.22623825073242188\n",
            "iteration 47705: loss: 0.2262382060289383\n",
            "iteration 47706: loss: 0.2262381762266159\n",
            "iteration 47707: loss: 0.2262381613254547\n",
            "iteration 47708: loss: 0.2262381613254547\n",
            "iteration 47709: loss: 0.2262381613254547\n",
            "iteration 47710: loss: 0.22623810172080994\n",
            "iteration 47711: loss: 0.22623810172080994\n",
            "iteration 47712: loss: 0.22623808681964874\n",
            "iteration 47713: loss: 0.22623808681964874\n",
            "iteration 47714: loss: 0.22623801231384277\n",
            "iteration 47715: loss: 0.22623801231384277\n",
            "iteration 47716: loss: 0.22623804211616516\n",
            "iteration 47717: loss: 0.2262379676103592\n",
            "iteration 47718: loss: 0.22623798251152039\n",
            "iteration 47719: loss: 0.2262379676103592\n",
            "iteration 47720: loss: 0.2262379229068756\n",
            "iteration 47721: loss: 0.22623786330223083\n",
            "iteration 47722: loss: 0.22623786330223083\n",
            "iteration 47723: loss: 0.22623786330223083\n",
            "iteration 47724: loss: 0.22623786330223083\n",
            "iteration 47725: loss: 0.22623784840106964\n",
            "iteration 47726: loss: 0.22623780369758606\n",
            "iteration 47727: loss: 0.22623774409294128\n",
            "iteration 47728: loss: 0.22623777389526367\n",
            "iteration 47729: loss: 0.22623777389526367\n",
            "iteration 47730: loss: 0.2262377291917801\n",
            "iteration 47731: loss: 0.2262377291917801\n",
            "iteration 47732: loss: 0.2262376844882965\n",
            "iteration 47733: loss: 0.2262377291917801\n",
            "iteration 47734: loss: 0.2262376844882965\n",
            "iteration 47735: loss: 0.22623765468597412\n",
            "iteration 47736: loss: 0.22623762488365173\n",
            "iteration 47737: loss: 0.22623758018016815\n",
            "iteration 47738: loss: 0.22623758018016815\n",
            "iteration 47739: loss: 0.22623758018016815\n",
            "iteration 47740: loss: 0.22623756527900696\n",
            "iteration 47741: loss: 0.22623750567436218\n",
            "iteration 47742: loss: 0.22623750567436218\n",
            "iteration 47743: loss: 0.226237490773201\n",
            "iteration 47744: loss: 0.22623750567436218\n",
            "iteration 47745: loss: 0.2262374609708786\n",
            "iteration 47746: loss: 0.2262374460697174\n",
            "iteration 47747: loss: 0.2262374609708786\n",
            "iteration 47748: loss: 0.2262374460697174\n",
            "iteration 47749: loss: 0.22623741626739502\n",
            "iteration 47750: loss: 0.22623737156391144\n",
            "iteration 47751: loss: 0.22623738646507263\n",
            "iteration 47752: loss: 0.22623734176158905\n",
            "iteration 47753: loss: 0.22623729705810547\n",
            "iteration 47754: loss: 0.22623732686042786\n",
            "iteration 47755: loss: 0.22623726725578308\n",
            "iteration 47756: loss: 0.2262372523546219\n",
            "iteration 47757: loss: 0.2262372076511383\n",
            "iteration 47758: loss: 0.2262372076511383\n",
            "iteration 47759: loss: 0.22623717784881592\n",
            "iteration 47760: loss: 0.22623713314533234\n",
            "iteration 47761: loss: 0.22623717784881592\n",
            "iteration 47762: loss: 0.22623710334300995\n",
            "iteration 47763: loss: 0.22623714804649353\n",
            "iteration 47764: loss: 0.22623710334300995\n",
            "iteration 47765: loss: 0.22623714804649353\n",
            "iteration 47766: loss: 0.22623708844184875\n",
            "iteration 47767: loss: 0.22623705863952637\n",
            "iteration 47768: loss: 0.2262369841337204\n",
            "iteration 47769: loss: 0.22623701393604279\n",
            "iteration 47770: loss: 0.22623701393604279\n",
            "iteration 47771: loss: 0.2262369841337204\n",
            "iteration 47772: loss: 0.2262369692325592\n",
            "iteration 47773: loss: 0.22623690962791443\n",
            "iteration 47774: loss: 0.22623693943023682\n",
            "iteration 47775: loss: 0.22623690962791443\n",
            "iteration 47776: loss: 0.22623690962791443\n",
            "iteration 47777: loss: 0.22623686492443085\n",
            "iteration 47778: loss: 0.22623682022094727\n",
            "iteration 47779: loss: 0.22623682022094727\n",
            "iteration 47780: loss: 0.22623677551746368\n",
            "iteration 47781: loss: 0.22623679041862488\n",
            "iteration 47782: loss: 0.2262367308139801\n",
            "iteration 47783: loss: 0.2262367308139801\n",
            "iteration 47784: loss: 0.2262367457151413\n",
            "iteration 47785: loss: 0.2262367308139801\n",
            "iteration 47786: loss: 0.2262367308139801\n",
            "iteration 47787: loss: 0.22623665630817413\n",
            "iteration 47788: loss: 0.22623667120933533\n",
            "iteration 47789: loss: 0.22623670101165771\n",
            "iteration 47790: loss: 0.22623662650585175\n",
            "iteration 47791: loss: 0.22623662650585175\n",
            "iteration 47792: loss: 0.22623658180236816\n",
            "iteration 47793: loss: 0.22623653709888458\n",
            "iteration 47794: loss: 0.22623653709888458\n",
            "iteration 47795: loss: 0.22623655200004578\n",
            "iteration 47796: loss: 0.2262364625930786\n",
            "iteration 47797: loss: 0.226236492395401\n",
            "iteration 47798: loss: 0.22623643279075623\n",
            "iteration 47799: loss: 0.226236492395401\n",
            "iteration 47800: loss: 0.2262364625930786\n",
            "iteration 47801: loss: 0.22623638808727264\n",
            "iteration 47802: loss: 0.22623637318611145\n",
            "iteration 47803: loss: 0.22623638808727264\n",
            "iteration 47804: loss: 0.22623637318611145\n",
            "iteration 47805: loss: 0.22623634338378906\n",
            "iteration 47806: loss: 0.22623634338378906\n",
            "iteration 47807: loss: 0.22623634338378906\n",
            "iteration 47808: loss: 0.22623629868030548\n",
            "iteration 47809: loss: 0.2262362688779831\n",
            "iteration 47810: loss: 0.2262362539768219\n",
            "iteration 47811: loss: 0.2262362241744995\n",
            "iteration 47812: loss: 0.22623619437217712\n",
            "iteration 47813: loss: 0.2262362241744995\n",
            "iteration 47814: loss: 0.22623619437217712\n",
            "iteration 47815: loss: 0.22623614966869354\n",
            "iteration 47816: loss: 0.22623614966869354\n",
            "iteration 47817: loss: 0.22623610496520996\n",
            "iteration 47818: loss: 0.22623607516288757\n",
            "iteration 47819: loss: 0.22623610496520996\n",
            "iteration 47820: loss: 0.22623606026172638\n",
            "iteration 47821: loss: 0.2262360155582428\n",
            "iteration 47822: loss: 0.22623606026172638\n",
            "iteration 47823: loss: 0.2262360155582428\n",
            "iteration 47824: loss: 0.2262360155582428\n",
            "iteration 47825: loss: 0.22623595595359802\n",
            "iteration 47826: loss: 0.22623595595359802\n",
            "iteration 47827: loss: 0.22623591125011444\n",
            "iteration 47828: loss: 0.22623591125011444\n",
            "iteration 47829: loss: 0.22623591125011444\n",
            "iteration 47830: loss: 0.22623586654663086\n",
            "iteration 47831: loss: 0.22623589634895325\n",
            "iteration 47832: loss: 0.22623582184314728\n",
            "iteration 47833: loss: 0.22623583674430847\n",
            "iteration 47834: loss: 0.2262357771396637\n",
            "iteration 47835: loss: 0.2262357920408249\n",
            "iteration 47836: loss: 0.2262357771396637\n",
            "iteration 47837: loss: 0.2262357473373413\n",
            "iteration 47838: loss: 0.22623570263385773\n",
            "iteration 47839: loss: 0.2262357473373413\n",
            "iteration 47840: loss: 0.22623570263385773\n",
            "iteration 47841: loss: 0.22623567283153534\n",
            "iteration 47842: loss: 0.22623565793037415\n",
            "iteration 47843: loss: 0.22623558342456818\n",
            "iteration 47844: loss: 0.22623562812805176\n",
            "iteration 47845: loss: 0.22623558342456818\n",
            "iteration 47846: loss: 0.22623559832572937\n",
            "iteration 47847: loss: 0.2262355536222458\n",
            "iteration 47848: loss: 0.2262355387210846\n",
            "iteration 47849: loss: 0.2262355387210846\n",
            "iteration 47850: loss: 0.22623547911643982\n",
            "iteration 47851: loss: 0.22623546421527863\n",
            "iteration 47852: loss: 0.22623546421527863\n",
            "iteration 47853: loss: 0.22623547911643982\n",
            "iteration 47854: loss: 0.22623541951179504\n",
            "iteration 47855: loss: 0.22623546421527863\n",
            "iteration 47856: loss: 0.22623538970947266\n",
            "iteration 47857: loss: 0.22623535990715027\n",
            "iteration 47858: loss: 0.22623538970947266\n",
            "iteration 47859: loss: 0.22623538970947266\n",
            "iteration 47860: loss: 0.2262353152036667\n",
            "iteration 47861: loss: 0.2262352705001831\n",
            "iteration 47862: loss: 0.2262353003025055\n",
            "iteration 47863: loss: 0.22623524069786072\n",
            "iteration 47864: loss: 0.22623524069786072\n",
            "iteration 47865: loss: 0.22623524069786072\n",
            "iteration 47866: loss: 0.22623519599437714\n",
            "iteration 47867: loss: 0.22623519599437714\n",
            "iteration 47868: loss: 0.22623518109321594\n",
            "iteration 47869: loss: 0.22623515129089355\n",
            "iteration 47870: loss: 0.22623512148857117\n",
            "iteration 47871: loss: 0.22623510658740997\n",
            "iteration 47872: loss: 0.22623510658740997\n",
            "iteration 47873: loss: 0.22623515129089355\n",
            "iteration 47874: loss: 0.2262350618839264\n",
            "iteration 47875: loss: 0.226235032081604\n",
            "iteration 47876: loss: 0.226235032081604\n",
            "iteration 47877: loss: 0.22623500227928162\n",
            "iteration 47878: loss: 0.226235032081604\n",
            "iteration 47879: loss: 0.22623498737812042\n",
            "iteration 47880: loss: 0.22623494267463684\n",
            "iteration 47881: loss: 0.22623495757579803\n",
            "iteration 47882: loss: 0.22623494267463684\n",
            "iteration 47883: loss: 0.22623488306999207\n",
            "iteration 47884: loss: 0.22623488306999207\n",
            "iteration 47885: loss: 0.22623488306999207\n",
            "iteration 47886: loss: 0.22623483836650848\n",
            "iteration 47887: loss: 0.22623486816883087\n",
            "iteration 47888: loss: 0.22623483836650848\n",
            "iteration 47889: loss: 0.22623476386070251\n",
            "iteration 47890: loss: 0.2262347936630249\n",
            "iteration 47891: loss: 0.22623476386070251\n",
            "iteration 47892: loss: 0.22623471915721893\n",
            "iteration 47893: loss: 0.22623471915721893\n",
            "iteration 47894: loss: 0.22623471915721893\n",
            "iteration 47895: loss: 0.22623467445373535\n",
            "iteration 47896: loss: 0.22623462975025177\n",
            "iteration 47897: loss: 0.22623459994792938\n",
            "iteration 47898: loss: 0.22623462975025177\n",
            "iteration 47899: loss: 0.2262345850467682\n",
            "iteration 47900: loss: 0.22623462975025177\n",
            "iteration 47901: loss: 0.2262345850467682\n",
            "iteration 47902: loss: 0.2262345850467682\n",
            "iteration 47903: loss: 0.22623446583747864\n",
            "iteration 47904: loss: 0.22623451054096222\n",
            "iteration 47905: loss: 0.22623448073863983\n",
            "iteration 47906: loss: 0.22623446583747864\n",
            "iteration 47907: loss: 0.22623448073863983\n",
            "iteration 47908: loss: 0.22623446583747864\n",
            "iteration 47909: loss: 0.22623443603515625\n",
            "iteration 47910: loss: 0.22623439133167267\n",
            "iteration 47911: loss: 0.22623439133167267\n",
            "iteration 47912: loss: 0.22623440623283386\n",
            "iteration 47913: loss: 0.22623439133167267\n",
            "iteration 47914: loss: 0.2262342870235443\n",
            "iteration 47915: loss: 0.2262343466281891\n",
            "iteration 47916: loss: 0.2262342870235443\n",
            "iteration 47917: loss: 0.2262342870235443\n",
            "iteration 47918: loss: 0.22623427212238312\n",
            "iteration 47919: loss: 0.22623422741889954\n",
            "iteration 47920: loss: 0.22623422741889954\n",
            "iteration 47921: loss: 0.22623419761657715\n",
            "iteration 47922: loss: 0.22623416781425476\n",
            "iteration 47923: loss: 0.22623416781425476\n",
            "iteration 47924: loss: 0.22623412311077118\n",
            "iteration 47925: loss: 0.22623412311077118\n",
            "iteration 47926: loss: 0.22623410820960999\n",
            "iteration 47927: loss: 0.22623410820960999\n",
            "iteration 47928: loss: 0.2262340784072876\n",
            "iteration 47929: loss: 0.2262340486049652\n",
            "iteration 47930: loss: 0.2262340486049652\n",
            "iteration 47931: loss: 0.22623400390148163\n",
            "iteration 47932: loss: 0.22623400390148163\n",
            "iteration 47933: loss: 0.22623398900032043\n",
            "iteration 47934: loss: 0.22623398900032043\n",
            "iteration 47935: loss: 0.22623391449451447\n",
            "iteration 47936: loss: 0.22623391449451447\n",
            "iteration 47937: loss: 0.22623391449451447\n",
            "iteration 47938: loss: 0.22623391449451447\n",
            "iteration 47939: loss: 0.22623386979103088\n",
            "iteration 47940: loss: 0.2262338399887085\n",
            "iteration 47941: loss: 0.22623386979103088\n",
            "iteration 47942: loss: 0.22623379528522491\n",
            "iteration 47943: loss: 0.2262338101863861\n",
            "iteration 47944: loss: 0.22623376548290253\n",
            "iteration 47945: loss: 0.22623376548290253\n",
            "iteration 47946: loss: 0.22623369097709656\n",
            "iteration 47947: loss: 0.22623375058174133\n",
            "iteration 47948: loss: 0.22623367607593536\n",
            "iteration 47949: loss: 0.22623367607593536\n",
            "iteration 47950: loss: 0.22623367607593536\n",
            "iteration 47951: loss: 0.22623363137245178\n",
            "iteration 47952: loss: 0.22623364627361298\n",
            "iteration 47953: loss: 0.226233571767807\n",
            "iteration 47954: loss: 0.226233571767807\n",
            "iteration 47955: loss: 0.226233571767807\n",
            "iteration 47956: loss: 0.2262335568666458\n",
            "iteration 47957: loss: 0.226233571767807\n",
            "iteration 47958: loss: 0.22623348236083984\n",
            "iteration 47959: loss: 0.22623351216316223\n",
            "iteration 47960: loss: 0.22623352706432343\n",
            "iteration 47961: loss: 0.22623348236083984\n",
            "iteration 47962: loss: 0.22623345255851746\n",
            "iteration 47963: loss: 0.22623348236083984\n",
            "iteration 47964: loss: 0.22623343765735626\n",
            "iteration 47965: loss: 0.22623339295387268\n",
            "iteration 47966: loss: 0.2262333631515503\n",
            "iteration 47967: loss: 0.2262333631515503\n",
            "iteration 47968: loss: 0.2262333184480667\n",
            "iteration 47969: loss: 0.2262333184480667\n",
            "iteration 47970: loss: 0.2262333184480667\n",
            "iteration 47971: loss: 0.22623328864574432\n",
            "iteration 47972: loss: 0.22623328864574432\n",
            "iteration 47973: loss: 0.22623324394226074\n",
            "iteration 47974: loss: 0.22623321413993835\n",
            "iteration 47975: loss: 0.22623324394226074\n",
            "iteration 47976: loss: 0.22623315453529358\n",
            "iteration 47977: loss: 0.22623319923877716\n",
            "iteration 47978: loss: 0.22623315453529358\n",
            "iteration 47979: loss: 0.22623315453529358\n",
            "iteration 47980: loss: 0.2262330949306488\n",
            "iteration 47981: loss: 0.2262330949306488\n",
            "iteration 47982: loss: 0.2262330800294876\n",
            "iteration 47983: loss: 0.2262330800294876\n",
            "iteration 47984: loss: 0.2262330949306488\n",
            "iteration 47985: loss: 0.22623303532600403\n",
            "iteration 47986: loss: 0.22623300552368164\n",
            "iteration 47987: loss: 0.22623300552368164\n",
            "iteration 47988: loss: 0.22623297572135925\n",
            "iteration 47989: loss: 0.22623296082019806\n",
            "iteration 47990: loss: 0.22623291611671448\n",
            "iteration 47991: loss: 0.2262328863143921\n",
            "iteration 47992: loss: 0.2262328565120697\n",
            "iteration 47993: loss: 0.2262328565120697\n",
            "iteration 47994: loss: 0.2262328565120697\n",
            "iteration 47995: loss: 0.2262328416109085\n",
            "iteration 47996: loss: 0.22623281180858612\n",
            "iteration 47997: loss: 0.22623279690742493\n",
            "iteration 47998: loss: 0.22623281180858612\n",
            "iteration 47999: loss: 0.22623281180858612\n",
            "iteration 48000: loss: 0.22623276710510254\n",
            "iteration 48001: loss: 0.22623273730278015\n",
            "iteration 48002: loss: 0.22623272240161896\n",
            "iteration 48003: loss: 0.22623267769813538\n",
            "iteration 48004: loss: 0.22623267769813538\n",
            "iteration 48005: loss: 0.22623269259929657\n",
            "iteration 48006: loss: 0.22623267769813538\n",
            "iteration 48007: loss: 0.2262326180934906\n",
            "iteration 48008: loss: 0.2262326031923294\n",
            "iteration 48009: loss: 0.2262326031923294\n",
            "iteration 48010: loss: 0.2262326180934906\n",
            "iteration 48011: loss: 0.22623257339000702\n",
            "iteration 48012: loss: 0.22623255848884583\n",
            "iteration 48013: loss: 0.22623252868652344\n",
            "iteration 48014: loss: 0.22623249888420105\n",
            "iteration 48015: loss: 0.22623245418071747\n",
            "iteration 48016: loss: 0.22623248398303986\n",
            "iteration 48017: loss: 0.22623248398303986\n",
            "iteration 48018: loss: 0.22623245418071747\n",
            "iteration 48019: loss: 0.2262324094772339\n",
            "iteration 48020: loss: 0.2262323796749115\n",
            "iteration 48021: loss: 0.2262323647737503\n",
            "iteration 48022: loss: 0.22623233497142792\n",
            "iteration 48023: loss: 0.22623229026794434\n",
            "iteration 48024: loss: 0.2262323796749115\n",
            "iteration 48025: loss: 0.22623229026794434\n",
            "iteration 48026: loss: 0.22623232007026672\n",
            "iteration 48027: loss: 0.22623224556446075\n",
            "iteration 48028: loss: 0.22623226046562195\n",
            "iteration 48029: loss: 0.22623224556446075\n",
            "iteration 48030: loss: 0.22623220086097717\n",
            "iteration 48031: loss: 0.22623217105865479\n",
            "iteration 48032: loss: 0.2262321412563324\n",
            "iteration 48033: loss: 0.2262321412563324\n",
            "iteration 48034: loss: 0.2262321412563324\n",
            "iteration 48035: loss: 0.2262321412563324\n",
            "iteration 48036: loss: 0.22623208165168762\n",
            "iteration 48037: loss: 0.22623205184936523\n",
            "iteration 48038: loss: 0.22623208165168762\n",
            "iteration 48039: loss: 0.22623205184936523\n",
            "iteration 48040: loss: 0.22623200714588165\n",
            "iteration 48041: loss: 0.22623200714588165\n",
            "iteration 48042: loss: 0.22623200714588165\n",
            "iteration 48043: loss: 0.22623197734355927\n",
            "iteration 48044: loss: 0.2262319028377533\n",
            "iteration 48045: loss: 0.22623193264007568\n",
            "iteration 48046: loss: 0.22623193264007568\n",
            "iteration 48047: loss: 0.2262318879365921\n",
            "iteration 48048: loss: 0.2262319028377533\n",
            "iteration 48049: loss: 0.22623181343078613\n",
            "iteration 48050: loss: 0.22623185813426971\n",
            "iteration 48051: loss: 0.22623181343078613\n",
            "iteration 48052: loss: 0.22623181343078613\n",
            "iteration 48053: loss: 0.22623176872730255\n",
            "iteration 48054: loss: 0.22623178362846375\n",
            "iteration 48055: loss: 0.22623172402381897\n",
            "iteration 48056: loss: 0.22623172402381897\n",
            "iteration 48057: loss: 0.22623173892498016\n",
            "iteration 48058: loss: 0.22623172402381897\n",
            "iteration 48059: loss: 0.22623169422149658\n",
            "iteration 48060: loss: 0.2262316644191742\n",
            "iteration 48061: loss: 0.2262316197156906\n",
            "iteration 48062: loss: 0.22623160481452942\n",
            "iteration 48063: loss: 0.22623157501220703\n",
            "iteration 48064: loss: 0.2262316197156906\n",
            "iteration 48065: loss: 0.22623157501220703\n",
            "iteration 48066: loss: 0.22623153030872345\n",
            "iteration 48067: loss: 0.22623150050640106\n",
            "iteration 48068: loss: 0.22623153030872345\n",
            "iteration 48069: loss: 0.22623150050640106\n",
            "iteration 48070: loss: 0.22623148560523987\n",
            "iteration 48071: loss: 0.22623148560523987\n",
            "iteration 48072: loss: 0.2262313812971115\n",
            "iteration 48073: loss: 0.2262314260005951\n",
            "iteration 48074: loss: 0.2262313812971115\n",
            "iteration 48075: loss: 0.2262314110994339\n",
            "iteration 48076: loss: 0.22623136639595032\n",
            "iteration 48077: loss: 0.22623133659362793\n",
            "iteration 48078: loss: 0.22623136639595032\n",
            "iteration 48079: loss: 0.22623129189014435\n",
            "iteration 48080: loss: 0.22623124718666077\n",
            "iteration 48081: loss: 0.22623129189014435\n",
            "iteration 48082: loss: 0.22623121738433838\n",
            "iteration 48083: loss: 0.22623121738433838\n",
            "iteration 48084: loss: 0.22623124718666077\n",
            "iteration 48085: loss: 0.22623121738433838\n",
            "iteration 48086: loss: 0.226231187582016\n",
            "iteration 48087: loss: 0.2262311726808548\n",
            "iteration 48088: loss: 0.2262311428785324\n",
            "iteration 48089: loss: 0.22623112797737122\n",
            "iteration 48090: loss: 0.22623109817504883\n",
            "iteration 48091: loss: 0.22623112797737122\n",
            "iteration 48092: loss: 0.22623106837272644\n",
            "iteration 48093: loss: 0.22623102366924286\n",
            "iteration 48094: loss: 0.22623105347156525\n",
            "iteration 48095: loss: 0.22623100876808167\n",
            "iteration 48096: loss: 0.22623102366924286\n",
            "iteration 48097: loss: 0.22623100876808167\n",
            "iteration 48098: loss: 0.2262309491634369\n",
            "iteration 48099: loss: 0.2262309342622757\n",
            "iteration 48100: loss: 0.2262309491634369\n",
            "iteration 48101: loss: 0.22623088955879211\n",
            "iteration 48102: loss: 0.2262309342622757\n",
            "iteration 48103: loss: 0.22623085975646973\n",
            "iteration 48104: loss: 0.22623088955879211\n",
            "iteration 48105: loss: 0.22623081505298615\n",
            "iteration 48106: loss: 0.22623078525066376\n",
            "iteration 48107: loss: 0.22623078525066376\n",
            "iteration 48108: loss: 0.22623077034950256\n",
            "iteration 48109: loss: 0.22623077034950256\n",
            "iteration 48110: loss: 0.22623077034950256\n",
            "iteration 48111: loss: 0.2262307107448578\n",
            "iteration 48112: loss: 0.22623074054718018\n",
            "iteration 48113: loss: 0.2262307107448578\n",
            "iteration 48114: loss: 0.2262306660413742\n",
            "iteration 48115: loss: 0.22623062133789062\n",
            "iteration 48116: loss: 0.22623062133789062\n",
            "iteration 48117: loss: 0.22623057663440704\n",
            "iteration 48118: loss: 0.226230651140213\n",
            "iteration 48119: loss: 0.22623057663440704\n",
            "iteration 48120: loss: 0.22623057663440704\n",
            "iteration 48121: loss: 0.22623053193092346\n",
            "iteration 48122: loss: 0.22623057663440704\n",
            "iteration 48123: loss: 0.2262304723262787\n",
            "iteration 48124: loss: 0.22623050212860107\n",
            "iteration 48125: loss: 0.2262304574251175\n",
            "iteration 48126: loss: 0.2262304574251175\n",
            "iteration 48127: loss: 0.2262304574251175\n",
            "iteration 48128: loss: 0.2262304276227951\n",
            "iteration 48129: loss: 0.22623038291931152\n",
            "iteration 48130: loss: 0.22623038291931152\n",
            "iteration 48131: loss: 0.22623035311698914\n",
            "iteration 48132: loss: 0.22623035311698914\n",
            "iteration 48133: loss: 0.22623030841350555\n",
            "iteration 48134: loss: 0.22623030841350555\n",
            "iteration 48135: loss: 0.22623029351234436\n",
            "iteration 48136: loss: 0.22623026371002197\n",
            "iteration 48137: loss: 0.22623023390769958\n",
            "iteration 48138: loss: 0.22623026371002197\n",
            "iteration 48139: loss: 0.226230189204216\n",
            "iteration 48140: loss: 0.22623014450073242\n",
            "iteration 48141: loss: 0.226230189204216\n",
            "iteration 48142: loss: 0.2262301743030548\n",
            "iteration 48143: loss: 0.22623014450073242\n",
            "iteration 48144: loss: 0.22623014450073242\n",
            "iteration 48145: loss: 0.22623011469841003\n",
            "iteration 48146: loss: 0.22623005509376526\n",
            "iteration 48147: loss: 0.22623006999492645\n",
            "iteration 48148: loss: 0.22623002529144287\n",
            "iteration 48149: loss: 0.22623005509376526\n",
            "iteration 48150: loss: 0.22622999548912048\n",
            "iteration 48151: loss: 0.2262299805879593\n",
            "iteration 48152: loss: 0.22622999548912048\n",
            "iteration 48153: loss: 0.2262299358844757\n",
            "iteration 48154: loss: 0.2262299507856369\n",
            "iteration 48155: loss: 0.2262299358844757\n",
            "iteration 48156: loss: 0.22622990608215332\n",
            "iteration 48157: loss: 0.22622990608215332\n",
            "iteration 48158: loss: 0.22622987627983093\n",
            "iteration 48159: loss: 0.22622981667518616\n",
            "iteration 48160: loss: 0.22622983157634735\n",
            "iteration 48161: loss: 0.22622981667518616\n",
            "iteration 48162: loss: 0.22622981667518616\n",
            "iteration 48163: loss: 0.22622975707054138\n",
            "iteration 48164: loss: 0.22622975707054138\n",
            "iteration 48165: loss: 0.22622975707054138\n",
            "iteration 48166: loss: 0.2262297123670578\n",
            "iteration 48167: loss: 0.2262297421693802\n",
            "iteration 48168: loss: 0.2262297123670578\n",
            "iteration 48169: loss: 0.2262296974658966\n",
            "iteration 48170: loss: 0.22622966766357422\n",
            "iteration 48171: loss: 0.22622962296009064\n",
            "iteration 48172: loss: 0.22622962296009064\n",
            "iteration 48173: loss: 0.22622963786125183\n",
            "iteration 48174: loss: 0.22622957825660706\n",
            "iteration 48175: loss: 0.22622957825660706\n",
            "iteration 48176: loss: 0.22622954845428467\n",
            "iteration 48177: loss: 0.22622951865196228\n",
            "iteration 48178: loss: 0.2262295037508011\n",
            "iteration 48179: loss: 0.2262295037508011\n",
            "iteration 48180: loss: 0.2262294590473175\n",
            "iteration 48181: loss: 0.2262294590473175\n",
            "iteration 48182: loss: 0.22622942924499512\n",
            "iteration 48183: loss: 0.22622942924499512\n",
            "iteration 48184: loss: 0.22622942924499512\n",
            "iteration 48185: loss: 0.22622939944267273\n",
            "iteration 48186: loss: 0.22622938454151154\n",
            "iteration 48187: loss: 0.22622933983802795\n",
            "iteration 48188: loss: 0.22622933983802795\n",
            "iteration 48189: loss: 0.22622928023338318\n",
            "iteration 48190: loss: 0.22622928023338318\n",
            "iteration 48191: loss: 0.2262292355298996\n",
            "iteration 48192: loss: 0.22622928023338318\n",
            "iteration 48193: loss: 0.2262292355298996\n",
            "iteration 48194: loss: 0.2262292206287384\n",
            "iteration 48195: loss: 0.22622919082641602\n",
            "iteration 48196: loss: 0.2262292206287384\n",
            "iteration 48197: loss: 0.22622914612293243\n",
            "iteration 48198: loss: 0.22622916102409363\n",
            "iteration 48199: loss: 0.22622910141944885\n",
            "iteration 48200: loss: 0.22622910141944885\n",
            "iteration 48201: loss: 0.22622911632061005\n",
            "iteration 48202: loss: 0.22622907161712646\n",
            "iteration 48203: loss: 0.22622904181480408\n",
            "iteration 48204: loss: 0.22622902691364288\n",
            "iteration 48205: loss: 0.2262289971113205\n",
            "iteration 48206: loss: 0.2262289822101593\n",
            "iteration 48207: loss: 0.22622895240783691\n",
            "iteration 48208: loss: 0.2262289822101593\n",
            "iteration 48209: loss: 0.22622895240783691\n",
            "iteration 48210: loss: 0.22622892260551453\n",
            "iteration 48211: loss: 0.22622890770435333\n",
            "iteration 48212: loss: 0.22622887790203094\n",
            "iteration 48213: loss: 0.22622890770435333\n",
            "iteration 48214: loss: 0.22622883319854736\n",
            "iteration 48215: loss: 0.22622886300086975\n",
            "iteration 48216: loss: 0.22622886300086975\n",
            "iteration 48217: loss: 0.2262287586927414\n",
            "iteration 48218: loss: 0.22622878849506378\n",
            "iteration 48219: loss: 0.2262287586927414\n",
            "iteration 48220: loss: 0.2262287437915802\n",
            "iteration 48221: loss: 0.2262287139892578\n",
            "iteration 48222: loss: 0.2262287139892578\n",
            "iteration 48223: loss: 0.22622866928577423\n",
            "iteration 48224: loss: 0.22622866928577423\n",
            "iteration 48225: loss: 0.22622866928577423\n",
            "iteration 48226: loss: 0.22622863948345184\n",
            "iteration 48227: loss: 0.22622863948345184\n",
            "iteration 48228: loss: 0.22622859477996826\n",
            "iteration 48229: loss: 0.22622855007648468\n",
            "iteration 48230: loss: 0.22622859477996826\n",
            "iteration 48231: loss: 0.22622856497764587\n",
            "iteration 48232: loss: 0.2262285202741623\n",
            "iteration 48233: loss: 0.2262285202741623\n",
            "iteration 48234: loss: 0.2262285053730011\n",
            "iteration 48235: loss: 0.2262284755706787\n",
            "iteration 48236: loss: 0.22622844576835632\n",
            "iteration 48237: loss: 0.22622843086719513\n",
            "iteration 48238: loss: 0.22622840106487274\n",
            "iteration 48239: loss: 0.22622843086719513\n",
            "iteration 48240: loss: 0.22622838616371155\n",
            "iteration 48241: loss: 0.22622835636138916\n",
            "iteration 48242: loss: 0.22622835636138916\n",
            "iteration 48243: loss: 0.22622835636138916\n",
            "iteration 48244: loss: 0.22622832655906677\n",
            "iteration 48245: loss: 0.226228266954422\n",
            "iteration 48246: loss: 0.226228266954422\n",
            "iteration 48247: loss: 0.2262282818555832\n",
            "iteration 48248: loss: 0.2262282371520996\n",
            "iteration 48249: loss: 0.22622820734977722\n",
            "iteration 48250: loss: 0.22622819244861603\n",
            "iteration 48251: loss: 0.22622816264629364\n",
            "iteration 48252: loss: 0.22622814774513245\n",
            "iteration 48253: loss: 0.22622811794281006\n",
            "iteration 48254: loss: 0.22622814774513245\n",
            "iteration 48255: loss: 0.22622808814048767\n",
            "iteration 48256: loss: 0.22622808814048767\n",
            "iteration 48257: loss: 0.22622808814048767\n",
            "iteration 48258: loss: 0.22622808814048767\n",
            "iteration 48259: loss: 0.2262280285358429\n",
            "iteration 48260: loss: 0.2262279987335205\n",
            "iteration 48261: loss: 0.2262279987335205\n",
            "iteration 48262: loss: 0.22622795403003693\n",
            "iteration 48263: loss: 0.22622796893119812\n",
            "iteration 48264: loss: 0.22622795403003693\n",
            "iteration 48265: loss: 0.22622790932655334\n",
            "iteration 48266: loss: 0.22622792422771454\n",
            "iteration 48267: loss: 0.22622787952423096\n",
            "iteration 48268: loss: 0.22622787952423096\n",
            "iteration 48269: loss: 0.22622787952423096\n",
            "iteration 48270: loss: 0.22622783482074738\n",
            "iteration 48271: loss: 0.226227805018425\n",
            "iteration 48272: loss: 0.226227805018425\n",
            "iteration 48273: loss: 0.226227805018425\n",
            "iteration 48274: loss: 0.22622771561145782\n",
            "iteration 48275: loss: 0.22622771561145782\n",
            "iteration 48276: loss: 0.22622771561145782\n",
            "iteration 48277: loss: 0.22622768580913544\n",
            "iteration 48278: loss: 0.22622767090797424\n",
            "iteration 48279: loss: 0.22622767090797424\n",
            "iteration 48280: loss: 0.22622767090797424\n",
            "iteration 48281: loss: 0.22622759640216827\n",
            "iteration 48282: loss: 0.22622767090797424\n",
            "iteration 48283: loss: 0.22622764110565186\n",
            "iteration 48284: loss: 0.2262275665998459\n",
            "iteration 48285: loss: 0.2262275218963623\n",
            "iteration 48286: loss: 0.2262275516986847\n",
            "iteration 48287: loss: 0.2262275665998459\n",
            "iteration 48288: loss: 0.22622749209403992\n",
            "iteration 48289: loss: 0.2262275218963623\n",
            "iteration 48290: loss: 0.22622747719287872\n",
            "iteration 48291: loss: 0.22622743248939514\n",
            "iteration 48292: loss: 0.22622743248939514\n",
            "iteration 48293: loss: 0.22622737288475037\n",
            "iteration 48294: loss: 0.22622743248939514\n",
            "iteration 48295: loss: 0.22622740268707275\n",
            "iteration 48296: loss: 0.22622732818126678\n",
            "iteration 48297: loss: 0.22622732818126678\n",
            "iteration 48298: loss: 0.2262273132801056\n",
            "iteration 48299: loss: 0.22622725367546082\n",
            "iteration 48300: loss: 0.22622732818126678\n",
            "iteration 48301: loss: 0.2262273132801056\n",
            "iteration 48302: loss: 0.2262272834777832\n",
            "iteration 48303: loss: 0.22622723877429962\n",
            "iteration 48304: loss: 0.22622719407081604\n",
            "iteration 48305: loss: 0.22622719407081604\n",
            "iteration 48306: loss: 0.22622716426849365\n",
            "iteration 48307: loss: 0.22622719407081604\n",
            "iteration 48308: loss: 0.22622716426849365\n",
            "iteration 48309: loss: 0.22622713446617126\n",
            "iteration 48310: loss: 0.22622708976268768\n",
            "iteration 48311: loss: 0.2262270748615265\n",
            "iteration 48312: loss: 0.2262270450592041\n",
            "iteration 48313: loss: 0.2262270450592041\n",
            "iteration 48314: loss: 0.2262270450592041\n",
            "iteration 48315: loss: 0.2262270450592041\n",
            "iteration 48316: loss: 0.22622700035572052\n",
            "iteration 48317: loss: 0.22622697055339813\n",
            "iteration 48318: loss: 0.22622697055339813\n",
            "iteration 48319: loss: 0.22622695565223694\n",
            "iteration 48320: loss: 0.22622692584991455\n",
            "iteration 48321: loss: 0.22622689604759216\n",
            "iteration 48322: loss: 0.22622688114643097\n",
            "iteration 48323: loss: 0.22622685134410858\n",
            "iteration 48324: loss: 0.22622685134410858\n",
            "iteration 48325: loss: 0.2262268364429474\n",
            "iteration 48326: loss: 0.22622685134410858\n",
            "iteration 48327: loss: 0.2262267768383026\n",
            "iteration 48328: loss: 0.226226806640625\n",
            "iteration 48329: loss: 0.2262267768383026\n",
            "iteration 48330: loss: 0.22622676193714142\n",
            "iteration 48331: loss: 0.22622673213481903\n",
            "iteration 48332: loss: 0.22622673213481903\n",
            "iteration 48333: loss: 0.22622671723365784\n",
            "iteration 48334: loss: 0.22622664272785187\n",
            "iteration 48335: loss: 0.22622665762901306\n",
            "iteration 48336: loss: 0.22622668743133545\n",
            "iteration 48337: loss: 0.22622661292552948\n",
            "iteration 48338: loss: 0.22622661292552948\n",
            "iteration 48339: loss: 0.2262265682220459\n",
            "iteration 48340: loss: 0.2262265682220459\n",
            "iteration 48341: loss: 0.2262265682220459\n",
            "iteration 48342: loss: 0.22622649371623993\n",
            "iteration 48343: loss: 0.2262265682220459\n",
            "iteration 48344: loss: 0.22622647881507874\n",
            "iteration 48345: loss: 0.22622649371623993\n",
            "iteration 48346: loss: 0.22622647881507874\n",
            "iteration 48347: loss: 0.22622644901275635\n",
            "iteration 48348: loss: 0.22622641921043396\n",
            "iteration 48349: loss: 0.22622640430927277\n",
            "iteration 48350: loss: 0.22622637450695038\n",
            "iteration 48351: loss: 0.22622635960578918\n",
            "iteration 48352: loss: 0.22622635960578918\n",
            "iteration 48353: loss: 0.2262263000011444\n",
            "iteration 48354: loss: 0.2262263000011444\n",
            "iteration 48355: loss: 0.2262263298034668\n",
            "iteration 48356: loss: 0.22622628509998322\n",
            "iteration 48357: loss: 0.22622624039649963\n",
            "iteration 48358: loss: 0.22622625529766083\n",
            "iteration 48359: loss: 0.22622621059417725\n",
            "iteration 48360: loss: 0.22622621059417725\n",
            "iteration 48361: loss: 0.22622618079185486\n",
            "iteration 48362: loss: 0.22622616589069366\n",
            "iteration 48363: loss: 0.22622618079185486\n",
            "iteration 48364: loss: 0.22622612118721008\n",
            "iteration 48365: loss: 0.22622612118721008\n",
            "iteration 48366: loss: 0.2262260913848877\n",
            "iteration 48367: loss: 0.2262260913848877\n",
            "iteration 48368: loss: 0.2262260615825653\n",
            "iteration 48369: loss: 0.2262260615825653\n",
            "iteration 48370: loss: 0.22622600197792053\n",
            "iteration 48371: loss: 0.22622600197792053\n",
            "iteration 48372: loss: 0.22622600197792053\n",
            "iteration 48373: loss: 0.22622600197792053\n",
            "iteration 48374: loss: 0.22622594237327576\n",
            "iteration 48375: loss: 0.22622594237327576\n",
            "iteration 48376: loss: 0.22622589766979218\n",
            "iteration 48377: loss: 0.22622589766979218\n",
            "iteration 48378: loss: 0.22622588276863098\n",
            "iteration 48379: loss: 0.226225808262825\n",
            "iteration 48380: loss: 0.2262258529663086\n",
            "iteration 48381: loss: 0.226225808262825\n",
            "iteration 48382: loss: 0.226225808262825\n",
            "iteration 48383: loss: 0.22622576355934143\n",
            "iteration 48384: loss: 0.22622576355934143\n",
            "iteration 48385: loss: 0.22622577846050262\n",
            "iteration 48386: loss: 0.22622576355934143\n",
            "iteration 48387: loss: 0.22622570395469666\n",
            "iteration 48388: loss: 0.22622570395469666\n",
            "iteration 48389: loss: 0.22622570395469666\n",
            "iteration 48390: loss: 0.22622568905353546\n",
            "iteration 48391: loss: 0.22622565925121307\n",
            "iteration 48392: loss: 0.22622564435005188\n",
            "iteration 48393: loss: 0.2262256145477295\n",
            "iteration 48394: loss: 0.2262256145477295\n",
            "iteration 48395: loss: 0.22622554004192352\n",
            "iteration 48396: loss: 0.22622549533843994\n",
            "iteration 48397: loss: 0.22622552514076233\n",
            "iteration 48398: loss: 0.22622552514076233\n",
            "iteration 48399: loss: 0.22622552514076233\n",
            "iteration 48400: loss: 0.22622546553611755\n",
            "iteration 48401: loss: 0.22622545063495636\n",
            "iteration 48402: loss: 0.22622542083263397\n",
            "iteration 48403: loss: 0.22622545063495636\n",
            "iteration 48404: loss: 0.2262253761291504\n",
            "iteration 48405: loss: 0.22622542083263397\n",
            "iteration 48406: loss: 0.2262253761291504\n",
            "iteration 48407: loss: 0.22622540593147278\n",
            "iteration 48408: loss: 0.2262253314256668\n",
            "iteration 48409: loss: 0.226225346326828\n",
            "iteration 48410: loss: 0.22622528672218323\n",
            "iteration 48411: loss: 0.22622528672218323\n",
            "iteration 48412: loss: 0.22622528672218323\n",
            "iteration 48413: loss: 0.22622525691986084\n",
            "iteration 48414: loss: 0.22622528672218323\n",
            "iteration 48415: loss: 0.22622522711753845\n",
            "iteration 48416: loss: 0.22622522711753845\n",
            "iteration 48417: loss: 0.22622521221637726\n",
            "iteration 48418: loss: 0.22622516751289368\n",
            "iteration 48419: loss: 0.2262251377105713\n",
            "iteration 48420: loss: 0.2262251079082489\n",
            "iteration 48421: loss: 0.22622501850128174\n",
            "iteration 48422: loss: 0.22622506320476532\n",
            "iteration 48423: loss: 0.22622504830360413\n",
            "iteration 48424: loss: 0.22622504830360413\n",
            "iteration 48425: loss: 0.22622504830360413\n",
            "iteration 48426: loss: 0.22622501850128174\n",
            "iteration 48427: loss: 0.22622497379779816\n",
            "iteration 48428: loss: 0.22622498869895935\n",
            "iteration 48429: loss: 0.22622498869895935\n",
            "iteration 48430: loss: 0.22622497379779816\n",
            "iteration 48431: loss: 0.22622492909431458\n",
            "iteration 48432: loss: 0.22622492909431458\n",
            "iteration 48433: loss: 0.2262248992919922\n",
            "iteration 48434: loss: 0.2262248992919922\n",
            "iteration 48435: loss: 0.22622482478618622\n",
            "iteration 48436: loss: 0.22622482478618622\n",
            "iteration 48437: loss: 0.22622482478618622\n",
            "iteration 48438: loss: 0.22622480988502502\n",
            "iteration 48439: loss: 0.22622480988502502\n",
            "iteration 48440: loss: 0.22622475028038025\n",
            "iteration 48441: loss: 0.22622475028038025\n",
            "iteration 48442: loss: 0.22622475028038025\n",
            "iteration 48443: loss: 0.22622473537921906\n",
            "iteration 48444: loss: 0.22622470557689667\n",
            "iteration 48445: loss: 0.2262246310710907\n",
            "iteration 48446: loss: 0.22622469067573547\n",
            "iteration 48447: loss: 0.22622466087341309\n",
            "iteration 48448: loss: 0.22622466087341309\n",
            "iteration 48449: loss: 0.22622457146644592\n",
            "iteration 48450: loss: 0.2262246310710907\n",
            "iteration 48451: loss: 0.22622454166412354\n",
            "iteration 48452: loss: 0.22622454166412354\n",
            "iteration 48453: loss: 0.22622454166412354\n",
            "iteration 48454: loss: 0.22622449696063995\n",
            "iteration 48455: loss: 0.22622449696063995\n",
            "iteration 48456: loss: 0.22622446715831757\n",
            "iteration 48457: loss: 0.22622446715831757\n",
            "iteration 48458: loss: 0.22622446715831757\n",
            "iteration 48459: loss: 0.22622442245483398\n",
            "iteration 48460: loss: 0.2262243926525116\n",
            "iteration 48461: loss: 0.2262243777513504\n",
            "iteration 48462: loss: 0.2262243926525116\n",
            "iteration 48463: loss: 0.2262243777513504\n",
            "iteration 48464: loss: 0.22622433304786682\n",
            "iteration 48465: loss: 0.22622433304786682\n",
            "iteration 48466: loss: 0.22622433304786682\n",
            "iteration 48467: loss: 0.22622427344322205\n",
            "iteration 48468: loss: 0.22622430324554443\n",
            "iteration 48469: loss: 0.22622425854206085\n",
            "iteration 48470: loss: 0.22622421383857727\n",
            "iteration 48471: loss: 0.22622421383857727\n",
            "iteration 48472: loss: 0.22622422873973846\n",
            "iteration 48473: loss: 0.22622421383857727\n",
            "iteration 48474: loss: 0.22622418403625488\n",
            "iteration 48475: loss: 0.2262241095304489\n",
            "iteration 48476: loss: 0.22622409462928772\n",
            "iteration 48477: loss: 0.22622409462928772\n",
            "iteration 48478: loss: 0.22622409462928772\n",
            "iteration 48479: loss: 0.22622409462928772\n",
            "iteration 48480: loss: 0.22622402012348175\n",
            "iteration 48481: loss: 0.22622402012348175\n",
            "iteration 48482: loss: 0.22622402012348175\n",
            "iteration 48483: loss: 0.22622399032115936\n",
            "iteration 48484: loss: 0.22622399032115936\n",
            "iteration 48485: loss: 0.2262239158153534\n",
            "iteration 48486: loss: 0.2262238711118698\n",
            "iteration 48487: loss: 0.2262239158153534\n",
            "iteration 48488: loss: 0.2262239158153534\n",
            "iteration 48489: loss: 0.2262239009141922\n",
            "iteration 48490: loss: 0.2262238711118698\n",
            "iteration 48491: loss: 0.2262238711118698\n",
            "iteration 48492: loss: 0.22622385621070862\n",
            "iteration 48493: loss: 0.22622379660606384\n",
            "iteration 48494: loss: 0.22622379660606384\n",
            "iteration 48495: loss: 0.22622379660606384\n",
            "iteration 48496: loss: 0.22622375190258026\n",
            "iteration 48497: loss: 0.22622375190258026\n",
            "iteration 48498: loss: 0.2262236624956131\n",
            "iteration 48499: loss: 0.22622373700141907\n",
            "iteration 48500: loss: 0.2262236326932907\n",
            "iteration 48501: loss: 0.2262236326932907\n",
            "iteration 48502: loss: 0.2262236624956131\n",
            "iteration 48503: loss: 0.2262236326932907\n",
            "iteration 48504: loss: 0.22622361779212952\n",
            "iteration 48505: loss: 0.22622358798980713\n",
            "iteration 48506: loss: 0.22622361779212952\n",
            "iteration 48507: loss: 0.22622354328632355\n",
            "iteration 48508: loss: 0.22622355818748474\n",
            "iteration 48509: loss: 0.22622351348400116\n",
            "iteration 48510: loss: 0.22622349858283997\n",
            "iteration 48511: loss: 0.22622349858283997\n",
            "iteration 48512: loss: 0.22622346878051758\n",
            "iteration 48513: loss: 0.2262234389781952\n",
            "iteration 48514: loss: 0.22622346878051758\n",
            "iteration 48515: loss: 0.2262233942747116\n",
            "iteration 48516: loss: 0.226223424077034\n",
            "iteration 48517: loss: 0.22622337937355042\n",
            "iteration 48518: loss: 0.22622334957122803\n",
            "iteration 48519: loss: 0.22622330486774445\n",
            "iteration 48520: loss: 0.22622331976890564\n",
            "iteration 48521: loss: 0.22622330486774445\n",
            "iteration 48522: loss: 0.22622327506542206\n",
            "iteration 48523: loss: 0.22622330486774445\n",
            "iteration 48524: loss: 0.22622323036193848\n",
            "iteration 48525: loss: 0.22622326016426086\n",
            "iteration 48526: loss: 0.22622323036193848\n",
            "iteration 48527: loss: 0.2262232005596161\n",
            "iteration 48528: loss: 0.2262231856584549\n",
            "iteration 48529: loss: 0.2262231558561325\n",
            "iteration 48530: loss: 0.2262231409549713\n",
            "iteration 48531: loss: 0.22622311115264893\n",
            "iteration 48532: loss: 0.2262231409549713\n",
            "iteration 48533: loss: 0.2262231409549713\n",
            "iteration 48534: loss: 0.22622303664684296\n",
            "iteration 48535: loss: 0.22622308135032654\n",
            "iteration 48536: loss: 0.22622306644916534\n",
            "iteration 48537: loss: 0.22622299194335938\n",
            "iteration 48538: loss: 0.22622303664684296\n",
            "iteration 48539: loss: 0.22622299194335938\n",
            "iteration 48540: loss: 0.2262229472398758\n",
            "iteration 48541: loss: 0.2262229472398758\n",
            "iteration 48542: loss: 0.2262229025363922\n",
            "iteration 48543: loss: 0.2262229025363922\n",
            "iteration 48544: loss: 0.2262229025363922\n",
            "iteration 48545: loss: 0.22622287273406982\n",
            "iteration 48546: loss: 0.22622287273406982\n",
            "iteration 48547: loss: 0.22622284293174744\n",
            "iteration 48548: loss: 0.22622282803058624\n",
            "iteration 48549: loss: 0.22622279822826385\n",
            "iteration 48550: loss: 0.22622278332710266\n",
            "iteration 48551: loss: 0.22622272372245789\n",
            "iteration 48552: loss: 0.22622275352478027\n",
            "iteration 48553: loss: 0.2262227088212967\n",
            "iteration 48554: loss: 0.2262227088212967\n",
            "iteration 48555: loss: 0.2262226641178131\n",
            "iteration 48556: loss: 0.2262226790189743\n",
            "iteration 48557: loss: 0.2262226641178131\n",
            "iteration 48558: loss: 0.2262226641178131\n",
            "iteration 48559: loss: 0.2262226641178131\n",
            "iteration 48560: loss: 0.22622263431549072\n",
            "iteration 48561: loss: 0.22622258961200714\n",
            "iteration 48562: loss: 0.22622260451316833\n",
            "iteration 48563: loss: 0.22622255980968475\n",
            "iteration 48564: loss: 0.22622255980968475\n",
            "iteration 48565: loss: 0.22622251510620117\n",
            "iteration 48566: loss: 0.2262224704027176\n",
            "iteration 48567: loss: 0.2262224704027176\n",
            "iteration 48568: loss: 0.22622248530387878\n",
            "iteration 48569: loss: 0.2262224704027176\n",
            "iteration 48570: loss: 0.2262224704027176\n",
            "iteration 48571: loss: 0.226222425699234\n",
            "iteration 48572: loss: 0.2262224406003952\n",
            "iteration 48573: loss: 0.22622239589691162\n",
            "iteration 48574: loss: 0.22622236609458923\n",
            "iteration 48575: loss: 0.22622235119342804\n",
            "iteration 48576: loss: 0.22622230648994446\n",
            "iteration 48577: loss: 0.22622230648994446\n",
            "iteration 48578: loss: 0.22622230648994446\n",
            "iteration 48579: loss: 0.22622227668762207\n",
            "iteration 48580: loss: 0.22622224688529968\n",
            "iteration 48581: loss: 0.2262222319841385\n",
            "iteration 48582: loss: 0.2262222319841385\n",
            "iteration 48583: loss: 0.2262221872806549\n",
            "iteration 48584: loss: 0.22622215747833252\n",
            "iteration 48585: loss: 0.22622215747833252\n",
            "iteration 48586: loss: 0.22622212767601013\n",
            "iteration 48587: loss: 0.22622211277484894\n",
            "iteration 48588: loss: 0.22622211277484894\n",
            "iteration 48589: loss: 0.22622212767601013\n",
            "iteration 48590: loss: 0.22622206807136536\n",
            "iteration 48591: loss: 0.22622203826904297\n",
            "iteration 48592: loss: 0.22622206807136536\n",
            "iteration 48593: loss: 0.22622203826904297\n",
            "iteration 48594: loss: 0.2262219488620758\n",
            "iteration 48595: loss: 0.2262219488620758\n",
            "iteration 48596: loss: 0.2262219488620758\n",
            "iteration 48597: loss: 0.22622191905975342\n",
            "iteration 48598: loss: 0.22622191905975342\n",
            "iteration 48599: loss: 0.22622188925743103\n",
            "iteration 48600: loss: 0.22622191905975342\n",
            "iteration 48601: loss: 0.22622188925743103\n",
            "iteration 48602: loss: 0.22622184455394745\n",
            "iteration 48603: loss: 0.22622188925743103\n",
            "iteration 48604: loss: 0.22622184455394745\n",
            "iteration 48605: loss: 0.22622182965278625\n",
            "iteration 48606: loss: 0.22622182965278625\n",
            "iteration 48607: loss: 0.22622175514698029\n",
            "iteration 48608: loss: 0.2262217104434967\n",
            "iteration 48609: loss: 0.2262217104434967\n",
            "iteration 48610: loss: 0.22622175514698029\n",
            "iteration 48611: loss: 0.2262217253446579\n",
            "iteration 48612: loss: 0.2262217104434967\n",
            "iteration 48613: loss: 0.2262217104434967\n",
            "iteration 48614: loss: 0.22622165083885193\n",
            "iteration 48615: loss: 0.22622159123420715\n",
            "iteration 48616: loss: 0.22622160613536835\n",
            "iteration 48617: loss: 0.22622156143188477\n",
            "iteration 48618: loss: 0.22622159123420715\n",
            "iteration 48619: loss: 0.22622159123420715\n",
            "iteration 48620: loss: 0.22622159123420715\n",
            "iteration 48621: loss: 0.2262214869260788\n",
            "iteration 48622: loss: 0.2262214869260788\n",
            "iteration 48623: loss: 0.22622151672840118\n",
            "iteration 48624: loss: 0.2262214720249176\n",
            "iteration 48625: loss: 0.22622141242027283\n",
            "iteration 48626: loss: 0.22622141242027283\n",
            "iteration 48627: loss: 0.22622139751911163\n",
            "iteration 48628: loss: 0.22622144222259521\n",
            "iteration 48629: loss: 0.22622136771678925\n",
            "iteration 48630: loss: 0.22622136771678925\n",
            "iteration 48631: loss: 0.22622139751911163\n",
            "iteration 48632: loss: 0.22622136771678925\n",
            "iteration 48633: loss: 0.22622129321098328\n",
            "iteration 48634: loss: 0.2262212485074997\n",
            "iteration 48635: loss: 0.22622127830982208\n",
            "iteration 48636: loss: 0.22622129321098328\n",
            "iteration 48637: loss: 0.2262212038040161\n",
            "iteration 48638: loss: 0.2262212038040161\n",
            "iteration 48639: loss: 0.22622117400169373\n",
            "iteration 48640: loss: 0.2262212038040161\n",
            "iteration 48641: loss: 0.22622115910053253\n",
            "iteration 48642: loss: 0.22622115910053253\n",
            "iteration 48643: loss: 0.22622112929821014\n",
            "iteration 48644: loss: 0.22622112929821014\n",
            "iteration 48645: loss: 0.22622108459472656\n",
            "iteration 48646: loss: 0.22622105479240417\n",
            "iteration 48647: loss: 0.22622108459472656\n",
            "iteration 48648: loss: 0.22622103989124298\n",
            "iteration 48649: loss: 0.2262210100889206\n",
            "iteration 48650: loss: 0.226220965385437\n",
            "iteration 48651: loss: 0.2262209951877594\n",
            "iteration 48652: loss: 0.226220965385437\n",
            "iteration 48653: loss: 0.2262209951877594\n",
            "iteration 48654: loss: 0.22622093558311462\n",
            "iteration 48655: loss: 0.22622093558311462\n",
            "iteration 48656: loss: 0.22622092068195343\n",
            "iteration 48657: loss: 0.22622089087963104\n",
            "iteration 48658: loss: 0.22622084617614746\n",
            "iteration 48659: loss: 0.22622080147266388\n",
            "iteration 48660: loss: 0.22622084617614746\n",
            "iteration 48661: loss: 0.2262207567691803\n",
            "iteration 48662: loss: 0.22622080147266388\n",
            "iteration 48663: loss: 0.2262207716703415\n",
            "iteration 48664: loss: 0.22622069716453552\n",
            "iteration 48665: loss: 0.2262207567691803\n",
            "iteration 48666: loss: 0.22622069716453552\n",
            "iteration 48667: loss: 0.22622069716453552\n",
            "iteration 48668: loss: 0.22622068226337433\n",
            "iteration 48669: loss: 0.22622065246105194\n",
            "iteration 48670: loss: 0.22622060775756836\n",
            "iteration 48671: loss: 0.22622063755989075\n",
            "iteration 48672: loss: 0.22622060775756836\n",
            "iteration 48673: loss: 0.22622063755989075\n",
            "iteration 48674: loss: 0.2262205332517624\n",
            "iteration 48675: loss: 0.2262205332517624\n",
            "iteration 48676: loss: 0.2262205332517624\n",
            "iteration 48677: loss: 0.2262205183506012\n",
            "iteration 48678: loss: 0.2262205183506012\n",
            "iteration 48679: loss: 0.22622044384479523\n",
            "iteration 48680: loss: 0.2262205183506012\n",
            "iteration 48681: loss: 0.22622044384479523\n",
            "iteration 48682: loss: 0.22622045874595642\n",
            "iteration 48683: loss: 0.22622044384479523\n",
            "iteration 48684: loss: 0.22622041404247284\n",
            "iteration 48685: loss: 0.22622036933898926\n",
            "iteration 48686: loss: 0.22622032463550568\n",
            "iteration 48687: loss: 0.22622032463550568\n",
            "iteration 48688: loss: 0.22622032463550568\n",
            "iteration 48689: loss: 0.22622032463550568\n",
            "iteration 48690: loss: 0.2262202501296997\n",
            "iteration 48691: loss: 0.2262202799320221\n",
            "iteration 48692: loss: 0.2262202501296997\n",
            "iteration 48693: loss: 0.2262202799320221\n",
            "iteration 48694: loss: 0.22622022032737732\n",
            "iteration 48695: loss: 0.22622020542621613\n",
            "iteration 48696: loss: 0.22622016072273254\n",
            "iteration 48697: loss: 0.22622022032737732\n",
            "iteration 48698: loss: 0.22622013092041016\n",
            "iteration 48699: loss: 0.22622013092041016\n",
            "iteration 48700: loss: 0.22622013092041016\n",
            "iteration 48701: loss: 0.22622008621692657\n",
            "iteration 48702: loss: 0.22622008621692657\n",
            "iteration 48703: loss: 0.2262200564146042\n",
            "iteration 48704: loss: 0.226220041513443\n",
            "iteration 48705: loss: 0.226220041513443\n",
            "iteration 48706: loss: 0.2262200117111206\n",
            "iteration 48707: loss: 0.22621996700763702\n",
            "iteration 48708: loss: 0.22621998190879822\n",
            "iteration 48709: loss: 0.22621993720531464\n",
            "iteration 48710: loss: 0.22621989250183105\n",
            "iteration 48711: loss: 0.22621992230415344\n",
            "iteration 48712: loss: 0.22621989250183105\n",
            "iteration 48713: loss: 0.22621989250183105\n",
            "iteration 48714: loss: 0.22621981799602509\n",
            "iteration 48715: loss: 0.22621981799602509\n",
            "iteration 48716: loss: 0.2262198030948639\n",
            "iteration 48717: loss: 0.22621981799602509\n",
            "iteration 48718: loss: 0.22621981799602509\n",
            "iteration 48719: loss: 0.2262197732925415\n",
            "iteration 48720: loss: 0.22621972858905792\n",
            "iteration 48721: loss: 0.22621974349021912\n",
            "iteration 48722: loss: 0.22621969878673553\n",
            "iteration 48723: loss: 0.22621968388557434\n",
            "iteration 48724: loss: 0.22621974349021912\n",
            "iteration 48725: loss: 0.22621960937976837\n",
            "iteration 48726: loss: 0.22621969878673553\n",
            "iteration 48727: loss: 0.22621960937976837\n",
            "iteration 48728: loss: 0.22621960937976837\n",
            "iteration 48729: loss: 0.22621960937976837\n",
            "iteration 48730: loss: 0.22621957957744598\n",
            "iteration 48731: loss: 0.2262195646762848\n",
            "iteration 48732: loss: 0.2262195646762848\n",
            "iteration 48733: loss: 0.22621950507164001\n",
            "iteration 48734: loss: 0.22621950507164001\n",
            "iteration 48735: loss: 0.22621950507164001\n",
            "iteration 48736: loss: 0.22621950507164001\n",
            "iteration 48737: loss: 0.22621944546699524\n",
            "iteration 48738: loss: 0.22621944546699524\n",
            "iteration 48739: loss: 0.22621938586235046\n",
            "iteration 48740: loss: 0.22621941566467285\n",
            "iteration 48741: loss: 0.22621941566467285\n",
            "iteration 48742: loss: 0.22621934115886688\n",
            "iteration 48743: loss: 0.2262193262577057\n",
            "iteration 48744: loss: 0.22621934115886688\n",
            "iteration 48745: loss: 0.22621934115886688\n",
            "iteration 48746: loss: 0.2262192666530609\n",
            "iteration 48747: loss: 0.2262193262577057\n",
            "iteration 48748: loss: 0.2262192666530609\n",
            "iteration 48749: loss: 0.22621925175189972\n",
            "iteration 48750: loss: 0.22621922194957733\n",
            "iteration 48751: loss: 0.22621925175189972\n",
            "iteration 48752: loss: 0.22621917724609375\n",
            "iteration 48753: loss: 0.22621914744377136\n",
            "iteration 48754: loss: 0.22621910274028778\n",
            "iteration 48755: loss: 0.22621913254261017\n",
            "iteration 48756: loss: 0.22621910274028778\n",
            "iteration 48757: loss: 0.2262190878391266\n",
            "iteration 48758: loss: 0.2262190580368042\n",
            "iteration 48759: loss: 0.2262190580368042\n",
            "iteration 48760: loss: 0.2262190282344818\n",
            "iteration 48761: loss: 0.22621901333332062\n",
            "iteration 48762: loss: 0.2262190282344818\n",
            "iteration 48763: loss: 0.2262190282344818\n",
            "iteration 48764: loss: 0.22621898353099823\n",
            "iteration 48765: loss: 0.22621893882751465\n",
            "iteration 48766: loss: 0.22621893882751465\n",
            "iteration 48767: loss: 0.22621889412403107\n",
            "iteration 48768: loss: 0.22621890902519226\n",
            "iteration 48769: loss: 0.22621886432170868\n",
            "iteration 48770: loss: 0.22621889412403107\n",
            "iteration 48771: loss: 0.22621884942054749\n",
            "iteration 48772: loss: 0.22621884942054749\n",
            "iteration 48773: loss: 0.2262188196182251\n",
            "iteration 48774: loss: 0.2262187898159027\n",
            "iteration 48775: loss: 0.2262188196182251\n",
            "iteration 48776: loss: 0.2262187898159027\n",
            "iteration 48777: loss: 0.22621870040893555\n",
            "iteration 48778: loss: 0.22621874511241913\n",
            "iteration 48779: loss: 0.22621870040893555\n",
            "iteration 48780: loss: 0.22621862590312958\n",
            "iteration 48781: loss: 0.22621867060661316\n",
            "iteration 48782: loss: 0.22621862590312958\n",
            "iteration 48783: loss: 0.22621862590312958\n",
            "iteration 48784: loss: 0.22621865570545197\n",
            "iteration 48785: loss: 0.22621853649616241\n",
            "iteration 48786: loss: 0.22621853649616241\n",
            "iteration 48787: loss: 0.22621850669384003\n",
            "iteration 48788: loss: 0.22621853649616241\n",
            "iteration 48789: loss: 0.2262185513973236\n",
            "iteration 48790: loss: 0.22621850669384003\n",
            "iteration 48791: loss: 0.22621849179267883\n",
            "iteration 48792: loss: 0.22621849179267883\n",
            "iteration 48793: loss: 0.22621846199035645\n",
            "iteration 48794: loss: 0.22621841728687286\n",
            "iteration 48795: loss: 0.22621838748455048\n",
            "iteration 48796: loss: 0.22621838748455048\n",
            "iteration 48797: loss: 0.22621838748455048\n",
            "iteration 48798: loss: 0.22621837258338928\n",
            "iteration 48799: loss: 0.2262183427810669\n",
            "iteration 48800: loss: 0.2262183427810669\n",
            "iteration 48801: loss: 0.2262183129787445\n",
            "iteration 48802: loss: 0.2262183129787445\n",
            "iteration 48803: loss: 0.22621826827526093\n",
            "iteration 48804: loss: 0.22621826827526093\n",
            "iteration 48805: loss: 0.22621825337409973\n",
            "iteration 48806: loss: 0.22621822357177734\n",
            "iteration 48807: loss: 0.22621819376945496\n",
            "iteration 48808: loss: 0.22621817886829376\n",
            "iteration 48809: loss: 0.22621817886829376\n",
            "iteration 48810: loss: 0.22621817886829376\n",
            "iteration 48811: loss: 0.22621814906597137\n",
            "iteration 48812: loss: 0.22621813416481018\n",
            "iteration 48813: loss: 0.22621813416481018\n",
            "iteration 48814: loss: 0.2262180745601654\n",
            "iteration 48815: loss: 0.2262180745601654\n",
            "iteration 48816: loss: 0.22621802985668182\n",
            "iteration 48817: loss: 0.22621802985668182\n",
            "iteration 48818: loss: 0.22621801495552063\n",
            "iteration 48819: loss: 0.22621801495552063\n",
            "iteration 48820: loss: 0.22621798515319824\n",
            "iteration 48821: loss: 0.22621794044971466\n",
            "iteration 48822: loss: 0.22621794044971466\n",
            "iteration 48823: loss: 0.22621794044971466\n",
            "iteration 48824: loss: 0.22621791064739227\n",
            "iteration 48825: loss: 0.2262178659439087\n",
            "iteration 48826: loss: 0.2262178361415863\n",
            "iteration 48827: loss: 0.2262178361415863\n",
            "iteration 48828: loss: 0.2262178361415863\n",
            "iteration 48829: loss: 0.22621777653694153\n",
            "iteration 48830: loss: 0.22621779143810272\n",
            "iteration 48831: loss: 0.22621779143810272\n",
            "iteration 48832: loss: 0.22621777653694153\n",
            "iteration 48833: loss: 0.22621774673461914\n",
            "iteration 48834: loss: 0.22621774673461914\n",
            "iteration 48835: loss: 0.22621771693229675\n",
            "iteration 48836: loss: 0.22621770203113556\n",
            "iteration 48837: loss: 0.22621765732765198\n",
            "iteration 48838: loss: 0.22621767222881317\n",
            "iteration 48839: loss: 0.22621765732765198\n",
            "iteration 48840: loss: 0.226217582821846\n",
            "iteration 48841: loss: 0.226217582821846\n",
            "iteration 48842: loss: 0.2262175977230072\n",
            "iteration 48843: loss: 0.22621753811836243\n",
            "iteration 48844: loss: 0.226217582821846\n",
            "iteration 48845: loss: 0.22621755301952362\n",
            "iteration 48846: loss: 0.22621753811836243\n",
            "iteration 48847: loss: 0.22621747851371765\n",
            "iteration 48848: loss: 0.22621746361255646\n",
            "iteration 48849: loss: 0.22621746361255646\n",
            "iteration 48850: loss: 0.22621746361255646\n",
            "iteration 48851: loss: 0.22621741890907288\n",
            "iteration 48852: loss: 0.22621741890907288\n",
            "iteration 48853: loss: 0.22621741890907288\n",
            "iteration 48854: loss: 0.2262173444032669\n",
            "iteration 48855: loss: 0.2262173593044281\n",
            "iteration 48856: loss: 0.22621731460094452\n",
            "iteration 48857: loss: 0.22621731460094452\n",
            "iteration 48858: loss: 0.22621729969978333\n",
            "iteration 48859: loss: 0.22621729969978333\n",
            "iteration 48860: loss: 0.22621724009513855\n",
            "iteration 48861: loss: 0.22621722519397736\n",
            "iteration 48862: loss: 0.22621724009513855\n",
            "iteration 48863: loss: 0.22621722519397736\n",
            "iteration 48864: loss: 0.22621718049049377\n",
            "iteration 48865: loss: 0.22621719539165497\n",
            "iteration 48866: loss: 0.2262171059846878\n",
            "iteration 48867: loss: 0.226217120885849\n",
            "iteration 48868: loss: 0.22621718049049377\n",
            "iteration 48869: loss: 0.22621707618236542\n",
            "iteration 48870: loss: 0.22621706128120422\n",
            "iteration 48871: loss: 0.22621707618236542\n",
            "iteration 48872: loss: 0.22621703147888184\n",
            "iteration 48873: loss: 0.22621700167655945\n",
            "iteration 48874: loss: 0.22621700167655945\n",
            "iteration 48875: loss: 0.22621695697307587\n",
            "iteration 48876: loss: 0.22621698677539825\n",
            "iteration 48877: loss: 0.22621700167655945\n",
            "iteration 48878: loss: 0.2262168824672699\n",
            "iteration 48879: loss: 0.2262168824672699\n",
            "iteration 48880: loss: 0.22621694207191467\n",
            "iteration 48881: loss: 0.2262168824672699\n",
            "iteration 48882: loss: 0.2262168675661087\n",
            "iteration 48883: loss: 0.22621683776378632\n",
            "iteration 48884: loss: 0.22621682286262512\n",
            "iteration 48885: loss: 0.22621683776378632\n",
            "iteration 48886: loss: 0.22621682286262512\n",
            "iteration 48887: loss: 0.22621682286262512\n",
            "iteration 48888: loss: 0.22621676325798035\n",
            "iteration 48889: loss: 0.22621674835681915\n",
            "iteration 48890: loss: 0.22621670365333557\n",
            "iteration 48891: loss: 0.22621676325798035\n",
            "iteration 48892: loss: 0.22621671855449677\n",
            "iteration 48893: loss: 0.22621670365333557\n",
            "iteration 48894: loss: 0.22621670365333557\n",
            "iteration 48895: loss: 0.2262166440486908\n",
            "iteration 48896: loss: 0.2262166291475296\n",
            "iteration 48897: loss: 0.22621659934520721\n",
            "iteration 48898: loss: 0.22621655464172363\n",
            "iteration 48899: loss: 0.22621659934520721\n",
            "iteration 48900: loss: 0.22621652483940125\n",
            "iteration 48901: loss: 0.22621652483940125\n",
            "iteration 48902: loss: 0.22621650993824005\n",
            "iteration 48903: loss: 0.22621648013591766\n",
            "iteration 48904: loss: 0.22621652483940125\n",
            "iteration 48905: loss: 0.22621648013591766\n",
            "iteration 48906: loss: 0.22621646523475647\n",
            "iteration 48907: loss: 0.2262164056301117\n",
            "iteration 48908: loss: 0.2262164056301117\n",
            "iteration 48909: loss: 0.2262163609266281\n",
            "iteration 48910: loss: 0.2262163907289505\n",
            "iteration 48911: loss: 0.2262163609266281\n",
            "iteration 48912: loss: 0.22621631622314453\n",
            "iteration 48913: loss: 0.22621631622314453\n",
            "iteration 48914: loss: 0.22621628642082214\n",
            "iteration 48915: loss: 0.22621628642082214\n",
            "iteration 48916: loss: 0.22621627151966095\n",
            "iteration 48917: loss: 0.22621631622314453\n",
            "iteration 48918: loss: 0.22621627151966095\n",
            "iteration 48919: loss: 0.22621622681617737\n",
            "iteration 48920: loss: 0.22621619701385498\n",
            "iteration 48921: loss: 0.2262161672115326\n",
            "iteration 48922: loss: 0.22621622681617737\n",
            "iteration 48923: loss: 0.22621619701385498\n",
            "iteration 48924: loss: 0.2262161523103714\n",
            "iteration 48925: loss: 0.2262161523103714\n",
            "iteration 48926: loss: 0.226216122508049\n",
            "iteration 48927: loss: 0.22621610760688782\n",
            "iteration 48928: loss: 0.22621604800224304\n",
            "iteration 48929: loss: 0.22621607780456543\n",
            "iteration 48930: loss: 0.22621603310108185\n",
            "iteration 48931: loss: 0.22621603310108185\n",
            "iteration 48932: loss: 0.22621595859527588\n",
            "iteration 48933: loss: 0.22621595859527588\n",
            "iteration 48934: loss: 0.22621598839759827\n",
            "iteration 48935: loss: 0.2262159287929535\n",
            "iteration 48936: loss: 0.2262159287929535\n",
            "iteration 48937: loss: 0.2262158840894699\n",
            "iteration 48938: loss: 0.2262158840894699\n",
            "iteration 48939: loss: 0.22621586918830872\n",
            "iteration 48940: loss: 0.22621586918830872\n",
            "iteration 48941: loss: 0.22621583938598633\n",
            "iteration 48942: loss: 0.22621576488018036\n",
            "iteration 48943: loss: 0.22621580958366394\n",
            "iteration 48944: loss: 0.22621579468250275\n",
            "iteration 48945: loss: 0.22621574997901917\n",
            "iteration 48946: loss: 0.22621579468250275\n",
            "iteration 48947: loss: 0.22621574997901917\n",
            "iteration 48948: loss: 0.2262156903743744\n",
            "iteration 48949: loss: 0.2262156754732132\n",
            "iteration 48950: loss: 0.2262156903743744\n",
            "iteration 48951: loss: 0.2262156456708908\n",
            "iteration 48952: loss: 0.2262156754732132\n",
            "iteration 48953: loss: 0.2262156456708908\n",
            "iteration 48954: loss: 0.22621560096740723\n",
            "iteration 48955: loss: 0.22621560096740723\n",
            "iteration 48956: loss: 0.22621560096740723\n",
            "iteration 48957: loss: 0.22621555626392365\n",
            "iteration 48958: loss: 0.22621555626392365\n",
            "iteration 48959: loss: 0.22621552646160126\n",
            "iteration 48960: loss: 0.22621551156044006\n",
            "iteration 48961: loss: 0.2262154519557953\n",
            "iteration 48962: loss: 0.2262154072523117\n",
            "iteration 48963: loss: 0.2262154519557953\n",
            "iteration 48964: loss: 0.2262154519557953\n",
            "iteration 48965: loss: 0.2262154370546341\n",
            "iteration 48966: loss: 0.2262154072523117\n",
            "iteration 48967: loss: 0.2262153923511505\n",
            "iteration 48968: loss: 0.2262153923511505\n",
            "iteration 48969: loss: 0.22621533274650574\n",
            "iteration 48970: loss: 0.22621531784534454\n",
            "iteration 48971: loss: 0.22621531784534454\n",
            "iteration 48972: loss: 0.22621528804302216\n",
            "iteration 48973: loss: 0.22621527314186096\n",
            "iteration 48974: loss: 0.2262152135372162\n",
            "iteration 48975: loss: 0.22621524333953857\n",
            "iteration 48976: loss: 0.2262152135372162\n",
            "iteration 48977: loss: 0.226215198636055\n",
            "iteration 48978: loss: 0.2262151539325714\n",
            "iteration 48979: loss: 0.22621512413024902\n",
            "iteration 48980: loss: 0.2262151539325714\n",
            "iteration 48981: loss: 0.22621512413024902\n",
            "iteration 48982: loss: 0.22621509432792664\n",
            "iteration 48983: loss: 0.22621512413024902\n",
            "iteration 48984: loss: 0.22621507942676544\n",
            "iteration 48985: loss: 0.22621504962444305\n",
            "iteration 48986: loss: 0.22621503472328186\n",
            "iteration 48987: loss: 0.22621503472328186\n",
            "iteration 48988: loss: 0.22621500492095947\n",
            "iteration 48989: loss: 0.22621500492095947\n",
            "iteration 48990: loss: 0.22621497511863708\n",
            "iteration 48991: loss: 0.22621500492095947\n",
            "iteration 48992: loss: 0.2262149155139923\n",
            "iteration 48993: loss: 0.2262149304151535\n",
            "iteration 48994: loss: 0.2262149155139923\n",
            "iteration 48995: loss: 0.22621488571166992\n",
            "iteration 48996: loss: 0.22621485590934753\n",
            "iteration 48997: loss: 0.22621484100818634\n",
            "iteration 48998: loss: 0.22621481120586395\n",
            "iteration 48999: loss: 0.22621484100818634\n",
            "iteration 49000: loss: 0.22621479630470276\n",
            "iteration 49001: loss: 0.22621476650238037\n",
            "iteration 49002: loss: 0.22621479630470276\n",
            "iteration 49003: loss: 0.22621479630470276\n",
            "iteration 49004: loss: 0.22621473670005798\n",
            "iteration 49005: loss: 0.2262146919965744\n",
            "iteration 49006: loss: 0.2262147217988968\n",
            "iteration 49007: loss: 0.2262147217988968\n",
            "iteration 49008: loss: 0.2262146919965744\n",
            "iteration 49009: loss: 0.22621461749076843\n",
            "iteration 49010: loss: 0.22621464729309082\n",
            "iteration 49011: loss: 0.22621460258960724\n",
            "iteration 49012: loss: 0.22621455788612366\n",
            "iteration 49013: loss: 0.22621457278728485\n",
            "iteration 49014: loss: 0.22621460258960724\n",
            "iteration 49015: loss: 0.22621449828147888\n",
            "iteration 49016: loss: 0.22621449828147888\n",
            "iteration 49017: loss: 0.22621452808380127\n",
            "iteration 49018: loss: 0.2262144535779953\n",
            "iteration 49019: loss: 0.2262144535779953\n",
            "iteration 49020: loss: 0.2262144535779953\n",
            "iteration 49021: loss: 0.22621440887451172\n",
            "iteration 49022: loss: 0.22621440887451172\n",
            "iteration 49023: loss: 0.22621440887451172\n",
            "iteration 49024: loss: 0.22621436417102814\n",
            "iteration 49025: loss: 0.22621436417102814\n",
            "iteration 49026: loss: 0.22621436417102814\n",
            "iteration 49027: loss: 0.22621433436870575\n",
            "iteration 49028: loss: 0.22621428966522217\n",
            "iteration 49029: loss: 0.22621428966522217\n",
            "iteration 49030: loss: 0.22621425986289978\n",
            "iteration 49031: loss: 0.22621428966522217\n",
            "iteration 49032: loss: 0.2262142449617386\n",
            "iteration 49033: loss: 0.2262142449617386\n",
            "iteration 49034: loss: 0.2262142151594162\n",
            "iteration 49035: loss: 0.226214200258255\n",
            "iteration 49036: loss: 0.226214200258255\n",
            "iteration 49037: loss: 0.22621417045593262\n",
            "iteration 49038: loss: 0.22621414065361023\n",
            "iteration 49039: loss: 0.22621412575244904\n",
            "iteration 49040: loss: 0.22621408104896545\n",
            "iteration 49041: loss: 0.22621412575244904\n",
            "iteration 49042: loss: 0.22621408104896545\n",
            "iteration 49043: loss: 0.22621402144432068\n",
            "iteration 49044: loss: 0.22621402144432068\n",
            "iteration 49045: loss: 0.2262139767408371\n",
            "iteration 49046: loss: 0.22621402144432068\n",
            "iteration 49047: loss: 0.2262139767408371\n",
            "iteration 49048: loss: 0.2262139618396759\n",
            "iteration 49049: loss: 0.22621393203735352\n",
            "iteration 49050: loss: 0.22621393203735352\n",
            "iteration 49051: loss: 0.22621390223503113\n",
            "iteration 49052: loss: 0.22621388733386993\n",
            "iteration 49053: loss: 0.22621388733386993\n",
            "iteration 49054: loss: 0.22621381282806396\n",
            "iteration 49055: loss: 0.22621384263038635\n",
            "iteration 49056: loss: 0.22621381282806396\n",
            "iteration 49057: loss: 0.22621378302574158\n",
            "iteration 49058: loss: 0.22621376812458038\n",
            "iteration 49059: loss: 0.226213738322258\n",
            "iteration 49060: loss: 0.22621376812458038\n",
            "iteration 49061: loss: 0.2262137234210968\n",
            "iteration 49062: loss: 0.226213738322258\n",
            "iteration 49063: loss: 0.22621369361877441\n",
            "iteration 49064: loss: 0.2262137234210968\n",
            "iteration 49065: loss: 0.22621364891529083\n",
            "iteration 49066: loss: 0.22621366381645203\n",
            "iteration 49067: loss: 0.22621361911296844\n",
            "iteration 49068: loss: 0.22621360421180725\n",
            "iteration 49069: loss: 0.22621354460716248\n",
            "iteration 49070: loss: 0.22621360421180725\n",
            "iteration 49071: loss: 0.22621354460716248\n",
            "iteration 49072: loss: 0.22621354460716248\n",
            "iteration 49073: loss: 0.22621352970600128\n",
            "iteration 49074: loss: 0.22621357440948486\n",
            "iteration 49075: loss: 0.2262134850025177\n",
            "iteration 49076: loss: 0.2262134552001953\n",
            "iteration 49077: loss: 0.2262134552001953\n",
            "iteration 49078: loss: 0.22621342539787292\n",
            "iteration 49079: loss: 0.22621341049671173\n",
            "iteration 49080: loss: 0.2262134552001953\n",
            "iteration 49081: loss: 0.22621341049671173\n",
            "iteration 49082: loss: 0.22621336579322815\n",
            "iteration 49083: loss: 0.22621336579322815\n",
            "iteration 49084: loss: 0.22621330618858337\n",
            "iteration 49085: loss: 0.22621330618858337\n",
            "iteration 49086: loss: 0.22621330618858337\n",
            "iteration 49087: loss: 0.2262132614850998\n",
            "iteration 49088: loss: 0.2262132465839386\n",
            "iteration 49089: loss: 0.22621318697929382\n",
            "iteration 49090: loss: 0.2262132465839386\n",
            "iteration 49091: loss: 0.2262132167816162\n",
            "iteration 49092: loss: 0.22621318697929382\n",
            "iteration 49093: loss: 0.22621314227581024\n",
            "iteration 49094: loss: 0.22621312737464905\n",
            "iteration 49095: loss: 0.22621314227581024\n",
            "iteration 49096: loss: 0.22621309757232666\n",
            "iteration 49097: loss: 0.22621306777000427\n",
            "iteration 49098: loss: 0.22621306777000427\n",
            "iteration 49099: loss: 0.22621305286884308\n",
            "iteration 49100: loss: 0.2262130230665207\n",
            "iteration 49101: loss: 0.2262130081653595\n",
            "iteration 49102: loss: 0.2262130081653595\n",
            "iteration 49103: loss: 0.2262130081653595\n",
            "iteration 49104: loss: 0.2262129783630371\n",
            "iteration 49105: loss: 0.2262129783630371\n",
            "iteration 49106: loss: 0.22621294856071472\n",
            "iteration 49107: loss: 0.22621293365955353\n",
            "iteration 49108: loss: 0.22621293365955353\n",
            "iteration 49109: loss: 0.22621285915374756\n",
            "iteration 49110: loss: 0.22621290385723114\n",
            "iteration 49111: loss: 0.22621288895606995\n",
            "iteration 49112: loss: 0.22621285915374756\n",
            "iteration 49113: loss: 0.22621282935142517\n",
            "iteration 49114: loss: 0.2262127846479416\n",
            "iteration 49115: loss: 0.2262127846479416\n",
            "iteration 49116: loss: 0.2262127846479416\n",
            "iteration 49117: loss: 0.22621271014213562\n",
            "iteration 49118: loss: 0.2262127697467804\n",
            "iteration 49119: loss: 0.22621269524097443\n",
            "iteration 49120: loss: 0.22621269524097443\n",
            "iteration 49121: loss: 0.22621269524097443\n",
            "iteration 49122: loss: 0.22621265053749084\n",
            "iteration 49123: loss: 0.22621269524097443\n",
            "iteration 49124: loss: 0.22621262073516846\n",
            "iteration 49125: loss: 0.22621262073516846\n",
            "iteration 49126: loss: 0.22621257603168488\n",
            "iteration 49127: loss: 0.2262125462293625\n",
            "iteration 49128: loss: 0.22621257603168488\n",
            "iteration 49129: loss: 0.2262125462293625\n",
            "iteration 49130: loss: 0.22621247172355652\n",
            "iteration 49131: loss: 0.2262125015258789\n",
            "iteration 49132: loss: 0.22621242702007294\n",
            "iteration 49133: loss: 0.22621245682239532\n",
            "iteration 49134: loss: 0.22621245682239532\n",
            "iteration 49135: loss: 0.22621242702007294\n",
            "iteration 49136: loss: 0.22621241211891174\n",
            "iteration 49137: loss: 0.22621238231658936\n",
            "iteration 49138: loss: 0.22621235251426697\n",
            "iteration 49139: loss: 0.22621235251426697\n",
            "iteration 49140: loss: 0.22621235251426697\n",
            "iteration 49141: loss: 0.22621235251426697\n",
            "iteration 49142: loss: 0.2262123078107834\n",
            "iteration 49143: loss: 0.2262123078107834\n",
            "iteration 49144: loss: 0.2262123078107834\n",
            "iteration 49145: loss: 0.2262122631072998\n",
            "iteration 49146: loss: 0.22621223330497742\n",
            "iteration 49147: loss: 0.22621221840381622\n",
            "iteration 49148: loss: 0.22621218860149384\n",
            "iteration 49149: loss: 0.22621217370033264\n",
            "iteration 49150: loss: 0.22621217370033264\n",
            "iteration 49151: loss: 0.22621217370033264\n",
            "iteration 49152: loss: 0.22621217370033264\n",
            "iteration 49153: loss: 0.22621211409568787\n",
            "iteration 49154: loss: 0.22621209919452667\n",
            "iteration 49155: loss: 0.22621206939220428\n",
            "iteration 49156: loss: 0.2262120246887207\n",
            "iteration 49157: loss: 0.22621197998523712\n",
            "iteration 49158: loss: 0.2262120246887207\n",
            "iteration 49159: loss: 0.2262120544910431\n",
            "iteration 49160: loss: 0.22621199488639832\n",
            "iteration 49161: loss: 0.22621193528175354\n",
            "iteration 49162: loss: 0.22621195018291473\n",
            "iteration 49163: loss: 0.22621197998523712\n",
            "iteration 49164: loss: 0.22621187567710876\n",
            "iteration 49165: loss: 0.22621193528175354\n",
            "iteration 49166: loss: 0.22621187567710876\n",
            "iteration 49167: loss: 0.22621187567710876\n",
            "iteration 49168: loss: 0.22621183097362518\n",
            "iteration 49169: loss: 0.226211816072464\n",
            "iteration 49170: loss: 0.2262117862701416\n",
            "iteration 49171: loss: 0.2262117862701416\n",
            "iteration 49172: loss: 0.226211816072464\n",
            "iteration 49173: loss: 0.2262117862701416\n",
            "iteration 49174: loss: 0.2262117564678192\n",
            "iteration 49175: loss: 0.22621174156665802\n",
            "iteration 49176: loss: 0.22621171176433563\n",
            "iteration 49177: loss: 0.22621171176433563\n",
            "iteration 49178: loss: 0.22621169686317444\n",
            "iteration 49179: loss: 0.22621166706085205\n",
            "iteration 49180: loss: 0.22621162235736847\n",
            "iteration 49181: loss: 0.2262115776538849\n",
            "iteration 49182: loss: 0.22621162235736847\n",
            "iteration 49183: loss: 0.22621163725852966\n",
            "iteration 49184: loss: 0.22621163725852966\n",
            "iteration 49185: loss: 0.2262115776538849\n",
            "iteration 49186: loss: 0.2262115478515625\n",
            "iteration 49187: loss: 0.2262115478515625\n",
            "iteration 49188: loss: 0.22621150314807892\n",
            "iteration 49189: loss: 0.2262115180492401\n",
            "iteration 49190: loss: 0.22621145844459534\n",
            "iteration 49191: loss: 0.22621145844459534\n",
            "iteration 49192: loss: 0.22621145844459534\n",
            "iteration 49193: loss: 0.22621139883995056\n",
            "iteration 49194: loss: 0.22621139883995056\n",
            "iteration 49195: loss: 0.22621139883995056\n",
            "iteration 49196: loss: 0.22621138393878937\n",
            "iteration 49197: loss: 0.22621135413646698\n",
            "iteration 49198: loss: 0.2262113094329834\n",
            "iteration 49199: loss: 0.2262113392353058\n",
            "iteration 49200: loss: 0.2262113094329834\n",
            "iteration 49201: loss: 0.22621126472949982\n",
            "iteration 49202: loss: 0.226211279630661\n",
            "iteration 49203: loss: 0.22621122002601624\n",
            "iteration 49204: loss: 0.22621123492717743\n",
            "iteration 49205: loss: 0.22621123492717743\n",
            "iteration 49206: loss: 0.22621116042137146\n",
            "iteration 49207: loss: 0.22621116042137146\n",
            "iteration 49208: loss: 0.22621114552021027\n",
            "iteration 49209: loss: 0.22621114552021027\n",
            "iteration 49210: loss: 0.22621114552021027\n",
            "iteration 49211: loss: 0.22621111571788788\n",
            "iteration 49212: loss: 0.22621111571788788\n",
            "iteration 49213: loss: 0.22621110081672668\n",
            "iteration 49214: loss: 0.2262110412120819\n",
            "iteration 49215: loss: 0.2262110412120819\n",
            "iteration 49216: loss: 0.22621098160743713\n",
            "iteration 49217: loss: 0.22621099650859833\n",
            "iteration 49218: loss: 0.22621098160743713\n",
            "iteration 49219: loss: 0.22621099650859833\n",
            "iteration 49220: loss: 0.22621098160743713\n",
            "iteration 49221: loss: 0.22621095180511475\n",
            "iteration 49222: loss: 0.22621090710163116\n",
            "iteration 49223: loss: 0.22621090710163116\n",
            "iteration 49224: loss: 0.22621087729930878\n",
            "iteration 49225: loss: 0.22621087729930878\n",
            "iteration 49226: loss: 0.22621086239814758\n",
            "iteration 49227: loss: 0.2262108027935028\n",
            "iteration 49228: loss: 0.2262108325958252\n",
            "iteration 49229: loss: 0.2262108027935028\n",
            "iteration 49230: loss: 0.22621075809001923\n",
            "iteration 49231: loss: 0.22621075809001923\n",
            "iteration 49232: loss: 0.22621074318885803\n",
            "iteration 49233: loss: 0.22621071338653564\n",
            "iteration 49234: loss: 0.22621068358421326\n",
            "iteration 49235: loss: 0.22621066868305206\n",
            "iteration 49236: loss: 0.22621068358421326\n",
            "iteration 49237: loss: 0.22621068358421326\n",
            "iteration 49238: loss: 0.22621062397956848\n",
            "iteration 49239: loss: 0.2262105941772461\n",
            "iteration 49240: loss: 0.2262105941772461\n",
            "iteration 49241: loss: 0.2262105494737625\n",
            "iteration 49242: loss: 0.2262105494737625\n",
            "iteration 49243: loss: 0.2262105494737625\n",
            "iteration 49244: loss: 0.22621051967144012\n",
            "iteration 49245: loss: 0.22621050477027893\n",
            "iteration 49246: loss: 0.22621050477027893\n",
            "iteration 49247: loss: 0.22621050477027893\n",
            "iteration 49248: loss: 0.22621047496795654\n",
            "iteration 49249: loss: 0.22621044516563416\n",
            "iteration 49250: loss: 0.22621043026447296\n",
            "iteration 49251: loss: 0.22621044516563416\n",
            "iteration 49252: loss: 0.22621040046215057\n",
            "iteration 49253: loss: 0.22621038556098938\n",
            "iteration 49254: loss: 0.226210355758667\n",
            "iteration 49255: loss: 0.2262103259563446\n",
            "iteration 49256: loss: 0.2262103259563446\n",
            "iteration 49257: loss: 0.2262103110551834\n",
            "iteration 49258: loss: 0.22621026635169983\n",
            "iteration 49259: loss: 0.22621026635169983\n",
            "iteration 49260: loss: 0.22621026635169983\n",
            "iteration 49261: loss: 0.22621026635169983\n",
            "iteration 49262: loss: 0.22621016204357147\n",
            "iteration 49263: loss: 0.22621020674705505\n",
            "iteration 49264: loss: 0.22621014714241028\n",
            "iteration 49265: loss: 0.22621019184589386\n",
            "iteration 49266: loss: 0.22621016204357147\n",
            "iteration 49267: loss: 0.2262101173400879\n",
            "iteration 49268: loss: 0.2262100875377655\n",
            "iteration 49269: loss: 0.2262100726366043\n",
            "iteration 49270: loss: 0.2262100875377655\n",
            "iteration 49271: loss: 0.2262100875377655\n",
            "iteration 49272: loss: 0.22621004283428192\n",
            "iteration 49273: loss: 0.22621002793312073\n",
            "iteration 49274: loss: 0.22620996832847595\n",
            "iteration 49275: loss: 0.22620996832847595\n",
            "iteration 49276: loss: 0.22620999813079834\n",
            "iteration 49277: loss: 0.22620996832847595\n",
            "iteration 49278: loss: 0.22620996832847595\n",
            "iteration 49279: loss: 0.22620990872383118\n",
            "iteration 49280: loss: 0.22620992362499237\n",
            "iteration 49281: loss: 0.22620990872383118\n",
            "iteration 49282: loss: 0.2262098342180252\n",
            "iteration 49283: loss: 0.2262098789215088\n",
            "iteration 49284: loss: 0.2262098342180252\n",
            "iteration 49285: loss: 0.2262098342180252\n",
            "iteration 49286: loss: 0.22620978951454163\n",
            "iteration 49287: loss: 0.22620978951454163\n",
            "iteration 49288: loss: 0.22620978951454163\n",
            "iteration 49289: loss: 0.22620975971221924\n",
            "iteration 49290: loss: 0.22620975971221924\n",
            "iteration 49291: loss: 0.22620971500873566\n",
            "iteration 49292: loss: 0.22620967030525208\n",
            "iteration 49293: loss: 0.22620967030525208\n",
            "iteration 49294: loss: 0.2262096405029297\n",
            "iteration 49295: loss: 0.2262096405029297\n",
            "iteration 49296: loss: 0.22620967030525208\n",
            "iteration 49297: loss: 0.2262095957994461\n",
            "iteration 49298: loss: 0.2262095957994461\n",
            "iteration 49299: loss: 0.2262095957994461\n",
            "iteration 49300: loss: 0.22620952129364014\n",
            "iteration 49301: loss: 0.22620956599712372\n",
            "iteration 49302: loss: 0.22620952129364014\n",
            "iteration 49303: loss: 0.22620949149131775\n",
            "iteration 49304: loss: 0.22620947659015656\n",
            "iteration 49305: loss: 0.22620949149131775\n",
            "iteration 49306: loss: 0.22620943188667297\n",
            "iteration 49307: loss: 0.22620944678783417\n",
            "iteration 49308: loss: 0.22620940208435059\n",
            "iteration 49309: loss: 0.22620943188667297\n",
            "iteration 49310: loss: 0.22620940208435059\n",
            "iteration 49311: loss: 0.2262093722820282\n",
            "iteration 49312: loss: 0.2262093722820282\n",
            "iteration 49313: loss: 0.22620931267738342\n",
            "iteration 49314: loss: 0.22620932757854462\n",
            "iteration 49315: loss: 0.22620931267738342\n",
            "iteration 49316: loss: 0.22620928287506104\n",
            "iteration 49317: loss: 0.22620925307273865\n",
            "iteration 49318: loss: 0.22620928287506104\n",
            "iteration 49319: loss: 0.22620923817157745\n",
            "iteration 49320: loss: 0.22620923817157745\n",
            "iteration 49321: loss: 0.22620919346809387\n",
            "iteration 49322: loss: 0.22620919346809387\n",
            "iteration 49323: loss: 0.2262091338634491\n",
            "iteration 49324: loss: 0.2262091338634491\n",
            "iteration 49325: loss: 0.22620908915996552\n",
            "iteration 49326: loss: 0.2262091189622879\n",
            "iteration 49327: loss: 0.22620908915996552\n",
            "iteration 49328: loss: 0.22620907425880432\n",
            "iteration 49329: loss: 0.22620901465415955\n",
            "iteration 49330: loss: 0.22620907425880432\n",
            "iteration 49331: loss: 0.22620899975299835\n",
            "iteration 49332: loss: 0.22620901465415955\n",
            "iteration 49333: loss: 0.22620899975299835\n",
            "iteration 49334: loss: 0.22620896995067596\n",
            "iteration 49335: loss: 0.22620892524719238\n",
            "iteration 49336: loss: 0.22620889544487\n",
            "iteration 49337: loss: 0.22620889544487\n",
            "iteration 49338: loss: 0.22620889544487\n",
            "iteration 49339: loss: 0.22620889544487\n",
            "iteration 49340: loss: 0.22620880603790283\n",
            "iteration 49341: loss: 0.22620880603790283\n",
            "iteration 49342: loss: 0.22620880603790283\n",
            "iteration 49343: loss: 0.22620880603790283\n",
            "iteration 49344: loss: 0.22620880603790283\n",
            "iteration 49345: loss: 0.22620880603790283\n",
            "iteration 49346: loss: 0.22620876133441925\n",
            "iteration 49347: loss: 0.22620873153209686\n",
            "iteration 49348: loss: 0.22620873153209686\n",
            "iteration 49349: loss: 0.22620873153209686\n",
            "iteration 49350: loss: 0.22620873153209686\n",
            "iteration 49351: loss: 0.22620868682861328\n",
            "iteration 49352: loss: 0.2262086421251297\n",
            "iteration 49353: loss: 0.2262086421251297\n",
            "iteration 49354: loss: 0.2262086421251297\n",
            "iteration 49355: loss: 0.22620859742164612\n",
            "iteration 49356: loss: 0.22620859742164612\n",
            "iteration 49357: loss: 0.22620853781700134\n",
            "iteration 49358: loss: 0.22620853781700134\n",
            "iteration 49359: loss: 0.22620852291584015\n",
            "iteration 49360: loss: 0.22620849311351776\n",
            "iteration 49361: loss: 0.22620844841003418\n",
            "iteration 49362: loss: 0.22620849311351776\n",
            "iteration 49363: loss: 0.22620847821235657\n",
            "iteration 49364: loss: 0.22620844841003418\n",
            "iteration 49365: loss: 0.2262084186077118\n",
            "iteration 49366: loss: 0.2262084037065506\n",
            "iteration 49367: loss: 0.22620835900306702\n",
            "iteration 49368: loss: 0.2262084037065506\n",
            "iteration 49369: loss: 0.2262083739042282\n",
            "iteration 49370: loss: 0.22620835900306702\n",
            "iteration 49371: loss: 0.22620829939842224\n",
            "iteration 49372: loss: 0.22620828449726105\n",
            "iteration 49373: loss: 0.22620825469493866\n",
            "iteration 49374: loss: 0.22620829939842224\n",
            "iteration 49375: loss: 0.22620825469493866\n",
            "iteration 49376: loss: 0.22620820999145508\n",
            "iteration 49377: loss: 0.22620820999145508\n",
            "iteration 49378: loss: 0.2262081801891327\n",
            "iteration 49379: loss: 0.22620823979377747\n",
            "iteration 49380: loss: 0.22620820999145508\n",
            "iteration 49381: loss: 0.2262081652879715\n",
            "iteration 49382: loss: 0.2262081354856491\n",
            "iteration 49383: loss: 0.22620812058448792\n",
            "iteration 49384: loss: 0.22620812058448792\n",
            "iteration 49385: loss: 0.22620806097984314\n",
            "iteration 49386: loss: 0.22620806097984314\n",
            "iteration 49387: loss: 0.22620801627635956\n",
            "iteration 49388: loss: 0.22620801627635956\n",
            "iteration 49389: loss: 0.22620801627635956\n",
            "iteration 49390: loss: 0.22620800137519836\n",
            "iteration 49391: loss: 0.22620797157287598\n",
            "iteration 49392: loss: 0.22620797157287598\n",
            "iteration 49393: loss: 0.2262079268693924\n",
            "iteration 49394: loss: 0.2262079417705536\n",
            "iteration 49395: loss: 0.2262079268693924\n",
            "iteration 49396: loss: 0.2262078821659088\n",
            "iteration 49397: loss: 0.22620782256126404\n",
            "iteration 49398: loss: 0.22620785236358643\n",
            "iteration 49399: loss: 0.22620785236358643\n",
            "iteration 49400: loss: 0.22620780766010284\n",
            "iteration 49401: loss: 0.22620780766010284\n",
            "iteration 49402: loss: 0.22620777785778046\n",
            "iteration 49403: loss: 0.22620777785778046\n",
            "iteration 49404: loss: 0.22620780766010284\n",
            "iteration 49405: loss: 0.22620776295661926\n",
            "iteration 49406: loss: 0.2262077033519745\n",
            "iteration 49407: loss: 0.2262076884508133\n",
            "iteration 49408: loss: 0.2262076884508133\n",
            "iteration 49409: loss: 0.2262076586484909\n",
            "iteration 49410: loss: 0.2262076437473297\n",
            "iteration 49411: loss: 0.2262076437473297\n",
            "iteration 49412: loss: 0.22620758414268494\n",
            "iteration 49413: loss: 0.22620761394500732\n",
            "iteration 49414: loss: 0.22620761394500732\n",
            "iteration 49415: loss: 0.22620753943920135\n",
            "iteration 49416: loss: 0.22620753943920135\n",
            "iteration 49417: loss: 0.22620752453804016\n",
            "iteration 49418: loss: 0.22620749473571777\n",
            "iteration 49419: loss: 0.22620749473571777\n",
            "iteration 49420: loss: 0.2262074500322342\n",
            "iteration 49421: loss: 0.2262074202299118\n",
            "iteration 49422: loss: 0.2262074202299118\n",
            "iteration 49423: loss: 0.2262074500322342\n",
            "iteration 49424: loss: 0.2262074053287506\n",
            "iteration 49425: loss: 0.22620737552642822\n",
            "iteration 49426: loss: 0.22620733082294464\n",
            "iteration 49427: loss: 0.2262074053287506\n",
            "iteration 49428: loss: 0.22620733082294464\n",
            "iteration 49429: loss: 0.22620730102062225\n",
            "iteration 49430: loss: 0.22620730102062225\n",
            "iteration 49431: loss: 0.22620728611946106\n",
            "iteration 49432: loss: 0.22620728611946106\n",
            "iteration 49433: loss: 0.22620725631713867\n",
            "iteration 49434: loss: 0.22620725631713867\n",
            "iteration 49435: loss: 0.2262072116136551\n",
            "iteration 49436: loss: 0.2262072116136551\n",
            "iteration 49437: loss: 0.2262071818113327\n",
            "iteration 49438: loss: 0.2262072116136551\n",
            "iteration 49439: loss: 0.2262071818113327\n",
            "iteration 49440: loss: 0.2262071669101715\n",
            "iteration 49441: loss: 0.22620713710784912\n",
            "iteration 49442: loss: 0.22620710730552673\n",
            "iteration 49443: loss: 0.22620709240436554\n",
            "iteration 49444: loss: 0.22620710730552673\n",
            "iteration 49445: loss: 0.22620701789855957\n",
            "iteration 49446: loss: 0.22620704770088196\n",
            "iteration 49447: loss: 0.226206973195076\n",
            "iteration 49448: loss: 0.22620701789855957\n",
            "iteration 49449: loss: 0.2262069284915924\n",
            "iteration 49450: loss: 0.2262069433927536\n",
            "iteration 49451: loss: 0.2262069284915924\n",
            "iteration 49452: loss: 0.2262069284915924\n",
            "iteration 49453: loss: 0.22620689868927002\n",
            "iteration 49454: loss: 0.22620686888694763\n",
            "iteration 49455: loss: 0.22620682418346405\n",
            "iteration 49456: loss: 0.22620686888694763\n",
            "iteration 49457: loss: 0.22620685398578644\n",
            "iteration 49458: loss: 0.22620680928230286\n",
            "iteration 49459: loss: 0.22620677947998047\n",
            "iteration 49460: loss: 0.22620677947998047\n",
            "iteration 49461: loss: 0.22620674967765808\n",
            "iteration 49462: loss: 0.22620677947998047\n",
            "iteration 49463: loss: 0.22620674967765808\n",
            "iteration 49464: loss: 0.2262067049741745\n",
            "iteration 49465: loss: 0.2262066900730133\n",
            "iteration 49466: loss: 0.2262066900730133\n",
            "iteration 49467: loss: 0.2262066900730133\n",
            "iteration 49468: loss: 0.22620661556720734\n",
            "iteration 49469: loss: 0.22620658576488495\n",
            "iteration 49470: loss: 0.22620658576488495\n",
            "iteration 49471: loss: 0.22620658576488495\n",
            "iteration 49472: loss: 0.22620657086372375\n",
            "iteration 49473: loss: 0.22620657086372375\n",
            "iteration 49474: loss: 0.22620654106140137\n",
            "iteration 49475: loss: 0.22620654106140137\n",
            "iteration 49476: loss: 0.22620651125907898\n",
            "iteration 49477: loss: 0.22620651125907898\n",
            "iteration 49478: loss: 0.2262064665555954\n",
            "iteration 49479: loss: 0.2262064516544342\n",
            "iteration 49480: loss: 0.22620642185211182\n",
            "iteration 49481: loss: 0.2262064516544342\n",
            "iteration 49482: loss: 0.22620639204978943\n",
            "iteration 49483: loss: 0.22620634734630585\n",
            "iteration 49484: loss: 0.22620637714862823\n",
            "iteration 49485: loss: 0.22620639204978943\n",
            "iteration 49486: loss: 0.22620634734630585\n",
            "iteration 49487: loss: 0.22620627284049988\n",
            "iteration 49488: loss: 0.22620630264282227\n",
            "iteration 49489: loss: 0.22620630264282227\n",
            "iteration 49490: loss: 0.2262062132358551\n",
            "iteration 49491: loss: 0.22620625793933868\n",
            "iteration 49492: loss: 0.2262062281370163\n",
            "iteration 49493: loss: 0.2262062281370163\n",
            "iteration 49494: loss: 0.2262062132358551\n",
            "iteration 49495: loss: 0.2262062281370163\n",
            "iteration 49496: loss: 0.22620615363121033\n",
            "iteration 49497: loss: 0.22620615363121033\n",
            "iteration 49498: loss: 0.22620610892772675\n",
            "iteration 49499: loss: 0.22620610892772675\n",
            "iteration 49500: loss: 0.22620610892772675\n",
            "iteration 49501: loss: 0.22620606422424316\n",
            "iteration 49502: loss: 0.22620601952075958\n",
            "iteration 49503: loss: 0.22620606422424316\n",
            "iteration 49504: loss: 0.22620601952075958\n",
            "iteration 49505: loss: 0.22620601952075958\n",
            "iteration 49506: loss: 0.22620601952075958\n",
            "iteration 49507: loss: 0.2262059450149536\n",
            "iteration 49508: loss: 0.2262059897184372\n",
            "iteration 49509: loss: 0.22620591521263123\n",
            "iteration 49510: loss: 0.22620591521263123\n",
            "iteration 49511: loss: 0.22620587050914764\n",
            "iteration 49512: loss: 0.22620585560798645\n",
            "iteration 49513: loss: 0.22620585560798645\n",
            "iteration 49514: loss: 0.22620582580566406\n",
            "iteration 49515: loss: 0.22620585560798645\n",
            "iteration 49516: loss: 0.22620582580566406\n",
            "iteration 49517: loss: 0.2262057512998581\n",
            "iteration 49518: loss: 0.22620578110218048\n",
            "iteration 49519: loss: 0.22620578110218048\n",
            "iteration 49520: loss: 0.2262057363986969\n",
            "iteration 49521: loss: 0.2262057512998581\n",
            "iteration 49522: loss: 0.2262057363986969\n",
            "iteration 49523: loss: 0.2262057363986969\n",
            "iteration 49524: loss: 0.2262057065963745\n",
            "iteration 49525: loss: 0.22620563209056854\n",
            "iteration 49526: loss: 0.22620566189289093\n",
            "iteration 49527: loss: 0.22620563209056854\n",
            "iteration 49528: loss: 0.22620561718940735\n",
            "iteration 49529: loss: 0.22620558738708496\n",
            "iteration 49530: loss: 0.22620555758476257\n",
            "iteration 49531: loss: 0.22620555758476257\n",
            "iteration 49532: loss: 0.22620555758476257\n",
            "iteration 49533: loss: 0.22620554268360138\n",
            "iteration 49534: loss: 0.2262054979801178\n",
            "iteration 49535: loss: 0.226205512881279\n",
            "iteration 49536: loss: 0.2262054681777954\n",
            "iteration 49537: loss: 0.22620543837547302\n",
            "iteration 49538: loss: 0.22620542347431183\n",
            "iteration 49539: loss: 0.22620543837547302\n",
            "iteration 49540: loss: 0.22620539367198944\n",
            "iteration 49541: loss: 0.22620542347431183\n",
            "iteration 49542: loss: 0.22620537877082825\n",
            "iteration 49543: loss: 0.22620537877082825\n",
            "iteration 49544: loss: 0.22620534896850586\n",
            "iteration 49545: loss: 0.22620531916618347\n",
            "iteration 49546: loss: 0.2262052744626999\n",
            "iteration 49547: loss: 0.2262052744626999\n",
            "iteration 49548: loss: 0.2262052297592163\n",
            "iteration 49549: loss: 0.2262052595615387\n",
            "iteration 49550: loss: 0.2262052595615387\n",
            "iteration 49551: loss: 0.2262052297592163\n",
            "iteration 49552: loss: 0.2262052297592163\n",
            "iteration 49553: loss: 0.22620518505573273\n",
            "iteration 49554: loss: 0.22620518505573273\n",
            "iteration 49555: loss: 0.22620511054992676\n",
            "iteration 49556: loss: 0.22620511054992676\n",
            "iteration 49557: loss: 0.22620511054992676\n",
            "iteration 49558: loss: 0.22620508074760437\n",
            "iteration 49559: loss: 0.22620506584644318\n",
            "iteration 49560: loss: 0.22620511054992676\n",
            "iteration 49561: loss: 0.22620506584644318\n",
            "iteration 49562: loss: 0.2262050360441208\n",
            "iteration 49563: loss: 0.2262050360441208\n",
            "iteration 49564: loss: 0.2262049913406372\n",
            "iteration 49565: loss: 0.2262049913406372\n",
            "iteration 49566: loss: 0.22620496153831482\n",
            "iteration 49567: loss: 0.22620491683483124\n",
            "iteration 49568: loss: 0.22620487213134766\n",
            "iteration 49569: loss: 0.22620491683483124\n",
            "iteration 49570: loss: 0.22620487213134766\n",
            "iteration 49571: loss: 0.22620487213134766\n",
            "iteration 49572: loss: 0.22620482742786407\n",
            "iteration 49573: loss: 0.22620482742786407\n",
            "iteration 49574: loss: 0.2262047827243805\n",
            "iteration 49575: loss: 0.2262047827243805\n",
            "iteration 49576: loss: 0.2262047827243805\n",
            "iteration 49577: loss: 0.2262047827243805\n",
            "iteration 49578: loss: 0.2262047529220581\n",
            "iteration 49579: loss: 0.2262047827243805\n",
            "iteration 49580: loss: 0.22620470821857452\n",
            "iteration 49581: loss: 0.22620467841625214\n",
            "iteration 49582: loss: 0.22620470821857452\n",
            "iteration 49583: loss: 0.22620466351509094\n",
            "iteration 49584: loss: 0.22620460391044617\n",
            "iteration 49585: loss: 0.22620463371276855\n",
            "iteration 49586: loss: 0.22620460391044617\n",
            "iteration 49587: loss: 0.22620458900928497\n",
            "iteration 49588: loss: 0.22620460391044617\n",
            "iteration 49589: loss: 0.22620458900928497\n",
            "iteration 49590: loss: 0.2262045443058014\n",
            "iteration 49591: loss: 0.22620455920696259\n",
            "iteration 49592: loss: 0.22620446979999542\n",
            "iteration 49593: loss: 0.22620443999767303\n",
            "iteration 49594: loss: 0.22620448470115662\n",
            "iteration 49595: loss: 0.22620443999767303\n",
            "iteration 49596: loss: 0.22620443999767303\n",
            "iteration 49597: loss: 0.22620442509651184\n",
            "iteration 49598: loss: 0.22620442509651184\n",
            "iteration 49599: loss: 0.22620436549186707\n",
            "iteration 49600: loss: 0.22620435059070587\n",
            "iteration 49601: loss: 0.22620439529418945\n",
            "iteration 49602: loss: 0.22620436549186707\n",
            "iteration 49603: loss: 0.22620432078838348\n",
            "iteration 49604: loss: 0.2262043058872223\n",
            "iteration 49605: loss: 0.2262043058872223\n",
            "iteration 49606: loss: 0.22620424628257751\n",
            "iteration 49607: loss: 0.2262043058872223\n",
            "iteration 49608: loss: 0.22620423138141632\n",
            "iteration 49609: loss: 0.22620418667793274\n",
            "iteration 49610: loss: 0.22620418667793274\n",
            "iteration 49611: loss: 0.22620412707328796\n",
            "iteration 49612: loss: 0.22620415687561035\n",
            "iteration 49613: loss: 0.22620412707328796\n",
            "iteration 49614: loss: 0.22620408236980438\n",
            "iteration 49615: loss: 0.22620415687561035\n",
            "iteration 49616: loss: 0.22620408236980438\n",
            "iteration 49617: loss: 0.22620408236980438\n",
            "iteration 49618: loss: 0.2262040674686432\n",
            "iteration 49619: loss: 0.2262040376663208\n",
            "iteration 49620: loss: 0.2262040376663208\n",
            "iteration 49621: loss: 0.2262040674686432\n",
            "iteration 49622: loss: 0.22620399296283722\n",
            "iteration 49623: loss: 0.22620396316051483\n",
            "iteration 49624: loss: 0.22620396316051483\n",
            "iteration 49625: loss: 0.22620396316051483\n",
            "iteration 49626: loss: 0.22620396316051483\n",
            "iteration 49627: loss: 0.22620394825935364\n",
            "iteration 49628: loss: 0.22620391845703125\n",
            "iteration 49629: loss: 0.22620387375354767\n",
            "iteration 49630: loss: 0.22620387375354767\n",
            "iteration 49631: loss: 0.2262038290500641\n",
            "iteration 49632: loss: 0.2262037992477417\n",
            "iteration 49633: loss: 0.2262037992477417\n",
            "iteration 49634: loss: 0.2262037694454193\n",
            "iteration 49635: loss: 0.2262037694454193\n",
            "iteration 49636: loss: 0.2262037694454193\n",
            "iteration 49637: loss: 0.22620372474193573\n",
            "iteration 49638: loss: 0.22620365023612976\n",
            "iteration 49639: loss: 0.22620370984077454\n",
            "iteration 49640: loss: 0.22620368003845215\n",
            "iteration 49641: loss: 0.22620365023612976\n",
            "iteration 49642: loss: 0.22620360553264618\n",
            "iteration 49643: loss: 0.22620360553264618\n",
            "iteration 49644: loss: 0.22620360553264618\n",
            "iteration 49645: loss: 0.22620359063148499\n",
            "iteration 49646: loss: 0.22620351612567902\n",
            "iteration 49647: loss: 0.2262035608291626\n",
            "iteration 49648: loss: 0.22620359063148499\n",
            "iteration 49649: loss: 0.2262035608291626\n",
            "iteration 49650: loss: 0.22620351612567902\n",
            "iteration 49651: loss: 0.22620348632335663\n",
            "iteration 49652: loss: 0.22620348632335663\n",
            "iteration 49653: loss: 0.22620348632335663\n",
            "iteration 49654: loss: 0.22620341181755066\n",
            "iteration 49655: loss: 0.22620347142219543\n",
            "iteration 49656: loss: 0.22620339691638947\n",
            "iteration 49657: loss: 0.22620335221290588\n",
            "iteration 49658: loss: 0.22620335221290588\n",
            "iteration 49659: loss: 0.22620335221290588\n",
            "iteration 49660: loss: 0.22620335221290588\n",
            "iteration 49661: loss: 0.22620335221290588\n",
            "iteration 49662: loss: 0.2262033224105835\n",
            "iteration 49663: loss: 0.22620327770709991\n",
            "iteration 49664: loss: 0.2262032926082611\n",
            "iteration 49665: loss: 0.22620327770709991\n",
            "iteration 49666: loss: 0.2262032926082611\n",
            "iteration 49667: loss: 0.2262032926082611\n",
            "iteration 49668: loss: 0.22620317339897156\n",
            "iteration 49669: loss: 0.22620317339897156\n",
            "iteration 49670: loss: 0.22620320320129395\n",
            "iteration 49671: loss: 0.22620317339897156\n",
            "iteration 49672: loss: 0.22620312869548798\n",
            "iteration 49673: loss: 0.22620311379432678\n",
            "iteration 49674: loss: 0.22620311379432678\n",
            "iteration 49675: loss: 0.226203054189682\n",
            "iteration 49676: loss: 0.226203054189682\n",
            "iteration 49677: loss: 0.2262030392885208\n",
            "iteration 49678: loss: 0.22620300948619843\n",
            "iteration 49679: loss: 0.2262030392885208\n",
            "iteration 49680: loss: 0.2262030392885208\n",
            "iteration 49681: loss: 0.22620299458503723\n",
            "iteration 49682: loss: 0.22620296478271484\n",
            "iteration 49683: loss: 0.22620296478271484\n",
            "iteration 49684: loss: 0.22620292007923126\n",
            "iteration 49685: loss: 0.22620296478271484\n",
            "iteration 49686: loss: 0.22620289027690887\n",
            "iteration 49687: loss: 0.22620292007923126\n",
            "iteration 49688: loss: 0.22620287537574768\n",
            "iteration 49689: loss: 0.22620287537574768\n",
            "iteration 49690: loss: 0.2262028008699417\n",
            "iteration 49691: loss: 0.2262028157711029\n",
            "iteration 49692: loss: 0.2262028008699417\n",
            "iteration 49693: loss: 0.22620275616645813\n",
            "iteration 49694: loss: 0.22620277106761932\n",
            "iteration 49695: loss: 0.22620277106761932\n",
            "iteration 49696: loss: 0.22620272636413574\n",
            "iteration 49697: loss: 0.22620272636413574\n",
            "iteration 49698: loss: 0.22620268166065216\n",
            "iteration 49699: loss: 0.2262026071548462\n",
            "iteration 49700: loss: 0.22620268166065216\n",
            "iteration 49701: loss: 0.22620263695716858\n",
            "iteration 49702: loss: 0.22620265185832977\n",
            "iteration 49703: loss: 0.2262025773525238\n",
            "iteration 49704: loss: 0.2262025624513626\n",
            "iteration 49705: loss: 0.2262025773525238\n",
            "iteration 49706: loss: 0.2262025773525238\n",
            "iteration 49707: loss: 0.2262025624513626\n",
            "iteration 49708: loss: 0.22620251774787903\n",
            "iteration 49709: loss: 0.22620251774787903\n",
            "iteration 49710: loss: 0.22620248794555664\n",
            "iteration 49711: loss: 0.22620245814323425\n",
            "iteration 49712: loss: 0.22620245814323425\n",
            "iteration 49713: loss: 0.22620244324207306\n",
            "iteration 49714: loss: 0.22620244324207306\n",
            "iteration 49715: loss: 0.22620239853858948\n",
            "iteration 49716: loss: 0.22620239853858948\n",
            "iteration 49717: loss: 0.2262023687362671\n",
            "iteration 49718: loss: 0.2262023389339447\n",
            "iteration 49719: loss: 0.2262023240327835\n",
            "iteration 49720: loss: 0.2262023687362671\n",
            "iteration 49721: loss: 0.22620227932929993\n",
            "iteration 49722: loss: 0.2262023389339447\n",
            "iteration 49723: loss: 0.22620224952697754\n",
            "iteration 49724: loss: 0.22620224952697754\n",
            "iteration 49725: loss: 0.22620220482349396\n",
            "iteration 49726: loss: 0.22620217502117157\n",
            "iteration 49727: loss: 0.22620217502117157\n",
            "iteration 49728: loss: 0.22620216012001038\n",
            "iteration 49729: loss: 0.22620221972465515\n",
            "iteration 49730: loss: 0.226202130317688\n",
            "iteration 49731: loss: 0.2262021005153656\n",
            "iteration 49732: loss: 0.226202130317688\n",
            "iteration 49733: loss: 0.2262020856142044\n",
            "iteration 49734: loss: 0.22620205581188202\n",
            "iteration 49735: loss: 0.22620205581188202\n",
            "iteration 49736: loss: 0.22620204091072083\n",
            "iteration 49737: loss: 0.22620205581188202\n",
            "iteration 49738: loss: 0.22620198130607605\n",
            "iteration 49739: loss: 0.22620198130607605\n",
            "iteration 49740: loss: 0.22620196640491486\n",
            "iteration 49741: loss: 0.22620196640491486\n",
            "iteration 49742: loss: 0.22620192170143127\n",
            "iteration 49743: loss: 0.22620193660259247\n",
            "iteration 49744: loss: 0.22620193660259247\n",
            "iteration 49745: loss: 0.2262018620967865\n",
            "iteration 49746: loss: 0.2262018471956253\n",
            "iteration 49747: loss: 0.2262018471956253\n",
            "iteration 49748: loss: 0.22620181739330292\n",
            "iteration 49749: loss: 0.22620180249214172\n",
            "iteration 49750: loss: 0.22620181739330292\n",
            "iteration 49751: loss: 0.22620177268981934\n",
            "iteration 49752: loss: 0.22620174288749695\n",
            "iteration 49753: loss: 0.22620180249214172\n",
            "iteration 49754: loss: 0.22620174288749695\n",
            "iteration 49755: loss: 0.22620172798633575\n",
            "iteration 49756: loss: 0.22620169818401337\n",
            "iteration 49757: loss: 0.22620168328285217\n",
            "iteration 49758: loss: 0.2262016087770462\n",
            "iteration 49759: loss: 0.22620165348052979\n",
            "iteration 49760: loss: 0.2262016087770462\n",
            "iteration 49761: loss: 0.22620157897472382\n",
            "iteration 49762: loss: 0.22620157897472382\n",
            "iteration 49763: loss: 0.22620157897472382\n",
            "iteration 49764: loss: 0.22620156407356262\n",
            "iteration 49765: loss: 0.22620156407356262\n",
            "iteration 49766: loss: 0.22620153427124023\n",
            "iteration 49767: loss: 0.22620153427124023\n",
            "iteration 49768: loss: 0.22620145976543427\n",
            "iteration 49769: loss: 0.22620148956775665\n",
            "iteration 49770: loss: 0.22620145976543427\n",
            "iteration 49771: loss: 0.22620144486427307\n",
            "iteration 49772: loss: 0.22620145976543427\n",
            "iteration 49773: loss: 0.22620141506195068\n",
            "iteration 49774: loss: 0.2262013703584671\n",
            "iteration 49775: loss: 0.2262013852596283\n",
            "iteration 49776: loss: 0.2262013852596283\n",
            "iteration 49777: loss: 0.22620129585266113\n",
            "iteration 49778: loss: 0.22620129585266113\n",
            "iteration 49779: loss: 0.22620129585266113\n",
            "iteration 49780: loss: 0.22620132565498352\n",
            "iteration 49781: loss: 0.22620132565498352\n",
            "iteration 49782: loss: 0.22620122134685516\n",
            "iteration 49783: loss: 0.22620125114917755\n",
            "iteration 49784: loss: 0.22620125114917755\n",
            "iteration 49785: loss: 0.22620120644569397\n",
            "iteration 49786: loss: 0.22620122134685516\n",
            "iteration 49787: loss: 0.22620120644569397\n",
            "iteration 49788: loss: 0.2262011468410492\n",
            "iteration 49789: loss: 0.22620117664337158\n",
            "iteration 49790: loss: 0.226201131939888\n",
            "iteration 49791: loss: 0.226201131939888\n",
            "iteration 49792: loss: 0.22620108723640442\n",
            "iteration 49793: loss: 0.22620108723640442\n",
            "iteration 49794: loss: 0.22620105743408203\n",
            "iteration 49795: loss: 0.22620105743408203\n",
            "iteration 49796: loss: 0.22620096802711487\n",
            "iteration 49797: loss: 0.22620102763175964\n",
            "iteration 49798: loss: 0.22620102763175964\n",
            "iteration 49799: loss: 0.22620098292827606\n",
            "iteration 49800: loss: 0.22620093822479248\n",
            "iteration 49801: loss: 0.2262009084224701\n",
            "iteration 49802: loss: 0.2262009084224701\n",
            "iteration 49803: loss: 0.2262009084224701\n",
            "iteration 49804: loss: 0.2262009084224701\n",
            "iteration 49805: loss: 0.22620081901550293\n",
            "iteration 49806: loss: 0.22620084881782532\n",
            "iteration 49807: loss: 0.22620081901550293\n",
            "iteration 49808: loss: 0.22620081901550293\n",
            "iteration 49809: loss: 0.22620081901550293\n",
            "iteration 49810: loss: 0.22620078921318054\n",
            "iteration 49811: loss: 0.22620081901550293\n",
            "iteration 49812: loss: 0.22620074450969696\n",
            "iteration 49813: loss: 0.22620072960853577\n",
            "iteration 49814: loss: 0.22620069980621338\n",
            "iteration 49815: loss: 0.22620072960853577\n",
            "iteration 49816: loss: 0.22620069980621338\n",
            "iteration 49817: loss: 0.2262006253004074\n",
            "iteration 49818: loss: 0.2262006551027298\n",
            "iteration 49819: loss: 0.2262006253004074\n",
            "iteration 49820: loss: 0.2262006253004074\n",
            "iteration 49821: loss: 0.2262006253004074\n",
            "iteration 49822: loss: 0.22620055079460144\n",
            "iteration 49823: loss: 0.22620053589344025\n",
            "iteration 49824: loss: 0.22620058059692383\n",
            "iteration 49825: loss: 0.22620053589344025\n",
            "iteration 49826: loss: 0.22620050609111786\n",
            "iteration 49827: loss: 0.22620049118995667\n",
            "iteration 49828: loss: 0.22620049118995667\n",
            "iteration 49829: loss: 0.22620046138763428\n",
            "iteration 49830: loss: 0.2262004315853119\n",
            "iteration 49831: loss: 0.2262004315853119\n",
            "iteration 49832: loss: 0.2262004315853119\n",
            "iteration 49833: loss: 0.2262003868818283\n",
            "iteration 49834: loss: 0.22620037198066711\n",
            "iteration 49835: loss: 0.22620037198066711\n",
            "iteration 49836: loss: 0.22620031237602234\n",
            "iteration 49837: loss: 0.22620034217834473\n",
            "iteration 49838: loss: 0.22620029747486115\n",
            "iteration 49839: loss: 0.22620029747486115\n",
            "iteration 49840: loss: 0.22620029747486115\n",
            "iteration 49841: loss: 0.22620029747486115\n",
            "iteration 49842: loss: 0.22620022296905518\n",
            "iteration 49843: loss: 0.22620025277137756\n",
            "iteration 49844: loss: 0.2262001931667328\n",
            "iteration 49845: loss: 0.2262001782655716\n",
            "iteration 49846: loss: 0.2262001782655716\n",
            "iteration 49847: loss: 0.2262001484632492\n",
            "iteration 49848: loss: 0.22620010375976562\n",
            "iteration 49849: loss: 0.22620005905628204\n",
            "iteration 49850: loss: 0.22620010375976562\n",
            "iteration 49851: loss: 0.226200133562088\n",
            "iteration 49852: loss: 0.22620005905628204\n",
            "iteration 49853: loss: 0.22620005905628204\n",
            "iteration 49854: loss: 0.22620001435279846\n",
            "iteration 49855: loss: 0.22620002925395966\n",
            "iteration 49856: loss: 0.22620005905628204\n",
            "iteration 49857: loss: 0.2261999398469925\n",
            "iteration 49858: loss: 0.2261999547481537\n",
            "iteration 49859: loss: 0.2261999100446701\n",
            "iteration 49860: loss: 0.2261999547481537\n",
            "iteration 49861: loss: 0.2261999100446701\n",
            "iteration 49862: loss: 0.2261999100446701\n",
            "iteration 49863: loss: 0.22619983553886414\n",
            "iteration 49864: loss: 0.22619986534118652\n",
            "iteration 49865: loss: 0.22619986534118652\n",
            "iteration 49866: loss: 0.22619982063770294\n",
            "iteration 49867: loss: 0.22619979083538055\n",
            "iteration 49868: loss: 0.22619979083538055\n",
            "iteration 49869: loss: 0.22619977593421936\n",
            "iteration 49870: loss: 0.22619979083538055\n",
            "iteration 49871: loss: 0.22619971632957458\n",
            "iteration 49872: loss: 0.22619977593421936\n",
            "iteration 49873: loss: 0.22619971632957458\n",
            "iteration 49874: loss: 0.226199671626091\n",
            "iteration 49875: loss: 0.2261996567249298\n",
            "iteration 49876: loss: 0.2261996567249298\n",
            "iteration 49877: loss: 0.22619962692260742\n",
            "iteration 49878: loss: 0.22619958221912384\n",
            "iteration 49879: loss: 0.22619962692260742\n",
            "iteration 49880: loss: 0.22619953751564026\n",
            "iteration 49881: loss: 0.22619959712028503\n",
            "iteration 49882: loss: 0.22619953751564026\n",
            "iteration 49883: loss: 0.22619950771331787\n",
            "iteration 49884: loss: 0.22619953751564026\n",
            "iteration 49885: loss: 0.22619947791099548\n",
            "iteration 49886: loss: 0.22619950771331787\n",
            "iteration 49887: loss: 0.22619947791099548\n",
            "iteration 49888: loss: 0.2261994630098343\n",
            "iteration 49889: loss: 0.2261994332075119\n",
            "iteration 49890: loss: 0.22619938850402832\n",
            "iteration 49891: loss: 0.2261994183063507\n",
            "iteration 49892: loss: 0.22619935870170593\n",
            "iteration 49893: loss: 0.22619935870170593\n",
            "iteration 49894: loss: 0.22619938850402832\n",
            "iteration 49895: loss: 0.22619934380054474\n",
            "iteration 49896: loss: 0.22619935870170593\n",
            "iteration 49897: loss: 0.22619929909706116\n",
            "iteration 49898: loss: 0.22619926929473877\n",
            "iteration 49899: loss: 0.22619929909706116\n",
            "iteration 49900: loss: 0.22619923949241638\n",
            "iteration 49901: loss: 0.2261991947889328\n",
            "iteration 49902: loss: 0.2261992245912552\n",
            "iteration 49903: loss: 0.22619923949241638\n",
            "iteration 49904: loss: 0.2261991947889328\n",
            "iteration 49905: loss: 0.2261991947889328\n",
            "iteration 49906: loss: 0.22619912028312683\n",
            "iteration 49907: loss: 0.22619915008544922\n",
            "iteration 49908: loss: 0.22619912028312683\n",
            "iteration 49909: loss: 0.22619906067848206\n",
            "iteration 49910: loss: 0.22619907557964325\n",
            "iteration 49911: loss: 0.22619907557964325\n",
            "iteration 49912: loss: 0.22619903087615967\n",
            "iteration 49913: loss: 0.22619906067848206\n",
            "iteration 49914: loss: 0.22619900107383728\n",
            "iteration 49915: loss: 0.22619900107383728\n",
            "iteration 49916: loss: 0.2261989563703537\n",
            "iteration 49917: loss: 0.2261989414691925\n",
            "iteration 49918: loss: 0.2261989563703537\n",
            "iteration 49919: loss: 0.2261989563703537\n",
            "iteration 49920: loss: 0.22619888186454773\n",
            "iteration 49921: loss: 0.22619891166687012\n",
            "iteration 49922: loss: 0.22619882225990295\n",
            "iteration 49923: loss: 0.22619886696338654\n",
            "iteration 49924: loss: 0.22619882225990295\n",
            "iteration 49925: loss: 0.22619882225990295\n",
            "iteration 49926: loss: 0.22619879245758057\n",
            "iteration 49927: loss: 0.22619876265525818\n",
            "iteration 49928: loss: 0.22619876265525818\n",
            "iteration 49929: loss: 0.22619882225990295\n",
            "iteration 49930: loss: 0.2261987179517746\n",
            "iteration 49931: loss: 0.22619874775409698\n",
            "iteration 49932: loss: 0.22619867324829102\n",
            "iteration 49933: loss: 0.2261987030506134\n",
            "iteration 49934: loss: 0.22619867324829102\n",
            "iteration 49935: loss: 0.22619864344596863\n",
            "iteration 49936: loss: 0.22619864344596863\n",
            "iteration 49937: loss: 0.22619859874248505\n",
            "iteration 49938: loss: 0.22619859874248505\n",
            "iteration 49939: loss: 0.22619859874248505\n",
            "iteration 49940: loss: 0.22619859874248505\n",
            "iteration 49941: loss: 0.22619855403900146\n",
            "iteration 49942: loss: 0.22619852423667908\n",
            "iteration 49943: loss: 0.22619855403900146\n",
            "iteration 49944: loss: 0.22619850933551788\n",
            "iteration 49945: loss: 0.22619850933551788\n",
            "iteration 49946: loss: 0.2261984795331955\n",
            "iteration 49947: loss: 0.2261984795331955\n",
            "iteration 49948: loss: 0.22619843482971191\n",
            "iteration 49949: loss: 0.22619836032390594\n",
            "iteration 49950: loss: 0.22619843482971191\n",
            "iteration 49951: loss: 0.22619839012622833\n",
            "iteration 49952: loss: 0.22619831562042236\n",
            "iteration 49953: loss: 0.22619831562042236\n",
            "iteration 49954: loss: 0.22619834542274475\n",
            "iteration 49955: loss: 0.22619828581809998\n",
            "iteration 49956: loss: 0.22619831562042236\n",
            "iteration 49957: loss: 0.22619828581809998\n",
            "iteration 49958: loss: 0.22619827091693878\n",
            "iteration 49959: loss: 0.2261982411146164\n",
            "iteration 49960: loss: 0.2261982262134552\n",
            "iteration 49961: loss: 0.2261982262134552\n",
            "iteration 49962: loss: 0.2261982262134552\n",
            "iteration 49963: loss: 0.2261982262134552\n",
            "iteration 49964: loss: 0.22619815170764923\n",
            "iteration 49965: loss: 0.2261981964111328\n",
            "iteration 49966: loss: 0.22619812190532684\n",
            "iteration 49967: loss: 0.22619812190532684\n",
            "iteration 49968: loss: 0.22619815170764923\n",
            "iteration 49969: loss: 0.22619812190532684\n",
            "iteration 49970: loss: 0.22619803249835968\n",
            "iteration 49971: loss: 0.22619803249835968\n",
            "iteration 49972: loss: 0.22619804739952087\n",
            "iteration 49973: loss: 0.22619803249835968\n",
            "iteration 49974: loss: 0.2261980026960373\n",
            "iteration 49975: loss: 0.2261979579925537\n",
            "iteration 49976: loss: 0.2261979877948761\n",
            "iteration 49977: loss: 0.22619792819023132\n",
            "iteration 49978: loss: 0.22619791328907013\n",
            "iteration 49979: loss: 0.22619788348674774\n",
            "iteration 49980: loss: 0.2261979579925537\n",
            "iteration 49981: loss: 0.22619786858558655\n",
            "iteration 49982: loss: 0.22619783878326416\n",
            "iteration 49983: loss: 0.22619788348674774\n",
            "iteration 49984: loss: 0.22619786858558655\n",
            "iteration 49985: loss: 0.2261977642774582\n",
            "iteration 49986: loss: 0.22619779407978058\n",
            "iteration 49987: loss: 0.2261977642774582\n",
            "iteration 49988: loss: 0.2261977642774582\n",
            "iteration 49989: loss: 0.226197749376297\n",
            "iteration 49990: loss: 0.226197749376297\n",
            "iteration 49991: loss: 0.226197749376297\n",
            "iteration 49992: loss: 0.22619768977165222\n",
            "iteration 49993: loss: 0.2261977195739746\n",
            "iteration 49994: loss: 0.22619764506816864\n",
            "iteration 49995: loss: 0.22619764506816864\n",
            "iteration 49996: loss: 0.22619763016700745\n",
            "iteration 49997: loss: 0.22619757056236267\n",
            "iteration 49998: loss: 0.22619763016700745\n",
            "iteration 49999: loss: 0.22619760036468506\n",
            "iteration 50000: loss: 0.22619757056236267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "024b97d5",
        "outputId": "45c1b278-964f-44c8-cd38-923193dda937"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import seaborn as sn # used for creating a heatmap for displaying confusion matrix\n",
        "\n",
        "# testing data torch.tensors\n",
        "X_test_t = torch.from_numpy(X_test).float()\n",
        "Y_test_t = torch.from_numpy(Y_test).float()\n",
        "\n",
        "# Eval mode -- Explain that it is used to \n",
        "# set the model into evaluation mode, which \n",
        "# turns off certain layers used during training\n",
        "model.eval()\n",
        "\n",
        "# prediction\n",
        "Y_pred_t = model(X_test_t)\n",
        "# convert probabilities to 0 or 1\n",
        "Y_pred = (Y_pred_t.detach().numpy() > 0.5).astype(np.int32)\n",
        "\n",
        "F1 = f1_score(Y_test, Y_pred)\n",
        "accuracy = np.sum(Y_pred == Y_test)/Y_test.size\n",
        "\n",
        "print(f\"F1 Score: {F1}\")\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "id": "024b97d5",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score: 0.8852459016393444\n",
            "Accuracy: 0.9125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "295c7dbe",
        "outputId": "0a95bf5b-f170-4aba-b355-a5abc6c93b75"
      },
      "source": [
        "def plot_conf_mat(Y_pred, Y_gt):\n",
        "    \"\"\"\n",
        "    Plots Confusion Matrix\n",
        "    Args:\n",
        "        Y_pred: predictions array (np.array)\n",
        "        Y_gt: ground truth array (np.array)\n",
        "    \"\"\"\n",
        "    conf_mat = pd.DataFrame(confusion_matrix(Y_gt, Y_pred, labels=[1,0]))\n",
        "    sn.heatmap(conf_mat, annot=True)\n",
        "    \n",
        "plot_conf_mat(Y_pred, Y_test)"
      ],
      "id": "295c7dbe",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "559c7c7c"
      },
      "source": [
        "### Observations\n",
        "\n",
        "After training our Logistic Regression Classifier, we observe a very high Accuracy and F1 Score, which is awesome! Lets keep track of our progress:\n",
        "\n",
        "| Model | Observations | Accuracy | F1 | \n",
        "|:- |:- |:- | :- |\n",
        "| Logistic Regression | High accuracy, very few false positives and false negatives | 88% | 88% |"
      ],
      "id": "559c7c7c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57d2328f"
      },
      "source": [
        "trace = torch.jit.trace(model, torch.rand((1, 20)))\n",
        "trace.save('suv_predictor.pt')\n",
        "torch.save(model.state_dict(), 'suv_predictor_weights.pt') "
      ],
      "id": "57d2328f",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34b085e1"
      },
      "source": [
        ""
      ],
      "id": "34b085e1",
      "execution_count": 11,
      "outputs": []
    }
  ]
}